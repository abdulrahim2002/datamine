[["1", "Add simple HashAggregation", "Dain Sundstrom", "dain", "08/17/12, 12:12:59 AM", "NaN", "NaN"], ["2", "Add byte[] backed tuple and block implementations", "Dain Sundstrom", "dain", "08/20/12, 05:14:52 PM", "The code only supportes fixed with columns , and has some hacks to get width information into the operators, but it does generally show the direction.", "NaN"], ["4", "add support for variable length fields", "Martin Traverso", "martint", "08/21/12, 06:34:25 PM", "NaN", "NaN"], ["5", "add tuple builder", "Martin Traverso", "martint", "08/21/12, 09:29:01 PM", "NaN", "NaN"], ["6", "Extract Slice interface", "David Phillips", "electrum", "08/21/12, 10:09:45 PM", "NaN", "NaN"], ["7", "remove redundant field", "Martin Traverso", "martint", "08/21/12, 10:50:15 PM", "NaN", "NaN"], ["8", "minor fixes", "Martin Traverso", "martint", "08/22/12, 01:13:59 AM", "NaN", "NaN"], ["9", "Add MaskedValueBlock", "Dain Sundstrom", "dain", "08/23/12, 02:44:34 AM", "Block filters now simply returns a masked value block\nAdd filter to position block", "NaN"], ["10", "First cut of Dictionary encoder (serde)", "Eric Hwang", "erichwang", "08/23/12, 12:32:49 AM", "Summary:\nThe surrounding API is still a little rough and can probably be abstracted some more, but the core functionality should be completely there.\n\nIt also includes a simple packed long serde to help with the encoding.", "NaN"], ["11", "Add ByteBuffer slices", "David Phillips", "electrum", "08/23/12, 12:44:14 AM", "NaN", "NaN"], ["12", "Add uncompressed serde", "Dain Sundstrom", "dain", "08/23/12, 10:18:11 PM", "NaN", "NaN"], ["15", "Get rid of empty blocks", "Martin Traverso", "martint", "08/23/12, 06:07:03 AM", "Filter methods now return Optional<> instead. Operators never produce an empty block.", "NaN"], ["17", "Fix up some of the earlier review comments", "Eric Hwang", "erichwang", "08/24/12, 06:28:09 PM", "NaN", "NaN"], ["18", "standalone jar and '\\u0001' style column delimiter ", "Dain Sundstrom", "dain", "08/24/12, 06:51:22 PM", "NaN", "NaN"], ["20", "Add simple console printer", "Dain Sundstrom", "dain", "08/24/12, 08:23:15 PM", "NaN", "NaN"], ["21", "Handle importing long millis encoded as floats", "David Phillips", "electrum", "08/24/12, 08:21:41 PM", "NaN", "NaN"], ["22", "Use Guava Objects.toStringHelper", "David Phillips", "electrum", "08/26/12, 02:17:37 AM", "NaN", "NaN"], ["23", "Added OutputStreamSliceOutput", "Eric Hwang", "erichwang", "08/28/12, 12:07:52 AM", "- Also consolidated writeZero(int length) generic impl into base class\n- Removed duplicate of readSlice from SliceInput", "NaN"], ["24", "Implement Cursor and BlockStream", "Martin Traverso", "martint", "08/28/12, 12:01:31 AM", "NaN", "NaN"], ["25", "More cursor-based stuff", "Martin Traverso", "martint", "08/28/12, 07:44:48 PM", "NaN", "NaN"], ["27", "Add BlockStream aggregation implementations and benchmarks", "Dain Sundstrom", "dain", "09/04/12, 06:54:22 PM", "NaN", "NaN"], ["28", "Move some classes to new packages", "Martin Traverso", "martint", "09/04/12, 10:05:19 PM", "NaN", "NaN"], ["29", "Remove old GroupBy and Aggregation code", "Dain Sundstrom", "dain", "09/04/12, 11:48:53 PM", "NaN", "NaN"], ["31", "Add more tests for RLE and uncompressed cursors", "Martin Traverso", "martint", "09/05/12, 07:06:15 PM", "NaN", "NaN"], ["32", "Simplify group by and aggregation by adding nextValueEquals", "Dain Sundstrom", "dain", "09/05/12, 08:22:30 PM", "NaN", "NaN"], ["33", "Implement advanceToPosition and getCurrentValueEndPosition", "Martin Traverso", "martint", "09/05/12, 11:23:15 PM", "NaN", "NaN"], ["34", "Improvements to DictionarySerde", "Eric Hwang", "erichwang", "09/07/12, 01:00:50 AM", "- DictionarySerde performance improvements with array lookup to avoid overhead of Map\n- Add BlockStreamSerde interface to normalize interactions between serdes\n- Factor out code to serialize and deserialize TupleInfo. Same code as before that is still hacky, but we can change this later.\n- Allow Slice to move data to SliceOutput", "NaN"], ["35", "Fix benchmark timing code", "Dain Sundstrom", "dain", "09/06/12, 08:32:32 PM", "NaN", "NaN"], ["36", "Rewrite ColumnProcessor and data import", "David Phillips", "electrum", "09/07/12, 08:05:55 PM", "NaN", "NaN"], ["37", "Simplify cursors ", "Dain Sundstrom", "dain", "09/11/12, 11:14:52 PM", "NaN", "NaN"], ["38", "Cursor redesign", "Dain Sundstrom", "dain", "09/13/12, 12:45:10 AM", "NaN", "NaN"], ["39", "Add query testing framework", "David Phillips", "electrum", "09/12/12, 06:17:37 PM", "NaN", "NaN"], ["40", "Add RLE Serde + some fixes for Dictionary Serde", "Eric Hwang", "erichwang", "09/13/12, 10:36:49 PM", "- Add new RLE Serde -- this utilizes the simplest format and should be good enough for testing. If we find out that we need to have a more compact format, we can explore other options (that would be more complex)\n- Allow TupleInfo  to extract Tuple slices from the head of SliceInputs. This enables us to put Tuples into SliceOutputs without having to always right the length of the Slice, saving us some work and space.\n- Some updates, and bug fixes to DictionarySerde", "NaN"], ["41", "Add double data type", "Dain Sundstrom", "dain", "09/13/12, 02:08:01 AM", "NaN", "NaN"], ["42", "Implement aggregation operator without group by", "Martin Traverso", "martint", "09/13/12, 01:14:53 AM", "NaN", "NaN"], ["43", "Update Merge operator to BlockStream api", "Dain Sundstrom", "dain", "09/13/12, 05:47:39 AM", "NaN", "NaN"], ["44", "Reorganize code", "Dain Sundstrom", "dain", "09/13/12, 05:07:13 PM", "Move all block related code to blocks with sub package for each encoding type\nMove cvs import code to inject package\nMove benchmark code to a benchmark package in tests", "NaN"], ["45", "Test isFinished and fix bug in RLE cursor", "Martin Traverso", "martint", "09/13/12, 08:29:45 PM", "NaN", "NaN"], ["46", "Add lineitem table for testing", "David Phillips", "electrum", "09/13/12, 08:17:54 PM", "NaN", "NaN"], ["47", "Reorder TypeInfo.Type enum so ordinals back to orginal value ", "Dain Sundstrom", "dain", "09/13/12, 10:37:37 PM", "Current serialized format for TupleInfo (incorrectly) writes the enum oridnal", "NaN"], ["49", "Cleanup", "Dain Sundstrom", "dain", "09/14/12, 12:18:47 AM", "NaN", "NaN"], ["50", "Additions to TestQueries and other minor fixes", "Martin Traverso", "martint", "09/14/12, 12:30:26 AM", "NaN", "NaN"], ["51", "Fix double parsing", "Eric Hwang", "erichwang", "09/14/12, 01:14:55 AM", "NaN", "NaN"], ["53", "Binary and comparison operators, tests and bug fixes", "Martin Traverso", "martint", "09/14/12, 07:36:26 PM", "Initial take on binary and comparison operators. For now, they can only operate on specific types. We need to figure out how to generalize them later.", "NaN"], ["54", "Merge Block and BlockStream apis", "Dain Sundstrom", "dain", "09/14/12, 10:18:25 PM", "NaN", "NaN"], ["55", "Update Cursor API comments", "Eric Hwang", "erichwang", "09/14/12, 10:44:48 PM", "NaN", "NaN"], ["56", "Add isValid method cursor", "Dain Sundstrom", "dain", "09/14/12, 10:56:43 PM", "NaN", "NaN"], ["57", "Implement And operator", "Martin Traverso", "martint", "09/15/12, 12:57:04 AM", "NaN", "NaN"], ["58", "Fix build failures", "Dain Sundstrom", "dain", "09/15/12, 12:47:30 AM", "NaN", "NaN"], ["59", "Bind Serdes to Main + TupleStreamWriter", "Eric Hwang", "erichwang", "09/17/12, 10:52:41 PM", "The main goal of this is to bind the Serdes to main to allow conversion of the data files. Along the way, added TupleStreamWriter paradigm which we can extend in the future to make data ingestion more efficient. Right now, each column is extracted one at a time (same as before).", "NaN"], ["60", "Implement Or operator + other minor fixes", "Martin Traverso", "martint", "09/17/12, 09:44:01 PM", "NaN", "NaN"], ["61", "Cleanup and consolidate cursor test code", "Dain Sundstrom", "dain", "09/17/12, 11:35:23 PM", "NaN", "NaN"], ["62", "Make changes requested in pull #59", "Eric Hwang", "erichwang", "09/17/12, 11:31:54 PM", "- Both DictionarySerde and RunLengthEncodedSerdes now work with multi column tuples + tests\n- Some bug fixes\n- Add TupleStreamSerdes\n- Make TupleStreamWriters extend Closeable", "NaN"], ["63", "Add RangeBoundedCursor to support Main data conversions", "Eric Hwang", "erichwang", "09/18/12, 07:43:11 PM", "- Added RangeBoundedCursor + TupleStreamChunker\n- Update UncompressedSerde with some position information\n- This should run now:\n  pv data/000000_0.gz | gzcat | head -10001 |  target/presto-0.1-SNAPSHOT-standalone.jar convert csv -o data/foo -d '\\u0001' -t long_raw -t string_raw -t string_raw -t fmillis_raw -t fmillis_raw -t string_raw -t string_raw", "NaN"], ["64", "Update benchmarks add dictionary aggregation", "Dain Sundstrom", "dain", "09/19/12, 01:10:50 AM", "NaN", "NaN"], ["65", "Early benchmark framework", "Eric Hwang", "erichwang", "09/28/12, 10:40:32 PM", "- Benchmarks will need to extend AbstractBenchmark class\n- This version hardcodes dropping data as csv and jmeter output files\n- jmeter *.jtl files can be picked up by Jenkins performance plugin for build perf history\n- csv files will be used to submit data with post processing command to ODS\n\nAll of the benchmarks can be run with:\nmvn clean install exec:java", "NaN"], ["66", "Add SQL parser", "David Phillips", "electrum", "09/27/12, 07:12:39 PM", "NaN", "NaN"], ["67", "Add joins to AST", "David Phillips", "electrum", "09/28/12, 06:33:32 PM", "NaN", "NaN"], ["68", "Add basic support for distributed tuple streams ", "Dain Sundstrom", "dain", "10/04/12, 11:47:04 PM", "NaN", "NaN"], ["69", "AST Visitor framework and basic semantic analysis", "Martin Traverso", "martint", "10/04/12, 11:37:33 PM", "NaN", "NaN"], ["70", "Add all columns to test data set", "David Phillips", "electrum", "10/03/12, 08:14:12 PM", "NaN", "NaN"], ["71", "Yield support", "Dain Sundstrom", "dain", "10/06/12, 12:27:27 AM", "NaN", "NaN"], ["72", "Fix issue where aggregations advance past end position and add tests", "Martin Traverso", "martint", "10/05/12, 08:58:54 PM", "NaN", "NaN"], ["73", "Build out more Benchmark framework + porting over existing benchmarks", "Eric Hwang", "erichwang", "10/18/12, 07:38:42 PM", "- Abstracted TupleStreamBenchmark framework + converted benchmarks\n- Add default serdes to TupleStreamSerdes\n- Add StatsCollectingTupleStreamSerde\n- Added SelfIdTupleStreamSerde + some other fixes\n- Improve BlockBuilder resizing behavior", "NaN"], ["74", "AST rewrites, SQL printer and some other minor fixes", "Martin Traverso", "martint", "10/18/12, 11:05:13 PM", "NaN", "NaN"], ["75", "Setup multi-module project structure", "David Phillips", "electrum", "10/18/12, 11:55:55 PM", "NaN", "NaN"], ["76", "Create dedicated ODS output file generator to improve ODS export speed & add benchmark warmups", "Eric Hwang", "erichwang", "10/19/12, 12:43:42 AM", "- Also adjust the iterations for the benchmarks to run a little faster", "NaN"], ["77", "Add basic Presto TupleStream operators that will be needed for imports or just generally useful", "Eric Hwang", "erichwang", "10/19/12, 10:03:23 PM", "- Add DelimitedTupleStream\n- Add MaterializingTupleStream\n- Migrate all cursors to calling: Cursors.checkReadablePosition(this) to validate read access availability", "NaN"], ["78", "Add framework for importing from Hive", "David Phillips", "electrum", "10/24/12, 06:19:33 PM", "NaN", "NaN"], ["79", "Minor fixes", "Martin Traverso", "martint", "10/24/12, 12:22:54 AM", "NaN", "NaN"], ["80", "Fix FilterOperator", "Dain Sundstrom", "dain", "10/24/12, 06:40:30 PM", "NaN", "NaN"], ["81", "Refactor Metadata interfaces", "David Phillips", "electrum", "10/24/12, 06:09:04 PM", "NaN", "NaN"], ["82", "Hive importer + bug fixes + performance optimizations", "Eric Hwang", "erichwang", "10/25/12, 12:34:59 AM", "Still a little rough around the edges, but does what we need for the demo.", "NaN"], ["83", "Add database storage for Metadata", "David Phillips", "electrum", "10/25/12, 06:04:07 PM", "NaN", "NaN"], ["84", "Fix bug in Benchmark data generation", "Eric Hwang", "erichwang", "10/25/12, 04:54:00 AM", "Summary:\nSo I finally tracked down why so many of our benchmarks seemed to have taken such a huge performance hit.\nIt turns out there is was a bug in the data import code that caused the wrong column to be used which had\ndifferent performance characteristic. This change returns it back to the correct and previous behavior.\nLocal tests have shown that this is was the cause of the seeming perf degredation in our benchmark suite.", "NaN"], ["85", "Correctly produce increasing positions for stored tuple streams", "Eric Hwang", "erichwang", "10/26/12, 12:03:28 AM", "- Add RepositioningTupleStream which compacts and reindexes the positions of an underlying stream\n- This is used to give consecutive files the right positioning", "NaN"], ["86", "Integrate Metadata with HiveImportManager", "David Phillips", "electrum", "10/26/12, 12:30:31 AM", "NaN", "NaN"], ["87", "Don't import partition name as column", "David Phillips", "electrum", "10/30/12, 07:39:41 PM", "NaN", "NaN"], ["88", "Fix slice compare and equals", "Dain Sundstrom", "dain", "10/29/12, 07:44:13 PM", "NaN", "NaN"], ["89", "Add basic interactive console", "David Phillips", "electrum", "11/01/12, 11:22:45 PM", "NaN", "NaN"], ["90", "Add table and column handles", "David Phillips", "electrum", "11/08/12, 11:34:10 PM", "NaN", "NaN"], ["93", "Limit and TopN operator", "Eric Hwang", "erichwang", "11/07/12, 08:26:47 PM", "NaN", "NaN"], ["94", "Actually use a filter in query benchmark", "Martin Traverso", "martint", "11/07/12, 02:01:40 AM", "NaN", "NaN"], ["95", "Add support for selecting with qualified wildcard from anonymous columns", "Martin Traverso", "martint", "11/07/12, 06:24:38 PM", "Queries of this form are now resolved correctly:\n\n``` sql\nSELECT T.*\nFROM (\n    SELECT a + b\n    FROM U\n) T\n```\n\nNote that the expression \"a + b\" doesn't have an alias", "NaN"], ["97", "Add support for LIMIT", "Martin Traverso", "martint", "11/08/12, 12:36:30 AM", "NaN", "NaN"], ["98", "TopN benchmark", "Eric Hwang", "erichwang", "11/07/12, 11:01:49 PM", "NaN", "NaN"], ["99", "Speed improvements", "Dain Sundstrom", "dain", "11/08/12, 11:00:41 PM", "NaN", "NaN"], ["100", "Add in memory order by operator", "Eric Hwang", "erichwang", "11/08/12, 08:57:24 PM", "This is a very ghetto in memory implementation of order by that materializes all tuples and sorts them. This should be good for what we need at this point in time.", "NaN"], ["103", "Fix perf regression", "Martin Traverso", "martint", "11/08/12, 11:12:52 PM", "NaN", "NaN"], ["106", "Null support", "Dain Sundstrom", "dain", "11/13/12, 12:34:23 AM", "Add isNull(field) to TupleReadable\nNull is stored in a bit vector at the head of the tuple\nAggregates (except) count skip null values", "NaN"], ["107", "Merge ColumnScan and Align plan nodes into a single logical TableScan node", "Martin Traverso", "martint", "11/10/12, 02:07:37 AM", "NaN", "NaN"], ["108", "Add sum(double) benchmark", "Martin Traverso", "martint", "11/10/12, 02:04:39 AM", "NaN", "NaN"], ["109", "Tuple info in operator", "Dain Sundstrom", "dain", "11/13/12, 09:59:18 PM", "NaN", "NaN"], ["110", "Add distributed data imports", "David Phillips", "electrum", "11/14/12, 06:19:47 AM", "NaN", "NaN"], ["112", "Intermediate aggregates", "Dain Sundstrom", "dain", "11/14/12, 08:29:51 PM", "NaN", "NaN"], ["113", "Update to SPI package and fix imports", "David Phillips", "electrum", "11/14/12, 10:59:03 PM", "NaN", "NaN"], ["114", "Add handles + jackson serialization formats", "Eric Hwang", "erichwang", "11/15/12, 03:38:37 AM", "NaN", "NaN"], ["115", "Add vectorized AggregationOperator", "Dain Sundstrom", "dain", "11/15/12, 03:59:28 AM", "NaN", "NaN"], ["117", "Add managers for metastore and datastreamproviders", "Eric Hwang", "erichwang", "11/19/12, 11:07:53 PM", "- Also make some needed  augmentations to metastore interface", "NaN"], ["118", "Aggregation function improvements", "Martin Traverso", "martint", "11/16/12, 08:44:59 PM", "- Add intermediate types to aggregation FunctionInfo (in preparation for distributed plan creation)\n- Rename FullAggregationFunction to AggregationFunction -- the \"public\" aggregation class \n- Rename AggregationFunction to AggregationFunctionStep -- an aggregation function specialized on one of the execution steps (single-node, combine, intermediate, final)\n- Mechanism for instantiating aggregation functions bound to inputs in physical plan. This is so that we can get rid of the hard-coded if-then-else checks in ExecutionPlanner\n\nAlso, fix source of potential bugs in Page.", "NaN"], ["119", "Add SplitManager", "David Phillips", "electrum", "11/19/12, 08:43:10 PM", "NaN", "NaN"], ["120", "Demonstrate integration of end-to-end query with static query", "Eric Hwang", "erichwang", "11/20/12, 01:07:02 AM", "NaN", "NaN"], ["123", "Don't contact metastore when reading data", "David Phillips", "electrum", "11/20/12, 08:14:07 PM", "NaN", "NaN"], ["124", "Use discovery instead of hard coded HiveClient", "David Phillips", "electrum", "11/21/12, 06:45:42 AM", "NaN", "NaN"], ["125", "Make catalog name a parameter", "David Phillips", "electrum", "11/21/12, 07:28:06 PM", "NaN", "NaN"], ["126", "Migrate Tpch test data system to be compliant with Metadata and DataStreamProvider", "Eric Hwang", "erichwang", "11/26/12, 10:03:53 PM", "- This affects the benchmark code as well as the TestQueries.\n- There is currently a temporary hack with the LegacyStorageManager to allow it to interface with the ExecutionPlanner, but all those hacks should be removed once ExecutionPlanner switches to using DataStreamProviders.", "NaN"], ["127", "Replace Slot/SlotReference with SQL-parseable Symbol", "Martin Traverso", "martint", "11/27/12, 12:55:51 AM", "Also, other minor changes in preparation for distributed planning and execution", "NaN"], ["128", "Schedule improvements", "Dain Sundstrom", "dain", "11/28/12, 05:11:31 PM", "NaN", "NaN"], ["130", "Various changes", "David Phillips", "electrum", "11/28/12, 07:09:35 PM", "NaN", "NaN"], ["131", "Distributed sql execution and other fixes", "Martin Traverso", "martint", "11/28/12, 11:43:40 PM", "NaN", "NaN"], ["132", "Initial feedback implementation", "Dain Sundstrom", "dain", "11/29/12, 12:09:09 AM", "NaN", "NaN"], ["133", "Fix NPE when printing null columns", "Martin Traverso", "martint", "11/29/12, 01:25:27 AM", "NaN", "NaN"], ["134", "Minor feedback fixes", "Dain Sundstrom", "dain", "11/29/12, 01:32:35 AM", "NaN", "NaN"], ["136", "Add more feedback data", "Dain Sundstrom", "dain", "11/29/12, 06:23:14 PM", "NaN", "NaN"], ["137", "Fix http timeout and reliability issues", "Dain Sundstrom", "dain", "11/30/12, 05:23:35 PM", "NaN", "NaN"], ["138", "Fix bug in reporting of running tasks", "Martin Traverso", "martint", "11/30/12, 01:49:16 AM", "NaN", "NaN"], ["139", "Fix issue where positions are counted multiple times", "Martin Traverso", "martint", "11/30/12, 05:44:43 AM", "NaN", "NaN"], ["140", "Add console as exec goal", "David Phillips", "electrum", "11/30/12, 07:48:13 PM", "NaN", "NaN"], ["141", "Use exec:java instead of exec:exec for console", "David Phillips", "electrum", "11/30/12, 08:07:25 PM", "NaN", "NaN"], ["142", "Join logical planning and other fixes", "Martin Traverso", "martint", "12/01/12, 01:55:50 AM", "NaN", "NaN"], ["143", "Add basic support for JOIN ... USING", "Martin Traverso", "martint", "12/01/12, 03:48:17 AM", "NaN", "NaN"], ["144", "Refactor ColumnPrinter into OutputProcessor", "David Phillips", "electrum", "12/03/12, 08:02:15 PM", "NaN", "NaN"], ["145", "New hash and sort", "Dain Sundstrom", "dain", "12/04/12, 12:49:24 AM", "NaN", "NaN"], ["146", "Single-node join plans, test and benchmark", "Martin Traverso", "martint", "12/04/12, 01:54:46 AM", "NaN", "NaN"], ["147", "Only create http client if needed", "Dain Sundstrom", "dain", "12/04/12, 04:16:56 AM", "This fixes most of the performance regression.  The HttpClient was initializing ssl for some reason.", "NaN"], ["148", "Devirtualize benchmark innerloop", "Dain Sundstrom", "dain", "12/04/12, 05:01:29 AM", "NaN", "NaN"], ["149", "Add aligned output", "David Phillips", "electrum", "12/04/12, 06:37:28 PM", "NaN", "NaN"], ["150", "Move uses of \"session\" up the analysis stack", "Martin Traverso", "martint", "12/04/12, 10:57:10 PM", "Split Session out of SessionMetadata and make all the components in the planning stack depend on Metadata except for the Analyzer.", "NaN"], ["153", "Add PageIterator interface with close method", "Dain Sundstrom", "dain", "12/05/12, 05:10:26 PM", "NaN", "NaN"], ["154", "Log errors reported to the QueryState object", "Dain Sundstrom", "dain", "12/05/12, 05:09:43 AM", "NaN", "NaN"], ["155", "Inject HttpClients", "Dain Sundstrom", "dain", "12/05/12, 05:27:03 PM", "NaN", "NaN"], ["156", "Add debug flag to console", "Martin Traverso", "martint", "12/05/12, 06:43:28 PM", "NaN", "NaN"], ["157", "When exchange is closed, abort results on remote nodes", "Dain Sundstrom", "dain", "12/06/12, 12:34:03 AM", "NaN", "NaN"], ["158", "Incremental imports", "Eric Hwang", "erichwang", "12/06/12, 10:53:20 PM", "Re-running the import job repeatedly produces incremental updates to synchronize with the corresponding remote database (Hive). It adds new partitions, and drops old ones no longer present in the remote system. It will also re-do any partitions that may have failed from previous import attempts.\n\nThere are still a few known race minor conditions if you decide to run the import job on the same table consecutively very quickly, but in general the state should still converge to the correct result after multiple retries.", "NaN"], ["160", "Fix broken subplan", "Martin Traverso", "martint", "12/06/12, 01:32:42 AM", "NaN", "NaN"], ["161", "Fix bug in group by column with output alias", "Martin Traverso", "martint", "12/06/12, 02:20:52 AM", "NaN", "NaN"], ["162", "Tiny fixes", "Dain Sundstrom", "dain", "12/06/12, 02:03:40 AM", "NaN", "NaN"], ["163", "Fix bug in count aggregation from inline view", "Martin Traverso", "martint", "12/06/12, 02:23:41 AM", "This query now works:\n\nSELECT COUNT(*) FROM (SELECT ... ) x", "NaN"], ["164", "Allow hash aggregation with no aggregation functions", "Martin Traverso", "martint", "12/06/12, 02:31:05 AM", "NaN", "NaN"], ["165", "Add internal table support", "David Phillips", "electrum", "12/06/12, 08:05:56 PM", "NaN", "NaN"], ["166", "Fix query state machine", "Dain Sundstrom", "dain", "12/06/12, 05:22:22 PM", "Fix the query and task state machines\nFix bugs with communicating \"no more data\" via http response codes", "NaN"], ["167", "Schedule internal splits on current node", "David Phillips", "electrum", "12/06/12, 10:47:22 PM", "NaN", "NaN"], ["168", "Implement GROUP BY w/o aggregations", "Martin Traverso", "martint", "12/07/12, 01:52:01 AM", "NaN", "NaN"], ["169", "Implement ORDER BY", "Martin Traverso", "martint", "12/07/12, 04:36:24 PM", "NaN", "NaN"], ["171", "Fixed bug when building Sort execution plan", "Martin Traverso", "martint", "12/07/12, 11:52:52 PM", "NaN", "NaN"], ["172", "Cleanup and organize", "Dain Sundstrom", "dain", "12/08/12, 12:04:16 AM", "NaN", "NaN"], ["173", "Add tests for BoundedExecutor and ShardBoundedExecutor", "Eric Hwang", "erichwang", "12/13/12, 08:18:35 PM", "NaN", "NaN"], ["174", "Console improvements", "David Phillips", "electrum", "12/09/12, 06:36:12 AM", "NaN", "NaN"], ["175", "Implement partition pruning for hive tables", "Martin Traverso", "martint", "12/09/12, 04:58:41 AM", "These changes depend on  https://github.com/facebook/presto-hive/pull/10", "NaN"], ["176", "Fix bug when partition key does not appear in predicate", "Martin Traverso", "martint", "12/09/12, 06:14:42 AM", "NaN", "NaN"], ["177", "Add dual table", "David Phillips", "electrum", "12/10/12, 03:14:33 AM", "NaN", "NaN"], ["178", "Add system tables and add internal tables to metadata", "David Phillips", "electrum", "12/10/12, 06:59:45 PM", "NaN", "NaN"], ["179", "Don't add special commands to history", "David Phillips", "electrum", "12/11/12, 07:50:27 PM", "NaN", "NaN"], ["180", "Unify expression interpreter and optimizer", "Martin Traverso", "martint", "12/12/12, 01:16:41 AM", "NaN", "NaN"], ["181", "Implement DISTINCT", "Martin Traverso", "martint", "12/12/12, 01:19:25 AM", "NaN", "NaN"], ["182", "Bookkeeping", "Dain Sundstrom", "dain", "12/12/12, 10:23:40 PM", "Rewrite bookkeeping objects to be correct and thread safe\nAdded Stage abstraction\nAdd more stats and cleanup stats collection\nRecord exceptions in info objects and propagate to client\nAdd multiline progress to console", "NaN"], ["183", "Move allocation out of the inner loop", "Martin Traverso", "martint", "12/12/12, 02:23:54 AM", "A bit of a hack, but hey...", "NaN"], ["184", "Performance improvements in expression evaluation", "Martin Traverso", "martint", "12/12/12, 06:55:11 PM", "NaN", "NaN"], ["185", "Convert query queued time to mills", "Dain Sundstrom", "dain", "12/12/12, 10:48:44 PM", "NaN", "NaN"], ["186", "Automatically add /v1/query to end of server uri", "Dain Sundstrom", "dain", "12/12/12, 10:58:10 PM", "NaN", "NaN"], ["187", "Add max and min aggregations", "Eric Hwang", "erichwang", "12/13/12, 02:16:51 AM", "NaN", "NaN"], ["188", "Add support for MySQL", "David Phillips", "electrum", "12/13/12, 04:48:14 AM", "NaN", "NaN"], ["189", "Add bug fixes for max/min and add varbinary support", "Eric Hwang", "erichwang", "12/13/12, 08:18:58 PM", "NaN", "NaN"], ["191", "Column aligned console status output format", "Dain Sundstrom", "dain", "12/13/12, 07:15:01 PM", "NaN", "NaN"], ["194", "Fix stuck task issue", "Martin Traverso", "martint", "12/13/12, 08:22:40 PM", "NaN", "NaN"], ["195", "Make LocalExecutionPlanner use a visitor", "Martin Traverso", "martint", "12/13/12, 10:49:10 PM", "NaN", "NaN"], ["196", "Properly handle the user aborting the pager", "David Phillips", "electrum", "12/15/12, 08:05:54 AM", "NaN", "NaN"], ["197", "Fix split reporting code", "Dain Sundstrom", "dain", "12/14/12, 10:49:31 PM", "NaN", "NaN"], ["198", "Get real file size for hive splits", "Dain Sundstrom", "dain", "12/15/12, 12:22:40 AM", "NaN", "NaN"], ["199", "Fix recording of actual size", "Dain Sundstrom", "dain", "12/15/12, 01:03:30 AM", "NaN", "NaN"], ["200", "Add benchmarks to varbinarymax + cleanup for tupleinfo", "Eric Hwang", "erichwang", "12/18/12, 08:50:08 PM", "NaN", "NaN"], ["201", "Make benchmark and console launcher respect maven java version", "Eric Hwang", "erichwang", "12/15/12, 02:03:14 AM", "NaN", "NaN"], ["202", "Upgrade to airlift 0.68-SNAPSHOT", "David Phillips", "electrum", "12/16/12, 01:08:58 AM", "Fix server tests on machines with 32 processors.", "NaN"], ["203", "Rename NewInMemoryOrderByOperator to InMemoryOrderByOperator", "Dain Sundstrom", "dain", "12/17/12, 07:51:34 PM", "Remove old InMemoryOrderByOperator code", "NaN"], ["204", "Add flag to disable import support", "Martin Traverso", "martint", "12/17/12, 07:59:03 PM", "NaN", "NaN"], ["205", "Hard limits", "Dain Sundstrom", "dain", "12/18/12, 02:09:53 AM", "NaN", "NaN"], ["206", "Show decimal values in console status", "Martin Traverso", "martint", "12/19/12, 12:04:25 AM", "NaN", "NaN"], ["207", "Flat hash aggregation", "Dain Sundstrom", "dain", "12/19/12, 11:42:16 PM", "NaN", "NaN"], ["208", "Nodes must now declare supporte import clients", "Dain Sundstrom", "dain", "12/19/12, 06:03:31 AM", "NaN", "NaN"], ["209", "Limit target nodes for presto imports", "Dain Sundstrom", "dain", "12/19/12, 06:11:01 AM", "NaN", "NaN"], ["210", "Limit threads", "Dain Sundstrom", "dain", "12/19/12, 11:51:08 PM", "NaN", "NaN"], ["211", "Query gc", "Dain Sundstrom", "dain", "12/20/12, 06:41:52 PM", "NaN", "NaN"], ["212", "Add Scribe/Nectar event logging framework", "Eric Hwang", "erichwang", "01/04/13, 01:06:23 AM", "- Adds the Scribe-backed Event handling service for Presto\n- Adds one QueryCompletionEvent for Presto\n\nTo integrate these changes:\n1) update config values: scribe.nectar.event-category-map with the event to scribe category mapping\n2) Update discovery with the local scribed on each machine:\ncurl -H 'Content-Type: application/json' --data-binary '{\"environment\":\"test\",\"type\":\"scribe\",\"pool\":\"general\",\"properties\":{\"thrift\":\"localhost:1456\"}}' http://localhost:8411/v1/announcement/static\n\nThe Scribe category to watch is:\nnectar_presto_query_stats", "NaN"], ["213", "Handle aborting queries in console", "David Phillips", "electrum", "12/20/12, 06:41:53 PM", "NaN", "NaN"], ["214", "Show query status during planning", "David Phillips", "electrum", "12/21/12, 02:31:10 AM", "NaN", "NaN"], ["215", "Pass list of columns to ImportClient", "Martin Traverso", "martint", "12/21/12, 01:30:29 AM", "NaN", "NaN"], ["217", "Record now returns byte[] for getString()", "Martin Traverso", "martint", "12/21/12, 07:07:56 PM", "This depends on https://github.com/facebook/presto-hive/pull/14", "NaN"], ["218", "Handle multi-line queries in console", "David Phillips", "electrum", "12/27/12, 02:23:54 AM", "NaN", "NaN"], ["219", "Add basic string functions", "David Phillips", "electrum", "01/03/13, 06:10:39 PM", "Yes, the function invocation is a horrible hack and needs to be implemented correctly.", "NaN"], ["220", "Handle user interrupt (ctrl-C) in console", "David Phillips", "electrum", "01/03/13, 06:13:13 PM", "NaN", "NaN"], ["221", "Change to RecordCursor and pass column ids instead of names to hive client API ", "Martin Traverso", "martint", "01/04/13, 01:04:32 AM", "NaN", "NaN"], ["222", "Remove QueryState from QueryCreatedEvent", "Eric Hwang", "erichwang", "01/04/13, 02:30:25 AM", "NaN", "NaN"], ["223", "Make catalog, schema and user configurable in client", "Dain Sundstrom", "dain", "01/04/13, 04:14:19 AM", "NaN", "NaN"], ["224", "Partial query cancel", "Dain Sundstrom", "dain", "01/04/13, 06:21:26 PM", "NaN", "NaN"], ["225", "Add proper cpu and user time to query stats", "Dain Sundstrom", "dain", "01/04/13, 06:41:55 PM", "NaN", "NaN"], ["226", "Remove duplicate jmxutils dependency", "David Phillips", "electrum", "01/04/13, 07:21:29 PM", "NaN", "NaN"], ["227", "Add some scribe error logging changes", "Eric Hwang", "erichwang", "01/05/13, 01:12:55 AM", "NaN", "NaN"], ["228", "Improve console status printing", "Martin Traverso", "martint", "01/07/13, 06:49:38 PM", "NaN", "NaN"], ["229", "Add disabled tests for order by that highlight broken semantics", "Martin Traverso", "martint", "01/07/13, 07:15:22 PM", "NaN", "NaN"], ["230", "Functions", "Dain Sundstrom", "dain", "01/08/13, 12:27:08 AM", "NaN", "NaN"], ["231", "Rewrite extract directly to function call", "Dain Sundstrom", "dain", "01/08/13, 01:22:48 AM", "NaN", "NaN"], ["232", "Implement like", "Dain Sundstrom", "dain", "01/08/13, 03:19:59 AM", "NaN", "NaN"], ["233", "Implement between", "Dain Sundstrom", "dain", "01/08/13, 06:14:10 PM", "NaN", "NaN"], ["235", "Implement IN list", "Dain Sundstrom", "dain", "01/08/13, 08:12:16 PM", "NaN", "NaN"], ["237", "Add CurrentTime node", "Martin Traverso", "martint", "01/08/13, 09:32:28 PM", "NaN", "NaN"], ["238", "Implement case statement", "Dain Sundstrom", "dain", "01/08/13, 11:20:58 PM", "NaN", "NaN"], ["239", "Implement cast", "Martin Traverso", "martint", "01/08/13, 10:43:51 PM", "NaN", "NaN"], ["242", "Implement count column", "Dain Sundstrom", "dain", "01/08/13, 11:33:59 PM", "NaN", "NaN"], ["243", "Python console launcher with Prism compatibility", "Eric Hwang", "erichwang", "01/08/13, 11:36:05 PM", "This pull request is for review only and will not actually be committed", "NaN"], ["245", "Make session available to expression interpreter and functions", "Dain Sundstrom", "dain", "01/09/13, 07:01:47 PM", "Session is passed from coordiator to tasks during creation\nAdd start time to session\nChange current_timestamp to use session start time", "NaN"], ["246", "Support SHOW TABLES and SHOW COLUMNS for Hive", "David Phillips", "electrum", "01/09/13, 10:10:03 PM", "NaN", "NaN"], ["248", "Fix null support in RecordProjectOperator", "Dain Sundstrom", "dain", "01/09/13, 10:22:24 PM", "NaN", "NaN"], ["249", "Fixes for metadata", "David Phillips", "electrum", "01/09/13, 10:27:03 PM", "NaN", "NaN"], ["251", "Add SHOW PARTITIONS for Hive", "David Phillips", "electrum", "01/10/13, 06:12:21 AM", "NaN", "NaN"], ["253", "Use dynamic columns for SHOW PARTITIONS output", "David Phillips", "electrum", "01/10/13, 06:57:11 PM", "NaN", "NaN"], ["254", "Announce presto coordinators", "Dain Sundstrom", "dain", "01/11/13, 01:40:35 AM", "NaN", "NaN"], ["255", "Make hive chunk size configurable", "Dain Sundstrom", "dain", "01/11/13, 03:11:24 AM", "NaN", "NaN"], ["256", "Add event metrics for cumulative split wall and cpu time etc", "Eric Hwang", "erichwang", "01/14/13, 10:22:43 PM", "Let me know if this is what you guys were looking for or if there are any other metrics you wanted.", "NaN"], ["257", "Fix parsing of non-reserved keywords", "David Phillips", "electrum", "01/11/13, 06:57:15 PM", "NaN", "NaN"], ["258", "Optimize projections and filters during logical planning", "Martin Traverso", "martint", "01/11/13, 06:08:23 PM", "NaN", "NaN"], ["259", "Fix aligned output for multi-line text", "David Phillips", "electrum", "01/14/13, 12:44:00 AM", "NaN", "NaN"], ["264", "Count column should return 0 if all values are null", "Martin Traverso", "martint", "01/14/13, 05:31:53 PM", "NaN", "NaN"], ["265", "Fix broken tests for count column", "Martin Traverso", "martint", "01/14/13, 08:06:40 PM", "NaN", "NaN"], ["266", "Remove unused variable", "Martin Traverso", "martint", "01/14/13, 08:08:50 PM", "NaN", "NaN"], ["267", "Allow stages with no tasks to fix issue #260", "Dain Sundstrom", "dain", "01/14/13, 09:02:03 PM", "NaN", "NaN"], ["268", "Optimize symbol -> channel/field mapping during expression interpretation", "Martin Traverso", "martint", "01/14/13, 08:39:13 PM", "NaN", "NaN"], ["269", "Print results normally for a query with no rows\t", "David Phillips", "electrum", "01/14/13, 10:21:48 PM", "NaN", "NaN"], ["270", "Allow null stageinfos when computing global stats", "Eric Hwang", "erichwang", "01/14/13, 11:13:31 PM", "NaN", "NaN"], ["272", "Add logging for user, catalog, schema", "Eric Hwang", "erichwang", "01/15/13, 12:56:19 AM", "NaN", "NaN"], ["274", "Fix NPE and remove OutputStats", "David Phillips", "electrum", "01/15/13, 08:17:07 PM", "NaN", "NaN"], ["275", "Add support for non-string partition keys to split pruning", "Martin Traverso", "martint", "01/15/13, 08:50:24 PM", "NaN", "NaN"], ["276", "Add start of a JDBC driver", "David Phillips", "electrum", "01/16/13, 07:30:34 PM", "NaN", "NaN"], ["277", "Covnert info stats objects to use DataSize and Duration", "Eric Hwang", "erichwang", "01/16/13, 04:58:26 AM", "NaN", "NaN"], ["278", "Fix console problems", "David Phillips", "electrum", "01/16/13, 07:04:12 PM", "NaN", "NaN"], ["279", "Remove unused class", "Martin Traverso", "martint", "01/16/13, 04:06:47 AM", "NaN", "NaN"], ["280", "Add per-node avg stats to console in debug mode", "Martin Traverso", "martint", "01/16/13, 04:08:17 AM", "NaN", "NaN"], ["281", "Improve split assignment algorithm", "Martin Traverso", "martint", "01/16/13, 05:35:31 AM", "The existing algorithm chooses a node for each split by drawing from a random uniform distribution.This results in a non-uniform (poisson-like) distribution for splits per node.\n\nThe updated algorithm emulates a least-load load balancing algorithm by assigning each split to the node with the smallest number of assignments.", "NaN"], ["282", "Fix issue when partition keys are strings", "Martin Traverso", "martint", "01/16/13, 05:57:38 AM", "NaN", "NaN"], ["285", "Make presto build in eclipse", "Henning Schmiedehausen", "hgschmie", "01/16/13, 08:40:46 PM", "exclude the dependency:copy goal because eclipse can not handle it.", "NaN"], ["286", "don't use snapshot for airlift", "Henning Schmiedehausen", "hgschmie", "01/16/13, 11:16:04 PM", "NaN", "NaN"], ["288", "Add SHOW FUNCTIONS", "David Phillips", "electrum", "01/17/13, 02:34:06 AM", "NaN", "NaN"], ["289", "Stopgap to prevent OOM when too many splits", "Eric Hwang", "erichwang", "01/17/13, 01:01:36 AM", "NaN", "NaN"], ["290", "Disable broken test", "Martin Traverso", "martint", "01/17/13, 12:49:09 AM", "NaN", "NaN"], ["291", "Use TupleInfo to access intermediate data, not implicit offsets.", "Henning Schmiedehausen", "hgschmie", "01/17/13, 07:32:13 AM", "Use TupleInfo to find the right offsets into the intermediate\ndata. Add setLong and setDouble with implicit offset 0 to TupleInfo to\nmatch the existing getLong and getDouble functions.", "NaN"], ["292", "Fix TestQueryManagerConfig test", "Eric Hwang", "erichwang", "01/17/13, 01:04:18 AM", "NaN", "NaN"], ["293", "Fix cancel for operators", "Dain Sundstrom", "dain", "01/17/13, 03:25:45 AM", "NaN", "NaN"], ["294", "Fix global stats computation and status display for completed stages", "Martin Traverso", "martint", "01/17/13, 04:36:32 AM", "NaN", "NaN"], ["295", "Fix task cancellation", "Dain Sundstrom", "dain", "01/18/13, 12:34:35 AM", "NaN", "NaN"], ["296", "Add disabled test for self joins", "Martin Traverso", "martint", "01/17/13, 09:56:33 PM", "NaN", "NaN"], ["297", "POM changes", "David Phillips", "electrum", "01/18/13, 12:43:33 AM", "NaN", "NaN"], ["298", "add statistic aggregation functions", "Henning Schmiedehausen", "hgschmie", "01/19/13, 04:34:14 AM", "add standard deviation and variance aggregation for int64 and double for sample and population. ", "NaN"], ["299", "Hive progress", "Dain Sundstrom", "dain", "01/18/13, 01:28:18 AM", "NaN", "NaN"], ["300", "Update dependencies to non-snapshot versions", "Martin Traverso", "martint", "01/18/13, 01:44:24 AM", "NaN", "NaN"], ["301", "Allow query cancel during planning", "Dain Sundstrom", "dain", "01/18/13, 03:09:47 AM", "NaN", "NaN"], ["302", "Move SPI to separate module", "David Phillips", "electrum", "01/18/13, 02:31:41 AM", "NaN", "NaN"], ["303", "Add get_json_scalar function", "Eric Hwang", "erichwang", "01/23/13, 09:22:07 PM", "NaN", "NaN"], ["304", "Add basic help command", "David Phillips", "electrum", "01/18/13, 10:07:37 PM", "NaN", "NaN"], ["305", "Push down predicates through joins for partition pruning", "Martin Traverso", "martint", "01/19/13, 01:30:42 AM", "NaN", "NaN"], ["306", "Add basic support for quoted identifiers", "David Phillips", "electrum", "01/19/13, 06:38:58 AM", "NaN", "NaN"], ["307", "Add support for non-deterministic functions", "Dain Sundstrom", "dain", "01/19/13, 02:10:29 AM", "NaN", "NaN"], ["308", "Minor JDBC driver improvements", "David Phillips", "electrum", "01/22/13, 07:57:43 PM", "NaN", "NaN"], ["310", "Plan rewriter framework + migrate optimizations to it", "Martin Traverso", "martint", "01/23/13, 05:13:09 AM", "NaN", "NaN"], ["311", "Fix bug when rewriting NegativeExpression", "Martin Traverso", "martint", "01/23/13, 05:15:17 AM", "It was discarding the rewritten subexpression of arithmetic negations", "NaN"], ["312", "Show proper messages for parser errors", "David Phillips", "electrum", "01/23/13, 07:22:21 PM", "NaN", "NaN"], ["313", "Bypass interpreter for identity projections", "Martin Traverso", "martint", "01/23/13, 05:08:54 PM", "NaN", "NaN"], ["314", "minor fixes", "Henning Schmiedehausen", "hgschmie", "01/23/13, 08:11:05 PM", "- bad import\n- released version ", "NaN"], ["315", "Move GraphvizPrinter to util with minor cleanup", "David Phillips", "electrum", "01/24/13, 12:12:19 AM", "NaN", "NaN"], ["316", "Add SHOW TABLES with LIKE predicate", "Eric Hwang", "erichwang", "01/24/13, 01:22:18 AM", "NaN", "NaN"], ["317", "Exchange operator", "Dain Sundstrom", "dain", "02/02/13, 01:03:40 AM", "NaN", "NaN"], ["318", "Improve cancelation of queries", "David Phillips", "electrum", "01/24/13, 06:46:51 PM", "NaN", "NaN"], ["319", "Add support for inputs at fields index > 0 for aggregations", "Martin Traverso", "martint", "01/25/13, 01:40:52 AM", "NaN", "NaN"], ["320", "Presto Main part for the Hive Metadata caching implementation", "Henning Schmiedehausen", "hgschmie", "01/24/13, 09:38:15 PM", "- Add Metadata SPI\n- Use CachingHiveClient in the HiveClientFactory.", "NaN"], ["322", "Code formatting", "David Phillips", "electrum", "01/25/13, 01:53:10 AM", "NaN", "NaN"], ["323", "Fix hash aggregation operator slice sizing bug", "Eric Hwang", "erichwang", "01/24/13, 11:32:40 PM", "Fixes this:\n\npresto> select ds, stddev(cpu_msec) from\nhive_silver.default.hivedba_query_stats group by ds limit 20;\nQuery 0 failed:\nTask 0.0.0 failed:\njava.lang.IndexOutOfBoundsException: end index (2625) must not be greater\nthan size (2621)\n    at\ncom.google.common.base.Preconditions.checkPositionIndexes(Preconditions.jav\na:388)\n    at com.facebook.presto.slice.Slice.checkIndexLength(Slice.java:914)\n    at com.facebook.presto.slice.Slice.setDouble(Slice.java:480)\n    at com.facebook.presto.tuple.TupleInfo.setDouble(TupleInfo.java:337)\n    at\ncom.facebook.presto.operator.aggregation.AbstractVarianceAggregation.initia\nlize(AbstractVarianceAggregation.java:69)\n    at\ncom.facebook.presto.operator.HashAggregationOperator$FixedWidthAggregator.i\nnitialize(HashAggregationOperator.java:350)\n    at\ncom.facebook.presto.operator.HashAggregationOperator$HashAggregationIterato\nr.aggregate(HashAggregationOperator.java:191)\n    at\ncom.facebook.presto.operator.HashAggregationOperator$HashAggregationIterato\nr.computeNext(HashAggregationOperator.java:234)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.tryToComputeNext(Abstract\nPageIterator.java:137)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.hasNext(AbstractPageItera\ntor.java:130)\n    at\ncom.facebook.presto.operator.FilterAndProjectOperator$FilterAndProjectItera\ntor.computeNext(FilterAndProjectOperator.java:84)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.tryToComputeNext(Abstract\nPageIterator.java:137)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.hasNext(AbstractPageItera\ntor.java:130)\n    at\ncom.facebook.presto.operator.LimitOperator$LimitIterator.computeNext(LimitO\nperator.java:60)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.tryToComputeNext(Abstract\nPageIterator.java:137)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.hasNext(AbstractPageItera\ntor.java:130)\n    at\ncom.facebook.presto.execution.SqlTaskExecution$SplitWorker.call(SqlTaskExec\nution.java:269)\n    at\ncom.facebook.presto.execution.SqlTaskExecution.run(SqlTaskExecution.java:12\n6)\n    at\ncom.facebook.presto.execution.SqlTaskManager$TaskStarter.run(SqlTaskManager\n.java:282)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n    at\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1\n110)\n    at\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:\n603)\n    at java.lang.Thread.run(Thread.java:722)\njava.lang.IndexOutOfBoundsException: end index (2625) must not be greater\nthan size (2621)\n    at\ncom.google.common.base.Preconditions.checkPositionIndexes(Preconditions.jav\na:388)\n    at com.facebook.presto.slice.Slice.checkIndexLength(Slice.java:914)\n    at com.facebook.presto.slice.Slice.setDouble(Slice.java:480)\n    at com.facebook.presto.tuple.TupleInfo.setDouble(TupleInfo.java:337)\n    at\ncom.facebook.presto.operator.aggregation.AbstractVarianceAggregation.initia\nlize(AbstractVarianceAggregation.java:69)\n    at\ncom.facebook.presto.operator.HashAggregationOperator$FixedWidthAggregator.i\nnitialize(HashAggregationOperator.java:350)\n    at\ncom.facebook.presto.operator.HashAggregationOperator$HashAggregationIterato\nr.aggregate(HashAggregationOperator.java:191)\n    at\ncom.facebook.presto.operator.HashAggregationOperator$HashAggregationIterato\nr.computeNext(HashAggregationOperator.java:234)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.tryToComputeNext(Abstract\nPageIterator.java:137)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.hasNext(AbstractPageItera\ntor.java:130)\n    at\ncom.facebook.presto.operator.FilterAndProjectOperator$FilterAndProjectItera\ntor.computeNext(FilterAndProjectOperator.java:84)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.tryToComputeNext(Abstract\nPageIterator.java:137)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.hasNext(AbstractPageItera\ntor.java:130)\n    at\ncom.facebook.presto.operator.LimitOperator$LimitIterator.computeNext(LimitO\nperator.java:60)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.tryToComputeNext(Abstract\nPageIterator.java:137)\n    at\ncom.facebook.presto.operator.AbstractPageIterator.hasNext(AbstractPageItera\ntor.java:130)\n    at\ncom.facebook.presto.execution.SqlTaskExecution$SplitWorker.call(SqlTaskExec\nution.java:269)\n    at\ncom.facebook.presto.execution.SqlTaskExecution.run(SqlTaskExecution.java:12\n6)\n    at\ncom.facebook.presto.execution.SqlTaskManager$TaskStarter.run(SqlTaskManager\n.java:282)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n    at\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1\n110)\n    at\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:\n603)\n    at java.lang.Thread.run(Thread.java:722)", "NaN"], ["325", "Fix table imports", "David Phillips", "electrum", "01/28/13, 05:57:57 AM", "NaN", "NaN"], ["326", "Add JsonAvgBenchmarkResultWriter", "Dain Sundstrom", "dain", "01/26/13, 01:30:09 AM", "NaN", "NaN"], ["327", "Upgrade to release version of jline (2.10)", "Martin Traverso", "martint", "01/28/13, 05:54:12 PM", "NaN", "NaN"], ["329", "Handle smaller terminal windows properly", "David Phillips", "electrum", "01/28/13, 11:15:50 PM", "NaN", "NaN"], ["332", "Configure release plugin", "Martin Traverso", "martint", "01/29/13, 06:34:12 PM", "NaN", "NaN"], ["333", "Show query with error portion highlighted", "David Phillips", "electrum", "01/29/13, 11:56:32 PM", "NaN", "NaN"], ["334", "Add dummy EOF token when highlighting errors", "David Phillips", "electrum", "01/30/13, 07:06:13 PM", "NaN", "NaN"], ["335", "Add support for complex join subexpressions and joins on multiple fields", "Martin Traverso", "martint", "01/30/13, 02:16:48 AM", "NaN", "NaN"], ["337", "Upgrade to latest airlift with latest jackson and switch to more efficient json", "Eric Hwang", "erichwang", "01/30/13, 06:29:24 AM", "jackson 1.9.12 fixes the bug we were seeing with nextFieldName method from jackson. The code no longer hangs when there is a space before a colon.\n\nThe JSON parsing function in Presto should be substantially more perfomant now.", "NaN"], ["338", "Add @JsonCreator annotation to PlanFragment", "Dain Sundstrom", "dain", "01/30/13, 07:05:06 AM", "NaN", "NaN"], ["339", "Add full syntax for SHOW TABLES to help", "Martin Traverso", "martint", "01/30/13, 04:43:47 PM", "NaN", "NaN"], ["341", "Handle parsing doubles with exponents", "David Phillips", "electrum", "01/31/13, 02:18:39 AM", "NaN", "NaN"], ["342", "Fix divide by zero when no nodes", "David Phillips", "electrum", "01/31/13, 05:17:55 AM", "NaN", "NaN"], ["343", "Make HiveClient PartitionChunk iteration bounded and pipelined", "Eric Hwang", "erichwang", "01/31/13, 09:21:22 PM", "Having threads block to fill a queue for an iterator turns out to be a little iffy since you need to deal with a way of cleaning up those thread resources if you never finish your iterator (especially if it is passed through an SPI).\n\nInstead, what this does is add a SuspendingExecutor that has the ability to halt and resume tasks submitted to it without holding up thread resources. Once you tie that into the partition chunk iterator, what you have is a queue that halts tasks when it fills up, and resumes the executor when enough tasks have been drained from it without holding any thread resources.", "NaN"], ["344", "Update to airlift 0.69 and fix dependencies", "David Phillips", "electrum", "01/31/13, 09:13:32 PM", "NaN", "NaN"], ["345", "Update timing stats incrementally", "Martin Traverso", "martint", "01/31/13, 08:56:36 PM", "They were only being updated at the end of processing a split.\nBytes and rows are updated incrementally so we need timings to be updated at the same rate to get consistent row and data rate reports.", "NaN"], ["347", "Add id to plan nodes and fix queries where table appears twice in the same fragment", "Martin Traverso", "martint", "02/01/13, 09:44:53 PM", "NaN", "NaN"], ["348", "Add basic endpoint for query execution", "David Phillips", "electrum", "02/01/13, 09:21:46 PM", "NaN", "NaN"], ["349", "Make plan fragment id a typed object", "Martin Traverso", "martint", "02/01/13, 11:10:06 PM", "NaN", "NaN"], ["350", "Upgrade to Jackson 2.x", "David Phillips", "electrum", "02/02/13, 12:02:50 AM", "NaN", "NaN"], ["352", "Improve benchmarks stats", "Dain Sundstrom", "dain", "02/03/13, 12:51:18 AM", "NaN", "NaN"], ["353", "Fixes for distributed execution", "Dain Sundstrom", "dain", "02/03/13, 01:31:07 AM", "NaN", "NaN"], ["355", "Add string concatenation operator", "David Phillips", "electrum", "02/05/13, 07:38:52 PM", "NaN", "NaN"], ["356", "Fix client latency issues ", "Dain Sundstrom", "dain", "02/05/13, 04:04:58 AM", "NaN", "NaN"], ["359", "Add distributed SQL integration tests", "Eric Hwang", "erichwang", "02/06/13, 08:03:30 PM", "Note: all of the std dev and variance tests are broken in distributed mode (currently tests are disabled for those)", "NaN"], ["360", "Handle lexer errors properly in console", "David Phillips", "electrum", "02/06/13, 06:08:49 PM", "NaN", "NaN"], ["361", "Fix Variance and Std Dev failing distributed tests", "Eric Hwang", "erichwang", "02/06/13, 09:15:27 PM", "NaN", "NaN"], ["362", "Fix duplicate dependencies", "David Phillips", "electrum", "02/06/13, 09:24:00 PM", "NaN", "NaN"], ["363", "Hackathon project - CSV output for the console", "Henning Schmiedehausen", "hgschmie", "02/07/13, 01:22:17 AM", "- add command line command execution to the console\n- add CSV / TSV output format to the console\n\nThis implements  --execute \"sql command\" and sends the output to stdout. Default format is \"CSV\". Available formats are\n\nCSV - comma separated\nTSV - tab separated\nCSV_HEADER - comma separated with an additional row that has the column names\nTSV_HEADER - tab separated with an additional row that has the column names", "NaN"], ["365", "Support SymlinkTextInputFormat for Hive", "David Phillips", "electrum", "02/12/13, 07:02:46 PM", "NaN", "NaN"], ["367", "Update to hive-presto-shaded 0.2", "David Phillips", "electrum", "02/14/13, 12:30:53 AM", "NaN", "NaN"], ["369", "Convert to Netty http client", "Dain Sundstrom", "dain", "02/14/13, 09:37:02 PM", "This depends on https://github.com/airlift/airlift/pull/39", "NaN"], ["370", "Push caching into HiveClient + proper Prism integration", "Eric Hwang", "erichwang", "02/16/13, 12:22:23 AM", "- Refactored HiveClient to extract out various libraries\n- A few bug fixes and HiveClient clean up\n- Push caching from ImportClient level down to thrift level of HiveClient\n- Upgrade Presto to swift 0.4.0\n- Add proper prism integration as a new module\n\nFollow ups:\n- Add JMX support to flush caches\n- Add a proper integration test for PrismHiveClient (currently depends on a lot of FB specific resources)", "NaN"], ["371", "Add cron based import into Presto.", "Henning Schmiedehausen", "hgschmie", "03/05/13, 11:02:18 PM", "This is driven by a configurable background service that accepts import requests through a rest endpoint.", "NaN"], ["372", "Add shutdown for executors", "Eric Hwang", "erichwang", "02/16/13, 12:39:34 AM", "NaN", "NaN"], ["374", "Add plugin system", "Dain Sundstrom", "dain", "02/22/13, 11:10:12 PM", "NaN", "NaN"], ["375", "Add SQL benchmark for TPC-H query 1", "David Phillips", "electrum", "02/19/13, 07:45:19 PM", "NaN", "NaN"], ["376", "Fix requiring Hadoop native libraries", "David Phillips", "electrum", "02/19/13, 09:49:45 PM", "NaN", "NaN"], ["377", "Tweak exception handling", "Henning Schmiedehausen", "hgschmie", "02/20/13, 06:31:15 PM", "NaN", "NaN"], ["378", "Add STRPOS function", "David Phillips", "electrum", "02/20/13, 07:49:41 PM", "NaN", "NaN"], ["379", "Update dependencies", "Martin Traverso", "martint", "02/20/13, 08:19:34 PM", "NaN", "NaN"], ["380", "Add support for prism table links", "Eric Hwang", "erichwang", "02/20/13, 08:34:09 PM", "Will add specific unit test in another commit to get this in first.", "NaN"], ["381", "Add integration tests for prism client", "David Phillips", "electrum", "02/20/13, 09:17:39 PM", "NaN", "NaN"], ["382", "Update to swift-smc 0.2", "Martin Traverso", "martint", "02/20/13, 10:57:33 PM", "NaN", "NaN"], ["383", "Type DeSer configurable for multiple types", "Henning Schmiedehausen", "hgschmie", "03/06/13, 12:15:42 AM", "Turns out that not only the TableHandle but also the ColumnHandles\nneed to be serialized and deserialized.  Factor out the common code\ninto a base class, add code for ColumnHandles.", "NaN"], ["384", "Cancel stops work", "Dain Sundstrom", "dain", "02/21/13, 09:29:27 PM", "NaN", "NaN"], ["385", "Add configuration for hdfs socks proxy", "Dain Sundstrom", "dain", "02/21/13, 09:31:51 PM", "NaN", "NaN"], ["386", "Add window functions", "David Phillips", "electrum", "02/28/13, 06:20:02 PM", "NaN", "NaN"], ["389", "move to airbase build.", "Henning Schmiedehausen", "hgschmie", "03/05/13, 05:21:26 PM", "this requires \n\nhttps://github.com/airlift/airbase/pull/4\nhttps://github.com/airlift/airbase/pull/5\nhttps://github.com/airlift/airlift/pull/44\n\nto be applied and the appropriate snapshots pushed to central.", "NaN"], ["390", "Add temporary support for MAP, ARRAY, and STRUCT by processing as strings", "Eric Hwang", "erichwang", "02/26/13, 11:40:08 PM", "NaN", "NaN"], ["391", "Implement approximate count distinct", "Martin Traverso", "martint", "02/27/13, 06:53:43 PM", "NaN", "NaN"], ["392", "Remove JMeter benchmark output", "David Phillips", "electrum", "02/27/13, 09:10:23 PM", "NaN", "NaN"], ["393", "Move presto-hive plugin asssembly to separate module", "Dain Sundstrom", "dain", "02/28/13, 01:08:06 AM", "NaN", "NaN"], ["394", "Upgrade resolver to 1.0 release", "Dain Sundstrom", "dain", "02/28/13, 05:21:02 PM", "NaN", "NaN"], ["395", "Fix dependencies", "David Phillips", "electrum", "02/28/13, 06:47:57 PM", "NaN", "NaN"], ["397", "Make batch size for fetching partition info configurable and default it to 500", "Martin Traverso", "martint", "03/01/13, 11:57:41 PM", "NaN", "NaN"], ["398", "fix hive plugin ref", "Henning Schmiedehausen", "hgschmie", "03/04/13, 09:35:36 PM", "NaN", "NaN"], ["399", "Pipeline hive client", "Dain Sundstrom", "dain", "03/06/13, 05:37:41 PM", "NaN", "NaN"], ["400", "Pipeline scheduling of query tasks", "Dain Sundstrom", "dain", "03/13/13, 12:06:05 AM", "NaN", "NaN"], ["401", "add missing dep so build succeeds.", "Henning Schmiedehausen", "hgschmie", "03/05/13, 11:27:09 PM", "NaN", "NaN"], ["403", "Minor fixes", "Dain Sundstrom", "dain", "03/06/13, 05:53:12 AM", "NaN", "NaN"], ["404", "Make constant final", "Martin Traverso", "martint", "03/06/13, 05:16:15 PM", "NaN", "NaN"], ["405", "Fix logging to match changes in airlift 0.71", "Martin Traverso", "martint", "03/06/13, 08:39:10 PM", "NaN", "NaN"], ["406", "switch to released versions", "Henning Schmiedehausen", "hgschmie", "03/06/13, 09:28:10 PM", "NaN", "NaN"], ["408", "Replace Jersey MediaType with Guava MediaType", "Dain Sundstrom", "dain", "03/07/13, 02:17:04 AM", "Jersey MediaType now requires internal classes only available in server jars", "NaN"], ["409", "Remove scribe event client", "Dain Sundstrom", "dain", "03/09/13, 04:33:40 PM", "NaN", "NaN"], ["411", "re-add main class to standalone jar", "Henning Schmiedehausen", "hgschmie", "03/08/13, 08:57:16 PM", "NaN", "NaN"], ["413", "Fix plugin class loader  getResources implementation", "Dain Sundstrom", "dain", "03/08/13, 11:50:55 PM", "NaN", "NaN"], ["416", "Remove server hive dep", "Dain Sundstrom", "dain", "03/09/13, 07:52:46 PM", "NaN", "NaN"], ["417", "Extensible main class", "Dain Sundstrom", "dain", "03/11/13, 04:56:19 PM", "NaN", "NaN"], ["418", "Add deps for Airlift split logging and log management apis", "Dain Sundstrom", "dain", "03/11/13, 05:58:15 PM", "NaN", "NaN"], ["419", "More task info", "Dain Sundstrom", "dain", "03/13/13, 05:17:38 PM", "NaN", "NaN"], ["423", "Minor fixes", "Dain Sundstrom", "dain", "03/14/13, 04:02:09 AM", "NaN", "NaN"], ["424", "Fixes", "David Phillips", "electrum", "03/14/13, 07:22:09 PM", "NaN", "NaN"], ["426", "Show proper error message when querying Hive views", "David Phillips", "electrum", "03/14/13, 07:36:35 PM", "NaN", "NaN"], ["427", "Make Hive view error message a constant", "David Phillips", "electrum", "03/14/13, 09:39:05 PM", "NaN", "NaN"], ["428", "Support queries with no matching partitions", "Dain Sundstrom", "dain", "03/14/13, 10:13:35 PM", "When setting no more splits on exchanges, mark all exchange sources not just the ones that had locations assigned", "NaN"], ["430", "Upgrage to Airbase 5 and Airlift 0.72", "Dain Sundstrom", "dain", "03/14/13, 11:44:31 PM", "NaN", "NaN"], ["431", "Upgrade versions", "Dain Sundstrom", "dain", "03/15/13, 06:32:57 AM", "NaN", "NaN"], ["432", "Various fixes", "Dain Sundstrom", "dain", "03/15/13, 07:53:45 PM", "NaN", "NaN"], ["433", "Add request scheduled/complete stats to exchange operator", "Martin Traverso", "martint", "03/15/13, 08:21:22 PM", "NaN", "NaN"], ["434", "Expose async http state in exchange info", "Dain Sundstrom", "dain", "03/15/13, 10:37:31 PM", "NaN", "NaN"], ["435", "Remove 100k split limit", "Dain Sundstrom", "dain", "03/18/13, 06:06:42 PM", "NaN", "NaN"], ["436", "Simple sql execution and create materialized view", "Henning Schmiedehausen", "hgschmie", "03/20/13, 12:52:36 AM", "NaN", "NaN"], ["437", "Use chunk schema", "Dain Sundstrom", "dain", "03/18/13, 06:37:31 PM", "NaN", "NaN"], ["438", "Change to new async configuration name", "Dain Sundstrom", "dain", "03/18/13, 07:03:12 PM", "NaN", "NaN"], ["439", "Upgrade to Airlift 0.73", "Dain Sundstrom", "dain", "03/18/13, 08:07:26 PM", "NaN", "NaN"], ["440", "Fixes", "Martin Traverso", "martint", "03/19/13, 06:09:46 AM", "NaN", "NaN"], ["441", "make threadpool sizes configurable", "Henning Schmiedehausen", "hgschmie", "03/20/13, 01:05:34 AM", "- query manager executor pool\n- import partition, chunk and shard pools", "NaN"], ["442", "Update and cancel tasks in parallel", "David Phillips", "electrum", "03/20/13, 08:28:58 PM", "NaN", "NaN"], ["443", "Upgrade to Airlift 0.74-SNAPSHOT", "David Phillips", "electrum", "03/21/13, 04:11:37 AM", "NaN", "NaN"], ["444", "Importer fixes", "Henning Schmiedehausen", "hgschmie", "03/21/13, 09:45:29 PM", "- fix node worker logic for imports\n- reduce size of buffered output streams to 64k", "NaN"], ["445", "Change memory allocation policy when growing slices.", "Henning Schmiedehausen", "hgschmie", "03/21/13, 11:45:31 PM", "Change the memory allocation policy when a slice needs memory beyond a\ngiven threshold (512k). Switch from doubling the current buffer size\nto an increase by 25%. Using this policy, a buffer of 512k that needs\nto hold 600k will grow only to 640k, not 1024k.\n\nBenchmarks seem to imply that queries run slightly faster:\n\noriginal:\n\n```\n                      count_agg ::    0.086 cpu ms :: in  1.5M,  12.9MB,   17.4B/s,   146GB/s :: out     1,      9B,   11.6K/s,   102KB/s\n                 double_sum_agg ::   10.211 cpu ms :: in  1.5M,  12.9MB,    147M/s,  1.23GB/s :: out     1,      9B,      97/s,    881B/s\n                       hash_agg ::  184.440 cpu ms :: in  1.5M,  21.5MB,   8.13M/s,   116MB/s :: out     3,     45B,      16/s,    243B/s\n               predicate_filter ::   99.070 cpu ms :: in  1.5M,  12.9MB,   15.1M/s,   130MB/s :: out 1.29M,  11.1MB,     13M/s,   112MB/s\n                     raw_stream ::    3.577 cpu ms :: in  1.5M,  12.9MB,    419M/s,  3.52GB/s :: out  1.5M,  12.9MB,    419M/s,  3.52GB/s\n                         top100 ::   33.175 cpu ms :: in  1.5M,  12.9MB,   45.2M/s,   388MB/s :: out   100,    900B,   3.01K/s,  26.5KB/s\n         in_memory_orderby_1.5M :: 1527.672 cpu ms :: in  1.5M,  41.5MB,    982K/s,  27.2MB/s :: out  1.5M,  28.6MB,    982K/s,  18.7MB/s\n                     hash_build ::  566.392 cpu ms :: in  1.5M,  25.8MB,   2.65M/s,  45.5MB/s :: out     0,      0B,       0/s,      0B/s\n                      hash_join :: 1645.129 cpu ms :: in    6M,   103MB,   3.65M/s,  62.6MB/s :: out    6M,   206MB,   3.65M/s,   125MB/s\n            hash_build_and_join :: 2005.875 cpu ms :: in  1.5M,   129MB,    748K/s,  64.2MB/s :: out    6M,   206MB,   2.99M/s,   103MB/s\nsql_groupby_agg_with_arithmetic :: 2057.132 cpu ms :: in    6M,   137MB,   2.92M/s,  66.8MB/s :: out     2,     30B,       0/s,     14B/s\n                  sql_count_agg ::    0.131 cpu ms :: in  1.5M,  12.9MB,   11.4B/s,  95.9GB/s :: out     1,      9B,   7.63K/s,  67.1KB/s\n             sql_double_sum_agg ::   10.315 cpu ms :: in  1.5M,  12.9MB,    145M/s,  1.22GB/s :: out     1,      9B,      96/s,    872B/s\n          sql_count_with_filter ::  163.948 cpu ms :: in  1.5M,  8.58MB,   9.15M/s,  52.4MB/s :: out     1,      9B,       6/s,     54B/s\n                sql_groupby_agg ::  191.831 cpu ms :: in  1.5M,  21.5MB,   7.82M/s,   112MB/s :: out     3,     45B,      15/s,    234B/s\n           sql_predicate_filter ::  154.564 cpu ms :: in  1.5M,  12.9MB,    9.7M/s,  83.3MB/s :: out 1.29M,  11.1MB,   8.34M/s,  71.6MB/s\n                 sql_raw_stream ::    3.649 cpu ms :: in  1.5M,  12.9MB,    411M/s,  3.45GB/s :: out  1.5M,  12.9MB,    411M/s,  3.45GB/s\n                    sql_top_100 ::   33.660 cpu ms :: in  1.5M,  12.9MB,   44.6M/s,   383MB/s :: out   100,    900B,   2.97K/s,  26.1KB/s\n                  sql_hash_join :: 1979.658 cpu ms :: in    6M,   129MB,   3.03M/s,  65.1MB/s :: out    6M,   206MB,   3.03M/s,   104MB/s\n              sql_varbinary_max ::  173.180 cpu ms :: in    6M,  97.3MB,   34.7M/s,   562MB/s :: out     1,     21B,       5/s,    121B/s\n             sql_distinct_multi ::  424.781 cpu ms :: in  1.5M,  27.8MB,   3.53M/s,  65.3MB/s :: out     5,     97B,      11/s,    228B/s\n            sql_distinct_single ::  126.152 cpu ms :: in  1.5M,  8.58MB,   11.9M/s,    68MB/s :: out     1,      6B,       7/s,     47B/s\n               sql_tpch_query_1 :: 15379.948 cpu ms :: in    6M,   361MB,    390K/s,  23.4MB/s :: out     4,    336B,       0/s,     21B/s\n             stat_long_variance ::   17.611 cpu ms :: in  1.5M,  12.9MB,   85.2M/s,   731MB/s :: out     1,      9B,      56/s,    511B/s\n         stat_long_variance_pop ::   17.593 cpu ms :: in  1.5M,  12.9MB,   85.3M/s,   732MB/s :: out     1,      9B,      56/s,    511B/s\n           stat_double_variance ::   17.744 cpu ms :: in  1.5M,  12.9MB,   84.5M/s,   726MB/s :: out     1,      9B,      56/s,    507B/s\n       stat_double_variance_pop ::   18.162 cpu ms :: in  1.5M,  12.9MB,   82.6M/s,   709MB/s :: out     1,      9B,      55/s,    495B/s\n               stat_long_stddev ::   19.123 cpu ms :: in  1.5M,  12.9MB,   78.4M/s,   673MB/s :: out     1,      9B,      52/s,    470B/s\n           stat_long_stddev_pop ::   18.837 cpu ms :: in  1.5M,  12.9MB,   79.6M/s,   684MB/s :: out     1,      9B,      53/s,    477B/s\n             stat_double_stddev ::   17.624 cpu ms :: in  1.5M,  12.9MB,   85.1M/s,   731MB/s :: out     1,      9B,      56/s,    510B/s\n         stat_double_stddev_pop ::   18.902 cpu ms :: in  1.5M,  12.9MB,   79.4M/s,   681MB/s :: out     1,      9B,      52/s,    476B/s\n sql_approx_count_distinct_long ::  139.656 cpu ms :: in  1.5M,  12.9MB,   10.7M/s,  92.2MB/s :: out     1,      9B,       7/s,     64B/s\n```\n\n   sql_approx_count_distinct_double ::  156.869 cpu ms :: in  1.5M,  12.9MB,   9.56M/s,  82.1MB/s :: out     1,      9B,       6/s,     57B/s\nsql_approx_count_distinct_varbinary ::  242.097 cpu ms :: in  1.5M,  21.5MB,    6.2M/s,  88.6MB/s :: out     1,      9B,       4/s,     37B/s\n\nwith this change:\n\n```\n                      count_agg ::    0.081 cpu ms :: in  1.5M,  12.9MB,   18.5B/s,   155GB/s :: out     1,      9B,   12.3K/s,   108KB/s\n                 double_sum_agg ::   10.186 cpu ms :: in  1.5M,  12.9MB,    147M/s,  1.23GB/s :: out     1,      9B,      98/s,    883B/s\n                       hash_agg ::  182.666 cpu ms :: in  1.5M,  21.5MB,   8.21M/s,   117MB/s :: out     3,     45B,      16/s,    246B/s\n               predicate_filter ::   98.264 cpu ms :: in  1.5M,  12.9MB,   15.3M/s,   131MB/s :: out 1.29M,  11.1MB,   13.1M/s,   113MB/s\n                     raw_stream ::    3.598 cpu ms :: in  1.5M,  12.9MB,    417M/s,  3.49GB/s :: out  1.5M,  12.9MB,    417M/s,  3.49GB/s\n                         top100 ::   33.915 cpu ms :: in  1.5M,  12.9MB,   44.2M/s,   380MB/s :: out   100,    900B,   2.95K/s,  25.9KB/s\n         in_memory_orderby_1.5M :: 1517.990 cpu ms :: in  1.5M,  41.5MB,    988K/s,  27.3MB/s :: out  1.5M,  28.6MB,    988K/s,  18.8MB/s\n                     hash_build ::  557.322 cpu ms :: in  1.5M,  25.8MB,   2.69M/s,  46.2MB/s :: out     0,      0B,       0/s,      0B/s\n                      hash_join :: 1605.892 cpu ms :: in    6M,   103MB,   3.74M/s,  64.2MB/s :: out    6M,   206MB,   3.74M/s,   128MB/s\n            hash_build_and_join :: 1950.674 cpu ms :: in  1.5M,   129MB,    769K/s,    66MB/s :: out    6M,   206MB,   3.08M/s,   106MB/s\nsql_groupby_agg_with_arithmetic :: 2007.107 cpu ms :: in    6M,   137MB,   2.99M/s,  68.4MB/s :: out     2,     30B,       0/s,     14B/s\n                  sql_count_agg ::    0.121 cpu ms :: in  1.5M,  12.9MB,   12.4B/s,   104GB/s :: out     1,      9B,   8.24K/s,  72.4KB/s\n             sql_double_sum_agg ::   10.238 cpu ms :: in  1.5M,  12.9MB,    147M/s,  1.23GB/s :: out     1,      9B,      97/s,    879B/s\n          sql_count_with_filter ::  158.863 cpu ms :: in  1.5M,  8.58MB,   9.44M/s,    54MB/s :: out     1,      9B,       6/s,     56B/s\n                sql_groupby_agg ::  186.075 cpu ms :: in  1.5M,  21.5MB,   8.06M/s,   115MB/s :: out     3,     45B,      16/s,    241B/s\n           sql_predicate_filter ::  152.790 cpu ms :: in  1.5M,  12.9MB,   9.82M/s,  84.3MB/s :: out 1.29M,  11.1MB,   8.44M/s,  72.4MB/s\n                 sql_raw_stream ::    3.702 cpu ms :: in  1.5M,  12.9MB,    405M/s,   3.4GB/s :: out  1.5M,  12.9MB,    405M/s,   3.4GB/s\n                    sql_top_100 ::   34.809 cpu ms :: in  1.5M,  12.9MB,   43.1M/s,   370MB/s :: out   100,    900B,   2.87K/s,  25.2KB/s\n                  sql_hash_join :: 1992.505 cpu ms :: in    6M,   129MB,   3.01M/s,  64.6MB/s :: out    6M,   206MB,   3.01M/s,   103MB/s\n              sql_varbinary_max ::  176.755 cpu ms :: in    6M,  97.3MB,     34M/s,   550MB/s :: out     1,     21B,       5/s,    118B/s\n             sql_distinct_multi ::  416.329 cpu ms :: in  1.5M,  27.8MB,    3.6M/s,  66.7MB/s :: out     5,     97B,      12/s,    232B/s\n            sql_distinct_single ::  130.377 cpu ms :: in  1.5M,  8.58MB,   11.5M/s,  65.8MB/s :: out     1,      6B,       7/s,     46B/s\n               sql_tpch_query_1 :: 15415.718 cpu ms :: in    6M,   361MB,    389K/s,  23.4MB/s :: out     4,    336B,       0/s,     21B/s\n             stat_long_variance ::   17.385 cpu ms :: in  1.5M,  12.9MB,   86.3M/s,   741MB/s :: out     1,      9B,      57/s,    517B/s\n         stat_long_variance_pop ::   17.379 cpu ms :: in  1.5M,  12.9MB,   86.3M/s,   741MB/s :: out     1,      9B,      57/s,    517B/s\n           stat_double_variance ::   17.366 cpu ms :: in  1.5M,  12.9MB,   86.4M/s,   741MB/s :: out     1,      9B,      57/s,    518B/s\n       stat_double_variance_pop ::   17.378 cpu ms :: in  1.5M,  12.9MB,   86.3M/s,   741MB/s :: out     1,      9B,      57/s,    517B/s\n               stat_long_stddev ::   17.433 cpu ms :: in  1.5M,  12.9MB,     86M/s,   739MB/s :: out     1,      9B,      57/s,    516B/s\n           stat_long_stddev_pop ::   17.483 cpu ms :: in  1.5M,  12.9MB,   85.8M/s,   736MB/s :: out     1,      9B,      57/s,    514B/s\n             stat_double_stddev ::   17.445 cpu ms :: in  1.5M,  12.9MB,     86M/s,   738MB/s :: out     1,      9B,      57/s,    515B/s\n         stat_double_stddev_pop ::   17.490 cpu ms :: in  1.5M,  12.9MB,   85.8M/s,   736MB/s :: out     1,      9B,      57/s,    514B/s\n sql_approx_count_distinct_long ::  129.374 cpu ms :: in  1.5M,  12.9MB,   11.6M/s,  99.5MB/s :: out     1,      9B,       7/s,     69B/s\n```\n\n   sql_approx_count_distinct_double ::  141.799 cpu ms :: in  1.5M,  12.9MB,   10.6M/s,  90.8MB/s :: out     1,      9B,       7/s,     63B/s\nsql_approx_count_distinct_varbinary ::  210.820 cpu ms :: in  1.5M,  21.5MB,   7.12M/s,   102MB/s :: out     1,      9B,       4/s,     42B/s", "NaN"], ["448", "Fix finding current server in execute endpoint", "David Phillips", "electrum", "03/23/13, 05:38:11 PM", "NaN", "NaN"], ["449", "Use TestingMBeanServer in TestingPrestoServer\t", "David Phillips", "electrum", "03/24/13, 02:18:04 AM", "NaN", "NaN"], ["450", "Upgrade to airbase 6 and airlift 0.74", "Dain Sundstrom", "dain", "03/25/13, 08:57:23 PM", "NaN", "NaN"], ["451", "Metadata api", "Henning Schmiedehausen", "hgschmie", "03/29/13, 07:50:35 PM", "NaN", "NaN"], ["452", "Optimizers api", "Henning Schmiedehausen", "hgschmie", "03/29/13, 07:53:23 PM", "Do not work on this before https://github.com/facebook/presto/pull/451 has been applied. This sits on top of that pull request.", "NaN"], ["453", "Allow duplicate requests to add buffers", "Dain Sundstrom", "dain", "03/27/13, 06:50:20 PM", "NaN", "NaN"], ["454", "Rename ShardBoundedExecutor to KeyBoundedExecutor", "David Phillips", "electrum", "03/28/13, 12:55:42 AM", "NaN", "NaN"], ["455", "Split stats", "Dain Sundstrom", "dain", "03/28/13, 10:05:08 PM", "NaN", "NaN"], ["456", "Drop table", "Henning Schmiedehausen", "hgschmie", "04/15/13, 05:32:54 PM", "NaN", "NaN"], ["457", "Fix NPE in ExecutionStats", "Dain Sundstrom", "dain", "03/29/13, 07:35:21 PM", "NaN", "NaN"], ["458", "fix missing import", "Henning Schmiedehausen", "hgschmie", "03/29/13, 08:22:57 PM", "NaN", "NaN"], ["459", "Statement resource", "Dain Sundstrom", "dain", "03/30/13, 12:02:02 AM", "NaN", "NaN"], ["460", "Make CLI use new client API", "David Phillips", "electrum", "04/03/13, 01:12:24 AM", "NaN", "NaN"], ["461", "Various fixes", "Dain Sundstrom", "dain", "04/03/13, 06:48:15 PM", "NaN", "NaN"], ["462", "Quote identifiers when formatting expressions", "Martin Traverso", "martint", "04/03/13, 05:00:37 PM", "This is to avoid issues with identifiers that are also SQL keywords", "NaN"], ["463", "Upgrade to Airlift 0.75", "Dain Sundstrom", "dain", "04/03/13, 07:50:41 PM", "NaN", "NaN"], ["464", "Simple heartbeat-based failure detector", "Martin Traverso", "martint", "04/04/13, 12:33:08 AM", "This is a naive failure detector that watches discovery for \"presto\" announcements and monitors the remote services using the http url.\n\nIt considers a service failed if the percentage of failed requests over the last minute (exponentially decayed) is higher than a configurable threshold. It also supports a configurable warmup period during which newly discovered nodes are considered failed.", "NaN"], ["465", "Fail remote tasks on bad responses or repeated failures", "Dain Sundstrom", "dain", "04/04/13, 01:47:28 AM", "NaN", "NaN"], ["466", "Move NoOpFailureDetector to tests", "Martin Traverso", "martint", "04/04/13, 05:04:54 PM", "NaN", "NaN"], ["468", "Fix overflow bug in status printer", "David Phillips", "electrum", "04/04/13, 04:57:55 AM", "NaN", "NaN"], ["469", "More updates for new client API", "David Phillips", "electrum", "04/05/13, 10:32:00 PM", "NaN", "NaN"], ["470", "Fixes", "Dain Sundstrom", "dain", "04/05/13, 12:34:36 AM", "NaN", "NaN"], ["471", "Optimize split assignment algorithm and tracking of nodes", "Martin Traverso", "martint", "04/05/13, 06:56:16 PM", "NaN", "NaN"], ["472", "Choose all local and rack-local nodes and fill with random if needed", "Martin Traverso", "martint", "04/05/13, 08:26:05 PM", "NaN", "NaN"], ["473", "Fix another deadlock in HttpRemoteTask", "Dain Sundstrom", "dain", "04/05/13, 11:04:36 PM", "NaN", "NaN"], ["474", "Tolerate connection errors in client", "Dain Sundstrom", "dain", "04/05/13, 11:19:32 PM", "NaN", "NaN"], ["475", "Fix console --execute", "David Phillips", "electrum", "04/08/13, 08:11:30 PM", "NaN", "NaN"], ["476", "Minor statement API cleanups", "David Phillips", "electrum", "04/08/13, 09:38:36 PM", "NaN", "NaN"], ["478", "Id improvements", "Dain Sundstrom", "dain", "04/09/13, 10:29:52 PM", "NaN", "NaN"], ["479", "Fix cancel", "Dain Sundstrom", "dain", "04/10/13, 03:46:31 PM", "NaN", "NaN"], ["480", "Add event logging for split completion", "Eric Hwang", "erichwang", "04/16/13, 06:24:13 PM", "The nectar payload in scribe will look like this:\n\n{\n  \"completed_positions_rate_ten_sec\": 437,\n  \"completed_data_size_count_one_min\": 1026901,\n  \"event_type\": \"SplitCompletion\",\n  \"task_id\": \"2.1.0\",\n  \"completed_data_size_total\": 1026901,\n  \"time_to_last_byte_ms\": 24719,\n  \"event_uuid\": \"fffb559f-7db3-40bb-aad4-56e001f1dbc8\",\n  \"completed_positions_count_ten_sec\": 4370,\n  \"time_to_first_byte_ms\": 24716,\n  \"completed_data_size_rate_thirty_sec\": 34230.033333333,\n  \"query_id\": \"2\",\n  \"execution_start_time\": \"2013-04-09T22:40:23.309Z\",\n  \"wall_time_ms\": 24720,\n  \"completed_positions_rate_one_min\": 72.833333333333,\n  \"completed_data_size_count_ten_sec\": 1026901,\n  \"completed_positions_count_thirty_sec\": 4370,\n  \"completed_positions_count_one_min\": 4370,\n  \"cpu_time_ms\": 210,\n  \"split_info_json\": \"[{\\\"path\\\":\\\"hdfs:\\/\\/dfs1.data.facebook.com:9000\\/user\\/facebook\\/warehouse\\/ad_account_bass_ad_obj_map\\/ds=2013-04-06\\/000000_0\\\",\\\"start\\\":0,\\\"length\\\":49792606,\\\"hosts\\\":[\\\"hadoop2913.snc5.facebook.com.\\/10.38.208.27\\\",\\\"hadoop417.snc5.facebook.com.\\/10.38.27.21\\\",\\\"hadoop1829.snc5.facebook.com.\\/10.38.106.27\\\"]}]\",\n  \"completed_positions_total\": 4370,\n  \"completed_data_size_count_thirty_sec\": 1026901,\n  \"user_time_ms\": 199,\n  \"event_time\": \"2013-04-09T22:40:48.033Z\",\n  \"completed_data_size_rate_ten_sec\": 102690.1,\n  \"completed_positions_rate_thirty_sec\": 145.66666666667,\n  \"completed_data_size_rate_one_min\": 17115.016666667,\n  \"event_host\": \"ehwang-mbp.local\",\n  \"stage_id\": \"2.1\"\n}", "NaN"], ["481", "Move all JaxRS code to server module and remove dependency", "Dain Sundstrom", "dain", "04/10/13, 05:36:20 PM", "NaN", "NaN"], ["483", "Move Event class to presto-facebook", "David Phillips", "electrum", "04/11/13, 06:41:05 AM", "NaN", "NaN"], ["484", "Enable JMX stats for DFSClient", "Eric Hwang", "erichwang", "04/12/13, 12:25:08 AM", "This took way too long", "NaN"], ["486", "Fix bug in StatementResource purger", "Dain Sundstrom", "dain", "04/12/13, 03:52:29 PM", "NaN", "NaN"], ["488", "Improve error message for backquoted identifiers", "David Phillips", "electrum", "04/16/13, 06:02:57 PM", "NaN", "NaN"], ["489", "Add snappy compression for native store.", "Henning Schmiedehausen", "hgschmie", "04/16/13, 06:01:26 PM", "Use Snappy to block compress data.", "NaN"], ["490", "Fix setting timeouts for Hive metastore client", "David Phillips", "electrum", "04/17/13, 01:17:07 AM", "NaN", "NaN"], ["492", "fix client hang for simple queries with no output stage", "Henning Schmiedehausen", "hgschmie", "04/18/13, 06:47:53 PM", "This makes e.g. \"DROP TABLE\" not hang in the cli.", "NaN"], ["493", "change default table store to use snappy", "Henning Schmiedehausen", "hgschmie", "04/18/13, 07:11:46 PM", "NaN", "NaN"], ["494", "only return data if query was successful", "Henning Schmiedehausen", "hgschmie", "04/18/13, 08:31:57 PM", "NaN", "NaN"], ["499", "Create alias", "Henning Schmiedehausen", "hgschmie", "04/20/13, 01:43:08 AM", "do not work on this pull request until https://github.com/facebook/presto/pull/498 was applied!", "NaN"], ["500", "Add simple query dashboard", "Martin Traverso", "martint", "04/20/13, 12:52:45 AM", "NaN", "NaN"], ["501", "Fix bug in TupleInfo.setNull/setNotNull", "Martin Traverso", "martint", "04/20/13, 12:41:50 AM", "NaN", "NaN"], ["502", "Flush buffered console results during slow queries", "David Phillips", "electrum", "04/20/13, 12:59:38 AM", "NaN", "NaN"], ["504", "Fix task error detection thresholds to work properly", "Eric Hwang", "erichwang", "04/25/13, 05:29:22 AM", "NaN", "NaN"], ["506", "Upgrade DFSClient and add slow node switching", "Eric Hwang", "erichwang", "04/26/13, 07:44:22 PM", "NaN", "NaN"], ["507", "Make EXTRACT field keywords non-reserved", "David Phillips", "electrum", "04/25/13, 08:26:32 PM", "NaN", "NaN"], ["508", "Remove import client", "Dain Sundstrom", "dain", "05/02/13, 02:08:26 AM", "NaN", "NaN"], ["509", "Fail tasks immediately on server 500 errors", "Eric Hwang", "erichwang", "04/26/13, 05:49:47 PM", "NaN", "NaN"], ["510", "Fix parsing of identifiers that start with a digit", "David Phillips", "electrum", "04/26/13, 05:49:48 PM", "NaN", "NaN"], ["511", "Add additional retention property for test tables", "David Phillips", "electrum", "04/26/13, 06:29:14 PM", "NaN", "NaN"], ["512", "Minor (hacky) refactoring to fix missing stats from completion event", "Martin Traverso", "martint", "04/26/13, 07:04:30 PM", "NaN", "NaN"], ["513", "Record failures for any error during task scheduling", "Martin Traverso", "martint", "04/26/13, 07:06:02 PM", "NaN", "NaN"], ["514", "ExpressionFormatter fixes", "Martin Traverso", "martint", "04/26/13, 07:50:08 PM", "NaN", "NaN"], ["515", "Fix bug when ordering by window function", "Martin Traverso", "martint", "04/26/13, 08:31:37 PM", "NaN", "NaN"], ["516", "Fix SQL formatter and add tests", "David Phillips", "electrum", "04/29/13, 08:02:37 PM", "NaN", "NaN"], ["518", "Fix issue when following symlinks across hdfs instances", "Martin Traverso", "martint", "04/30/13, 05:34:42 PM", "NaN", "NaN"], ["519", "Ask for partition info in exponentially increasing batch sizes", "Martin Traverso", "martint", "04/30/13, 06:18:42 PM", "This should help speed up trivial queries (e.g., limit) when the number of candidate partitions is large and the partition information hasn't been cached yet.", "NaN"], ["520", "Fast trivial queries", "Dain Sundstrom", "dain", "05/02/13, 06:21:01 PM", "NaN", "NaN"], ["521", "Show error for SHOW COLUMNS if table does not exist", "Martin Traverso", "martint", "05/02/13, 05:11:09 AM", "NaN", "NaN"], ["522", "Show whether columns are partition keys in DESCRIBE", "Martin Traverso", "martint", "05/02/13, 05:11:10 AM", "NaN", "NaN"], ["523", "Fix benchmarks", "Dain Sundstrom", "dain", "05/02/13, 05:22:33 PM", "NaN", "NaN"], ["524", "Add benchmark for LIKE", "Martin Traverso", "martint", "05/06/13, 06:46:16 PM", "NaN", "NaN"], ["525", "Fix starvation (task should run if the query is not done)", "Martin Traverso", "martint", "05/06/13, 06:46:16 PM", "NaN", "NaN"], ["526", "Add retry code back into hive client", "Dain Sundstrom", "dain", "05/06/13, 07:31:24 PM", "Remove other unnecessary uses of RetryDriver\nMove RetryDriver to hive module", "NaN"], ["527", "Fix show partitions bug", "Dain Sundstrom", "dain", "05/06/13, 07:35:53 PM", "Supply a default value to bindings optional", "NaN"], ["528", "Fix calls to Optional.get() without Optional.isPresent()", "Dain Sundstrom", "dain", "05/06/13, 09:39:13 PM", "NaN", "NaN"], ["529", "Return multiple pages from statement resource", "Dain Sundstrom", "dain", "05/09/13, 07:56:48 PM", "NaN", "NaN"], ["530", "Use UNIX timestamps in seconds for time functions", "David Phillips", "electrum", "05/06/13, 10:49:56 PM", "NaN", "NaN"], ["531", "Skip split address hints that do not resolve", "Dain Sundstrom", "dain", "05/08/13, 05:35:27 PM", "NaN", "NaN"], ["532", "Fix symbol to QualifiedName conversion", "Eric Hwang", "erichwang", "05/08/13, 01:50:09 AM", "NaN", "NaN"], ["533", "Implement aggregation analysis for various SQL constructs", "Martin Traverso", "martint", "05/09/13, 01:43:23 AM", "NaN", "NaN"], ["534", "Minor fixes", "Dain Sundstrom", "dain", "05/08/13, 06:24:23 PM", "NaN", "NaN"], ["535", "Generate better error message if nodes are unavailable", "Martin Traverso", "martint", "05/09/13, 01:43:23 AM", "Right now the call to Ordering.min() fails with NoSuchElementException and show \"failed: null\" to the user", "NaN"], ["536", "Improve metadata system", "Dain Sundstrom", "dain", "05/10/13, 12:31:36 AM", "Replace internal schema with connector metadata\nAdd sys.query and sys.task tables\nAdd jmx connector\nMove local tests and benchmarks to use standard execution apis", "NaN"], ["537", "Add test tables for offline tables and partitions", "David Phillips", "electrum", "05/09/13, 01:43:23 AM", "NaN", "NaN"], ["539", "Slight refactor Relation and Subquery ANTLR parsing", "Eric Hwang", "erichwang", "05/13/13, 07:18:54 PM", "NaN", "NaN"], ["541", "Hive connector fixes", "Dain Sundstrom", "dain", "05/10/13, 12:58:41 AM", "NaN", "NaN"], ["542", "Add more data to sys.query and sys.task", "Dain Sundstrom", "dain", "05/10/13, 05:17:47 AM", "Rename heartBeat to heartbeat", "NaN"], ["543", "Expose hive flush cache via JMX", "Dain Sundstrom", "dain", "05/10/13, 06:05:21 PM", "NaN", "NaN"], ["544", "Stop exchange operator immediately when task output is done", "Dain Sundstrom", "dain", "05/10/13, 05:00:55 PM", "NaN", "NaN"], ["545", "Cache regexps for LIKE and switch to joni", "Martin Traverso", "martint", "05/10/13, 06:03:19 PM", "NaN", "NaN"], ["548", "Limit the amount of memory for TopN operator", "Martin Traverso", "martint", "05/11/13, 06:16:46 AM", "NaN", "NaN"], ["549", "Optimize LIKE expressions", "Martin Traverso", "martint", "05/11/13, 03:05:30 AM", "- If pattern does not have % or _, replace with comparison expression\n- Return new like expression with optimized operands", "NaN"], ["550", "Fix null handling in LIKE expressions", "Martin Traverso", "martint", "05/11/13, 05:00:37 AM", "NaN", "NaN"], ["551", "Add support for Hive binary type", "Dain Sundstrom", "dain", "05/13/13, 05:04:09 PM", "NaN", "NaN"], ["552", "Benchmark for IN with constant list", "Martin Traverso", "martint", "05/11/13, 06:56:24 PM", "NaN", "NaN"], ["554", "Enable IN benchmark", "Martin Traverso", "martint", "05/12/13, 07:28:31 PM", "I forgot to add it to the benchmark suite...", "NaN"], ["555", "Do not assume metastore.get_partitions_by_names returns partitions in the same order as requested", "Dain Sundstrom", "dain", "05/13/13, 07:29:17 PM", "NaN", "NaN"], ["557", "Add more information to events", "Dain Sundstrom", "dain", "05/14/13, 12:14:43 AM", "add source, remote user address and user agent to session\nadd partition id to hive split info ", "NaN"], ["558", "Add function type to show functions command", "Dain Sundstrom", "dain", "05/14/13, 12:16:42 AM", "NaN", "NaN"], ["559", "Add max split size to hive client", "Dain Sundstrom", "dain", "05/14/13, 05:16:59 PM", "NaN", "NaN"], ["561", "Alter queries to support a QueryBody structure", "Eric Hwang", "erichwang", "05/22/13, 12:17:24 AM", "NaN", "NaN"], ["562", "Support multiple wildcards in select list", "David Phillips", "electrum", "05/14/13, 08:02:29 PM", "NaN", "NaN"], ["563", "Expose some query/task manager stats via jmx", "Martin Traverso", "martint", "05/14/13, 10:02:20 PM", "NaN", "NaN"], ["564", "Add table name to hive split info", "Martin Traverso", "martint", "05/15/13, 01:31:35 AM", "NaN", "NaN"], ["565", "Unpartitioned hive tables have no partition keys not no partitions", "Dain Sundstrom", "dain", "05/15/13, 10:59:11 PM", "NaN", "NaN"], ["566", "Upgrade to hive-presto-shaded 0.5", "David Phillips", "electrum", "05/17/13, 12:08:55 AM", "NaN", "NaN"], ["567", "Add max page size limit", "Dain Sundstrom", "dain", "05/17/13, 11:59:13 PM", "NaN", "NaN"], ["568", "Implement mysql-compatible date format/parse functions", "Martin Traverso", "martint", "05/20/13, 06:15:47 PM", "NaN", "NaN"], ["569", "Better constrain buffer memory usage", "Dain Sundstrom", "dain", "05/21/13, 07:51:58 PM", "Add soft limit to exchange client buffer\nAdd soft limit to output sink buffer (SharedBuffer)\nSize buffers in PageBuilder based on the size of previous page produced\nAdd more testing to exchange client", "NaN"], ["570", "Add missing @JsonProperty annotation to session source field", "Dain Sundstrom", "dain", "05/20/13, 08:58:15 PM", "NaN", "NaN"], ["571", "Add com.fasterxml.jackson to parent-first packages", "Tim Williamson", "twilliamson", "05/20/13, 10:21:49 PM", "NaN", "NaN"], ["572", "Sort queries in reverse-chronological order", "Martin Traverso", "martint", "05/21/13, 07:43:51 PM", "NaN", "NaN"], ["574", "Add UNION [ALL] support", "Eric Hwang", "erichwang", "05/24/13, 11:08:44 PM", "NaN", "NaN"], ["575", "Add masterSequenceId to SharedBufferInfo", "Martin Traverso", "martint", "05/22/13, 10:24:25 PM", "NaN", "NaN"], ["576", "UI fixes", "Martin Traverso", "martint", "05/22/13, 10:26:40 PM", "NaN", "NaN"], ["578", "Limit size of response based on bytes not pages", "Dain Sundstrom", "dain", "05/23/13, 03:24:15 AM", "NaN", "NaN"], ["580", "Use new shaded Hadoop and Hive artifacts", "David Phillips", "electrum", "05/23/13, 06:41:00 PM", "NaN", "NaN"], ["581", "Make query id actually unique", "Dain Sundstrom", "dain", "05/24/13, 09:45:29 PM", "NaN", "NaN"], ["583", "Task manager fixes", "Dain Sundstrom", "dain", "05/24/13, 11:04:44 PM", "NaN", "NaN"], ["584", "Change date_format and date_parse to use server local time as mysql does", "Dain Sundstrom", "dain", "05/24/13, 11:04:44 PM", "NaN", "NaN"], ["585", "Some extra commits for the UNION stuff", "Eric Hwang", "erichwang", "05/25/13, 01:47:28 AM", "NaN", "NaN"], ["586", "Assure task info end time is set when task is done", "Dain Sundstrom", "dain", "05/28/13, 07:59:03 PM", "NaN", "NaN"], ["587", "Provides a quick fix for the UNION ordering issue", "Eric Hwang", "erichwang", "05/28/13, 10:32:57 PM", "NaN", "NaN"], ["588", "Collapse project on filter ", "Dain Sundstrom", "dain", "05/28/13, 10:49:34 PM", "NaN", "NaN"], ["589", "Update joni to 2.0.0", "Martin Traverso", "martint", "05/28/13, 11:01:42 PM", "NaN", "NaN"], ["591", "Clear operator in worker after work is completed", "Dain Sundstrom", "dain", "05/29/13, 01:17:39 AM", "NaN", "NaN"], ["592", "Add tpch query 6 to benchmark", "Dain Sundstrom", "dain", "05/29/13, 01:23:55 AM", "NaN", "NaN"], ["593", "Fix planning issue when grouping by repeated fields", "Martin Traverso", "martint", "05/29/13, 05:53:19 PM", "NaN", "NaN"], ["594", "Avoid exponential backtracking when parsing nested expressions", "Martin Traverso", "martint", "05/29/13, 06:47:35 PM", "NaN", "NaN"], ["595", "Add queryId/stageId/taskId to worker thread names", "Dain Sundstrom", "dain", "05/29/13, 08:42:59 PM", "NaN", "NaN"], ["597", "Really fix issue when grouping by repeated fields", "Martin Traverso", "martint", "05/29/13, 08:26:12 PM", "The previous fix was in the wrong place", "NaN"], ["598", "Normalize UNION input if the underlying projection differs from expected", "Eric Hwang", "erichwang", "06/04/13, 12:02:12 AM", "NaN", "NaN"], ["600", "Add basic regexp functions", "David Phillips", "electrum", "05/30/13, 07:58:58 PM", "NaN", "NaN"], ["601", "Add Limit push down optimizer", "Eric Hwang", "erichwang", "06/03/13, 10:58:56 PM", "NaN", "NaN"], ["603", "Function cleanups", "David Phillips", "electrum", "05/30/13, 08:38:45 PM", "NaN", "NaN"], ["604", "Function fixes", "David Phillips", "electrum", "05/30/13, 11:37:30 PM", "NaN", "NaN"], ["605", "Fix hang in LIKE operator for invalid UTF-8", "David Phillips", "electrum", "05/31/13, 07:07:06 PM", "NaN", "NaN"], ["606", "Add support for IS DISTINCT FROM", null, "amleshjk", "05/31/13, 10:10:12 PM", "NaN", "NaN"], ["607", "Fix comparison test for IS DISTINCT FROM", null, "amleshjk", "05/31/13, 10:40:34 PM", "NaN", "NaN"], ["608", "Fix Hadoop FileSystem caching", "David Phillips", "electrum", "06/04/13, 06:24:33 PM", "NaN", "NaN"], ["609", "Some minor optimizations", "Martin Traverso", "martint", "06/05/13, 06:37:05 PM", "NaN", "NaN"], ["610", "Compiler", "Dain Sundstrom", "dain", "06/11/13, 01:48:31 AM", "NaN", "NaN"], ["611", "Make SlowDatanodeSwitcher compatible with cdh4", "Eric Hwang", "erichwang", "06/06/13, 12:21:05 AM", "NaN", "NaN"], ["612", "Add the split_part string function", null, "amleshjk", "06/14/13, 07:37:45 PM", "NaN", "NaN"], ["613", "Add url_extract and url_extract_param", null, "amleshjk", "06/14/13, 07:52:33 PM", "NaN", "NaN"], ["614", "Full boolean support", "Dain Sundstrom", "dain", "06/11/13, 02:11:35 AM", "NaN", "NaN"], ["615", "Codegen filter to record set", "Dain Sundstrom", "dain", "06/19/13, 07:54:40 PM", "NaN", "NaN"], ["616", "Make error message say \"column\" instead of \"attribute\"", "Martin Traverso", "martint", "06/11/13, 11:44:01 PM", "NaN", "NaN"], ["617", "Add exception cause before transitioning to FAILED state", "Dain Sundstrom", "dain", "06/12/13, 06:21:52 PM", "NaN", "NaN"], ["618", "Excape single quotes when outputing string literal", "Dain Sundstrom", "dain", "06/12/13, 06:15:50 PM", "NaN", "NaN"], ["619", "Refresh hive cached values in background", "Dain Sundstrom", "dain", "06/12/13, 11:44:16 PM", "NaN", "NaN"], ["620", "Throw specific message for identifiers containing a colon", "Dain Sundstrom", "dain", "06/12/13, 10:49:46 PM", "NaN", "NaN"], ["622", "Improve error messages", "Dain Sundstrom", "dain", "06/13/13, 01:23:02 AM", "NaN", "NaN"], ["623", "Add environment to event objects", "Dain Sundstrom", "dain", "06/13/13, 05:17:50 PM", "NaN", "NaN"], ["624", "Add json_array_length and json_array_contains", "David Phillips", "electrum", "06/13/13, 11:34:37 PM", "NaN", "NaN"], ["625", "Change sum(Boolean) to count_if(boolean) with same null handeling as count", "Dain Sundstrom", "dain", "06/14/13, 09:02:46 PM", "NaN", "NaN"], ["626", "Split URL extraction into separate functions", "David Phillips", "electrum", "06/14/13, 11:17:52 PM", "NaN", "NaN"], ["627", "Properly handle null in filter funtion result", "Dain Sundstrom", "dain", "06/15/13, 04:27:41 AM", "NaN", "NaN"], ["628", "Fix comparisons of longs as doubles in interpreter", "David Phillips", "electrum", "06/15/13, 04:23:14 AM", "NaN", "NaN"], ["630", "Update to slice 0.2", "Martin Traverso", "martint", "06/17/13, 07:11:17 PM", "NaN", "NaN"], ["631", "Fix parsing of current_timestamp as a single expression", "Martin Traverso", "martint", "06/17/13, 10:07:23 PM", "NaN", "NaN"], ["632", "Update to airlift 0.77", "Martin Traverso", "martint", "06/17/13, 10:47:38 PM", "NaN", "NaN"], ["633", "Align compiler interpreter", "Dain Sundstrom", "dain", "06/18/13, 12:18:10 AM", "NaN", "NaN"], ["634", "Fix bug in MAX/MIN aggregations with null strings", "Martin Traverso", "martint", "06/18/13, 07:26:27 PM", "NaN", "NaN"], ["635", "Left Join + Predicate Pushdown", "Eric Hwang", "erichwang", "06/21/13, 01:41:05 AM", "NaN", "NaN"], ["636", "Reduce the number of expected groups", "Martin Traverso", "martint", "06/18/13, 10:56:28 PM", "The current value of 100k means that each aggregation requires at least 400 KB of memory. For a query running ~120 splits per node, this adds up.", "NaN"], ["638", "Fix Eclipse compile errors", "Henning Schmiedehausen", "hgschmie", "06/19/13, 04:29:36 PM", "The Eclipse incremental compiler can not deal with the short expressions. Hint with some casts.", "NaN"], ["639", "Fix reversed precondition that broke aliases", "Henning Schmiedehausen", "hgschmie", "06/19/13, 06:49:07 PM", "NaN", "NaN"], ["641", "Fix isNull call for classes generated against RecordSet", "Dain Sundstrom", "dain", "06/20/13, 12:45:15 AM", "NaN", "NaN"], ["642", "Fix broken ConcatProjection function", "Dain Sundstrom", "dain", "06/20/13, 06:44:06 AM", "NaN", "NaN"], ["643", "Update operator stats when reading directly from RecordSet", "Dain Sundstrom", "dain", "06/20/13, 09:15:53 PM", "NaN", "NaN"], ["644", "Add EXPLAIN functionality to presto", null, "amleshjk", "06/21/13, 10:36:09 PM", "NaN", "NaN"], ["646", "Add boolean support to render() function", "Martin Traverso", "martint", "06/21/13, 11:50:50 PM", "NaN", "NaN"], ["649", "implement sys.alias", "Henning Schmiedehausen", "hgschmie", "06/22/13, 12:18:32 AM", "list all the table aliases in the system.\n\nselect \\* from sys.alias;\n source_connector_id | source_schema_name |              source_table_name               | destination_connector_id | destination_schema_name | destination_table_name\n---------------------+--------------------+----------------------------------------------+--------------------------+-------------------------+------------------------\n ailabs              | bi_carolina        | tmp_presto_polaris_lu_psychographic_segments | default                  | default                 | tpp\n(1 row)", "NaN"], ["650", "Fix deadlock between stage with too many splits and substage with full output buffers", "Dain Sundstrom", "dain", "06/24/13, 07:13:35 PM", "NaN", "NaN"], ["651", "Add test for count with boolean argument", "Martin Traverso", "martint", "06/24/13, 06:34:54 PM", "NaN", "NaN"], ["652", "JDBC fixes", "David Phillips", "electrum", "06/23/13, 05:16:39 AM", "NaN", "NaN"], ["653", "Fix development server config", "David Phillips", "electrum", "06/24/13, 06:24:30 PM", "NaN", "NaN"], ["654", "Add SHOW SCHEMAS function", null, "amleshjk", "06/25/13, 05:56:37 PM", "NaN", "NaN"], ["656", "Add split worker id to thread name", "Martin Traverso", "martint", "06/25/13, 05:56:37 PM", "NaN", "NaN"], ["657", "Limit memory for entire task instead of per operator", "Dain Sundstrom", "dain", "06/25/13, 07:49:14 PM", "NaN", "NaN"], ["658", "Rework tab complete to run a query and add tab complete for functions", null, "amleshjk", "07/16/13, 06:03:36 PM", "NaN", "NaN"], ["659", "Add support for Right join", "Eric Hwang", "erichwang", "07/03/13, 12:56:07 AM", "NaN", "NaN"], ["660", "Fix cascading of job to run table.", "Henning Schmiedehausen", "hgschmie", "07/01/13, 05:05:54 PM", "Any run for a job will hold the row in the job table and jobs can not\nbe deleted. Change foreign key to drop all run records if a job gets\ndeleted.", "NaN"], ["664", "Change operators to have pre allocated memory instead of a min flush size", "Dain Sundstrom", "dain", "06/26/13, 01:15:25 AM", "NaN", "NaN"], ["667", "Run prune unreferenced outputs one more time for predicate pushdown", "Eric Hwang", "erichwang", "06/28/13, 03:23:47 AM", "NaN", "NaN"], ["668", "Show warning message if FloatingDecimal patch to fix monitoring contention is not installed", "Martin Traverso", "martint", "06/28/13, 11:06:12 PM", "NaN", "NaN"], ["669", "fix bungled imports", "Henning Schmiedehausen", "hgschmie", "07/01/13, 10:09:12 PM", "NaN", "NaN"], ["670", "Fix compilation of IF and CASE statements with nulls", "Dain Sundstrom", "dain", "07/02/13, 06:40:50 PM", "NaN", "NaN"], ["671", "Add namespace to CLI output", null, "amleshjk", "07/02/13, 08:45:42 PM", "NaN", "NaN"], ["672", "Add FORMAT to EXPLAIN queries, and add graphviz explain functionality", null, "amleshjk", "07/03/13, 02:11:04 AM", "NaN", "NaN"], ["673", "Filter active nodes by node version", "Eric Hwang", "erichwang", "07/02/13, 10:30:53 PM", "NaN", "NaN"], ["674", "Disable Jackson field name canonicalization (string intern cache)", "Dain Sundstrom", "dain", "07/03/13, 01:05:33 AM", "Close Jackson parsers after use", "NaN"], ["675", "Fix compiler var-args warning", "Dain Sundstrom", "dain", "07/03/13, 01:53:05 AM", "NaN", "NaN"], ["676", "Allow per-host configuration for HDFS", "David Phillips", "electrum", "07/04/13, 05:45:11 AM", "NaN", "NaN"], ["677", "stop processing if operatorStats says we are done", "Henning Schmiedehausen", "hgschmie", "07/04/13, 04:57:18 AM", "NaN", "NaN"], ["678", "Parse Hive column values as needed instead of all up front", "Dain Sundstrom", "dain", "07/04/13, 06:46:42 AM", "NaN", "NaN"], ["680", "allow empty address lists in native split", "Henning Schmiedehausen", "hgschmie", "07/16/13, 05:49:44 PM", "NativeSplit only needs an address when reading data. When data is\nwritten, the node that will hold the data will be known only after the\ndata was written (and the split was consumed). This was masked till\nnow by using the list of addresses from the source split (the hdfs\nnodes that hold the source data) but the raid changes now may return\nempty lists.\n\nFix is to allow empty lists, set the list explicitly to empty when\ncreating a write split and check to make sure that the list of\naddresses returned in the native split manager is not empty.", "NaN"], ["681", "Exception fixes", "Henning Schmiedehausen", "hgschmie", "07/16/13, 05:47:32 PM", "NaN", "NaN"], ["682", "Adjust the partition fetch batch characteristics ", "Eric Hwang", "erichwang", "07/09/13, 05:50:22 PM", "NaN", "NaN"], ["683", "Only print wrap indicator for multiple rows", "David Phillips", "electrum", "07/09/13, 05:44:14 PM", "NaN", "NaN"], ["684", "Add function annotation for more verbose descriptions", null, "amleshjk", "08/09/13, 10:58:17 PM", "NaN", "NaN"], ["685", "Hive timestamp fixes", "David Phillips", "electrum", "07/10/13, 02:19:55 AM", "NaN", "NaN"], ["686", "Use SQL types for metadata", "David Phillips", "electrum", "07/11/13, 06:09:59 PM", "NaN", "NaN"], ["687", "More verbose message for 'function not found' exception", null, "amleshjk", "07/17/13, 06:54:11 PM", "NaN", "NaN"], ["688", "Add -f <filename> option to run query from file", null, "amleshjk", "07/15/13, 11:12:44 PM", "NaN", "NaN"], ["689", "Add vertical output", "David Phillips", "electrum", "07/16/13, 12:06:08 AM", "NaN", "NaN"], ["690", "Fix tests", "David Phillips", "electrum", "07/16/13, 01:29:20 AM", "NaN", "NaN"], ["691", "Make pager configurable", "David Phillips", "electrum", "07/16/13, 10:24:16 PM", "NaN", "NaN"], ["692", "Stop operators when query is done", "Dain Sundstrom", "dain", "07/18/13, 12:07:43 AM", "NaN", "NaN"], ["693", "Reduce requests", "Dain Sundstrom", "dain", "07/17/13, 11:46:20 PM", "NaN", "NaN"], ["694", "Add unsupported nodes to graphviz printer", null, "amleshjk", "07/18/13, 07:21:20 PM", "NaN", "NaN"], ["695", "Make distributed tests using TestingPrestoServer\t", "David Phillips", "electrum", "07/19/13, 12:19:57 AM", "NaN", "NaN"], ["696", "Predicate move around", "Eric Hwang", "erichwang", "08/08/13, 11:03:12 PM", "NaN", "NaN"], ["697", "FileSystemCache include the fsType as part of the cache key", "Eric Hwang", "erichwang", "07/20/13, 12:49:17 AM", "NaN", "NaN"], ["698", "Fix hangs in failing unit tests", "David Phillips", "electrum", "07/19/13, 01:37:45 AM", "NaN", "NaN"], ["699", "Update to ResponseHandler API change in Airlift", "Dain Sundstrom", "dain", "07/20/13, 12:49:17 AM", "NaN", "NaN"], ["700", "Add missing catalog configs for development", "David Phillips", "electrum", "07/21/13, 03:44:06 AM", "NaN", "NaN"], ["701", "Extract TestingDiscoveryServer to upper level", "David Phillips", "electrum", "07/21/13, 07:26:38 AM", "NaN", "NaN"], ["702", "Make QueryAbortedException an IOException", null, "amleshjk", "07/26/13, 06:45:46 PM", "NaN", "NaN"], ["703", "Fix infinite loop when LIKE pattern has utf-8 chars", "Martin Traverso", "martint", "07/26/13, 05:19:05 PM", "NaN", "NaN"], ["704", "Add missing dependency", "Martin Traverso", "martint", "07/26/13, 06:09:23 PM", "NaN", "NaN"], ["707", "Fixes for problems found in verifier", "Dain Sundstrom", "dain", "07/28/13, 11:17:50 PM", "NaN", "NaN"], ["708", "Fix eager evaluation of conditions", "David Phillips", "electrum", "08/01/13, 08:09:07 PM", "NaN", "NaN"], ["709", "Fix LIKE escaping", "Eric Hwang", "erichwang", "08/01/13, 07:37:09 PM", "Makes it compliant to ANSI", "NaN"], ["710", "Add failed method to shared buffer to free writers on query failure", "Dain Sundstrom", "dain", "08/01/13, 09:44:49 PM", "NaN", "NaN"], ["712", "Add unixtime conversion functions", "David Phillips", "electrum", "08/02/13, 06:04:59 PM", "NaN", "NaN"], ["713", "Datetime function cleanup", "David Phillips", "electrum", "08/02/13, 07:55:55 PM", "NaN", "NaN"], ["714", "Retry createRecordReader", "Eric Hwang", "erichwang", "08/02/13, 07:35:55 PM", "NaN", "NaN"], ["715", "Fix EXTRACT tests", "David Phillips", "electrum", "08/02/13, 08:58:32 PM", "NaN", "NaN"], ["716", "Add documentation", "David Phillips", "electrum", "08/06/13, 10:46:53 PM", "NaN", "NaN"], ["719", "Use SQL types in error messages", "David Phillips", "electrum", "08/07/13, 05:40:10 PM", "NaN", "NaN"], ["720", "Move webapp libraries to vendor directory", "David Phillips", "electrum", "08/07/13, 06:51:14 PM", "NaN", "NaN"], ["721", "A few improvements for predicate move around", "Eric Hwang", "erichwang", "08/09/13, 07:47:20 PM", "NaN", "NaN"], ["722", "Use escaping rather than enclosing quotes for TSV", "David Phillips", "electrum", "08/09/13, 08:06:30 PM", "NaN", "NaN"], ["723", "Support WHERE and ORDER BY for SHOW PARTITIONS", "David Phillips", "electrum", "08/12/13, 09:43:34 PM", "NaN", "NaN"], ["725", "Support LIMIT for SHOW PARTITIONS", "David Phillips", "electrum", "08/13/13, 08:33:47 PM", "NaN", "NaN"], ["726", "Add SemiJoins", "Eric Hwang", "erichwang", "08/29/13, 07:28:41 PM", "NaN", "NaN"], ["727", "Workaround for Code Cache eviction problems in Java 7 Hotspot", "Martin Traverso", "martint", "08/15/13, 01:24:10 AM", "NaN", "NaN"], ["728", "Handle StackOverflowError from parser", "David Phillips", "electrum", "08/17/13, 11:00:19 PM", "NaN", "NaN"], ["729", "Handle StackOverflowError during analysis", "David Phillips", "electrum", "08/20/13, 11:26:18 PM", "NaN", "NaN"], ["730", "Fix non-deterministic predicate pushdown for aggregates and table scans", "Eric Hwang", "erichwang", "08/19/13, 10:55:05 PM", "NaN", "NaN"], ["731", "Install the code cache gc trigger on run()", "Martin Traverso", "martint", "08/20/13, 04:35:33 AM", "NaN", "NaN"], ["732", "Use Supplier instead of Provider for ExchangeClient", "David Phillips", "electrum", "08/22/13, 08:14:09 PM", "NaN", "NaN"], ["733", "Use Supplier instead of Provider", "David Phillips", "electrum", "08/22/13, 11:26:08 PM", "NaN", "NaN"], ["734", "Add benchmark for sql join queries with predicates on the join clause", "Eric Hwang", "erichwang", "08/23/13, 07:38:59 PM", "NaN", "NaN"], ["735", "Upgrade to Airlift 0.79-SNAPSHOT", "David Phillips", "electrum", "08/23/13, 01:22:03 AM", "NaN", "NaN"], ["736", "Add Hive partition columns last to match Hive", "David Phillips", "electrum", "08/26/13, 04:17:29 AM", "NaN", "NaN"], ["738", "Update maven repository urls", "Martin Traverso", "martint", "08/25/13, 05:00:53 AM", "NaN", "NaN"], ["741", "New operator framework", "Dain Sundstrom", "dain", "09/12/13, 08:35:18 PM", "The commits are structured as follows\n- Add new operator and testing framework\n- Add new versions of all existing operators\n- Convert execution engine to use new operators\n- Add new scheduler that favors interactive queries\n- Add blocking support to operators\n- Rewrite the context and stats system \n- Convert compiler to new operators\n- Convert benchmarks to new operators\n- Convert storage manager to new operators\n- Convert a few remaining tests\n- Remove old operators\n\nAfter this main body of work is reviewed, I will add additional commits to repackage and rename classes to the old style, because doing this before the review will make fixes more difficult.", "NaN"], ["742", "Fix bad error message when response is empty in HttpRemoteTask", "Dain Sundstrom", "dain", "08/27/13, 09:43:22 PM", "NaN", "NaN"], ["744", "Make jackson dependencies provided", "David Phillips", "electrum", "08/29/13, 02:14:56 AM", "NaN", "NaN"], ["745", "Switch back to resolver until Aether is fixed", "David Phillips", "electrum", "08/29/13, 07:28:41 PM", "NaN", "NaN"], ["746", "Instrument thread pools", "Dain Sundstrom", "dain", "08/30/13, 12:38:59 AM", "NaN", "NaN"], ["747", "Use more efficient API for finding split locations", "David Phillips", "electrum", "08/30/13, 01:24:04 AM", "NaN", "NaN"], ["751", "Parser changes for table sample", "Nileema Shingte", "nileema", "09/09/13, 11:08:15 PM", "This request only has the changes made to the grammar for implementing tablesample. \n\nUsed the syntax for tablesample from here: http://www.neilconway.org/talks/hacking/ottawa/sql_standard.pdf\nParse tree for a sample query: https://gist.github.com/nileema/6458383", "NaN"], ["752", "Fix for all columns with alias", "Nileema Shingte", "nileema", "09/06/13, 07:02:45 PM", "For selecting all columns with alias (eg: SELECT A.\\* FROM orders a) we were building it as select true.\\* \nuse the node.toString instead which implements this correctly. ", "NaN"], ["754", "Add logging for TestExpressionCompiler", "David Phillips", "electrum", "09/09/13, 04:40:07 PM", "NaN", "NaN"], ["755", "Reduce memory usage on semi join test", "Eric Hwang", "erichwang", "09/09/13, 08:36:20 PM", "It was failing sporadically on the distributed test due to heap space", "NaN"], ["756", "Fix HiveClient call to getFileBlockLocations", "Eric Hwang", "erichwang", "09/09/13, 11:52:17 PM", "NaN", "NaN"], ["757", "Remove native store related runtime configuration", "David Phillips", "electrum", "09/10/13, 10:09:37 PM", "NaN", "NaN"], ["758", "Fix hash join handling of nulls", "Eric Hwang", "erichwang", "09/11/13, 11:01:15 PM", "NaN", "NaN"], ["762", "Add license headers", "David Phillips", "electrum", "09/14/13, 04:56:17 AM", "NaN", "NaN"], ["763", "Fix code formatting and optimize imports", "David Phillips", "electrum", "09/16/13, 09:47:33 PM", "NaN", "NaN"], ["764", "Various fixes", "Dain Sundstrom", "dain", "09/18/13, 12:24:30 AM", "NaN", "NaN"], ["765", "Updates", "David Phillips", "electrum", "09/18/13, 01:21:15 AM", "NaN", "NaN"], ["766", "CLI changes", "David Phillips", "electrum", "09/18/13, 03:01:38 AM", "NaN", "NaN"], ["767", "Rename presto-hive-plugin to presto-hive-cdh4", "David Phillips", "electrum", "09/18/13, 06:48:12 PM", "NaN", "NaN"], ["768", "Set JVM heap size for running unit tests", "David Phillips", "electrum", "09/18/13, 07:36:49 PM", "NaN", "NaN"], ["770", "Limit the number of queries remembered after they are finished", "Martin Traverso", "martint", "09/19/13, 06:04:23 AM", "Prune the set of remembered queries down to a configurable number, but only consider queries that haven't heartbeat within the client timeout period.", "NaN"], ["772", "Add README", "David Phillips", "electrum", "09/20/13, 01:00:15 AM", "NaN", "NaN"], ["773", "Fix bug in computeRate", "Martin Traverso", "martint", "09/20/13, 04:10:20 PM", "NaN", "NaN"], ["774", "Convert to most succinct unit", "Martin Traverso", "martint", "09/20/13, 04:09:53 PM", "NaN", "NaN"], ["775", "Expose number of \"live\" nodes", "Martin Traverso", "martint", "09/20/13, 10:07:47 PM", "NaN", "NaN"], ["776", "Lazily load RecordCursor in RecordProjectOpereator", "Eric Hwang", "erichwang", "09/21/13, 12:44:21 AM", "NaN", "NaN"], ["777", "Update to hadoop-cdh4 0.3 and hive-apache 0.3", "David Phillips", "electrum", "09/21/13, 12:27:05 AM", "NaN", "NaN"], ["778", "Allow having multiple Hive plugins", "David Phillips", "electrum", "09/22/13, 03:44:40 AM", "NaN", "NaN"], ["779", "Allow using Hive plugin without discovery", "David Phillips", "electrum", "09/23/13, 03:23:58 AM", "NaN", "NaN"], ["780", "Fix network issue", "Dain Sundstrom", "dain", "09/24/13, 06:25:42 AM", "Only the last commit actually fixes this issue.  The others are useful fixes made while trying to fix the core issue", "NaN"], ["783", "Taking care of boundary case conditions in the TABLESAMPLE operator", "Sameer Agarwal", "sameeragarwal", "09/24/13, 02:53:32 AM", "Changing the comparison expression between `rand()` and `sampleRatio` from `<=` to `<`. `rand()` returns a value between [0,1). So `rand() < 0` will return no values and `rand() < 1` will return all the values.", "NaN"], ["784", "Fix JMX stats for new operators", "Dain Sundstrom", "dain", "09/24/13, 05:59:46 PM", "NaN", "NaN"], ["785", "Add failure info to completion events", "Dain Sundstrom", "dain", "09/24/13, 08:04:19 PM", "NaN", "NaN"], ["786", "Don't truncate rate calculations in the UI", "Dain Sundstrom", "dain", "09/24/13, 10:29:12 PM", "NaN", "NaN"], ["787", "Add counters for calls to operator addInput, getOutput, and finish", "Dain Sundstrom", "dain", "09/24/13, 11:12:24 PM", "NaN", "NaN"], ["788", "Tablesample system implementation", "Nileema Shingte", "nileema", "09/25/13, 08:15:42 PM", "Recreating the pull request as it got closed earlier. ", "NaN"], ["789", "Allow running locally without discovery", "David Phillips", "electrum", "09/25/13, 05:49:18 AM", "NaN", "NaN"], ["790", "Make cpu time tracking in OperatorStats configurable", "Christopher Berner", "cberner", "09/26/13, 05:22:19 PM", "Added task.cpu-timer-enabled, which can be used to turn off cpu time tracking", "NaN"], ["792", "Random cleanups", "David Phillips", "electrum", "09/29/13, 03:23:59 AM", "NaN", "NaN"], ["793", "Add functions for NaN and Infinity", "Nileema Shingte", "nileema", "10/01/13, 04:57:56 PM", "Add math functions related to NaN and Infinity. \n- nan()\n- is_nan(value)\n- infinity()\n- is_infinity(value)\n- is_finite(value)\n\nTested it on various combinations:\nhttps://gist.github.com/nileema/6734480", "NaN"], ["794", "Close recordReader when advance reaches the end or throws", "Martin Traverso", "martint", "09/27/13, 11:47:15 PM", "NaN", "NaN"], ["795", "Scheduling fixes", "Dain Sundstrom", "dain", "09/28/13, 04:42:52 AM", "NaN", "NaN"], ["796", "Protect against bad Hadoop input formats", "Dain Sundstrom", "dain", "09/28/13, 07:14:50 PM", "NaN", "NaN"], ["797", "Update README", "David Phillips", "electrum", "10/01/13, 02:47:37 AM", "NaN", "NaN"], ["798", "Remove legacy config names", "David Phillips", "electrum", "09/30/13, 05:51:12 PM", "NaN", "NaN"], ["799", "Improve speed of TABLESAMPLE tests", "Dain Sundstrom", "dain", "09/30/13, 08:21:44 PM", "NaN", "NaN"], ["800", "Partitioned hash aggregations", "Dain Sundstrom", "dain", "12/04/13, 08:58:09 PM", "This implements a fixed size partitioning for hash aggregations set by the `query.initial-hash-partitions` configuration option (default 8)", "NaN"], ["801", "Fix license of ClassInfoLoader", "Dain Sundstrom", "dain", "10/01/13, 01:42:23 AM", "NaN", "NaN"], ["802", "Upgrade dependencies and fix SHOW SCHEMAS", "David Phillips", "electrum", "10/01/13, 04:52:07 PM", "NaN", "NaN"], ["804", "Hive plugin cleanup", "David Phillips", "electrum", "10/02/13, 07:33:46 PM", "NaN", "NaN"], ["806", "Improve query performance for hive bucketed tables", "David Phillips", "electrum", "10/04/13, 12:45:00 AM", "NaN", "NaN"], ["807", "Allow parsing CREATE TABLE AS SELECT", "David Phillips", "electrum", "10/04/13, 01:27:45 AM", "NaN", "NaN"], ["808", "Fix bucket hashing test", "David Phillips", "electrum", "10/04/13, 10:20:41 PM", "NaN", "NaN"], ["809", "Split up QueryManagerConfig", "David Phillips", "electrum", "10/11/13, 04:32:34 AM", "This renames the following property:\n\n  `query.shard.max-threads` -> `task.shard.max-threads`\n\nAdditionally, a new config property `task.info.max-age` is split out\nfrom the `query.info.max-age` property. The task property controls\nhow often the coordinator must heartbeat tasks on the workers. The\nquery property controls how often the end user (statement client)\nmust heartbeat the query on the coordinator.", "NaN"], ["810", "Fixes", "David Phillips", "electrum", "10/09/13, 04:30:57 PM", "NaN", "NaN"], ["811", "Parser changes for creating stratified samples with TABLESAMPLE", "Sameer Agarwal", "sameeragarwal", "11/09/13, 03:48:20 AM", "This diff adds an optional `STRATIFY ON` clause to `TABLESAMPLE` in the parser.\n\nExamples:\n\n`SELECT * FROM temp TABLESAMPLE BERNOULLI (50) STRATIFY ON (col_A)`\n`SELECT * FROM temp TABLESAMPLE SYSTEM (50) STRATIFY ON (col_A, col_B)`", "NaN"], ["812", "Fix overly broad exception handling", "David Phillips", "electrum", "10/11/13, 04:14:33 PM", "NaN", "NaN"], ["816", "Improve layout and readability of new theme", "David Phillips", "electrum", "10/18/13, 09:52:56 PM", "![screen shot 2013-10-17 at 7 07 19 pm](https://f.cloud.github.com/assets/9230/1357524/1eced93c-379a-11e3-8245-c99e3dbb1a0b.png)\n![screen shot 2013-10-17 at 7 07 28 pm](https://f.cloud.github.com/assets/9230/1357523/1ecd463a-379a-11e3-95e6-0f8522ff63c0.png)\n![screen shot 2013-10-17 at 7 07 49 pm](https://f.cloud.github.com/assets/9230/1357525/1edbf392-379a-11e3-9192-727db5de2829.png)", "NaN"], ["817", "Fix CLI history behavior on user interrupt", "David Phillips", "electrum", "10/20/13, 05:21:24 PM", "This is a temporary fix until a new jline is released.", "NaN"], ["818", "Update to Airlift with Jetty 9 client and server", "Dain Sundstrom", "dain", "02/15/14, 11:05:39 PM", "NaN", "NaN"], ["821", "Fix for hive bucketed tables", "Nileema Shingte", "nileema", "10/23/13, 07:47:20 PM", "Additional checks to ensure that we apply bucketing only when a table is bucketed. ", "NaN"], ["822", "Allow subclasses to use different Partition types", "David Phillips", "electrum", "10/23/13, 10:39:21 PM", "NaN", "NaN"], ["824", "Change name of CLI executable JAR", "David Phillips", "electrum", "10/28/13, 10:47:08 PM", "NaN", "NaN"], ["825", "Only announce coordinator if configured as one", "David Phillips", "electrum", "10/28/13, 10:52:22 PM", "This fixes a bug introduced in the CoordinatorModule refactoring\nthat causes every node to announce itself as a coordinator.", "NaN"], ["827", "Add NOTICE file for binary distribution", "Martin Traverso", "martint", "10/29/13, 12:26:05 AM", "NaN", "NaN"], ["828", "Upgrade to Airbase 13 and Airlift 0.84", "Martin Traverso", "martint", "10/29/13, 04:43:02 AM", "NaN", "NaN"], ["829", "Move installation instructions to docs", "David Phillips", "electrum", "10/29/13, 05:45:41 PM", "NaN", "NaN"], ["831", "Make interpreter stateless", "Martin Traverso", "martint", "10/29/13, 11:17:50 PM", "NaN", "NaN"], ["832", "Modify SPI to enable range predicate pushdown and negotiation with connectors", "Eric Hwang", "erichwang", "12/02/13, 06:44:40 PM", "As a simple extension of this new API, all of our connectors are now able to feed back domain information from Partitions during the optimization phase to enable greater plan optimizations.", "NaN"], ["833", "Minimum viable mobile styling", "James Pearce", "jamesgpearce", "11/01/13, 03:16:08 PM", "Makes the home page render sensibly on mobile devices assuming media query support\n\n![image](https://f.cloud.github.com/assets/90942/1442098/a2c934a0-41b6-11e3-85a2-8199c1117d06.png)", "NaN"], ["834", "Handle binary RCFiles correctly", "David Phillips", "electrum", "10/31/13, 04:28:19 AM", "This also changes the unit tests to work correctly when the timezone is set to that of the Hive installation.", "NaN"], ["835", "Fix tests when run with non-UTC timezone", "David Phillips", "electrum", "10/31/13, 06:16:46 PM", "NaN", "NaN"], ["836", "Rename JDBC classes to start with Presto", "David Phillips", "electrum", "11/01/13, 03:22:24 PM", "NaN", "NaN"], ["837", "Allow running embedded version of Discovery", "David Phillips", "electrum", "11/04/13, 10:50:09 PM", "Simplify deployment by allowing Presto to run an embedded version of the Discovery service. This embedded version only supports dynamic announcements. It does not have a static store.", "NaN"], ["840", "Fix download link in docs", "David Phillips", "electrum", "11/06/13, 07:06:01 AM", "NaN", "NaN"], ["846", "Example plugin", "Dain Sundstrom", "dain", "11/23/13, 01:01:05 AM", "This connector exposes schemas defined in the `metadata-uri` catalog property.  The `metadata-uri` can reference a local file or a remote http resource.  The table data files are simple CSV (no escaping supported) fetched from a location relative to the `metadata-uri`.  The sample `etc/catalog/example.properties` file has URIs for local file data and a sample remote dataset hosted on Amazon s3.", "NaN"], ["848", "Add FB links and like button", "Martin Traverso", "martint", "11/08/13, 08:51:05 PM", "NaN", "NaN"], ["851", "Upgrade to discovery-server 1.16", "David Phillips", "electrum", "11/14/13, 07:01:16 PM", "NaN", "NaN"], ["857", "Cli hangs when the server goes away", "Nileema Shingte", "nileema", "11/15/13, 10:22:18 PM", "Presto cli hangs for a long time when the server goes away. Add a check to see if the client is closed so that we can get out of this state by pressing ctrl-C", "NaN"], ["861", "Fix logging for full code cache condition", "David Phillips", "electrum", "11/18/13, 05:49:06 PM", "NaN", "NaN"], ["862", "Verify JVM requirements", "David Phillips", "electrum", "11/18/13, 06:43:57 PM", "NaN", "NaN"], ["863", "Show catalogs", "Nileema Shingte", "nileema", "11/21/13, 07:46:09 PM", "List all the loaded catalogs - creates an in memory system table for all the catalog names.", "NaN"], ["865", "Add Hive plugin for Apache Hadoop 1.x", "David Phillips", "electrum", "11/20/13, 06:43:58 PM", "NaN", "NaN"], ["866", "Restore binding for node resource", "David Phillips", "electrum", "11/20/13, 06:23:05 PM", "NaN", "NaN"], ["869", "Show correct error message when catalog name is incorrect", "Nileema Shingte", "nileema", "11/22/13, 02:23:35 AM", "NaN", "NaN"], ["870", "Fix test", "Nileema Shingte", "nileema", "11/22/13, 02:51:08 AM", "NaN", "NaN"], ["871", "Rename hive-apache1 to hive-hadoop1 and bundle with server", "David Phillips", "electrum", "11/23/13, 12:08:45 AM", "NaN", "NaN"], ["872", "Add release notes for 0.54", "David Phillips", "electrum", "11/23/13, 04:37:14 AM", "NaN", "NaN"], ["873", "Fix tests for example-http plugin", "David Phillips", "electrum", "11/23/13, 06:20:48 AM", "NaN", "NaN"], ["874", "Remove duplicate dependency", "David Phillips", "electrum", "11/25/13, 06:34:00 PM", "NaN", "NaN"], ["875", "Shade dependencies for JDBC standalone jar", "David Phillips", "electrum", "11/25/13, 08:20:07 PM", "We need some way to test that the standalone jar is complete, but this is tricky because the Presto test server will include the dependencies that might be missing from the jar. Maybe have the tests load it via a separate classloader that skips directly to the system classloader for anything that it doesn't have in itself.", "NaN"], ["877", "Fix the location for presto-example-http/pom.xml", "Nileema Shingte", "nileema", "11/27/13, 03:35:58 AM", "NaN", "NaN"], ["878", "Cross joins", "Nileema Shingte", "nileema", "12/23/13, 09:01:13 PM", "NaN", "NaN"], ["879", "Rename TableWriter to MaterializedViewWriter\t", "David Phillips", "electrum", "11/27/13, 05:48:18 PM", "NaN", "NaN"], ["883", "Add --source option to CLI", "Christopher Berner", "cberner", "12/03/13, 06:38:51 PM", "This lets you set the source of the query for logging purposes. Tested\nby running the CLI with --help to make sure the option showed up, and\nadded a unit test", "NaN"], ["884", "Use UUID for native shard IDs", "David Phillips", "electrum", "12/03/13, 09:11:55 PM", "NaN", "NaN"], ["885", "Implement SHOW SCHEMAS FROM syntax", "Christopher Berner", "cberner", "12/05/13, 10:46:58 PM", "Allows you to show schemas from a specific catalog. Tested by running\nSHOW SCHEMAS; and SHOW SCHEMAS FROM multifeed;", "NaN"], ["888", "Execution performance improvements", "Dain Sundstrom", "dain", "01/03/14, 04:04:46 AM", "Change blocks and TupleInfo to only contain a single field value\nMove looping into aggregation implementations\nUse ClassLoader magic to share base classes of aggregations without creating megamorphic call sites\nChange aggregation to support multi-channel grouping\nChange orderBy, topN, and window to support multi-channel sorting\nChange join to support multi-channel keys\nPipeline schedule splits in batches instead of one at a time ", "NaN"], ["889", "Add custom cursor for rc binary format", "Christopher Berner", "cberner", "12/06/13, 02:14:10 AM", "Ran the unit tests, and also ran a simple query against a table in the hivedev cluster", "NaN"], ["890", "Add a shade pattern for com.google.inject in presto jdbc driver jar", "Nileema Shingte", "nileema", "12/05/13, 02:42:29 AM", "NaN", "NaN"], ["891", "Add empty strings to Hive integration test", "Christopher Berner", "cberner", "12/05/13, 11:29:23 PM", "NaN", "NaN"], ["895", "Add catalog and schema to jdbc string", "Nileema Shingte", "nileema", "01/11/14, 03:58:39 AM", "NaN", "NaN"], ["898", "Refactor HiveRecordSet to make it easier to extend with new file formats", "Christopher Berner", "cberner", "12/17/13, 12:07:07 AM", "NaN", "NaN"], ["904", "Change DATE, TIME, TIMESTAMP and INTERVAL to be non-reserved words", "Dain Sundstrom", "dain", "12/10/13, 06:25:46 PM", "NaN", "NaN"], ["905", "Add unit tests for rc binary and rc text", "Christopher Berner", "cberner", "12/11/13, 02:01:49 AM", "NaN", "NaN"], ["906", "Implement distinct limit operator", "Nileema Shingte", "nileema", "01/31/14, 10:49:27 PM", "NaN", "NaN"], ["907", "Implement some functions in Presto JDBC driver", "Nileema Shingte", "nileema", "01/07/14, 08:02:54 PM", "Got basic queries running in Squirrel SQL Client", "NaN"], ["909", "Implement use catalog and use schema", "Nileema Shingte", "nileema", "01/31/14, 10:40:54 PM", "NaN", "NaN"], ["911", "Add a summarized print of TableScanNode TupleDomain to cut down on logging size", "Eric Hwang", "erichwang", "12/18/13, 08:00:39 PM", "Formatting:\n\nWhere there are a small number of range segments:\n\npresto> explain SELECT ds, id, COUNT(1) AS count FROM table WHERE ds >= '2013-11-15' AND (type='A' OR type='B') GROUP BY ds, id;\n\n---\n- Output[ds, id, count]\n       id := id\n       count := count\n  - Aggregate[ds, id] => [ds:varchar, id:varchar, count:bigint]\n         count := count(\"expr\")\n    - Project => [ds:varchar, id:varchar, expr:bigint]\n           expr := 1\n      - TableScan[default:default:table, original constrant=((\"ds\" >= '2013-11-15') AND ((\"type\" = 'A') OR (\"type\" = 'B'))), domain={ds => Domain{ranges=[[2013-11-15], [2013-11-16], [2013-11-17], [2013-11-18], [2013-11-19], [2013-11-20]], nullAllowed=false}, type => Domain{ranges=[[A], [B]], nullAllowed=false}}] => [id:varchar, ds:varchar]\n             id := HiveColumnHandle{clientId=default, name=id, ordinalPosition=1, hiveType=STRING, hiveColumnIndex=6, partitionKey=false}\n             ds := HiveColumnHandle{clientId=default, name=ds, ordinalPosition=2, hiveType=STRING, hiveColumnIndex=-1, partitionKey=true}\n             type := HiveColumnHandle{clientId=default, name=type, ordinalPosition=3, hiveType=STRING, hiveColumnIndex=-1, partitionKey=true}\n\n(1 row)\n\nWhen there are a large number of range segments:\n\npresto> explain SELECT ds, id, COUNT(1) AS count FROM table WHERE ds >= '2012-11-15' AND (type='A' OR type='B') GROUP BY ds, id;\n##                                                                                                                                                                                                                                                               Query Plan\n- Output[ds, id, count]\n       id := id\n       count := count\n  - Aggregate[ds, id] => [ds:varchar, id:varchar, count:bigint]\n         count := count(\"expr\")\n    - Project => [ds:varchar, id:varchar, expr:bigint]\n           expr := 1\n      - TableScan[default:default:table, original constrant=((\"ds\" >= '2012-11-15') AND ((\"type\" = 'A') OR (\"type\" = 'B'))), domain={ds => Summarized:Domain{ranges=[[2013-08-04, 2013-12-15]], nullAllowed=false}, type => Domain{ranges=[[A], [B]], nullAllowed=false}}] => [id:varchar, ds:varchar]\n             id := HiveColumnHandle{clientId=default, name=id, ordinalPosition=1, hiveType=STRING, hiveColumnIndex=1, partitionKey=false}\n             ds := HiveColumnHandle{clientId=default, name=ds, ordinalPosition=2, hiveType=STRING, hiveColumnIndex=-1, partitionKey=true}\n             type := HiveColumnHandle{clientId=default, name=type, ordinalPosition=3, hiveType=STRING, hiveColumnIndex=-1, partitionKey=true}\n\n(1 row)", "NaN"], ["912", "Add support for DISTINCT in aggregations for simple cases", "Christopher Berner", "cberner", "12/17/13, 09:36:42 PM", "Supports COUNT(DISTINCT) and DISTINCT in other aggregations, as long as\nthere is only one DISTINCT expression, and all the aggregations use this\nexpression. Also, using it with GROUP BY is not supported yet.", "NaN"], ["913", "Documentation for SHOW CATALOGS", "Nileema Shingte", "nileema", "12/18/13, 09:36:08 PM", "NaN", "NaN"], ["915", "Set no more buffers for hash distributed node", "Dain Sundstrom", "dain", "12/19/13, 02:46:29 AM", "NaN", "NaN"], ["916", "Add CREATE TABLE AS SELECT for native tables", "David Phillips", "electrum", "12/22/13, 09:16:47 PM", "NaN", "NaN"], ["917", "Fix querying tables with missing bucket columns", "David Phillips", "electrum", "12/19/13, 03:53:54 AM", "NaN", "NaN"], ["919", "Fix handling of Hive bucketed tables", "David Phillips", "electrum", "12/20/13, 03:17:31 AM", "This fixes #918.", "NaN"], ["920", "Fix hash distributed aggregations", "Dain Sundstrom", "dain", "12/21/13, 01:06:20 AM", "Split distribution strategy and output buffer partition strategy", "NaN"], ["924", "0.55 release notes", "Dain Sundstrom", "dain", "12/21/13, 12:21:03 AM", "NaN", "NaN"], ["927", "Add table creation for Hive", "David Phillips", "electrum", "12/27/13, 06:47:16 PM", "NaN", "NaN"], ["929", "Fix table creation", "David Phillips", "electrum", "12/29/13, 04:53:38 AM", "NaN", "NaN"], ["930", "Don't swallow exception when dropping table", "David Phillips", "electrum", "12/30/13, 04:00:27 AM", "NaN", "NaN"], ["932", "Remove support for implicit cross joins", "David Phillips", "electrum", "12/30/13, 08:42:24 PM", "The pre-ANSI join syntax makes it very easy to accidentally perform a\ncross join when an inner join was intended, not to mention resulting in\nharder to read queries, so we need a way to disable it via configuration.", "NaN"], ["933", "Add 0.56 release notes", "David Phillips", "electrum", "12/31/13, 01:54:25 AM", "NaN", "NaN"], ["935", "Guarantee that semaphore is always released", "David Phillips", "electrum", "12/31/13, 07:57:27 AM", "NaN", "NaN"], ["938", "Add Presto logo files", "Martin Traverso", "martint", "01/02/14, 07:27:56 PM", "NaN", "NaN"], ["940", "Simplify handling multiple Hadoop versions", "David Phillips", "electrum", "01/03/14, 09:29:50 PM", "NaN", "NaN"], ["941", "Fix optimizer performance regression", "Eric Hwang", "erichwang", "01/03/14, 08:26:01 PM", "This was a subtle one.", "NaN"], ["942", "Specify canonical page URL", "David Phillips", "electrum", "01/03/14, 06:11:40 AM", "NaN", "NaN"], ["943", "Add Hive plugin for Apache Hadoop 2.x", "David Phillips", "electrum", "01/05/14, 04:00:55 AM", "NaN", "NaN"], ["944", "TPCH plugin", "Dain Sundstrom", "dain", "02/05/14, 12:06:41 AM", "Add a plugin based on the airlift TPCH data generator.  This also converts test and benchmarks to use generated data instead of static files.\n\nFor benchmarks this code replaces TpchBlocksProvider with CREATE TABLE commands using LocalQueryRunner.  The means that benchmarks now use the native plugin which adds about 1-3ms to the benchmarks times.  For super fast benchmarks (e.g., count_agg and raw_stream), this appears as a huge performance drop.\n\nBefore:\n\n```\n                          count_agg ::    0.697 cpu ms :: in  1.5M,  12.9MB,   2.15B/s,    18GB/s :: out     1,      9B,   1.43K/s,  12.6KB/s\n                     double_sum_agg ::   10.741 cpu ms :: in  1.5M,  12.9MB,    140M/s,  1.17GB/s :: out     1,      9B,      93/s,    837B/s\n                           hash_agg ::  137.724 cpu ms :: in  1.5M,  21.5MB,   10.9M/s,   156MB/s :: out     3,     45B,      21/s,    326B/s\n                   predicate_filter ::   55.698 cpu ms :: in  1.5M,  12.9MB,   26.9M/s,   231MB/s :: out 1.29M,  11.1MB,   23.1M/s,   199MB/s\n                         raw_stream ::    0.810 cpu ms :: in  1.5M,  12.9MB,   1.85B/s,  15.5GB/s :: out  1.5M,  12.9MB,   1.85B/s,  15.5GB/s\n                             top100 ::   41.238 cpu ms :: in  1.5M,  12.9MB,   36.4M/s,   312MB/s :: out   100,    900B,   2.42K/s,  21.3KB/s\n             in_memory_orderby_1.5M :: 1521.210 cpu ms :: in  1.5M,  41.5MB,    986K/s,  27.3MB/s :: out  1.5M,  28.6MB,    986K/s,  18.8MB/s\n                         hash_build ::  529.348 cpu ms :: in  1.5M,  25.7MB,   2.83M/s,  48.6MB/s :: out  1.5M,  25.7MB,   2.83M/s,  48.6MB/s\n                          hash_join :: 1643.575 cpu ms :: in    6M,   103MB,   3.65M/s,  62.7MB/s :: out    6M,   206MB,   3.65M/s,   125MB/s\n                hash_build_and_join :: 2215.610 cpu ms :: in  7.5M,   129MB,   3.39M/s,  58.1MB/s :: out    6M,   206MB,   2.71M/s,    93MB/s\n                  hand_tpch_query_1 :: 2450.150 cpu ms :: in    6M,   361MB,   2.45M/s,   147MB/s :: out     4,    300B,       1/s,    122B/s\n                  hand_tpch_query_6 ::  289.359 cpu ms :: in    6M,   240MB,   20.7M/s,   831MB/s :: out     1,      9B,       3/s,     31B/s\n    sql_groupby_agg_with_arithmetic ::  891.984 cpu ms :: in    6M,   137MB,   6.73M/s,   154MB/s :: out     2,     30B,       2/s,     33B/s\n                      sql_count_agg ::    1.578 cpu ms :: in  1.5M,  12.9MB,    950M/s,  7.97GB/s :: out     1,      9B,     633/s,  5.57KB/s\n                 sql_double_sum_agg ::   12.196 cpu ms :: in  1.5M,  12.9MB,    123M/s,  1.03GB/s :: out     1,      9B,      81/s,    737B/s\n              sql_count_with_filter ::   61.586 cpu ms :: in  1.5M,  8.58MB,   24.4M/s,   139MB/s :: out     1,      9B,      16/s,    146B/s\n                    sql_groupby_agg ::  139.857 cpu ms :: in  1.5M,  21.5MB,   10.7M/s,   153MB/s :: out     3,     45B,      21/s,    321B/s\n               sql_predicate_filter ::   37.860 cpu ms :: in  1.5M,  12.9MB,   39.6M/s,   340MB/s :: out 1.29M,  11.1MB,     34M/s,   292MB/s\n                     sql_raw_stream ::    1.508 cpu ms :: in  1.5M,  12.9MB,    995M/s,  8.34GB/s :: out  1.5M,  12.9MB,    995M/s,  8.34GB/s\n                        sql_top_100 ::   42.490 cpu ms :: in  1.5M,  12.9MB,   35.3M/s,   303MB/s :: out   100,    900B,   2.35K/s,  20.7KB/s\n                      sql_hash_join :: 3281.675 cpu ms :: in  7.5M,   129MB,   2.29M/s,  39.2MB/s :: out    6M,   206MB,   1.83M/s,  62.8MB/s\n            sql_join_with_predicate ::  787.630 cpu ms :: in  7.5M,   116MB,   9.52M/s,   147MB/s :: out     1,      9B,       1/s,     11B/s\n                  sql_varbinary_max ::  182.818 cpu ms :: in    6M,  97.3MB,   32.8M/s,   532MB/s :: out     1,     21B,       5/s,    114B/s\n                 sql_distinct_multi ::  199.495 cpu ms :: in  1.5M,    32MB,   7.52M/s,   161MB/s :: out     5,    112B,      25/s,    561B/s\n                sql_distinct_single ::   93.074 cpu ms :: in  1.5M,  12.9MB,   16.1M/s,   138MB/s :: out     1,      9B,      10/s,     96B/s\n                   sql_tpch_query_1 :: 2503.541 cpu ms :: in    6M,   361MB,    2.4M/s,   144MB/s :: out     4,    336B,       1/s,    134B/s\n                   sql_tpch_query_6 ::  268.098 cpu ms :: in    6M,   240MB,   22.4M/s,   897MB/s :: out     1,      9B,       3/s,     33B/s\n                           sql_like :: 2812.936 cpu ms :: in    6M,   232MB,   2.13M/s,  82.4MB/s :: out 1.15M,  9.84MB,    408K/s,   3.5MB/s\n                             sql_in ::   67.976 cpu ms :: in    6M,  51.5MB,   88.3M/s,   758MB/s :: out    25,    225B,     367/s,  3.23KB/s\n                    sql_semijoin_in :: 1660.280 cpu ms :: in  7.5M,  64.4MB,   4.52M/s,  38.8MB/s :: out    3M,  25.8MB,   1.81M/s,  15.5MB/s\n                    sql_regexp_like :: 1853.437 cpu ms :: in  1.5M,  76.6MB,    809K/s,  41.3MB/s :: out     1,      9B,       0/s,      4B/s\n         sql_approx_percentile_long ::  559.526 cpu ms :: in  1.5M,  12.9MB,   2.68M/s,    23MB/s :: out     1,      9B,       1/s,     16B/s\n                 stat_long_variance ::   21.974 cpu ms :: in  1.5M,  12.9MB,   68.3M/s,   586MB/s :: out     1,      9B,      45/s,    409B/s\n             stat_long_variance_pop ::   21.928 cpu ms :: in  1.5M,  12.9MB,   68.4M/s,   587MB/s :: out     1,      9B,      45/s,    410B/s\n               stat_double_variance ::   19.587 cpu ms :: in  1.5M,  12.9MB,   76.6M/s,   657MB/s :: out     1,      9B,      51/s,    459B/s\n           stat_double_variance_pop ::   19.382 cpu ms :: in  1.5M,  12.9MB,   77.4M/s,   664MB/s :: out     1,      9B,      51/s,    464B/s\n                   stat_long_stddev ::   22.252 cpu ms :: in  1.5M,  12.9MB,   67.4M/s,   579MB/s :: out     1,      9B,      44/s,    404B/s\n               stat_long_stddev_pop ::   22.540 cpu ms :: in  1.5M,  12.9MB,   66.5M/s,   571MB/s :: out     1,      9B,      44/s,    399B/s\n                 stat_double_stddev ::   19.305 cpu ms :: in  1.5M,  12.9MB,   77.7M/s,   667MB/s :: out     1,      9B,      51/s,    466B/s\n             stat_double_stddev_pop ::   19.459 cpu ms :: in  1.5M,  12.9MB,   77.1M/s,   662MB/s :: out     1,      9B,      51/s,    462B/s\n     sql_approx_count_distinct_long ::  128.087 cpu ms :: in  1.5M,  12.9MB,   11.7M/s,   101MB/s :: out     1,      9B,       7/s,     70B/s\n   sql_approx_count_distinct_double ::  134.577 cpu ms :: in  1.5M,  12.9MB,   11.1M/s,  95.7MB/s :: out     1,      9B,       7/s,     66B/s\nsql_approx_count_distinct_varbinary ::  227.544 cpu ms :: in  1.5M,  21.5MB,   6.59M/s,  94.3MB/s :: out     1,      9B,       4/s,     39B/s\n```\n\nAfter:\n\n```\n                          count_agg ::    1.781 cpu ms :: in  1.5M,  12.9MB,    842M/s,  7.06GB/s :: out     1,      9B,     561/s,  4.93KB/s\n                     double_sum_agg ::   12.181 cpu ms :: in  1.5M,  12.9MB,    123M/s,  1.03GB/s :: out     1,      9B,      82/s,    738B/s\n                           hash_agg ::  141.757 cpu ms :: in  1.5M,  21.5MB,   10.6M/s,   151MB/s :: out     3,     45B,      21/s,    317B/s\n                   predicate_filter ::   58.120 cpu ms :: in  1.5M,  12.9MB,   25.8M/s,   222MB/s :: out 1.29M,  11.1MB,   22.2M/s,   190MB/s\n                         raw_stream ::    1.696 cpu ms :: in  1.5M,  12.9MB,    885M/s,  7.41GB/s :: out  1.5M,  12.9MB,    885M/s,  7.41GB/s\n                             top100 ::   45.253 cpu ms :: in  1.5M,  12.9MB,   33.1M/s,   285MB/s :: out   100,    900B,   2.21K/s,  19.4KB/s\n             in_memory_orderby_1.5M :: 1531.159 cpu ms :: in  1.5M,  41.5MB,    980K/s,  27.1MB/s :: out  1.5M,  28.6MB,    980K/s,  18.7MB/s\n                         hash_build ::  541.420 cpu ms :: in  1.5M,  25.7MB,   2.77M/s,  47.6MB/s :: out  1.5M,  25.7MB,   2.77M/s,  47.6MB/s\n                          hash_join :: 1673.213 cpu ms :: in    6M,   103MB,   3.59M/s,  61.6MB/s :: out    6M,   206MB,   3.59M/s,   123MB/s\n                hash_build_and_join :: 2204.910 cpu ms :: in  7.5M,   129MB,    3.4M/s,  58.4MB/s :: out    6M,   206MB,   2.72M/s,  93.4MB/s\n                  hand_tpch_query_1 :: 2464.116 cpu ms :: in    6M,   361MB,   2.44M/s,   146MB/s :: out     4,    300B,       1/s,    121B/s\n                  hand_tpch_query_6 ::  293.306 cpu ms :: in    6M,   240MB,   20.5M/s,   820MB/s :: out     1,      9B,       3/s,     30B/s\n    sql_groupby_agg_with_arithmetic ::  904.943 cpu ms :: in    6M,   137MB,   6.63M/s,   152MB/s :: out     2,     30B,       2/s,     33B/s\n                      sql_count_agg ::    3.073 cpu ms :: in  1.5M,  12.9MB,    488M/s,  4.09GB/s :: out     1,      9B,     325/s,  2.86KB/s\n                 sql_double_sum_agg ::   14.070 cpu ms :: in  1.5M,  12.9MB,    107M/s,   915MB/s :: out     1,      9B,      71/s,    639B/s\n              sql_count_with_filter ::   64.687 cpu ms :: in  1.5M,  8.58MB,   23.2M/s,   133MB/s :: out     1,      9B,      15/s,    139B/s\n                    sql_groupby_agg ::  142.734 cpu ms :: in  1.5M,  21.5MB,   10.5M/s,   150MB/s :: out     3,     45B,      21/s,    315B/s\n               sql_predicate_filter ::   39.810 cpu ms :: in  1.5M,  12.9MB,   37.7M/s,   323MB/s :: out 1.29M,  11.1MB,   32.4M/s,   278MB/s\n                     sql_raw_stream ::    3.085 cpu ms :: in  1.5M,  12.9MB,    486M/s,  4.08GB/s :: out  1.5M,  12.9MB,    486M/s,  4.08GB/s\n                        sql_top_100 ::   44.824 cpu ms :: in  1.5M,  12.9MB,   33.5M/s,   287MB/s :: out   100,    900B,   2.23K/s,  19.6KB/s\n                      sql_hash_join :: 3308.330 cpu ms :: in  7.5M,   129MB,   2.27M/s,  38.9MB/s :: out    6M,   206MB,   1.81M/s,  62.3MB/s\n            sql_join_with_predicate ::  794.320 cpu ms :: in  7.5M,   116MB,   9.44M/s,   146MB/s :: out     1,      9B,       1/s,     11B/s\n                  sql_varbinary_max ::  184.251 cpu ms :: in    6M,  97.3MB,   32.6M/s,   528MB/s :: out     1,     21B,       5/s,    113B/s\n                 sql_distinct_multi ::  213.832 cpu ms :: in  1.5M,    32MB,   7.01M/s,   150MB/s :: out     5,    112B,      23/s,    523B/s\n                sql_distinct_single ::   94.300 cpu ms :: in  1.5M,  12.9MB,   15.9M/s,   137MB/s :: out     1,      9B,      10/s,     95B/s\n                   sql_tpch_query_1 :: 2532.160 cpu ms :: in    6M,   361MB,   2.37M/s,   142MB/s :: out     4,    336B,       1/s,    132B/s\n                   sql_tpch_query_6 ::  248.466 cpu ms :: in    6M,   240MB,   24.2M/s,   967MB/s :: out     1,      9B,       4/s,     36B/s\n                           sql_like :: 2732.147 cpu ms :: in    6M,   232MB,    2.2M/s,  84.8MB/s :: out 1.15M,  9.84MB,    420K/s,   3.6MB/s\n                             sql_in ::   66.849 cpu ms :: in    6M,  51.5MB,   89.8M/s,   771MB/s :: out    25,    225B,     373/s,  3.29KB/s\n                    sql_semijoin_in :: 1675.488 cpu ms :: in  7.5M,  64.4MB,   4.48M/s,  38.4MB/s :: out    3M,  25.8MB,   1.79M/s,  15.4MB/s\n                    sql_regexp_like :: 1891.597 cpu ms :: in  1.5M,  76.6MB,    793K/s,  40.5MB/s :: out     1,      9B,       0/s,      4B/s\n         sql_approx_percentile_long ::  560.500 cpu ms :: in  1.5M,  12.9MB,   2.68M/s,    23MB/s :: out     1,      9B,       1/s,     16B/s\n                 stat_long_variance ::   24.178 cpu ms :: in  1.5M,  12.9MB,     62M/s,   532MB/s :: out     1,      9B,      41/s,    372B/s\n             stat_long_variance_pop ::   23.890 cpu ms :: in  1.5M,  12.9MB,   62.8M/s,   539MB/s :: out     1,      9B,      41/s,    376B/s\n               stat_double_variance ::   21.324 cpu ms :: in  1.5M,  12.9MB,   70.3M/s,   604MB/s :: out     1,      9B,      46/s,    422B/s\n           stat_double_variance_pop ::   21.284 cpu ms :: in  1.5M,  12.9MB,   70.5M/s,   605MB/s :: out     1,      9B,      46/s,    422B/s\n                   stat_long_stddev ::   23.768 cpu ms :: in  1.5M,  12.9MB,   63.1M/s,   542MB/s :: out     1,      9B,      42/s,    378B/s\n               stat_long_stddev_pop ::   23.717 cpu ms :: in  1.5M,  12.9MB,   63.2M/s,   543MB/s :: out     1,      9B,      42/s,    379B/s\n                 stat_double_stddev ::   21.165 cpu ms :: in  1.5M,  12.9MB,   70.9M/s,   608MB/s :: out     1,      9B,      47/s,    425B/s\n             stat_double_stddev_pop ::   21.314 cpu ms :: in  1.5M,  12.9MB,   70.4M/s,   604MB/s :: out     1,      9B,      46/s,    422B/s\n     sql_approx_count_distinct_long ::  132.816 cpu ms :: in  1.5M,  12.9MB,   11.3M/s,  96.9MB/s :: out     1,      9B,       7/s,     67B/s\n   sql_approx_count_distinct_double ::  136.426 cpu ms :: in  1.5M,  12.9MB,     11M/s,  94.4MB/s :: out     1,      9B,       7/s,     65B/s\nsql_approx_count_distinct_varbinary ::  218.193 cpu ms :: in  1.5M,  21.5MB,   6.87M/s,  98.3MB/s :: out     1,      9B,       4/s,     41B/s\n```", "NaN"], ["945", "Propagate config from split to RecordReader", "David Phillips", "electrum", "01/07/14, 08:24:13 PM", "NaN", "NaN"], ["947", "Add support for reading Hive data from S3", "David Phillips", "electrum", "01/08/14, 06:39:34 PM", "NaN", "NaN"], ["950", "Fix NullPointerException when reading from S3", "Zhenxiao Luo", "zhenxiao", "01/09/14, 06:43:46 PM", "When presto reading from S3, get the following NullPointerException:\n\npresto:benchmark> select dateint, hour, hostname, count(*) as cnt from streaming_client_log where dateint=20130701 GROUP BY dateint, hour, hostname order by cnt;\n\nQuery 20140109_060239_00014_ddzak, FAILED, 8 nodes\nhttp://ec2-54-205-226-69.compute-1.amazonaws.com:8080/v1/query/20140109_060239_00014_ddzak?pretty\nSplits: 10 total, 8 done (80.00%)\nCPU Time: 0.0s total,     0 rows/s,     0B/s, 50% active\nPer Node: 0.0 parallelism,     0 rows/s,     0B/s\nParallelism: 0.0\n0:01 [0 rows, 0B] [0 rows/s, 0B/s]\n\nQuery 20140109_060239_00014_ddzak failed: null\njava.lang.NullPointerException\n    at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:50)\n    at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:41)\n    at org.apache.hadoop.io.SequenceFile$Reader.getPosition(SequenceFile.java:2257)\n    at org.apache.hadoop.mapred.SequenceFileRecordReader.getProgress(SequenceFileRecordReader.java:114)\n    at com.facebook.presto.hive.GenericHiveRecordCursor.getCompletedBytes(GenericHiveRecordCursor.java:194)\n    at com.facebook.presto.operator.RecordProjectOperator.getOutput(RecordProjectOperator.java:157)\n    at com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:201)\n    at com.facebook.presto.operator.Driver.process(Driver.java:214)\n    at com.facebook.presto.operator.Driver.processFor(Driver.java:243)\n    at com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:636)\n    at com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:436)\n    at com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:570)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n\nThis root reason is, in RecordProjectOperator.getOutput(), advanceNextPosition() will close the cursor when reaching the end of file, the following cursor.getCompletedBytes() will get NullPointerException, since cursor is already closed.\n\nThis patch fixes the bug.", "NaN"], ["951", "Add support for connectors with sampled data", "Christopher Berner", "cberner", "02/07/14, 08:13:15 PM", "NaN", "NaN"], ["952", "Fix use of closed RecordReader", "David Phillips", "electrum", "01/09/14, 09:16:34 PM", "NaN", "NaN"], ["953", "Cassandra Plugin", "Dain Sundstrom", "dain", "01/18/14, 02:14:06 AM", "This pull request contain an updated version of Martin Weindel's Cassandra plugin for Presto.  The changes include the following:\n\nUpdated the code to the Presto coding style. (https://github.com/airlift/codestyle)\n\nSplit CassandraClient into separate services.  This is how the other connectors are designed, and Hive is only this way for legacy reasons.\n\nRemove the ClassLoader wrappers because Cassandra client does not seem to load class dynamically (like Hive does).  We can add this back if we find out the client is loading classes.\n\nAdded some initial tests that start an embedded Cassandra server.  We need much more extensive tests, but I think this is a good starting point.", "NaN"], ["956", "Refactor LocalExecutionPlanner", "Christopher Berner", "cberner", "01/10/14, 11:09:23 PM", "Replace Multimap with Map, now that all our channels contain only a\nsingle symbol", "NaN"], ["957", "Add unit test for DISTINCT aggregation on a JOIN", "Christopher Berner", "cberner", "01/10/14, 11:10:27 PM", "NaN", "NaN"], ["960", "Add Checkstyle configuration", "David Phillips", "electrum", "01/17/14, 02:11:53 AM", "This tries to catch as much as possible using the limited checks available in Checkstyle. If this turns out to be useful we can move it to airbase.", "NaN"], ["965", "Fix warnings in unit tests, and reformat some files", "Christopher Berner", "cberner", "01/17/14, 04:36:28 AM", "NaN", "NaN"], ["966", "Add 0.57 release notes", "David Phillips", "electrum", "01/15/14, 03:33:29 AM", "NaN", "NaN"], ["971", "EXPLAIN (format json, type source)", "Rishi Kesh Dwivedi", "rishidwivedi", "02/11/14, 11:03:23 PM", "NaN", "NaN"], ["972", "Enable Checkstyle for tests", "David Phillips", "electrum", "01/18/14, 07:21:16 AM", "NaN", "NaN"], ["973", "Fix thread safety for FunctionRegistry", "David Phillips", "electrum", "01/22/14, 09:29:40 PM", "NaN", "NaN"], ["977", "Fix handling of non-splittable files without blocks", "David Phillips", "electrum", "01/23/14, 03:27:29 AM", "NaN", "NaN"], ["979", "Fix hive connector semaphore release bug", "Eric Hwang", "erichwang", "01/23/14, 08:15:44 PM", "NaN", "NaN"], ["980", "Add 0.58 release notes", "David Phillips", "electrum", "01/24/14, 03:13:39 AM", "NaN", "NaN"], ["983", "Add new S3 FileSystem using AWS SDK", "David Phillips", "electrum", "02/13/14, 05:52:46 AM", "This is a new proof-of-concept S3 FileSystem implementation that uses the AWS SDK. It passes the basic unit tests but has not been tested otherwise.", "NaN"], ["985", "Fix hang in hive split source", "Dain Sundstrom", "dain", "01/29/14, 02:18:57 AM", "Correctly decrement the outstandingSplitCount in getNextBatch", "NaN"], ["987", "SerDeUtils", "Rishi Kesh Dwivedi", "rishidwivedi", "02/10/14, 09:21:55 PM", "Rewrite of hive SerDeUtils.getJSONString() using jackson. ", "NaN"], ["988", "Fix memory (estimate) leak in TopNOperator", "Christopher Berner", "cberner", "01/30/14, 08:14:52 PM", "NaN", "NaN"], ["998", "Add ordering and pattern matching to DatabaseMetaData", "Dain Sundstrom", "dain", "02/04/14, 06:50:49 PM", "NaN", "NaN"], ["1000", "Improve encapsulation of native connector interfaces", "Martin Traverso", "martint", "02/05/14, 12:35:46 AM", "- Inject NativeMetadata into NativeConnectorFactory, just like DataStreamProvider et al.\n- Make NativeSplitManager depend on the connector-specific NativeMetadata instead of global Metadata", "NaN"], ["1001", "Predicate optimization improvements", "Eric Hwang", "erichwang", "02/06/14, 12:35:37 AM", "- Between ranges\n- Simplified effective predicates from partitions", "NaN"], ["1002", "Update to latest airbase and airlift", "David Phillips", "electrum", "02/10/14, 06:41:00 PM", "NaN", "NaN"], ["1003", "Fix state machine and info objects", "Dain Sundstrom", "dain", "02/05/14, 07:09:02 AM", "NaN", "NaN"], ["1004", "Replace generic \"getService\" method in Connector with explicit methods", "Martin Traverso", "martint", "02/05/14, 07:29:27 AM", "NaN", "NaN"], ["1005", "Synchronize before copying the output items", "Dain Sundstrom", "dain", "02/05/14, 08:16:47 PM", "NaN", "NaN"], ["1006", "Fully stop tasks", "Dain Sundstrom", "dain", "02/06/14, 12:32:31 AM", "NaN", "NaN"], ["1007", "Fix overflow bug in HashPagePartitionFunction", "Dain Sundstrom", "dain", "02/06/14, 02:30:58 AM", "NaN", "NaN"], ["1008", "Add experimental support for approximate queries", "Christopher Berner", "cberner", "02/08/14, 06:04:29 AM", "Note, most of these are from my sample weight PR. The one that needs review is \"Experimental support for approximate queries\"", "NaN"], ["1009", "Do not close output buffers on failure", "Dain Sundstrom", "dain", "02/06/14, 06:56:21 AM", "NaN", "NaN"], ["1014", "Distribute splits evenly among nodes", "Nileema Shingte", "nileema", "02/08/14, 03:57:27 AM", "NaN", "NaN"], ["1016", "Add checkstyle rule for colon in foreach loops", "Christopher Berner", "cberner", "02/07/14, 08:57:26 PM", "NaN", "NaN"], ["1017", "Refactor sampled test queries into a separate abstract class", "Christopher Berner", "cberner", "02/08/14, 02:28:27 AM", "NaN", "NaN"], ["1018", "Fix LocalExecutionPlanner handling of null literal projections", "Dain Sundstrom", "dain", "02/10/14, 08:31:51 PM", "Disable compiler tests involving untyped nulls", "NaN"], ["1026", "Update to airbase 15", "David Phillips", "electrum", "02/11/14, 01:05:40 AM", "NaN", "NaN"], ["1028", "Add bootstrapped aggregation", "Christopher Berner", "cberner", "02/24/14, 09:56:55 PM", "NaN", "NaN"], ["1029", "Remove materialized view and alias support", "David Phillips", "electrum", "02/13/14, 02:06:17 AM", "NaN", "NaN"], ["1030", "Allow optimization of subexpressions in certain cases", "Eric Hwang", "erichwang", "02/11/14, 02:05:25 AM", "NaN", "NaN"], ["1032", "Fix information schema predicate extraction", "David Phillips", "electrum", "02/11/14, 03:30:20 AM", "NaN", "NaN"], ["1034", "Fix JDBC tests", "David Phillips", "electrum", "02/11/14, 06:46:16 PM", "NaN", "NaN"], ["1035", "Add missing dependency", "Christopher Berner", "cberner", "02/11/14, 08:39:47 PM", "NaN", "NaN"], ["1036", "Add poissonized sampling", "Christopher Berner", "cberner", "02/25/14, 06:48:16 PM", "NaN", "NaN"], ["1037", "Fix scheduling bug where drivers queued in TaskExecutor are not counted", "Dain Sundstrom", "dain", "02/12/14, 02:14:41 AM", "Create DriverContext when split is added instead of waiting until a thread is associated with the split.", "NaN"], ["1038", "POM updates", "David Phillips", "electrum", "02/12/14, 02:18:17 AM", "NaN", "NaN"], ["1039", "Fix dependency scope for commons-math", "David Phillips", "electrum", "02/12/14, 03:02:54 AM", "NaN", "NaN"], ["1040", "Check for no tpch nodes when generating splits", "Dain Sundstrom", "dain", "02/12/14, 03:16:44 AM", "NaN", "NaN"], ["1042", "Add support for sampled tables to Hive connector", "Christopher Berner", "cberner", "02/27/14, 07:44:12 PM", "NaN", "NaN"], ["1046", "Add tracking of failures for metastore API calls", "Christopher Berner", "cberner", "02/19/14, 10:02:13 PM", "NaN", "NaN"], ["1050", "Implement VALUES", "Dain Sundstrom", "dain", "02/27/14, 03:28:47 AM", "NaN", "NaN"], ["1054", "Add tracking of latency and failures for hadoop api calls", "Christopher Berner", "cberner", "02/19/14, 07:44:38 PM", "NaN", "NaN"], ["1056", "Fix EffectivePredicateExtractor range summary bug", "Eric Hwang", "erichwang", "02/19/14, 07:32:07 AM", "Occurs when the RangeSet has no ranges.", "NaN"], ["1061", "Fix JOINs that have ORDER BY rand()", "Christopher Berner", "cberner", "02/21/14, 07:39:06 AM", "NaN", "NaN"], ["1064", "Fix build failure", "Christopher Berner", "cberner", "02/21/14, 07:11:38 PM", "AsyncHttpClient.AsyncHttpResponseFuture was removed from airlift", "NaN"], ["1065", "Fix handling of Hive timestamp in maps/lists", "Christopher Berner", "cberner", "02/24/14, 08:23:27 PM", "Also, add more unit tests for RCFile formats", "NaN"], ["1069", "Rename analyzer.approximate-queries-enabled", "Christopher Berner", "cberner", "02/25/14, 10:32:59 PM", "Change name of config flag to analyzer.experimental-syntax-enabled,\nsince it's used to gate more than just approximate queries (rescaled\nsampling)", "NaN"], ["1075", "Make unit tests faster", "Christopher Berner", "cberner", "02/26/14, 11:45:22 PM", "NaN", "NaN"], ["1077", "Add more details to time logging in test queries", "Christopher Berner", "cberner", "02/27/14, 03:35:57 AM", "NaN", "NaN"], ["1078", "Fix logging for Hive connectors", "David Phillips", "electrum", "02/27/14, 01:57:02 AM", "NaN", "NaN"], ["1079", "Add documentation for migrating from Hive", "Christopher Berner", "cberner", "02/27/14, 09:02:47 PM", "NaN", "NaN"], ["1080", "Update to airbase 16", "David Phillips", "electrum", "02/27/14, 05:52:04 AM", "NaN", "NaN"], ["1087", "Change default read timeout to 1 second", "Dain Sundstrom", "dain", "05/06/15, 01:14:07 AM", "I'm not sure if the values are correct, but this shows how the default config system works", "NaN"], ["1088", "Add error codes", "Christopher Berner", "cberner", "03/11/14, 12:44:54 AM", "NaN", "NaN"], ["1089", "Parallelize test queries in presto-main", "Christopher Berner", "cberner", "03/04/14, 12:52:40 AM", "Make all unit tests in presto-main run in parallel by default, and\nannotate non-threadsafe tests with singleThreaded=true", "NaN"], ["1090", "Add backpressure to InMemoryExchange", "David Phillips", "electrum", "03/05/14, 11:46:18 PM", "NaN", "NaN"], ["1099", "closed form SUM and COUNT", "Christopher Berner", "cberner", "03/17/14, 11:01:47 PM", "NaN", "NaN"], ["1100", "Run all tests in parallel by default", "Christopher Berner", "cberner", "03/06/14, 02:37:48 AM", "Annotate all tests that use @BeforeMethod with singleThreaded=true", "NaN"], ["1105", "Remove potential deadlock with broken connectors", "Dain Sundstrom", "dain", "03/11/14, 12:09:03 AM", "Replace synchronized lock with a custom lock with timeout support\n\nFor close and updateSource stage change and then attempt to acquire\nlock.  If lock is acquired, process change; otherwise lock holder\nwill process change before releasing lock.\n\nFor close, if lock can not be acquired, interrupt lock holder.", "NaN"], ["1108", "Fix division by zero in VarianceAggregation", "Christopher Berner", "cberner", "03/10/14, 06:36:27 PM", "NaN", "NaN"], ["1109", "Update S3 FileSystem", "David Phillips", "electrum", "03/11/14, 12:12:20 AM", "This is an updated version of #1044.", "NaN"], ["1115", "Fix null values for DatabaseMetaData.getColumns", "David Phillips", "electrum", "03/11/14, 07:18:24 PM", "NaN", "NaN"], ["1116", "Fixes", "Christopher Berner", "cberner", "03/12/14, 12:02:41 AM", "NaN", "NaN"], ["1121", "Revert scheduler changes", "Martin Traverso", "martint", "03/12/14, 10:38:05 PM", "The scheduler changes can result in smaller queries being starved\nby bigger queries due to the introduction of global per-node queues.\nReverting for now so we can release. We'll revisit later.", "NaN"], ["1122", "Indivisible", "Christopher Berner", "cberner", "03/13/14, 06:08:22 PM", "NaN", "NaN"], ["1125", "Read timing", "Christopher Berner", "cberner", "03/24/14, 09:21:51 PM", "NaN", "NaN"], ["1128", "Add DirectoryLister interface", "David Phillips", "electrum", "03/19/14, 07:24:52 PM", "Use dependency injection instead of a static method call.", "NaN"], ["1129", "Refactor failure handling", "Christopher Berner", "cberner", "03/20/14, 12:21:43 AM", "- Split FailureInfo into ClientFailureInfo and FailureInfo, so that error codes can be propagated back from workers\n- Change ErrorCode to a concrete class, instead of an interface, to allow it to be json serialized", "NaN"], ["1131", "Record \"started\" counter for queries that fail to parse", "Martin Traverso", "martint", "03/17/14, 10:38:21 PM", "In the last release we started tracking parsing failures for error categorization,\nbut the change was missing the call to increment the started counter.\n\nAs a result, the started and completed counters get permanently out of sync\nand cause the number of running queries to be reported as 0.\n\nFixes https://github.com/facebook/presto/issues/1130", "NaN"], ["1132", "Rename HadoopApiStats and move it out of util package", "Christopher Berner", "cberner", "03/19/14, 06:27:23 PM", "NaN", "NaN"], ["1134", "Categorize user errors", "Christopher Berner", "cberner", "03/19/14, 05:13:17 PM", "- Categorize a bunch of illegal argument exceptions as user errors\n- Categorize division by zero as a user error", "NaN"], ["1136", "Include metastore host name in exception message", "Christopher Berner", "cberner", "03/24/14, 06:59:16 PM", "NaN", "NaN"], ["1139", "Upgrade to airlift 0.87-SNAPSHOT", "Christopher Berner", "cberner", "03/20/14, 05:13:55 AM", "NaN", "NaN"], ["1140", "Add counter for total failed queries", "Christopher Berner", "cberner", "03/20/14, 05:00:26 AM", "NaN", "NaN"], ["1143", "Use a private I/O thread pool for failure detector", "Martin Traverso", "martint", "03/20/14, 05:22:51 PM", "NaN", "NaN"], ["1145", "Allow additional modules with TestingPrestoServer", "David Phillips", "electrum", "03/20/14, 07:25:49 PM", "NaN", "NaN"], ["1148", "Add release notes for 0.62", "Martin Traverso", "martint", "03/21/14, 04:07:25 AM", "NaN", "NaN"], ["1149", "Cleanup data types documentation", "David Phillips", "electrum", "03/21/14, 04:49:49 AM", "NaN", "NaN"], ["1150", "Truncate SQL query text in index page (take 2)", "Martin Traverso", "martint", "03/21/14, 09:33:42 PM", "Forgot to apply the change to the \"running\" queries table :(", "NaN"], ["1151", "Add experimental syntax flag to FunctionRegistry", "Christopher Berner", "cberner", "03/25/14, 01:29:34 AM", "- Gate approximate functions with experimental syntax flag\n- Will also be used to gate future experimental functions, like the ML functions", "NaN"], ["1153", "Categorize errors and improve PrestoException message", "Christopher Berner", "cberner", "03/24/14, 07:40:15 PM", "NaN", "NaN"], ["1154", "Categorize partition offline exceptions", "Christopher Berner", "cberner", "03/26/14, 07:35:24 PM", "Also preserve their exception type by making PartitionOfflineException extend PrestoException. This fixes our internal tests", "NaN"], ["1156", "Rename AbstractHiveRecordCursor to HiveRecordCursor", "Christopher Berner", "cberner", "03/25/14, 02:16:58 AM", "NaN", "NaN"], ["1158", "Categorize bad date parse format string errors", "Christopher Berner", "cberner", "03/25/14, 08:12:02 PM", "NaN", "NaN"], ["1160", "Fix NPE in PlanPrinter", "Eric Hwang", "erichwang", "03/27/14, 12:39:36 AM", "NaN", "NaN"], ["1162", "Rewrite identical projection expressions in terms of the same symbol", "Eric Hwang", "erichwang", "03/27/14, 04:58:33 AM", "This is an existing gap in our optimizer whereby a projection may have the same deterministic expression being generated for multiple output symbols. This results in the expression being computed and stored multiple times unnecessarily.\n\nFor example, consider this query:\nhttps://gist.github.com/erichwang/8f9dea33396c2d85869d\n\nPreviously our plan would look like this:\nhttps://gist.github.com/erichwang/3f5c82c13a9c79f998ef\n\nWith the fix it looks like this:\nhttps://gist.github.com/erichwang/e08606ae5fe17dc52a6c", "NaN"], ["1163", "Fix Range bug in SPI", "Eric Hwang", "erichwang", "03/27/14, 04:48:13 PM", "This is a nasty one.", "NaN"], ["1165", "Add 0.63 release notes", "Martin Traverso", "martint", "03/28/14, 12:33:30 AM", "NaN", "NaN"], ["1167", "Fix race condition in Hive tests", "David Phillips", "electrum", "03/29/14, 02:11:20 AM", "NaN", "NaN"], ["1168", "Fix error bounds for approximate SUM and AVG", "Christopher Berner", "cberner", "04/01/14, 02:43:37 AM", "NaN", "NaN"], ["1171", "Update jmxutils to 1.15", "Martin Traverso", "martint", "04/01/14, 10:53:46 PM", "This fixes an issue when registering duplicate mbean names", "NaN"], ["1172", "Errors", "Christopher Berner", "cberner", "04/01/14, 11:58:52 PM", "NaN", "NaN"], ["1173", "Resize group by builder", "Christopher Berner", "cberner", "04/02/14, 07:31:25 PM", "NaN", "NaN"], ["1178", "Add helper methods similar to checkArgument", "Christopher Berner", "cberner", "04/05/14, 12:06:00 AM", "NaN", "NaN"], ["1179", "Fix bug in CREATE TABLE AS SELECT", "Martin Traverso", "martint", "04/06/14, 05:29:26 AM", "NaN", "NaN"], ["1181", "Fix thread UI and add release notes", "David Phillips", "electrum", "04/08/14, 02:23:47 AM", "NaN", "NaN"], ["1185", "Fix NPE in SqlTaskExecution.isFinished", "Martin Traverso", "martint", "04/08/14, 10:55:39 PM", "NaN", "NaN"], ["1187", "Update jmxutils to 1.16", "David Phillips", "electrum", "04/09/14, 12:23:52 AM", "NaN", "NaN"], ["1190", "Fix NumberFormatException killing cli bug", "Rishi Kesh Dwivedi", "rishidwivedi", "04/10/14, 02:52:09 AM", "NaN", "NaN"], ["1194", "Reuse pre computed partitionDomainSummary", "Rishi Kesh Dwivedi", "rishidwivedi", "04/25/14, 06:54:31 PM", "PartitionDomainSummary computation is an expensive operation(250ms for 12k partitions). Instead of computing it every time a new instance of TableScanNode is created, we can use pre computed partitionDomainSummary value.", "NaN"], ["1197", "Allow setting time zone for Hive data", "David Phillips", "electrum", "04/16/14, 08:03:42 PM", "NaN", "NaN"], ["1200", "Update to airbase 18", "Martin Traverso", "martint", "04/16/14, 11:05:33 PM", "NaN", "NaN"], ["1201", "Full type support updated with feedback", "Dain Sundstrom", "dain", "04/17/14, 12:28:56 AM", "This included all code from UDT and  full-type-support and has been updated based on feedback from the reviews.", "NaN"], ["1202", "Index joins", "Dain Sundstrom", "dain", "04/19/14, 05:32:35 AM", "Updated for new type system", "NaN"], ["1203", "Update to Hive 0.12", "David Phillips", "electrum", "04/17/14, 05:46:08 PM", "NaN", "NaN"], ["1205", "Add date_trunc function", "Dain Sundstrom", "dain", "04/19/14, 06:28:26 AM", "NaN", "NaN"], ["1206", "Add Color type and use it in color functions", "Martin Traverso", "martint", "04/21/14, 06:23:11 PM", "NaN", "NaN"], ["1208", "Explain missing column type bug fix", "Rishi Kesh Dwivedi", "rishidwivedi", "04/25/14, 08:11:16 PM", "NaN", "NaN"], ["1210", "Fix regression with VARCHAR partitions in Hive", "Christopher Berner", "cberner", "04/21/14, 09:38:10 PM", "Fix the Hive connector to return Slice objects for the values of VARCHAR\npartition columns instead of String objects. Also fix the Hive tests to\nuse Slice instead of String.", "NaN"], ["1211", "Jdbc date time types", "Dain Sundstrom", "dain", "04/22/14, 06:27:35 AM", "NaN", "NaN"], ["1213", "Minor fixes", "David Phillips", "electrum", "04/22/14, 12:34:30 AM", "NaN", "NaN"], ["1215", "Simplify test table creation for S3", "David Phillips", "electrum", "04/22/14, 12:40:32 AM", "NaN", "NaN"], ["1216", "Spi cleanup", "Dain Sundstrom", "dain", "04/29/14, 01:55:10 AM", "Remove unnecessary methods\nUse consistent naming\nAdd javadocs to interfaces\nChange RecordCursor getString to getSlice returning a Slice\nAllow plugins to register new Types and BlockEncodingFactories", "NaN"], ["1218", "Fix exchange client leak when queries are abandoned", "Martin Traverso", "martint", "04/22/14, 05:37:50 PM", "The query purger in StatementResource wasn't calling close() on query objects when purging\nthem from the list of active queries. As a result, abandonded queries would result\nin StatementResource leaking an exchange client", "NaN"], ["1219", "Update to airbase 19", "Martin Traverso", "martint", "04/22/14, 05:44:53 PM", "This updates Jetty to 9.1.4, which we hope will fix https://github.com/facebook/presto/issues/1138", "NaN"], ["1223", "Fix implicit coercion of aggregates", "Christopher Berner", "cberner", "04/24/14, 05:31:29 PM", "NaN", "NaN"], ["1224", "PagesSerde fixes", "David Phillips", "electrum", "04/24/14, 04:35:05 AM", "NaN", "NaN"], ["1225", "Fix SHOW PARTITIONS", "Christopher Berner", "cberner", "04/24/14, 05:38:18 PM", "Connectors (such as the Hive connector) that return Slices for their TupleDomains ended up with the\ntoString() representation of Slice in SHOW PARTITIONS", "NaN"], ["1226", "Fix a few thread-safety/concurrency issues", "Martin Traverso", "martint", "04/24/14, 05:46:57 PM", "NaN", "NaN"], ["1227", "Implement toString for hll", "Nileema Shingte", "nileema", "04/24/14, 06:36:43 PM", "NaN", "NaN"], ["1230", "Fix incorrect output from SHOW SCHEMAS", "Martin Traverso", "martint", "04/25/14, 06:24:24 AM", "There was a bug in the way MetadataManager keeps track and resolves information schema\nconnector registrations that caused the wrong ConnectorMetadata to be picked. As a result,\nSHOW SCHEMAS would display schemas from another catalog.", "NaN"], ["1232", "Cleanup SerDeUtils", "David Phillips", "electrum", "04/29/14, 10:44:11 PM", "NaN", "NaN"], ["1233", "Minor improvements to BootstrapFunctionBinder", "Martin Traverso", "martint", "04/25/14, 10:33:54 PM", "NaN", "NaN"], ["1234", "Fix memory leak in BootstrapFunctionBinder", "Martin Traverso", "martint", "04/27/14, 06:57:57 AM", "Callsites were being registered in a global BootstrapFunctionBinder but were never removed.\n\nIn this implementation, we create one BootrapFunctionBinder per dynamic classloader and load\nan \"isolated\" version of the Bootstrap class used by invokedynamic calls. When the classloader\nis garbage-collected (i.e., the compiled code is no longer needed), the callsite registrations\nare collected, too.", "NaN"], ["1236", "Use ValuesNode instead of dual table for planning queries w/o FROM", "Martin Traverso", "martint", "04/28/14, 08:27:21 PM", "NaN", "NaN"], ["1237", "Fix warnings in BlockEncodingManager", "David Phillips", "electrum", "04/28/14, 11:01:55 PM", "NaN", "NaN"], ["1238", "Exchange client improvements", "David Phillips", "electrum", "04/29/14, 05:01:04 AM", "NaN", "NaN"], ["1239", "Fix memory (estimate) leak and reduce memory usage of GROUP BY and DISTINCT", "Christopher Berner", "cberner", "04/29/14, 06:32:08 AM", "NaN", "NaN"], ["1240", "Add support for plugins to register new Types and BlockEncodingFactories", "Dain Sundstrom", "dain", "04/29/14, 04:03:04 AM", "NaN", "NaN"], ["1241", "Minor fixes", "Dain Sundstrom", "dain", "04/29/14, 05:45:38 AM", "NaN", "NaN"], ["1242", "Fix Hive client tests using getSlice as byte array", "Dain Sundstrom", "dain", "04/29/14, 05:58:39 PM", "NaN", "NaN"], ["1243", "Update docs for color functions", "Martin Traverso", "martint", "04/29/14, 11:09:53 PM", "NaN", "NaN"], ["1245", "Provide locale via X-Presto-Language header", "Martin Traverso", "martint", "04/29/14, 11:25:13 PM", "The expected format is as defined by IETF BCP 47 (http://tools.ietf.org/html/bcp47)", "NaN"], ["1247", "Hive timestamp fixes", "David Phillips", "electrum", "04/30/14, 12:34:18 AM", "NaN", "NaN"], ["1248", "Add Hive plugin for CDH 5", "David Phillips", "electrum", "04/30/14, 03:33:40 AM", "NaN", "NaN"], ["1249", "Add support for plugin provided operators", "Dain Sundstrom", "dain", "04/30/14, 06:29:13 AM", "NaN", "NaN"], ["1250", "Fix planning issues with non-trivial types and date/time constructs in VALUES ", "Martin Traverso", "martint", "04/30/14, 05:43:48 AM", "NaN", "NaN"], ["1252", "Add backoff for HttpPageBufferClient", "David Phillips", "electrum", "04/30/14, 06:40:19 PM", "NaN", "NaN"], ["1253", "Fix tracking of time for splitSource.getNextBatch", "Martin Traverso", "martint", "04/30/14, 04:44:46 PM", "NaN", "NaN"], ["1254", "Allow non-grouped aggregations to report their memory usage", "Christopher Berner", "cberner", "04/30/14, 08:27:32 PM", "NaN", "NaN"], ["1255", "Add config to disable proximity scheduling", "Nileema Shingte", "nileema", "04/30/14, 09:00:04 PM", "Add a config parameter to disable scheduling based on proximity. This will enable us to remove a variable and help us drill down into the cause of uneven split distribution. ", "NaN"], ["1256", "Hide internal functions\t", "David Phillips", "electrum", "04/30/14, 08:46:31 PM", "NaN", "NaN"], ["1257", "Session cleanups", "David Phillips", "electrum", "04/30/14, 11:29:53 PM", "NaN", "NaN"], ["1258", "Cleanup date time functions", "Dain Sundstrom", "dain", "04/30/14, 11:44:14 PM", "NaN", "NaN"], ["1259", "Use ResponseTooLargeException", "David Phillips", "electrum", "05/01/14, 12:29:56 AM", "NaN", "NaN"], ["1260", "Implement weekyear (%x) in date_parse and date_format", "Dain Sundstrom", "dain", "05/01/14, 12:34:41 AM", "NaN", "NaN"], ["1261", "CLI and example-http fixes", "David Phillips", "electrum", "05/01/14, 02:49:52 PM", "NaN", "NaN"], ["1263", "Add Hive format test for null lists", "David Phillips", "electrum", "05/01/14, 08:30:23 PM", "NaN", "NaN"], ["1267", "Fix invalid expression types", "Christopher Berner", "cberner", "05/01/14, 08:33:59 PM", "This can cause a NPE in the FunctionRegistry because we end up with\narguments that have no type", "NaN"], ["1268", "Fixes", "David Phillips", "electrum", "05/01/14, 10:06:16 PM", "NaN", "NaN"], ["1269", "Update documentation for datetime", "David Phillips", "electrum", "05/02/14, 02:44:08 AM", "NaN", "NaN"], ["1271", "Fix resource leak in HiveSplitSource when queries terminate early", "Martin Traverso", "martint", "05/02/14, 11:17:45 PM", "NaN", "NaN"], ["1272", "Minor cleanup", "David Phillips", "electrum", "05/06/14, 01:08:26 AM", "NaN", "NaN"], ["1273", "Categorize invalid VARCHAR casts", "Christopher Berner", "cberner", "05/05/14, 07:16:09 PM", "NaN", "NaN"], ["1274", "Add formatting for every statement type", "David Phillips", "electrum", "05/06/14, 01:09:38 AM", "NaN", "NaN"], ["1275", "Add machine learning functions plugin", "Christopher Berner", "cberner", "05/15/14, 12:01:57 AM", "Add functions to train and use machine learning models (classifiers and\nregressors) in Presto. This is currently only a proof of concept, and is\nnot ready for use in production. Example usage is as follows:\n\n``` sql\nSELECT evaluate_classifier_predictions(label, classify(features, model))\nFROM (\n    SELECT learn_classifier(label, features) AS model\n    FROM my_training_data;\n)\nCROSS JOIN my_validation_data;\n```", "NaN"], ["1276", "Add toString to HiveBucket", "David Phillips", "electrum", "05/06/14, 10:36:32 PM", "NaN", "NaN"], ["1279", "Categorize more errors", "Christopher Berner", "cberner", "05/06/14, 07:50:54 PM", "NaN", "NaN"], ["1280", "Error code improvements", "David Phillips", "electrum", "05/06/14, 10:37:47 PM", "NaN", "NaN"], ["1281", "Finish properly parameterizing TupleDomain", "Eric Hwang", "erichwang", "05/06/14, 11:06:58 PM", "NaN", "NaN"], ["1282", "Http500", "Christopher Berner", "cberner", "05/07/14, 08:02:43 PM", "NaN", "NaN"], ["1287", "Don't double count bytes and rows in RecordProjectOperator", "Christopher Berner", "cberner", "05/09/14, 05:42:50 PM", "NaN", "NaN"], ["1289", "Update to Hive 0.13", "David Phillips", "electrum", "06/13/14, 11:50:47 PM", "NaN", "NaN"], ["1290", "Increase parallelism in unit tests to 4", "Christopher Berner", "cberner", "05/10/14, 04:29:40 PM", "NaN", "NaN"], ["1293", "Fix release notes for 0.67", "David Phillips", "electrum", "05/09/14, 07:34:21 PM", "NaN", "NaN"], ["1294", "Fix handling of Hive tables bucketed on string", "David Phillips", "electrum", "05/10/14, 01:36:05 AM", "NaN", "NaN"], ["1296", "Add timestamp with time zone minus interval year month operator", "Dain Sundstrom", "dain", "05/13/14, 04:03:27 PM", "NaN", "NaN"], ["1297", "Various cleanup", "David Phillips", "electrum", "05/13/14, 04:56:52 PM", "NaN", "NaN"], ["1302", "Fix build", "Christopher Berner", "cberner", "05/15/14, 03:54:07 PM", "NaN", "NaN"], ["1303", "Fix MappedRecordCursor bug", "Eric Hwang", "erichwang", "05/15/14, 10:07:53 PM", "NaN", "NaN"], ["1305", "Make TestDateTimeFunctions single threaded", "David Phillips", "electrum", "05/16/14, 04:22:23 AM", "NaN", "NaN"], ["1306", "Implement varbinary type", "Martin Traverso", "martint", "05/19/14, 06:49:51 PM", "Adds a varbinary type and basic operators/functions", "NaN"], ["1309", "Add option to disable scheduling tasks on coordinator", "Christopher Berner", "cberner", "05/20/14, 12:03:43 AM", "NaN", "NaN"], ["1310", "Simplify SortedRangeSet.Builder and add benchmark", "Martin Traverso", "martint", "06/10/14, 08:03:37 PM", "Collect ranges and do one pass at the end to merge adjacent and overlapping ranges.\n\nThis improves performance for this class by about 6x for a synthetic benchmark with 10000 ranges\n\n```\n        Benchmark                                            Mode   Samples   Mean  Mean error   Units\nBefore: c.f.p.s.BenchmarkSortedRangeSet.benchmarkBuilder     avgt        10  5.416       0.150   ms/op\nAfter:  c.f.p.s.BenchmarkSortedRangeSet.benchmarkBuilder     avgt        10  0.867       0.016   ms/op\n```", "NaN"], ["1314", "Add jmx stats for abandoned and canceled queries", "Christopher Berner", "cberner", "05/19/14, 06:51:32 PM", "NaN", "NaN"], ["1316", "Hidden columns", "Dain Sundstrom", "dain", "05/28/14, 08:44:59 PM", "Allow connectors to expose columns that are hidden.  A hidden column will not appear in describe table or information schema `columns` table, and will not be selected with `*`.  The only way to access a hidden column is explicitly by name.\n\nAdditionally, I have added a hidden column, `row_number`, to the tpch connector for testing.", "NaN"], ["1317", "Fix varbinary-related test failures", "Martin Traverso", "martint", "05/19/14, 11:52:14 PM", "NaN", "NaN"], ["1319", "Extract native connector into separate plugin", "David Phillips", "electrum", "05/21/14, 08:37:26 PM", "NaN", "NaN"], ["1320", "Refactor aggregations", "Christopher Berner", "cberner", "05/29/14, 10:24:21 PM", "Introduce a new base class for aggregation functions that makes them\nsimpler to write.\n\nIt looks like there's a small (~3% performance regression), but given how much this simplifies the code, I think it's worthwhile. We could also try to get that back with byte code generation.\n\nmaster:\n                  hand_tpch_query_1 :: 2907.023 cpu ms :: in    6M,   361MB,   2.06M/s,   124MB/s :: out     4,    300B,       1/s,    103B/s\n                  hand_tpch_query_6 ::  308.901 cpu ms :: in    6M,   240MB,   19.4M/s,   778MB/s :: out     1,      9B,       3/s,     29B/s\n\nmy branch:\n                  hand_tpch_query_1 :: 2937.362 cpu ms :: in    6M,   361MB,   2.04M/s,   123MB/s :: out     4,    300B,       1/s,    102B/s\n                  hand_tpch_query_6 ::  321.828 cpu ms :: in    6M,   240MB,   18.6M/s,   747MB/s :: out     1,      9B,       3/s,     27B/s", "NaN"], ["1321", "Fix SHOW PARTITIONS and dual catalog with include-coordinator=false", "Christopher Berner", "cberner", "05/22/14, 01:04:56 AM", "Mark dual and information schema as system datasources. Also add more unit tests for disabling coordinator work scheduling", "NaN"], ["1322", "Categorize invalid argument errors to approx_percentile", "Christopher Berner", "cberner", "05/22/14, 07:10:55 PM", "NaN", "NaN"], ["1324", "Add more configurable learn methods for ML plugin", "Christopher Berner", "cberner", "05/30/14, 11:08:10 PM", "NaN", "NaN"], ["1325", "Fix parsing and formatting of sign expressions", "David Phillips", "electrum", "05/22/14, 07:24:50 PM", "NaN", "NaN"], ["1326", "Deprecate the datasources configuration property", "David Phillips", "electrum", "05/22/14, 10:52:26 PM", "This property, if not set, is now configured automatically at startup based on\nthe list of installed catalogs.  This will be completely removed later.", "NaN"], ["1327", "Remove dual connector", "Martin Traverso", "martint", "05/24/14, 04:29:17 AM", "NaN", "NaN"], ["1328", "Rewrite EXPLAIN to a VALUES query", "Martin Traverso", "martint", "05/24/14, 04:28:43 AM", "This helps avoid bytecode generation failures when plan string is too large due\nto max constant pool size being exceeded.", "NaN"], ["1329", "Fix tests for datasources change", "David Phillips", "electrum", "05/23/14, 04:15:18 PM", "NaN", "NaN"], ["1333", "Fix failing unit tests in java 8", "Martin Traverso", "martint", "05/28/14, 05:38:28 AM", "The user.timezone property for the surefire plugin must match\nthe setting via argLine. The pom was inheriting the property from\nairbase set to UTC, but argLine was set to Asia/Katmandu.\n\nAs far as I can tell, the surefire plugin executes the following steps:\n- create a VM using the argLine arguments\n- call setProperty for each systemPropertyVariable\n- run the tests\n\nUnlike Java 7, calling System.setProperty for user.timezone in Java 8 after the VM is initialized\nhas no effect. See d2fee8d93952054883bf94a65576cead632314cf for additional details", "NaN"], ["1334", "Extract integration tests into separate module for reuse in connectors", "Dain Sundstrom", "dain", "05/28/14, 06:18:02 PM", "NaN", "NaN"], ["1335", "Replace Unsafe constants with Slice SizeOf", "David Phillips", "electrum", "05/28/14, 11:21:43 PM", "NaN", "NaN"], ["1336", "Fix SQL formatter", "David Phillips", "electrum", "05/29/14, 12:04:05 AM", "NaN", "NaN"], ["1338", "Replace Unsafe with new Slice ByteArrays utility", "David Phillips", "electrum", "05/30/14, 09:52:48 PM", "NaN", "NaN"], ["1339", "Add missing presto-main dependency to presto-server", "Martin Traverso", "martint", "05/30/14, 06:19:02 PM", "This was missed in the refactoring that moved the server code to presto-main.\nAs a result, the server fails to start when run via the launcher", "NaN"], ["1340", "Fix hidden columns in union planning", "Dain Sundstrom", "dain", "05/31/14, 01:18:51 AM", "NaN", "NaN"], ["1341", "Fix Hive tests for VARBINARY", "David Phillips", "electrum", "05/31/14, 01:02:41 AM", "NaN", "NaN"], ["1342", "Cassandra patches", "Dain Sundstrom", "dain", "06/27/14, 02:52:45 AM", "NaN", "NaN"], ["1337", "Comment at end of file causes CLI error", "David Phillips", "electrum", "06/03/14, 11:41:23 PM", "```\ndescribe abtest; /* describe abtest; */\n```", "NaN"], ["1345", "Update README for presto-server change", "David Phillips", "electrum", "06/02/14, 05:51:43 PM", "NaN", "NaN"], ["1346", "Update connector documentation", "David Phillips", "electrum", "06/03/14, 11:38:42 PM", "NaN", "NaN"], ["1348", "Filter nulls before fetching values from an index", "Dain Sundstrom", "dain", "06/03/14, 01:06:24 AM", "NaN", "NaN"], ["1351", "Replace DistributionStat with TimeStat", "Martin Traverso", "martint", "06/06/14, 06:52:43 PM", "NaN", "NaN"], ["1353", "Fix handling of Hive maps with null keys", "David Phillips", "electrum", "06/03/14, 10:43:38 PM", "NaN", "NaN"], ["1354", "Summarize stats", "Dain Sundstrom", "dain", "06/10/14, 12:29:54 AM", "NaN", "NaN"], ["1355", "Implement simple version of greatest/least", "Martin Traverso", "martint", "06/04/14, 05:36:25 AM", "Only 2-arg versions for common types: double, bigint, timestamp (with tz), date\n\nOnce we support varargs we'll replace them with a generic version.", "NaN"], ["1358", "Add initial support for views", "David Phillips", "electrum", "06/11/14, 10:57:27 PM", "This is a basic working implementation. However, it lacks support for\nmetadata queries in Presto (the connector support is already in place).", "NaN"], ["1359", "Minor cleanup and (potential) optimization in compiler", "Martin Traverso", "martint", "06/04/14, 06:14:46 PM", "Currently, the code generates calls to getCursor for every channel from 0 up to the\nmax channel referenced in filters/projections. With this change, the code now only\ncalls getCursor on channels that are actually needed for evaluation.", "NaN"], ["1362", "Rename getHandle() to getSignature() in FunctionInfo", "Martin Traverso", "martint", "06/04/14, 06:53:30 PM", "NaN", "NaN"], ["1363", "Add support for optimistic splitting in hive connector", "Christopher Berner", "cberner", "06/06/14, 09:40:40 PM", "NaN", "NaN"], ["1364", "Minor cleanup and optimization in compiler (second try)", "Martin Traverso", "martint", "06/04/14, 09:07:41 PM", "Also adds a flag to disable interpreter fallback so that unit tests catch compiler regressions.", "NaN"], ["1365", "Consolidate FunctionInfo and OperatorInfo", "Martin Traverso", "martint", "06/04/14, 11:36:59 PM", "This is a precursor to the new expression tree. The goal is that\nat the core, operators and functions are both represented in the\nsame way and only the sql metadata layer is aware of the distinction\nbetween them.", "NaN"], ["1366", "Remove unused methods", "Martin Traverso", "martint", "06/04/14, 11:45:08 PM", "NaN", "NaN"], ["1368", "More refactoring of aggregations", "Christopher Berner", "cberner", "06/11/14, 10:10:45 PM", "NaN", "NaN"], ["1369", "Add Presto verifier", "Christopher Berner", "cberner", "06/11/14, 05:44:36 PM", "NaN", "NaN"], ["1374", "Allow Presto Hive connector to offline tables", "Christopher Berner", "cberner", "06/11/14, 04:28:48 PM", "NaN", "NaN"], ["1375", "Fix benchmarks", "Dain Sundstrom", "dain", "06/11/14, 02:49:35 AM", "NaN", "NaN"], ["1378", "Updates for PR #999 - Window functions with arguments", "Dain Sundstrom", "dain", "06/11/14, 05:38:53 PM", "- Move specification of argument channels to window function construction\n- Split commit into one that adds the feature and one that adds the functions\n- Simplify `first_value` and `last_value` by writing the value directly from the input pages index\n- Add `nth_value` function", "NaN"], ["1379", "Make all blocks random access", "Dain Sundstrom", "dain", "06/18/14, 07:09:10 PM", "NaN", "NaN"], ["1381", "Disallow at signs in unquoted identifiers", "Dain Sundstrom", "dain", "06/27/14, 06:41:04 AM", "Change SqlParser to be configurable which requires the parser to no longer be static.", "NaN"], ["1383", "Fix performance regression in variance aggregation", "Christopher Berner", "cberner", "06/12/14, 07:00:24 PM", "NaN", "NaN"], ["1385", "Fix TopN distributed plan to wrap partial TopN plan in merge TopN plan", "Sadayuki Furuhashi", "frsyuki", "06/13/14, 12:26:20 AM", "I'm creating a distributed query plan formatter and realized that TopNNode with partial=true wraps TopNNode with partial=false.\nFor example, with this query: `select r_regionkey from tpch.region order by r_regionkey limit 2`\ndistributed query plan is:\n\n```\n- Stage-1\n    -> Output[6]\n        Columns: r_regionkey = r:bigint\n        -> PartialTopN[9]\n            Count: 2\n            OrderBy: r ASC NULLS LAST\n            -> Exchange[8]\n                Sources: Stage-0\n- Stage-0\n    -> Sink[7]\n        -> FinalTopN[2]\n            Count: 2\n            OrderBy: r ASC NULLS LAST\n            -> TableScan[0]\n                Table: tpch.region\n                Columns: r:bigint = r_regionkey\n```\n\nPartialTopN plan node wraps FinalTopN plan, which should be opposite.\nI think DistributedLogicalPlanner.visitTopN is creating wrong TopNNode combination.", "NaN"], ["1386", "Allow checking if catalogs are loaded", "David Phillips", "electrum", "06/13/14, 06:26:22 PM", "NaN", "NaN"], ["1387", "Add explicit Guice binder for jax-rs services", "Dain Sundstrom", "dain", "07/01/14, 08:35:15 PM", "NaN", "NaN"], ["1388", "Add documentation for first_value, last_value and nth_value window functions", "Dain Sundstrom", "dain", "06/13/14, 09:03:28 PM", "NaN", "NaN"], ["1389", "Add 0.70 release notes", "Dain Sundstrom", "dain", "06/13/14, 11:22:55 PM", "NaN", "NaN"], ["1390", "Document GREATEST/LEAST", "Martin Traverso", "martint", "06/13/14, 11:21:35 PM", "NaN", "NaN"], ["1391", "Release notes for 0.70", "Christopher Berner", "cberner", "06/13/14, 11:16:05 PM", "NaN", "NaN"], ["1392", "Add view tests for unknown schema", "David Phillips", "electrum", "06/14/14, 12:44:11 AM", "NaN", "NaN"], ["1393", "More refactoring of aggregation framework", "Christopher Berner", "cberner", "06/23/14, 06:42:53 PM", "Simplify serialization of state, which will allow the approximate aggregations to be ported to the new framework as well.\n\nBenchmark results look unchanged, as expected:\n\nmaster:\n                          count_agg ::   50.170 cpu ms :: in  1.5M,  12.9MB,   29.9M/s,   257MB/s :: out     1,      9B,      19/s,    179B/s\n                     double_sum_agg ::   81.820 cpu ms :: in  1.5M,  12.9MB,   18.3M/s,   157MB/s :: out     1,      9B,      12/s,    109B/s\n                           hash_agg ::  237.592 cpu ms :: in  1.5M,  21.5MB,   6.31M/s,  90.3MB/s :: out     3,     45B,      12/s,    189B/s\n                   predicate_filter ::  107.258 cpu ms :: in  1.5M,  12.9MB,     14M/s,   120MB/s :: out 1.29M,  11.1MB,     12M/s,   103MB/s\n                         raw_stream ::   51.917 cpu ms :: in  1.5M,  12.9MB,   28.9M/s,   248MB/s :: out  1.5M,  12.9MB,   28.9M/s,   248MB/s\n                             top100 ::   82.947 cpu ms :: in  1.5M,  12.9MB,   18.1M/s,   155MB/s :: out   100,    900B,   1.21K/s,  10.6KB/s\n             in_memory_orderby_1.5M :: 1619.428 cpu ms :: in  1.5M,  41.5MB,    926K/s,  25.6MB/s :: out  1.5M,  28.6MB,    926K/s,  17.7MB/s\n                         hash_build ::  452.355 cpu ms :: in  1.5M,  25.7MB,   3.32M/s,  56.9MB/s :: out  1.5M,  25.7MB,   3.32M/s,  56.9MB/s\n                          hash_join :: 1754.806 cpu ms :: in    6M,   103MB,   3.42M/s,  58.7MB/s :: out    6M,   206MB,   3.42M/s,   117MB/s\n                hash_build_and_join :: 2215.103 cpu ms :: in  7.5M,   129MB,   3.39M/s,  58.1MB/s :: out    6M,   206MB,   2.71M/s,    93MB/s\n                  hand_tpch_query_1 :: 4192.773 cpu ms :: in    6M,   361MB,   1.43M/s,    86MB/s :: out     4,    300B,       0/s,     71B/s\n                  hand_tpch_query_6 ::  882.611 cpu ms :: in    6M,   240MB,    6.8M/s,   272MB/s :: out     1,      9B,       1/s,     10B/s\n    sql_groupby_agg_with_arithmetic :: 1529.683 cpu ms :: in    6M,   137MB,   3.92M/s,  89.8MB/s :: out     2,     30B,       1/s,     19B/s\n                      sql_count_agg ::   51.709 cpu ms :: in  1.5M,  12.9MB,     29M/s,   249MB/s :: out     1,      9B,      19/s,    174B/s\n                 sql_double_sum_agg ::   82.969 cpu ms :: in  1.5M,  12.9MB,   18.1M/s,   155MB/s :: out     1,      9B,      12/s,    108B/s\n              sql_count_with_filter ::  104.301 cpu ms :: in  1.5M,  8.58MB,   14.4M/s,  82.3MB/s :: out     1,      9B,       9/s,     86B/s\n                    sql_groupby_agg ::  238.003 cpu ms :: in  1.5M,  21.5MB,    6.3M/s,  90.2MB/s :: out     3,     45B,      12/s,    189B/s\n               sql_predicate_filter ::   87.894 cpu ms :: in  1.5M,  12.9MB,   17.1M/s,   146MB/s :: out 1.29M,  11.1MB,   14.7M/s,   126MB/s\n                     sql_raw_stream ::   53.471 cpu ms :: in  1.5M,  12.9MB,   28.1M/s,   241MB/s :: out  1.5M,  12.9MB,   28.1M/s,   241MB/s\n                        sql_top_100 ::   83.675 cpu ms :: in  1.5M,  12.9MB,   17.9M/s,   154MB/s :: out   100,    900B,    1.2K/s,  10.5KB/s\n                      sql_hash_join :: 3648.007 cpu ms :: in  7.5M,   129MB,   2.06M/s,  35.3MB/s :: out    6M,   206MB,   1.65M/s,  56.5MB/s\n            sql_join_with_predicate :: 1066.835 cpu ms :: in  7.5M,   116MB,   7.03M/s,   109MB/s :: out     1,      9B,       0/s,      8B/s\n                  sql_varbinary_max ::  374.124 cpu ms :: in    6M,  97.3MB,     16M/s,   260MB/s :: out     1,     21B,       2/s,     56B/s\n                 sql_distinct_multi ::  264.386 cpu ms :: in  1.5M,    32MB,   5.67M/s,   121MB/s :: out     5,    112B,      18/s,    423B/s\n                sql_distinct_single ::  120.448 cpu ms :: in  1.5M,  12.9MB,   12.5M/s,   107MB/s :: out     1,      9B,       8/s,     74B/s\n                   sql_tpch_query_1 :: 4378.585 cpu ms :: in    6M,   361MB,   1.37M/s,  82.3MB/s :: out     4,    336B,       0/s,     76B/s\n                   sql_tpch_query_6 ::  857.912 cpu ms :: in    6M,   240MB,      7M/s,   280MB/s :: out     1,      9B,       1/s,     10B/s\n                           sql_like :: 3290.110 cpu ms :: in    6M,   232MB,   1.82M/s,  70.4MB/s :: out 1.15M,  9.84MB,    349K/s,  2.99MB/s\n                             sql_in ::  211.881 cpu ms :: in    6M,  51.5MB,   28.3M/s,   243MB/s :: out    25,    225B,     117/s,  1.04KB/s\n                    sql_semijoin_in :: 2004.108 cpu ms :: in  7.5M,  64.4MB,   3.74M/s,  32.1MB/s :: out    3M,  25.8MB,    1.5M/s,  12.9MB/s\n                    sql_regexp_like :: 2038.071 cpu ms :: in  1.5M,  76.6MB,    736K/s,  37.6MB/s :: out     1,      9B,       0/s,      4B/s\n         sql_approx_percentile_long ::  630.795 cpu ms :: in  1.5M,  12.9MB,   2.38M/s,  20.4MB/s :: out     1,      9B,       1/s,     14B/s\n                   sql_between_long ::   89.900 cpu ms :: in  1.5M,  12.9MB,   16.7M/s,   143MB/s :: out     1,      9B,      11/s,    100B/s\n                 stat_long_variance ::   70.905 cpu ms :: in  1.5M,  12.9MB,   21.2M/s,   182MB/s :: out     1,      9B,      14/s,    126B/s\n             stat_long_variance_pop ::   71.026 cpu ms :: in  1.5M,  12.9MB,   21.1M/s,   181MB/s :: out     1,      9B,      14/s,    126B/s\n               stat_double_variance ::   68.704 cpu ms :: in  1.5M,  12.9MB,   21.8M/s,   187MB/s :: out     1,      9B,      14/s,    130B/s\n           stat_double_variance_pop ::   68.713 cpu ms :: in  1.5M,  12.9MB,   21.8M/s,   187MB/s :: out     1,      9B,      14/s,    130B/s\n                   stat_long_stddev ::   70.825 cpu ms :: in  1.5M,  12.9MB,   21.2M/s,   182MB/s :: out     1,      9B,      14/s,    127B/s\n               stat_long_stddev_pop ::   70.617 cpu ms :: in  1.5M,  12.9MB,   21.2M/s,   182MB/s :: out     1,      9B,      14/s,    127B/s\n                 stat_double_stddev ::   68.808 cpu ms :: in  1.5M,  12.9MB,   21.8M/s,   187MB/s :: out     1,      9B,      14/s,    130B/s\n             stat_double_stddev_pop ::   68.520 cpu ms :: in  1.5M,  12.9MB,   21.9M/s,   188MB/s :: out     1,      9B,      14/s,    131B/s\n     sql_approx_count_distinct_long ::   76.101 cpu ms :: in  1.5M,  12.9MB,   19.7M/s,   169MB/s :: out     1,      9B,      13/s,    118B/s\n   sql_approx_count_distinct_double ::   78.641 cpu ms :: in  1.5M,  12.9MB,   19.1M/s,   164MB/s :: out     1,      9B,      12/s,    114B/s\nsql_approx_count_distinct_varbinary ::  111.079 cpu ms :: in  1.5M,  21.5MB,   13.5M/s,   193MB/s :: out     1,      9B,       9/s,     81B/s\n\nmy branch:\n                          count_agg ::   50.360 cpu ms :: in  1.5M,  12.9MB,   29.8M/s,   256MB/s :: out     1,      9B,      19/s,    178B/s\n                     double_sum_agg ::   82.227 cpu ms :: in  1.5M,  12.9MB,   18.2M/s,   157MB/s :: out     1,      9B,      12/s,    109B/s\n                           hash_agg ::  231.548 cpu ms :: in  1.5M,  21.5MB,   6.48M/s,  92.7MB/s :: out     3,     45B,      12/s,    194B/s\n                   predicate_filter ::  113.306 cpu ms :: in  1.5M,  12.9MB,   13.2M/s,   114MB/s :: out 1.29M,  11.1MB,   11.4M/s,  97.6MB/s\n                         raw_stream ::   53.801 cpu ms :: in  1.5M,  12.9MB,   27.9M/s,   239MB/s :: out  1.5M,  12.9MB,   27.9M/s,   239MB/s\n                             top100 ::   93.899 cpu ms :: in  1.5M,  12.9MB,     16M/s,   137MB/s :: out   100,    900B,   1.06K/s,  9.36KB/s\n             in_memory_orderby_1.5M :: 1640.738 cpu ms :: in  1.5M,  41.5MB,    914K/s,  25.3MB/s :: out  1.5M,  28.6MB,    914K/s,  17.4MB/s\n                         hash_build ::  443.263 cpu ms :: in  1.5M,  25.7MB,   3.38M/s,  58.1MB/s :: out  1.5M,  25.7MB,   3.38M/s,  58.1MB/s\n                          hash_join :: 1725.499 cpu ms :: in    6M,   103MB,   3.48M/s,  59.7MB/s :: out    6M,   206MB,   3.48M/s,   119MB/s\n                hash_build_and_join :: 2171.789 cpu ms :: in  7.5M,   129MB,   3.45M/s,  59.3MB/s :: out    6M,   206MB,   2.76M/s,  94.9MB/s\n                  hand_tpch_query_1 :: 4021.287 cpu ms :: in    6M,   361MB,   1.49M/s,  89.7MB/s :: out     4,    300B,       0/s,     74B/s\n                  hand_tpch_query_6 ::  879.131 cpu ms :: in    6M,   240MB,   6.83M/s,   273MB/s :: out     1,      9B,       1/s,     10B/s\n    sql_groupby_agg_with_arithmetic :: 1505.557 cpu ms :: in    6M,   137MB,   3.99M/s,  91.2MB/s :: out     2,     30B,       1/s,     19B/s\n                      sql_count_agg ::   51.752 cpu ms :: in  1.5M,  12.9MB,     29M/s,   249MB/s :: out     1,      9B,      19/s,    173B/s\n                 sql_double_sum_agg ::   83.592 cpu ms :: in  1.5M,  12.9MB,   17.9M/s,   154MB/s :: out     1,      9B,      11/s,    107B/s\n              sql_count_with_filter ::   90.698 cpu ms :: in  1.5M,  8.58MB,   16.5M/s,  94.6MB/s :: out     1,      9B,      11/s,     99B/s\n                    sql_groupby_agg ::  226.872 cpu ms :: in  1.5M,  21.5MB,   6.61M/s,  94.6MB/s :: out     3,     45B,      13/s,    198B/s\n               sql_predicate_filter ::   87.474 cpu ms :: in  1.5M,  12.9MB,   17.1M/s,   147MB/s :: out 1.29M,  11.1MB,   14.7M/s,   126MB/s\n                     sql_raw_stream ::   53.081 cpu ms :: in  1.5M,  12.9MB,   28.3M/s,   243MB/s :: out  1.5M,  12.9MB,   28.3M/s,   243MB/s\n                        sql_top_100 ::   84.713 cpu ms :: in  1.5M,  12.9MB,   17.7M/s,   152MB/s :: out   100,    900B,   1.18K/s,  10.4KB/s\n                      sql_hash_join :: 3572.770 cpu ms :: in  7.5M,   129MB,    2.1M/s,    36MB/s :: out    6M,   206MB,   1.68M/s,  57.7MB/s\n            sql_join_with_predicate :: 1065.000 cpu ms :: in  7.5M,   116MB,   7.04M/s,   109MB/s :: out     1,      9B,       0/s,      8B/s\n                  sql_varbinary_max ::  367.233 cpu ms :: in    6M,  97.3MB,   16.3M/s,   265MB/s :: out     1,     21B,       2/s,     57B/s\n                 sql_distinct_multi ::  259.716 cpu ms :: in  1.5M,    32MB,   5.78M/s,   123MB/s :: out     5,    112B,      19/s,    431B/s\n                sql_distinct_single ::  117.946 cpu ms :: in  1.5M,  12.9MB,   12.7M/s,   109MB/s :: out     1,      9B,       8/s,     76B/s\n                   sql_tpch_query_1 :: 4180.204 cpu ms :: in    6M,   361MB,   1.44M/s,  86.3MB/s :: out     4,    336B,       0/s,     80B/s\n                   sql_tpch_query_6 ::  828.214 cpu ms :: in    6M,   240MB,   7.25M/s,   290MB/s :: out     1,      9B,       1/s,     10B/s\n                           sql_like :: 3205.596 cpu ms :: in    6M,   232MB,   1.87M/s,  72.3MB/s :: out 1.15M,  9.84MB,    358K/s,  3.07MB/s\n                             sql_in ::  209.179 cpu ms :: in    6M,  51.5MB,   28.7M/s,   246MB/s :: out    25,    225B,     119/s,  1.05KB/s\n                    sql_semijoin_in :: 1936.731 cpu ms :: in  7.5M,  64.4MB,   3.87M/s,  33.2MB/s :: out    3M,  25.8MB,   1.55M/s,  13.3MB/s\n                    sql_regexp_like :: 1919.071 cpu ms :: in  1.5M,  76.6MB,    782K/s,  39.9MB/s :: out     1,      9B,       0/s,      4B/s\n         sql_approx_percentile_long ::  598.998 cpu ms :: in  1.5M,  12.9MB,    2.5M/s,  21.5MB/s :: out     1,      9B,       1/s,     15B/s\n                   sql_between_long ::   92.386 cpu ms :: in  1.5M,  12.9MB,   16.2M/s,   139MB/s :: out     1,      9B,      10/s,     97B/s\n                 stat_long_variance ::   70.629 cpu ms :: in  1.5M,  12.9MB,   21.2M/s,   182MB/s :: out     1,      9B,      14/s,    127B/s\n             stat_long_variance_pop ::   70.358 cpu ms :: in  1.5M,  12.9MB,   21.3M/s,   183MB/s :: out     1,      9B,      14/s,    127B/s\n               stat_double_variance ::   68.072 cpu ms :: in  1.5M,  12.9MB,     22M/s,   189MB/s :: out     1,      9B,      14/s,    132B/s\n           stat_double_variance_pop ::   68.484 cpu ms :: in  1.5M,  12.9MB,   21.9M/s,   188MB/s :: out     1,      9B,      14/s,    131B/s\n                   stat_long_stddev ::   70.448 cpu ms :: in  1.5M,  12.9MB,   21.3M/s,   183MB/s :: out     1,      9B,      14/s,    127B/s\n               stat_long_stddev_pop ::   70.352 cpu ms :: in  1.5M,  12.9MB,   21.3M/s,   183MB/s :: out     1,      9B,      14/s,    127B/s\n                 stat_double_stddev ::   68.108 cpu ms :: in  1.5M,  12.9MB,     22M/s,   189MB/s :: out     1,      9B,      14/s,    132B/s\n             stat_double_stddev_pop ::   68.316 cpu ms :: in  1.5M,  12.9MB,     22M/s,   188MB/s :: out     1,      9B,      14/s,    131B/s\n     sql_approx_count_distinct_long ::   75.399 cpu ms :: in  1.5M,  12.9MB,   19.9M/s,   171MB/s :: out     1,      9B,      13/s,    119B/s\n   sql_approx_count_distinct_double ::   78.593 cpu ms :: in  1.5M,  12.9MB,   19.1M/s,   164MB/s :: out     1,      9B,      12/s,    114B/s\nsql_approx_count_distinct_varbinary ::  104.867 cpu ms :: in  1.5M,  21.5MB,   14.3M/s,   205MB/s :: out     1,      9B,       9/s,     85B/s", "NaN"], ["1394", "Update documentation for 0.70 release", "David Phillips", "electrum", "06/16/14, 05:21:36 PM", "NaN", "NaN"], ["1395", "Fix build: remove unused import", "Christopher Berner", "cberner", "06/14/14, 04:26:06 PM", "NaN", "NaN"], ["1397", "Update documentation", "David Phillips", "electrum", "06/16/14, 06:01:05 PM", "NaN", "NaN"], ["1398", "Fix listing columns from Presto views in Hive", "David Phillips", "electrum", "06/16/14, 06:38:35 PM", "NaN", "NaN"], ["1399", "Verifier", "Christopher Berner", "cberner", "06/18/14, 07:13:37 PM", "NaN", "NaN"], ["1400", "Add Hive distributed queries test and benchmarks", "Dain Sundstrom", "dain", "06/28/14, 06:40:05 AM", "NaN", "NaN"], ["1405", "Make ChannelSet immutable", "Dain Sundstrom", "dain", "07/01/14, 06:55:48 PM", "Only last commit is new.  Other commits are from #1379 \n\nRewrite ChannelSet using GroupByHash as a template\n\nBefore:\n\n```\nsql_semijoin_in :: 1905.036 cpu ms :: in  7.5M,  64.4MB,   3.94M/s,  33.8MB/s :: out    3M,  25.8MB,   1.58M/s,  13.5MB/s\n```\n\nAfter:\n\n```\nsql_semijoin_in :: 1222.121 cpu ms :: in  7.5M,  64.4MB,   6.14M/s,  52.7MB/s :: out    3M,  25.8MB,   2.46M/s,  21.1MB/s\n```", "NaN"], ["1406", "Fix outer index joins when multiple overlapping probe keys are supplied", "Eric Hwang", "erichwang", "06/18/14, 03:30:08 AM", "For example, when doing:\nA LEFT JOIN B ON a1 = b1 AND a2 = b1\nwe need to make sure that we only supply keys to the index lookup where a1 == a2 to get the correct semantics from the index. ", "NaN"], ["1417", "Fix build", "David Phillips", "electrum", "06/20/14, 02:45:30 AM", "NaN", "NaN"], ["1420", "Minor cleanup in ExpressionCompiler", "Martin Traverso", "martint", "06/28/14, 02:24:08 AM", "NaN", "NaN"], ["1421", "Add release notes for 0.71", "David Phillips", "electrum", "06/21/14, 02:07:55 AM", "NaN", "NaN"], ["1422", "Various cleanups", "David Phillips", "electrum", "06/24/14, 12:11:17 AM", "NaN", "NaN"], ["1423", "Cleanup aggregation function framework", "Christopher Berner", "cberner", "07/01/14, 09:46:59 PM", "NaN", "NaN"], ["1424", "Update to 0.9-SNAPSHOT presto-hive-apache", "Dain Sundstrom", "dain", "06/24/14, 05:46:46 AM", "Add test to verify entries in LazyMap with null key are ignored", "NaN"], ["1426", "Multipart upload support for Presto S3 file system implementation", "Nezih Yigitbasi", "nezihyigitbasi", "08/01/14, 10:13:08 PM", "PrestoS3FileSystem implementation currently uses the putObject call to upload objects to S3, which is inefficient in terms of resource utilization as it's a single threaded upload implementation. This pull request adds support for S3 client's multipart upload capability (see http://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html), which is a multithreaded upload implementation that will especially be useful for large objects.", "NaN"], ["1427", "Change verifier to skip queries that timeout", "Christopher Berner", "cberner", "06/25/14, 12:39:24 AM", "NaN", "NaN"], ["1428", "0.72 release notes", "Dain Sundstrom", "dain", "06/25/14, 02:19:13 AM", "NaN", "NaN"], ["1429", "Fix NPE in verifier", "Christopher Berner", "cberner", "06/25/14, 05:51:20 PM", "NaN", "NaN"], ["1432", "Remove Input class", "Christopher Berner", "cberner", "07/01/14, 07:45:29 PM", "NaN", "NaN"], ["1440", "Verify at least one worker is selected in scheduleFixedNodeCount", "Dain Sundstrom", "dain", "07/01/14, 06:59:44 PM", "NaN", "NaN"], ["1441", "HiveClient.listTableColumns exclude views instead of throwing HiveViewNotSupportedException", "Sadayuki Furuhashi", "frsyuki", "06/30/14, 07:01:58 PM", "Select from information_schema.columns fails with \"Hive views are not supported\" message if Hive has a VIEW.\nThis patch changes HiveClient.listTableColumns to exclude VIEW.\nRelated to https://github.com/treasure-data/prestogres/issues/12", "NaN"], ["1447", "Fetch all column handles in single call to metadata", "Dain Sundstrom", "dain", "06/30/14, 07:53:56 PM", "Current ConnectorMetadata implementations are not efficient at fetching single\ncolumns at a time, so for now switch back to fetching all column handles ahead\nof time.", "NaN"], ["1448", "Remove BlockCursor", "Dain Sundstrom", "dain", "07/02/14, 10:53:27 PM", "NaN", "NaN"], ["1449", "0.73 release notes", "Dain Sundstrom", "dain", "06/30/14, 10:18:22 PM", "NaN", "NaN"], ["1452", "Upgrade to Jax-rs 2.0 and Jersey 2.9.1", "Dain Sundstrom", "dain", "07/04/14, 01:24:33 AM", "NaN", "NaN"], ["1455", "Fix resource binding for embedded discovery.", "Henning Schmiedehausen", "hgschmie", "07/02/14, 05:22:40 PM", "Use the JaxrsBinder.\n\nFixes #1454.", "NaN"], ["1458", "Minor metastore updates", "David Phillips", "electrum", "07/03/14, 12:27:29 AM", "NaN", "NaN"], ["1459", "Fix NULLIF semantics", "Martin Traverso", "martint", "07/03/14, 12:44:52 AM", "NULLIF now correctly returns a result with the type of the first argument. For instance:\n\n```\nNULLIF(1.0, 2) => 1.0\nNULLIF(1, 2.0) => 1\n```\n\nPreviously, the type of the result was the common supertype of both arguments", "NaN"], ["1465", "Change column comment in DESC to be empty string instead of null", "Dain Sundstrom", "dain", "07/03/14, 07:59:21 PM", "NaN", "NaN"], ["1466", "Json path tokenizer", "Dain Sundstrom", "dain", "07/26/14, 07:35:07 PM", "NaN", "NaN"], ["1468", "Change blocks to not be type specific", "Dain Sundstrom", "dain", "07/29/14, 05:16:36 AM", "This change moves all type specific methods on `Block` and `BlockBuilder` to `Type` leaving blocks to be generic data transports.", "NaN"], ["1469", "Change \"verifier-test-\" to \"verifier-test:\"", "Christopher Berner", "cberner", "07/07/14, 07:32:15 PM", "NaN", "NaN"], ["1470", "Fix valueIsNull creation in Block.getRegion", "Dain Sundstrom", "dain", "07/07/14, 08:17:46 PM", "NaN", "NaN"], ["1477", "Optimize row_number window function ", "Nileema Shingte", "nileema", "08/13/14, 06:59:59 PM", "Identify the query shape with a filter on the row_number() and rewrite it for optimal execution.\nExamples of query shapes affected by this: \nhttps://gist.github.com/nileema/71d0099d804a10cb172b\n\nNote: Optimization applies only if the all the following criteria are met: \n1. There is a single row number function in the query \n2. The filter is a simple comparison with a literal (compound expressions are not supported in this revision) \n3. The filter operation is directly above the window  function", "NaN"], ["1480", "Fix analyzer issue when \"is not null\" is used in join clause", "Martin Traverso", "martint", "07/10/14, 05:40:09 PM", "Fixes #1479", "NaN"], ["1481", "Improve \"too many failures\" error message", "Martin Traverso", "martint", "07/10/14, 06:41:25 PM", "NaN", "NaN"], ["1483", "Add better feature normalizer to ML plugin", "Christopher Berner", "cberner", "07/11/14, 09:02:33 PM", "NaN", "NaN"], ["1484", "Add try_cast", "David Phillips", "electrum", "07/12/14, 12:08:23 AM", "NaN", "NaN"], ["1485", "Remove remnants of run length support", "Christopher Berner", "cberner", "07/24/14, 12:21:22 AM", "NaN", "NaN"], ["1486", "Remove unused logback-test.xml", "David Phillips", "electrum", "07/14/14, 06:31:23 PM", "NaN", "NaN"], ["1487", "Use Threads instead of ThreadFactoryBuilder", "David Phillips", "electrum", "07/14/14, 06:30:57 PM", "NaN", "NaN"], ["1488", "New expression tree and bytecode compiler", "Martin Traverso", "martint", "07/15/14, 10:57:00 PM", "First step in splitting SQL expression tree from expression IR", "NaN"], ["1489", "Add checkType that takes an error code", "Christopher Berner", "cberner", "08/06/14, 09:29:16 PM", "NaN", "NaN"], ["1490", "Fix IS NOT NULL example", "Dain Sundstrom", "dain", "07/14/14, 06:48:09 PM", "NaN", "NaN"], ["1492", "Add javax.annotation to PARENT_FIRST_CLASSES", "Nileema Shingte", "nileema", "07/14/14, 10:07:57 PM", "When this is omitted the PostConstruct methods are not called. ", "NaN"], ["1494", "Canonicalize expressions before analyzing join criteria", "Martin Traverso", "martint", "07/16/14, 10:53:14 PM", "The expression interpreter only understands a subset of expression types", "NaN"], ["1495", "Fix multithreading for Hive tests", "David Phillips", "electrum", "07/16/14, 08:55:20 PM", "NaN", "NaN"], ["1496", "Simplify generation of IN with huge lists", "Martin Traverso", "martint", "07/17/14, 05:25:05 PM", "Don't use a custom FunctionBinder. Instead, bind the set as a constant using invokeDynamic\nand emit an invokeStatic call to the helper \"in\" method.", "NaN"], ["1499", "Refactor verifier to use Guice", "Christopher Berner", "cberner", "07/21/14, 07:38:38 PM", "NaN", "NaN"], ["1503", "Improvements to aggregation framework", "Christopher Berner", "cberner", "07/29/14, 02:47:45 AM", "Benchmarks show a 10-30% performance gain\n\n```\nBenchmarks (master):\n                          count_agg ::    3.606 cpu ms :: in  1.5M,  12.9MB,    416M/s,  3.49GB/s :: out     1,  1.02KB,     277/s,   284KB/s\n                     double_sum_agg ::   26.284 cpu ms :: in  1.5M,  12.9MB,   57.1M/s,   490MB/s :: out     1,  1.02KB,      38/s,  38.9KB/s\n                           hash_agg ::  148.530 cpu ms :: in  1.5M,  23.4MB,   10.1M/s,   157MB/s :: out     3,  1.09KB,      20/s,  7.35KB/s\n                   predicate_filter ::   73.680 cpu ms :: in  1.5M,  12.9MB,   20.4M/s,   175MB/s :: out 1.29M,  11.3MB,   17.5M/s,   153MB/s\n                         raw_stream ::    3.631 cpu ms :: in  1.5M,  12.9MB,    413M/s,  3.46GB/s :: out  1.5M,  12.9MB,    413M/s,  3.46GB/s\n                             top100 ::   27.935 cpu ms :: in  1.5M,  12.9MB,   53.7M/s,   461MB/s :: out   100,   1.8KB,   3.58K/s,  64.3KB/s\n             in_memory_orderby_1.5M :: 1504.026 cpu ms :: in  1.5M,  50.1MB,    997K/s,  33.3MB/s :: out  1.5M,  28.6MB,    997K/s,    19MB/s\n                         hash_build ::  320.783 cpu ms :: in  1.5M,  25.8MB,   4.68M/s,  80.3MB/s :: out  1.5M,  25.8MB,   4.68M/s,  80.3MB/s\n                          hash_join :: 1235.543 cpu ms :: in    6M,   103MB,   4.86M/s,  83.4MB/s :: out    6M,   229MB,   4.86M/s,   185MB/s\n                hash_build_and_join :: 1611.270 cpu ms :: in  7.5M,   129MB,   4.66M/s,  79.9MB/s :: out    6M,   229MB,   3.72M/s,   142MB/s\n                  hand_tpch_query_1 :: 2710.860 cpu ms :: in    6M,   458MB,   2.21M/s,   169MB/s :: out     4,  7.45KB,       1/s,  2.75KB/s\n                  hand_tpch_query_6 ::  273.196 cpu ms :: in    6M,   271MB,     22M/s,   992MB/s :: out     1,  1.02KB,       3/s,  3.75KB/s\n    sql_groupby_agg_with_arithmetic ::  931.339 cpu ms :: in    6M,   145MB,   6.44M/s,   156MB/s :: out     2,  1.08KB,       2/s,  1.16KB/s\n                      sql_count_agg ::    5.100 cpu ms :: in  1.5M,  12.9MB,    294M/s,  2.47GB/s :: out     1,  1.02KB,     196/s,   201KB/s\n                 sql_double_sum_agg ::   27.523 cpu ms :: in  1.5M,  12.9MB,   54.5M/s,   468MB/s :: out     1,  1.02KB,      36/s,  37.2KB/s\n              sql_count_with_filter ::   66.517 cpu ms :: in  1.5M,  8.59MB,   22.6M/s,   129MB/s :: out     1,  1.02KB,      15/s,  15.4KB/s\n                    sql_groupby_agg ::  148.829 cpu ms :: in  1.5M,  23.4MB,   10.1M/s,   157MB/s :: out     3,  1.09KB,      20/s,  7.33KB/s\n               sql_predicate_filter ::   32.651 cpu ms :: in  1.5M,  12.9MB,   45.9M/s,   394MB/s :: out 1.29M,  11.3MB,   39.5M/s,   345MB/s\n                     sql_raw_stream ::    4.794 cpu ms :: in  1.5M,  12.9MB,    313M/s,  2.62GB/s :: out  1.5M,  12.9MB,    313M/s,  2.62GB/s\n                        sql_top_100 ::   28.930 cpu ms :: in  1.5M,  12.9MB,   51.8M/s,   445MB/s :: out   100,   1.8KB,   3.45K/s,  62.1KB/s\n                      sql_hash_join :: 2799.406 cpu ms :: in  7.5M,   129MB,   2.68M/s,    46MB/s :: out    6M,   229MB,   2.14M/s,  81.8MB/s\n            sql_join_with_predicate ::  588.991 cpu ms :: in  7.5M,   116MB,   12.7M/s,   197MB/s :: out     1,  1.02KB,       1/s,  1.74KB/s\n                  sql_varbinary_max ::  246.238 cpu ms :: in    6M,  97.3MB,   24.4M/s,   395MB/s :: out     1,     57B,       4/s,    231B/s\n                 sql_distinct_multi ::  188.266 cpu ms :: in  1.5M,  39.2MB,   7.97M/s,   208MB/s :: out     5,  1.16KB,      26/s,  6.14KB/s\n                sql_distinct_single ::   85.986 cpu ms :: in  1.5M,  12.9MB,   17.4M/s,   150MB/s :: out     1,  1.02KB,      11/s,  11.9KB/s\n                   sql_tpch_query_1 :: 2749.494 cpu ms :: in    6M,   458MB,   2.18M/s,   167MB/s :: out     4,  8.49KB,       1/s,  3.09KB/s\n                   sql_tpch_query_6 ::  211.896 cpu ms :: in    6M,   271MB,   28.3M/s,  1.25GB/s :: out     1,  1.02KB,       4/s,  4.83KB/s\n                           sql_like :: 2945.558 cpu ms :: in    6M,   270MB,   2.04M/s,  91.8MB/s :: out 1.15M,  10.9MB,    389K/s,   3.7MB/s\n                             sql_in ::   49.545 cpu ms :: in    6M,  51.5MB,    121M/s,  1.02GB/s :: out    25,  1.21KB,     504/s,  24.4KB/s\n                    sql_semijoin_in :: 1040.881 cpu ms :: in  7.5M,  64.4MB,   7.21M/s,  61.9MB/s :: out    3M,  27.5MB,   2.88M/s,  26.4MB/s\n                    sql_regexp_like :: 2288.169 cpu ms :: in  1.5M,  76.6MB,    656K/s,  33.5MB/s :: out     1,  1.02KB,       0/s,    458B/s\n         sql_approx_percentile_long ::  560.223 cpu ms :: in  1.5M,  12.9MB,   2.68M/s,    23MB/s :: out     1,  1.02KB,       1/s,  1.83KB/s\n                   sql_between_long ::   31.808 cpu ms :: in  1.5M,  12.9MB,   47.2M/s,   405MB/s :: out     1,  1.02KB,      31/s,  32.2KB/s\n                 stat_long_variance ::   23.990 cpu ms :: in  1.5M,  12.9MB,   62.5M/s,   537MB/s :: out     1,  1.02KB,      41/s,  42.7KB/s\n             stat_long_variance_pop ::   23.977 cpu ms :: in  1.5M,  12.9MB,   62.6M/s,   537MB/s :: out     1,  1.02KB,      41/s,  42.7KB/s\n               stat_double_variance ::   20.221 cpu ms :: in  1.5M,  12.9MB,   74.2M/s,   637MB/s :: out     1,  1.02KB,      49/s,  50.6KB/s\n           stat_double_variance_pop ::   20.209 cpu ms :: in  1.5M,  12.9MB,   74.2M/s,   637MB/s :: out     1,  1.02KB,      49/s,  50.6KB/s\n                   stat_long_stddev ::   23.884 cpu ms :: in  1.5M,  12.9MB,   62.8M/s,   539MB/s :: out     1,  1.02KB,      41/s,  42.8KB/s\n               stat_long_stddev_pop ::   23.878 cpu ms :: in  1.5M,  12.9MB,   62.8M/s,   539MB/s :: out     1,  1.02KB,      41/s,  42.9KB/s\n                 stat_double_stddev ::   20.006 cpu ms :: in  1.5M,  12.9MB,     75M/s,   644MB/s :: out     1,  1.02KB,      49/s,  51.2KB/s\n             stat_double_stddev_pop ::   20.205 cpu ms :: in  1.5M,  12.9MB,   74.2M/s,   637MB/s :: out     1,  1.02KB,      49/s,  50.7KB/s\n     sql_approx_count_distinct_long ::   24.346 cpu ms :: in  1.5M,  12.9MB,   61.6M/s,   529MB/s :: out     1,  1.02KB,      41/s,    42KB/s\n   sql_approx_count_distinct_double ::   26.044 cpu ms :: in  1.5M,  12.9MB,   57.6M/s,   494MB/s :: out     1,  1.02KB,      38/s,  39.3KB/s\nJul 21, 2014 11:57:50 AM io.airlift.log.Logger info\nsql_approx_count_distinct_varbinary ::   75.750 cpu ms :: in  1.5M,  21.5MB,   19.8M/s,   283MB/s :: out     1,  1.02KB,      13/s,  13.5KB/s\n\n\nBenchmarks on my branch:\n                          count_agg ::    3.584 cpu ms :: in  1.5M,  12.9MB,    418M/s,  3.51GB/s :: out     1,  1.02KB,     278/s,   286KB/s\n                     double_sum_agg ::   18.778 cpu ms :: in  1.5M,  12.9MB,   79.9M/s,   686MB/s :: out     1,  1.02KB,      53/s,  54.5KB/s\n                           hash_agg ::  136.159 cpu ms :: in  1.5M,  23.4MB,     11M/s,   172MB/s :: out     3,  1.09KB,      22/s,  8.02KB/s\n                   predicate_filter ::   71.093 cpu ms :: in  1.5M,  12.9MB,   21.1M/s,   181MB/s :: out 1.29M,  11.3MB,   18.1M/s,   158MB/s\n                         raw_stream ::    3.500 cpu ms :: in  1.5M,  12.9MB,    429M/s,  3.59GB/s :: out  1.5M,  12.9MB,    429M/s,  3.59GB/s\n                             top100 ::   28.027 cpu ms :: in  1.5M,  12.9MB,   53.5M/s,   459MB/s :: out   100,   1.8KB,   3.57K/s,  64.1KB/s\n             in_memory_orderby_1.5M :: 1469.982 cpu ms :: in  1.5M,  50.1MB,   1.02M/s,  34.1MB/s :: out  1.5M,  28.6MB,   1.02M/s,  19.5MB/s\n                         hash_build ::  330.285 cpu ms :: in  1.5M,  25.8MB,   4.54M/s,    78MB/s :: out  1.5M,  25.8MB,   4.54M/s,    78MB/s\n                          hash_join :: 1260.090 cpu ms :: in    6M,   103MB,   4.76M/s,  81.8MB/s :: out    6M,   229MB,   4.76M/s,   182MB/s\n                hash_build_and_join :: 1607.804 cpu ms :: in  7.5M,   129MB,   4.67M/s,  80.1MB/s :: out    6M,   229MB,   3.73M/s,   142MB/s\n                  hand_tpch_query_1 :: 2544.824 cpu ms :: in    6M,   458MB,   2.36M/s,   180MB/s :: out     4,  7.45KB,       1/s,  2.92KB/s\n                  hand_tpch_query_6 ::  276.046 cpu ms :: in    6M,   271MB,   21.7M/s,   982MB/s :: out     1,  1.02KB,       3/s,  3.71KB/s\n    sql_groupby_agg_with_arithmetic ::  867.701 cpu ms :: in    6M,   145MB,   6.92M/s,   167MB/s :: out     2,  1.08KB,       2/s,  1.24KB/s\n                      sql_count_agg ::    4.525 cpu ms :: in  1.5M,  12.9MB,    332M/s,  2.78GB/s :: out     1,  1.02KB,     221/s,   226KB/s\n                 sql_double_sum_agg ::   19.782 cpu ms :: in  1.5M,  12.9MB,   75.8M/s,   651MB/s :: out     1,  1.02KB,      50/s,  51.7KB/s\n              sql_count_with_filter ::   63.298 cpu ms :: in  1.5M,  8.59MB,   23.7M/s,   136MB/s :: out     1,  1.02KB,      15/s,  16.2KB/s\n                    sql_groupby_agg ::  138.630 cpu ms :: in  1.5M,  23.4MB,   10.8M/s,   169MB/s :: out     3,  1.09KB,      21/s,  7.88KB/s\n               sql_predicate_filter ::   27.900 cpu ms :: in  1.5M,  12.9MB,   53.8M/s,   462MB/s :: out 1.29M,  11.3MB,   46.2M/s,   404MB/s\n                     sql_raw_stream ::    4.854 cpu ms :: in  1.5M,  12.9MB,    309M/s,  2.59GB/s :: out  1.5M,  12.9MB,    309M/s,  2.59GB/s\n                        sql_top_100 ::   27.996 cpu ms :: in  1.5M,  12.9MB,   53.6M/s,   460MB/s :: out   100,   1.8KB,   3.57K/s,  64.2KB/s\n                      sql_hash_join :: 2751.243 cpu ms :: in  7.5M,   129MB,   2.73M/s,  46.8MB/s :: out    6M,   229MB,   2.18M/s,  83.2MB/s\n            sql_join_with_predicate ::  577.633 cpu ms :: in  7.5M,   116MB,     13M/s,   201MB/s :: out     1,  1.02KB,       1/s,  1.77KB/s\n                  sql_varbinary_max ::  213.180 cpu ms :: in    6M,  97.3MB,   28.2M/s,   457MB/s :: out     1,     57B,       4/s,    267B/s\n                 sql_distinct_multi ::  164.355 cpu ms :: in  1.5M,  39.2MB,   9.13M/s,   239MB/s :: out     5,  1.16KB,      30/s,  7.03KB/s\n                sql_distinct_single ::   85.907 cpu ms :: in  1.5M,  12.9MB,   17.5M/s,   150MB/s :: out     1,  1.02KB,      11/s,  11.9KB/s\n                   sql_tpch_query_1 :: 2580.721 cpu ms :: in    6M,   458MB,   2.33M/s,   178MB/s :: out     4,  8.49KB,       1/s,  3.29KB/s\n                   sql_tpch_query_6 ::  206.511 cpu ms :: in    6M,   271MB,   29.1M/s,  1.28GB/s :: out     1,  1.02KB,       4/s,  4.96KB/s\n                           sql_like :: 2841.709 cpu ms :: in    6M,   270MB,   2.11M/s,  95.2MB/s :: out 1.15M,  10.9MB,    404K/s,  3.83MB/s\n                             sql_in ::   48.315 cpu ms :: in    6M,  51.5MB,    124M/s,  1.04GB/s :: out    25,  1.21KB,     517/s,  25.1KB/s\n                    sql_semijoin_in :: 1024.990 cpu ms :: in  7.5M,  64.4MB,   7.32M/s,  62.8MB/s :: out    3M,  27.5MB,   2.93M/s,  26.8MB/s\n                    sql_regexp_like :: 2268.136 cpu ms :: in  1.5M,  76.6MB,    661K/s,  33.8MB/s :: out     1,  1.02KB,       0/s,    462B/s\n         sql_approx_percentile_long ::  576.011 cpu ms :: in  1.5M,  12.9MB,    2.6M/s,  22.4MB/s :: out     1,  1.02KB,       1/s,  1.78KB/s\n                   sql_between_long ::   31.243 cpu ms :: in  1.5M,  12.9MB,     48M/s,   412MB/s :: out     1,  1.02KB,      32/s,  32.8KB/s\n                 stat_long_variance ::   23.549 cpu ms :: in  1.5M,  12.9MB,   63.7M/s,   547MB/s :: out     1,  1.02KB,      42/s,  43.5KB/s\n             stat_long_variance_pop ::   23.575 cpu ms :: in  1.5M,  12.9MB,   63.6M/s,   546MB/s :: out     1,  1.02KB,      42/s,  43.4KB/s\n               stat_double_variance ::   19.829 cpu ms :: in  1.5M,  12.9MB,   75.6M/s,   649MB/s :: out     1,  1.02KB,      50/s,  51.6KB/s\n           stat_double_variance_pop ::   20.099 cpu ms :: in  1.5M,  12.9MB,   74.6M/s,   641MB/s :: out     1,  1.02KB,      49/s,  50.9KB/s\n                   stat_long_stddev ::   23.630 cpu ms :: in  1.5M,  12.9MB,   63.5M/s,   545MB/s :: out     1,  1.02KB,      42/s,  43.3KB/s\n               stat_long_stddev_pop ::   23.619 cpu ms :: in  1.5M,  12.9MB,   63.5M/s,   545MB/s :: out     1,  1.02KB,      42/s,  43.3KB/s\n                 stat_double_stddev ::   19.740 cpu ms :: in  1.5M,  12.9MB,     76M/s,   652MB/s :: out     1,  1.02KB,      50/s,  51.8KB/s\n             stat_double_stddev_pop ::   19.748 cpu ms :: in  1.5M,  12.9MB,     76M/s,   652MB/s :: out     1,  1.02KB,      50/s,  51.8KB/s\n     sql_approx_count_distinct_long ::   23.760 cpu ms :: in  1.5M,  12.9MB,   63.1M/s,   542MB/s :: out     1,  1.02KB,      42/s,  43.1KB/s\n   sql_approx_count_distinct_double ::   25.405 cpu ms :: in  1.5M,  12.9MB,     59M/s,   507MB/s :: out     1,  1.02KB,      39/s,  40.3KB/s\nsql_approx_count_distinct_varbinary ::   74.604 cpu ms :: in  1.5M,  21.5MB,   20.1M/s,   288MB/s :: out     1,  1.02KB,      13/s,  13.7KB/s\n```", "NaN"], ["1505", "Update to slice 0.7", "Martin Traverso", "martint", "07/22/14, 07:29:14 PM", "NaN", "NaN"], ["1506", "Async task response", "Dain Sundstrom", "dain", "08/30/14, 01:27:12 AM", "Change task info and results to use jax-rs async http responses.", "NaN"], ["1507", "Add release notes for 0.74", "Martin Traverso", "martint", "07/23/14, 01:38:30 AM", "NaN", "NaN"], ["1509", "Add support for JDBC binary types", "Dain Sundstrom", "dain", "07/25/14, 01:21:40 AM", "NaN", "NaN"], ["1510", "Add option to disable checksum verification for HDFS", "Martin Traverso", "martint", "07/25/14, 01:47:05 AM", "The option is hive.dfs.verify-checksum and it defaults to \"true\". ", "NaN"], ["1511", "Add 'Kill' button to query.html", "Sadayuki Furuhashi", "frsyuki", "07/25/14, 12:44:45 AM", "This patch adds 'Kill' button to query.html page.\n\nThe button looks like this if the query is running:\n![running](http://gyazo.com/ea79154feecb520895a940f476cab4a6.png)\n\nWhen you click the button and reloads the page:\n![killed](http://gyazo.com/e1f9721086d0aedba4baf0982a7f4b30.png)", "NaN"], ["1512", "Upgrade to Discovery 1.20-SNAPSHOT", "Dain Sundstrom", "dain", "07/26/14, 04:58:46 AM", "NaN", "NaN"], ["1513", "Add support for HLL to be returned in a string encoded format", "Eric Hwang", "erichwang", "07/28/14, 06:54:17 AM", "NaN", "NaN"], ["1516", "Rewrite count(constant) to count(*)", "Dain Sundstrom", "dain", "07/28/14, 07:52:26 PM", "This adds an optimizer to replace `count(constant)` with `count(*)` since it is much cheaper to compute.  This results in the removal of the projection of the expression, but does not end up removing an empty projection.  This is because the `PruneRedundantProjections` will not remove a projection that \"constrains the output tuple from the underlying operator\", so we end up with the following plan:\n\n```\nSELECT COUNT(42) FROM ORDERS\n\n- Output[_col0]\n        _col0 := count\n    - Aggregate => [count:bigint]\n            count := \"count\"(*)\n        - Project => []\n            - TableScan[local:tpch:orders:sf0.01, original constraint=true] => [orderkey:bigint]\n                    orderkey := local:tpch:orderkey:0\n```\n\nI'm not sure why we ever end up with the `orderkey` symbol in the first place, since we can support empty source operators.  Once that is fixed the empty projection should be eliminated.", "NaN"], ["1519", "Remove automatic column addition for empty scan", "Dain Sundstrom", "dain", "07/29/14, 11:00:46 PM", "Originally a column had to be selected to scan a table, so we always added a \"cheap\" column to scan.  When we removed this restriction we forgot to remove the code that automatically added a column to an empty table scan. For the Raptor connector, we must still choose a \"cheap\" column, but this is an internal detail of the Raptor connector.", "NaN"], ["1521", "Add table rename support", "David Phillips", "electrum", "08/08/14, 04:13:43 PM", "Syntax: `ALTER TABLE old RENAME TO new`\n\nThis feature is disabled by default for the Hive connector.\nEnable by setting `hive.allow-rename-table` to `true`.", "NaN"], ["1522", "Extract base classes for Type", "Dain Sundstrom", "dain", "08/06/14, 09:24:02 PM", "NaN", "NaN"], ["1528", "Use aggregation classloader", "Christopher Berner", "cberner", "07/31/14, 08:13:34 PM", "This fixes a ClassNotFound error when compiling aggregations from a plugin", "NaN"], ["1529", "Fix StateCompiler to use classloader of interface", "Christopher Berner", "cberner", "08/01/14, 10:13:09 PM", "Fixes an issue loading the ML plugin", "NaN"], ["1531", "Implement MAX_BY and further improvements to aggregations", "Christopher Berner", "cberner", "08/07/14, 11:58:50 PM", "NaN", "NaN"], ["1537", "Add initial insert support", "David Phillips", "electrum", "08/08/14, 10:36:55 PM", "NaN", "NaN"], ["1539", "Allow Verifier to specify the jdbc driver for control and test", "Nileema Shingte", "nileema", "08/19/14, 05:08:28 AM", "Ran this on the impala cluster that I set up. Verifier completed successfully, however I need to look into the results. ", "NaN"], ["1543", "Fix LIKE for values with multiple lines", "David Phillips", "electrum", "08/09/14, 12:32:43 AM", "NaN", "NaN"], ["1546", "Final cleanup of aggregation framework", "Christopher Berner", "cberner", "08/26/14, 09:54:17 PM", "NaN", "NaN"], ["1550", "Fix CROSS JOIN bug", "Eric Hwang", "erichwang", "08/13/14, 09:17:27 PM", "The main fix is in: \npresto-main/src/main/java/com/facebook/presto/sql/planner/optimizations/PredicatePushDown.java", "NaN"], ["1552", "Add support for distributed index joins", "Eric Hwang", "erichwang", "08/22/14, 08:06:09 PM", "This is enabled via a config option and is default off.", "NaN"], ["1553", "Update documentation", "David Phillips", "electrum", "08/19/14, 01:25:36 AM", "NaN", "NaN"], ["1556", "Add support for scalar functions that accept nulls", "Christopher Berner", "cberner", "08/27/14, 12:26:49 AM", "NaN", "NaN"], ["1558", "Fix parsing of UNION", "Christopher Berner", "cberner", "08/15/14, 10:28:19 PM", "Previously SELECT 123 UNION DISTINCT SELECT 123 UNION ALL SELECT 123 was\nparsed as SELECT 123 UNION DISTINCT SELECT 123 UNION DISTINCT SELECT 123", "NaN"], ["1559", "Add test query for previously broken UNION query", "Christopher Berner", "cberner", "08/16/14, 12:15:15 AM", "NaN", "NaN"], ["1560", "Fix parsing of INTERSECT", "Christopher Berner", "cberner", "08/16/14, 06:31:26 PM", "NaN", "NaN"], ["1561", "Fix grouping semantics", "Dain Sundstrom", "dain", "08/20/14, 08:38:50 PM", "In the last release of Presto when we wanted equals or hashCode we would do something like this:\n\n``` java\n  block.hash(position)\n```\n\nIn the new code I changed all of these to:\n\n``` java\n type.hash(block, position)\n```\n\nThe problem is the block was handling nulls, but the Type code does not.  This pull request consolidates the uses of equals and hashCode for Type and updates these places to handle null.\n\nTo simplify the code generation I've added a ByteCodeExpression abstraction to the byte code library.  The abstraction is described in the commit.", "NaN"], ["1563", "Increase test timeout to 60secs to resolve random failures", "Christopher Berner", "cberner", "08/18/14, 07:11:01 PM", "NaN", "NaN"], ["1564", "Move accumulator state size to a static constant", "Christopher Berner", "cberner", "08/19/14, 12:53:35 AM", "NaN", "NaN"], ["1565", "Improve support for numeric fields in json extract", "Dain Sundstrom", "dain", "08/19/14, 05:36:51 PM", "Allow numeric fields without quoting in path expressions\nNumeric fields match both array elements and object fields", "NaN"], ["1566", "Add configuration for s3 max connections", "Dain Sundstrom", "dain", "08/19/14, 01:29:12 AM", "Change default s3 msx connections to 500 and make it configurable", "NaN"], ["1567", "Add 0.75 release notes", "Dain Sundstrom", "dain", "08/20/14, 12:29:14 AM", "NaN", "NaN"], ["1568", "Expose queryId in PrestoResultSet", "Eric Hwang", "erichwang", "08/19/14, 02:57:27 PM", "NaN", "NaN"], ["1569", "Release notes for 0.75 ", "Nileema Shingte", "nileema", "08/20/14, 10:09:04 PM", "NaN", "NaN"], ["1572", "Document MAX_BY and update 0.75 release notes", "Christopher Berner", "cberner", "08/20/14, 05:43:03 PM", "NaN", "NaN"], ["1573", "Allow underscore in json path", "Dain Sundstrom", "dain", "08/20/14, 08:41:50 PM", "NaN", "NaN"], ["1574", "Remove unnecessary .gitignore", "David Phillips", "electrum", "08/20/14, 06:34:21 PM", "NaN", "NaN"], ["1575", "Upgrade to Airlift 0.93 and Discovery 1.20", "Dain Sundstrom", "dain", "08/20/14, 10:46:32 PM", "NaN", "NaN"], ["1576", "Documentation updates", "David Phillips", "electrum", "08/21/14, 01:14:20 AM", "NaN", "NaN"], ["1578", "Allow ':' in JsonPath path segments but not subscripts", "Dain Sundstrom", "dain", "08/21/14, 06:01:02 PM", "NaN", "NaN"], ["1579", "Update JSON path", "David Phillips", "electrum", "08/21/14, 08:24:53 PM", "NaN", "NaN"], ["1580", "Index cache flush", "Eric Hwang", "erichwang", "08/22/14, 10:44:01 PM", "Will squash when committing", "NaN"], ["1581", "Add parametric types", "Christopher Berner", "cberner", "09/19/14, 06:05:56 PM", "NaN", "NaN"], ["1582", "Make chr() work with unicode", "Christopher Berner", "cberner", "08/25/14, 10:12:33 PM", "- Update docs to reference chr() instead of char()\n- Make chr() work with unicode code points instead of ascii code points to match the documentation", "NaN"], ["1588", "Allow binding multiple event clients", "Christopher Berner", "cberner", "08/27/14, 12:36:58 AM", "Allow extensions of the verifier to add their own event client\nimplementations, by not failing to start if an un-recognized event\nclient is found.", "NaN"], ["1590", "Add base JDBC and MySQL connectors", "David Phillips", "electrum", "08/29/14, 04:06:28 PM", "NaN", "NaN"], ["1592", "More byte code expressions", "Dain Sundstrom", "dain", "08/28/14, 08:04:03 PM", "NaN", "NaN"], ["1597", "Fix typo in stats computation", "Martin Traverso", "martint", "08/26/14, 10:22:12 PM", "NaN", "NaN"], ["1598", "Update airbase with new joda-time", "David Phillips", "electrum", "08/27/14, 01:28:39 AM", "NaN", "NaN"], ["1599", "Fix TimeZoneKey tests after updating index file", "David Phillips", "electrum", "08/27/14, 04:11:55 AM", "NaN", "NaN"], ["1600", "Fix cassandra timestamp type (from bigint to timestamp)", null, "toxeh", "08/27/14, 05:29:15 PM", "NaN", "NaN"], ["1602", "Make testDuplicateFunctions more robust", "Christopher Berner", "cberner", "08/27/14, 05:29:15 PM", "NaN", "NaN"], ["1603", "Update README for server tarball", "David Phillips", "electrum", "08/27/14, 06:13:37 PM", "NaN", "NaN"], ["1606", "Fix race condition in HttpRemoteTask", "Eric Hwang", "erichwang", "09/03/14, 06:18:56 PM", "There is a race condition in HttpRemoteTask where it can race on setting the value of the needsUpdate flag. The potential result of this is that we can silently drop a failed HTTP request and not notify any of the callers.", "NaN"], ["1607", "Remove getColumnHandle from ConnectorMetadata", "David Phillips", "electrum", "08/28/14, 09:07:44 PM", "NaN", "NaN"], ["1610", "Fix incorrect parsing of function arguments", "Martin Traverso", "martint", "08/30/14, 04:17:23 AM", "The current parser incorrectly allows function calls with the following syntax:\n\n```\nfoo(,1,2)\nfoo(DISTINCT ,x)\nfoo(DISTINCT)\n```", "NaN"], ["1613", "Skip call to get partitions if domain is NONE", "Martin Traverso", "martint", "08/30/14, 04:16:43 AM", "NaN", "NaN"], ["1615", "Remove unused parser rule", "Martin Traverso", "martint", "08/30/14, 06:52:56 AM", "NaN", "NaN"], ["1617", "JDBC connector fixes", "David Phillips", "electrum", "09/02/14, 01:24:21 AM", "NaN", "NaN"], ["1618", "Fix cut'n'paste error in the presto-server pom.", "Henning Schmiedehausen", "hgschmie", "09/02/14, 01:21:01 AM", "NaN", "NaN"], ["1623", "Throttle Presto concurrent queries", "Eric Hwang", "erichwang", "09/07/14, 01:36:18 AM", "Add a sanity throttle for the maximum number of concurrent queries in Presto.", "NaN"], ["1624", "Don't generate so many invokedynamic callsite names", "Martin Traverso", "martint", "09/04/14, 12:20:10 AM", "Java 7 doesn't seem to like too many unique callsite names in a single class (Java 8 works ok, though)\nIt causes execution for queries with many projections to fail with:\n\n```\nException in thread \"main\" java.lang.BootstrapMethodError: call site initialization exception\n    at java.lang.invoke.CallSite.makeSite(CallSite.java:298)\n    at java.lang.invoke.MethodHandleNatives.linkCallSite(MethodHandleNatives.java:295)\n    at Runnable_1.run404(Unknown Source)\n    at Runnable_1.run(Unknown Source)\n    at com.facebook.indytest.Main.main(Main.java:8)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)\nCaused by: java.lang.NullPointerException\n    at sun.invoke.util.ValueConversions.unboxInteger(ValueConversions.java:77)\n    at java.lang.invoke.CallSite.makeSite(CallSite.java:282)\n    ... 9 more\n```\n\nFor future reference, here's a stand-alone test case that reproduces the issue: https://github.com/martint/indytest", "NaN"], ["1625", "Generate all bytecode under a well-known package", "Martin Traverso", "martint", "09/04/14, 03:47:29 AM", "It makes it easier to analyze heap dumps and look for class memory leaks", "NaN"], ["1626", "Update Hive integration tests", "David Phillips", "electrum", "09/04/14, 11:17:38 PM", "NaN", "NaN"], ["1629", "Make commitInsert final in ReadOnlyConnectorMetadata", "David Phillips", "electrum", "09/06/14, 02:07:48 AM", "NaN", "NaN"], ["1630", "Minor compiler refactorings", "Martin Traverso", "martint", "09/09/14, 08:33:21 PM", "NaN", "NaN"], ["1633", "Change @SqlType to take a String", "Christopher Berner", "cberner", "09/09/14, 08:44:11 PM", "NaN", "NaN"], ["1639", "Preparatory changes to support parametric types", "Christopher Berner", "cberner", "09/10/14, 12:28:54 AM", "NaN", "NaN"], ["1641", "Add Hive tests for newly added columns", "David Phillips", "electrum", "09/10/14, 04:51:19 AM", "Verify that all of the file formats work correctly if a column is\nadded to the table after the file is already created.", "NaN"], ["1643", "Fix bug in operator resolution", "Christopher Berner", "cberner", "09/10/14, 10:21:48 PM", "NaN", "NaN"], ["1645", "Add support for parametric functions", "Christopher Berner", "cberner", "09/11/14, 06:02:08 AM", "NaN", "NaN"], ["1646", "Update tests", "David Phillips", "electrum", "09/11/14, 08:54:12 PM", "NaN", "NaN"], ["1647", "Always call LifeCycleManager.stop()", "Christopher Berner", "cberner", "09/11/14, 06:56:10 PM", "Fixes a liveness failure in the verifier, where it might never exit if\nan exception is thrown", "NaN"], ["1649", "Update tests", "David Phillips", "electrum", "09/11/14, 09:51:05 PM", "NaN", "NaN"], ["1650", "Create AsyncResponseHandler builder pattern", "Eric Hwang", "erichwang", "09/22/14, 10:55:35 PM", "- Replaces AsyncResponseUtils\n- Adds ability to asynchronously cancel on client disconnect", "NaN"], ["1651", "Add Kafka connector", "David Phillips", "electrum", "09/16/14, 02:06:31 AM", "NaN", "NaN"], ["1656", "Add docs for MySQL and PostgreSQL connectors", "David Phillips", "electrum", "09/12/14, 07:01:41 PM", "NaN", "NaN"], ["1657", "Refactor HiveType", "Christopher Berner", "cberner", "09/16/14, 07:33:50 PM", "Move Type out of HiveType, in preparation for supporting parametric\ntypes", "NaN"], ["1660", "Change Cassandra properties to use Duration", "David Phillips", "electrum", "09/15/14, 07:59:19 PM", "NaN", "NaN"], ["1661", "Fix TestSqlStageExecution.testSplitAssignment()", "Dain Sundstrom", "dain", "09/15/14, 09:42:22 PM", "NaN", "NaN"], ["1665", "Add release notes", "Christopher Berner", "cberner", "09/16/14, 04:56:27 PM", "NaN", "NaN"], ["1666", "Speed up announcement", "Dain Sundstrom", "dain", "09/19/14, 07:27:06 AM", "This reduces test time of presto-cassandra, presto-hive, presto-raptor, presto-mysql and presto-postgres by one minute each.  It also reduces the other modules, to a lesser extent.", "NaN"], ["1667", "Add option for double scheduling", "Christopher Berner", "cberner", "09/16/14, 08:19:40 PM", "Allows nodes to be selected multiple times during selection for a\nparticular stage", "NaN"], ["1668", "Move some presto-main dependencies into Raptor", "David Phillips", "electrum", "09/16/14, 05:43:17 AM", "NaN", "NaN"], ["1670", "Upgrade provisio plugin to 0.1.9", "David Phillips", "electrum", "09/16/14, 04:29:32 PM", "NaN", "NaN"], ["1671", "Fix JDBC client hanging the JVM on shutdown", "David Phillips", "electrum", "09/16/14, 08:08:02 PM", "NaN", "NaN"], ["1672", "Update 0.76 release notes", "Martin Traverso", "martint", "09/17/14, 08:00:47 PM", "NaN", "NaN"], ["1674", "Upgrade to Airlift 0.94", "David Phillips", "electrum", "09/17/14, 02:20:38 AM", "NaN", "NaN"], ["1675", "Fix dependencies for Hive connector", "David Phillips", "electrum", "09/17/14, 09:02:42 PM", "A normal connector should not depend on presto-main. Doing so requires\ncareful vetting of all dependencies to avoid class loader issues.", "NaN"], ["1677", "Move type names to their own class", "Christopher Berner", "cberner", "09/17/14, 07:28:07 PM", "NaN", "NaN"], ["1678", "Upgrade to testing-postgresql-server 0.2", "David Phillips", "electrum", "09/17/14, 09:05:09 PM", "NaN", "NaN"], ["1679", "Verify Hive partition schema against table schema", "David Phillips", "electrum", "09/17/14, 11:41:57 PM", "Fixes #1562", "NaN"], ["1681", "Update 0.76 release notes", "David Phillips", "electrum", "09/18/14, 03:38:32 AM", "NaN", "NaN"], ["1683", "Add support for distributed joins", "Christopher Berner", "cberner", "09/24/14, 04:59:47 PM", "NaN", "NaN"], ["1684", "Update provisio plugin to 0.1.11", "Martin Traverso", "martint", "09/18/14, 06:59:12 PM", "This fixes an issue where the executable flags for the launcher were't being preserved", "NaN"], ["1686", "Upgrade to testing-postgresql-server 0.3", "David Phillips", "electrum", "09/19/14, 03:19:06 PM", "NaN", "NaN"], ["1687", "Hive arrays", "Christopher Berner", "cberner", "10/02/14, 03:24:00 AM", "NaN", "NaN"], ["1688", "Disable broken hive.storage-format property", "David Phillips", "electrum", "09/18/14, 10:11:59 PM", "NaN", "NaN"], ["1691", "Fix race condition when updating output buffers", "David Phillips", "electrum", "09/19/14, 07:29:53 PM", "Fixes #1689", "NaN"], ["1692", "Add ConnectorPageSource to SPI", "Dain Sundstrom", "dain", "09/20/14, 04:24:08 AM", "Move Page and PageBuilder to SPI\nRemove InternalConnector", "NaN"], ["1693", "Remove duplicate dependency in presto-hive", "David Phillips", "electrum", "09/19/14, 07:28:31 PM", "NaN", "NaN"], ["1695", "Change output buffers to use task ids", "Christopher Berner", "cberner", "09/24/14, 06:21:32 PM", "NaN", "NaN"], ["1697", "Don't forward requests for well-known schemas to connectors", "Martin Traverso", "martint", "09/22/14, 06:43:39 PM", "NaN", "NaN"], ["1698", "Fix race condition when updating sources", "Martin Traverso", "martint", "09/20/14, 12:07:54 AM", "The current implementation has a potential race condition under the following scenario:\n\n```\nTask updater              Driver\n\n                          acquire lock\n                          process new sources\n                          ...\n                          process new sources in lock release code\nadd new source\nfail to acquire lock\n (sources not processed)\n                          release lock\n```\n\nThis results in new sources not being processed and the query getting stuck\nif the task updater never call update sources again and the driver becomes\nblocked because data from all current exchanges are consumed", "NaN"], ["1700", "Remove verifier limit of 50 threads", "Eric Hwang", "erichwang", "09/22/14, 06:04:43 PM", "NaN", "NaN"], ["1701", "Hive 13 changes", "Dain Sundstrom", "dain", "09/26/14, 02:19:50 AM", "NaN", "NaN"], ["1702", "Include response body in remote task error message", "David Phillips", "electrum", "09/23/14, 01:19:54 AM", "NaN", "NaN"], ["1705", "Add streaming index join support", "Eric Hwang", "erichwang", "09/24/14, 01:20:36 AM", "Streaming index joins are activated as a last resort when index results cannot fit into the cache.", "NaN"], ["1707", "Always clear thread context class loader for plugins", "David Phillips", "electrum", "09/24/14, 01:34:01 AM", "The default context class loader for the main thread is the application\nclass loader, which is never correct for plugins. Ideally, plugins should\nbe loading code relative to their own classes rather than relying on the\ncontext class loader, but many libraries prefer the context class loader.", "NaN"], ["1708", "Fix hidden flag for Kafka getColumnMetadata", "David Phillips", "electrum", "09/25/14, 06:49:23 PM", "NaN", "NaN"], ["1710", "Cap maximum number of remote task callback threads", "Eric Hwang", "erichwang", "09/24/14, 01:56:35 AM", "NaN", "NaN"], ["1714", "Cache HiveType category during construction", "Dain Sundstrom", "dain", "09/24/14, 06:12:15 PM", "The category is checked for each Slice read in the record cursors, and this causes 2x regression in performance for varchar reads.", "NaN"], ["1715", "Fix TypeRegistry to return null for unknown types", "Christopher Berner", "cberner", "09/25/14, 02:01:01 AM", "NaN", "NaN"], ["1716", "Add missing JMX export for RemoteTaskFactory", "Eric Hwang", "erichwang", "09/24/14, 06:59:13 PM", "NaN", "NaN"], ["1717", "Fix COUNT and MAX_BY to be decomposable", "Christopher Berner", "cberner", "09/25/14, 01:36:17 AM", "NaN", "NaN"], ["1719", "Update for FullJsonResponseHandler API change", "David Phillips", "electrum", "09/25/14, 08:30:40 PM", "NaN", "NaN"], ["1725", "More Hive support", "Dain Sundstrom", "dain", "10/02/14, 12:12:06 AM", "Add support for more partition key types and values.\nAdd support for writing more types.", "NaN"], ["1726", "Record input stats for PageSource", "Dain Sundstrom", "dain", "09/26/14, 07:24:31 PM", "NaN", "NaN"], ["1729", "Development improvements for PluginManager", "David Phillips", "electrum", "09/26/14, 11:55:21 PM", "NaN", "NaN"], ["1730", "Add support for UNNEST", "Christopher Berner", "cberner", "10/10/14, 01:08:38 AM", "NaN", "NaN"], ["1733", "Simplify Session in preparation for adding session properties", "Dain Sundstrom", "dain", "09/29/14, 08:45:27 PM", "NaN", "NaN"], ["1735", "Orc page source", "Dain Sundstrom", "dain", "10/16/14, 05:45:29 AM", "Just the last three commits need to be reviewed:\n- Add metadata abstraction for ORC\n- Extend custom ORC reader to support DWRF\n- Add new ORC reader implementation", "NaN"], ["1736", "Rc page source", "Dain Sundstrom", "dain", "10/09/14, 08:46:36 PM", "This also fixes the size calculation of lazy blocks", "NaN"], ["1737", "Switch to AsyncResponseHandler in airlift", "Eric Hwang", "erichwang", "09/29/14, 11:01:42 PM", "NaN", "NaN"], ["1738", "Update release notes", "Christopher Berner", "cberner", "09/30/14, 05:17:59 PM", "NaN", "NaN"], ["1739", "Fix HashAggregation infinite loop during multiple partial agg flushes", "Eric Hwang", "erichwang", "09/30/14, 03:01:16 AM", "NaN", "NaN"], ["1740", "Session properties", "Dain Sundstrom", "dain", "10/04/14, 07:00:24 PM", "System and each catalog have a separate namespace for session properties\nCatalog specific session properties are passed to ConnectorMetadata\nNo properties are passed to functions or types\nAdd session properties to REST interface\nAdd CLI and experimental JDBC support for setting session properties", "NaN"], ["1741", "Release notes for 0.77", "Nileema Shingte", "nileema", "09/30/14, 06:30:43 PM", "NaN", "NaN"], ["1743", "Fix Hive S3 test", "David Phillips", "electrum", "09/30/14, 08:00:55 PM", "NaN", "NaN"], ["1745", "Fix initialization of Hadoop native code", "David Phillips", "electrum", "10/01/14, 12:32:35 AM", "NaN", "NaN"], ["1746", "Fix Exception self-suppression", "Dain Sundstrom", "dain", "10/01/14, 05:28:21 PM", "An exception can not add itself to the suppression list.  This can happen\nwhen code throws the same exception instance in close.", "NaN"], ["1747", "Fix race condition in SqlTask createTaskInfo", "Dain Sundstrom", "dain", "10/02/14, 12:11:18 AM", "This fixes a race condition in the task info management of SqlTask.  When a\nSqlTask finishes the TaskInfo is permanently, and all calls to getTaskInfo\nreturn the final instance.  The problem is while the final task info is being\nupdated another thread may create a new task info with a higher version number\nin the running state.  The client will receive the higher version number and\nreject the final task info with a lower version number.  The query will never\nfinish because it the final task info version number never increases.\n\nThe fix changes the SqlTask to always return a new TaskInfo for each call to\ngetTaskInfo with a new version number.", "NaN"], ["1748", "Fix multi scheduling", "Christopher Berner", "cberner", "10/07/14, 08:14:34 PM", "NaN", "NaN"], ["1751", "Add catch all exception mapper for HTTP", "Dain Sundstrom", "dain", "10/02/14, 05:54:34 PM", "Mapper logs the exception and request URI\nMapper returns the stacktrace a text/plain response", "NaN"], ["1752", "Remove schema from ConnectorSession", "Dain Sundstrom", "dain", "10/02/14, 05:21:23 AM", "NaN", "NaN"], ["1753", "Workaround for segfault in 8u20 due to JDK-8059299", "Martin Traverso", "martint", "10/02/14, 04:31:53 PM", "NaN", "NaN"], ["1754", "Add support for collection types to the verifier", "Christopher Berner", "cberner", "10/02/14, 07:35:35 PM", "NaN", "NaN"], ["1755", "Fix bug in byte code generation for parametric types", "Christopher Berner", "cberner", "10/02/14, 09:39:53 PM", "NaN", "NaN"], ["1756", "Add more documentation for arrays", "Christopher Berner", "cberner", "10/04/14, 02:20:16 AM", "NaN", "NaN"], ["1757", "Add support for Maps to verifier", "Christopher Berner", "cberner", "10/02/14, 10:45:35 PM", "NaN", "NaN"], ["1758", "Plugin fixes", "David Phillips", "electrum", "10/02/14, 10:49:49 PM", "NaN", "NaN"], ["1759", "Add cast from varbinary to HLL", "Christopher Berner", "cberner", "10/02/14, 11:48:43 PM", "NaN", "NaN"], ["1761", "Local planing failure", "Dain Sundstrom", "dain", "10/03/14, 05:33:29 PM", "NaN", "NaN"], ["1762", "Add to_json function", "Christopher Berner", "cberner", "10/04/14, 05:14:36 AM", "NaN", "NaN"], ["1764", "Exclude joda-time from aws-java-sdk", "David Phillips", "electrum", "10/03/14, 07:51:20 PM", "Their POM uses a version range which drags in the wrong version.", "NaN"], ["1765", "Fix quadratic running time in expression optimizer", "Christopher Berner", "cberner", "10/03/14, 11:09:54 PM", "NaN", "NaN"], ["1766", "Fix exponential running time in expression translator", "Christopher Berner", "cberner", "10/04/14, 02:17:09 AM", "It now runs in linear time", "NaN"], ["1767", "Add option to verifier to only run EXPLAIN for queries", "Martin Traverso", "martint", "10/04/14, 05:25:35 AM", "This is useful for testing parsing/analysis/planning without having to execute the queries.", "NaN"], ["1769", "Hive storage format", "Dain Sundstrom", "dain", "10/07/14, 01:12:54 AM", "Reenable the `hive.storage-format` configuration property\nAdd `storage-format` connector property", "NaN"], ["1770", "Add option to only schedule Hive splits locally", "Dain Sundstrom", "dain", "10/09/14, 09:39:55 PM", "Add a configuration and session property to only schedule Hive splits locally", "NaN"], ["1772", "Verbose stats", "Dain Sundstrom", "dain", "10/15/14, 10:11:36 PM", "NaN", "NaN"], ["1773", "Fix intermittent test failures", "Christopher Berner", "cberner", "10/06/14, 07:13:53 PM", "NaN", "NaN"], ["1774", "Jdbc updates", "Christopher Berner", "cberner", "10/08/14, 06:37:14 AM", "NaN", "NaN"], ["1775", "Fix map to json tests", "Christopher Berner", "cberner", "10/06/14, 08:29:08 PM", "This should prevent them from failing", "NaN"], ["1776", "Hive drop test tables", "David Phillips", "electrum", "10/07/14, 03:45:50 AM", "NaN", "NaN"], ["1777", "Fix precondition in ClientSession", "David Phillips", "electrum", "10/07/14, 04:44:54 AM", "NaN", "NaN"], ["1778", "Fix effective predicate extraction of UNION ALL", "Eric Hwang", "erichwang", "10/07/14, 09:43:44 PM", "Occurs because a source plan node of a UNION ALL can have a single symbol that maps to multiple output symbols of the UNION ALL.", "NaN"], ["1779", "Fix wrapping for release notes", "David Phillips", "electrum", "10/08/14, 07:46:58 PM", "NaN", "NaN"], ["1780", "Add 0.78 release notes", "Dain Sundstrom", "dain", "10/08/14, 07:41:07 PM", "NaN", "NaN"], ["1781", "Update release notes", "Christopher Berner", "cberner", "10/08/14, 07:48:30 PM", "NaN", "NaN"], ["1783", "Specify table format in Hive test script", "David Phillips", "electrum", "10/08/14, 11:30:42 PM", "NaN", "NaN"], ["1784", "Fix Hive test for DWRF", "David Phillips", "electrum", "10/09/14, 01:47:32 AM", "NaN", "NaN"], ["1785", "Support index joins probing with subsets of join keys", "Eric Hwang", "erichwang", "10/10/14, 12:56:12 AM", "To do this, there are three main considerations:\n1) The index join optimizer must be able to funnel index join symbols through various plan structures\n2) The index loader needs to deduplicate the keys based only on the symbols that are fed to the index source (not necessarily the entire join keys)\n3) Because fetching with reduced keys may produce additionally unnecessary results (e.g. fetching with A,B vs. A,B,C) we need to filter out extraneous results in the resultset (especially when in streaming mode).", "NaN"], ["1789", "Use normal code path for dropping test tables", "David Phillips", "electrum", "10/09/14, 08:26:20 PM", "NaN", "NaN"], ["1791", "Update UnloadedIndexKeyRecordSet usage of GroupByHash", "Eric Hwang", "erichwang", "10/10/14, 03:12:59 AM", "NaN", "NaN"], ["1792", "Fix issue where ScanFilterAndProject with PageSource might skip data", "Martin Traverso", "martint", "10/10/14, 06:47:08 PM", "This can happen when processing the last page due to a broken check\nin the isFinished() method. The implementation needs to check whether\nthere's a partial page pending processing before indicating the operator\nis done.", "NaN"], ["1793", "Raptor changes", "David Phillips", "electrum", "10/11/14, 04:40:56 PM", "NaN", "NaN"], ["1795", "Hive page source updates", "Dain Sundstrom", "dain", "10/10/14, 07:40:52 PM", "NaN", "NaN"], ["1796", "Expand UNNEST documentation", "David Phillips", "electrum", "10/10/14, 07:57:36 PM", "NaN", "NaN"], ["1797", "Unnest", "Christopher Berner", "cberner", "10/10/14, 08:19:42 PM", "NaN", "NaN"], ["1798", "TypeSignature", "Christopher Berner", "cberner", "10/13/14, 06:28:16 PM", "NaN", "NaN"], ["1799", "Update 0.79 release notes", "Martin Traverso", "martint", "10/11/14, 02:32:21 AM", "NaN", "NaN"], ["1801", "Add support for implicit joins", "Martin Traverso", "martint", "10/11/14, 08:17:22 PM", "NaN", "NaN"], ["1805", "Fix failure when output of UNNEST is not used", "Christopher Berner", "cberner", "10/13/14, 08:03:20 PM", "NaN", "NaN"], ["1806", "Aggregation and ML plugin improvements", "Christopher Berner", "cberner", "10/14/14, 10:44:20 PM", "NaN", "NaN"], ["1807", "Make ErrorCode an interface", "Christopher Berner", "cberner", "10/14/14, 11:42:01 PM", "NaN", "NaN"], ["1808", "Raptor changes", "David Phillips", "electrum", "10/14/14, 10:02:03 PM", "NaN", "NaN"], ["1811", "Allow verifier to run multiple suites", "Nileema Shingte", "nileema", "10/14/14, 08:57:52 PM", "NaN", "NaN"], ["1813", "Check that expressions used in GROUP BY are comparable", "Christopher Berner", "cberner", "10/15/14, 12:27:02 AM", "NaN", "NaN"], ["1815", "Add external batch support to Raptor connector", "David Phillips", "electrum", "10/15/14, 04:36:52 PM", "NaN", "NaN"], ["1818", "Preserve the value of argLine for surefire plugin", "Martin Traverso", "martint", "10/16/14, 01:22:56 AM", "This variable is set by the JaCoCo plugin to configure the agent.", "NaN"], ["1819", "Add OrcInputStream resetStream ", "Dain Sundstrom", "dain", "10/16/14, 04:30:32 PM", "Share OrcInputStreams between row groups which prevents blocks from being decompressed multiple times.", "NaN"], ["1820", "Move static methods to a separate class", "Martin Traverso", "martint", "10/16/14, 06:53:50 PM", "This seems to confuse TestNG, which tries to invoke some of the public\nstatic methods during setup", "NaN"], ["1821", "Add dummy field to TestingSplit", "Martin Traverso", "martint", "10/16/14, 07:54:56 PM", "This is to avoid exceptions due to Jackson refusing to serialize/deserialize an object with no fields", "NaN"], ["1822", "Move ORC reader to new presto-orc module", "Dain Sundstrom", "dain", "10/16/14, 08:34:30 PM", "NaN", "NaN"], ["1824", "Fix ORC MapJsonReader timezone", "Dain Sundstrom", "dain", "10/16/14, 10:22:29 PM", "NaN", "NaN"], ["1825", "Cleanup hive session properties", "Dain Sundstrom", "dain", "10/16/14, 10:31:13 PM", "NaN", "NaN"], ["1826", "Update to floatingdecimal 0.2", "Martin Traverso", "martint", "10/16/14, 10:53:52 PM", "This version prints a better error message when attempting to use it with Java 8", "NaN"], ["1827", "Set offset and length in RcFilePageSource", "Dain Sundstrom", "dain", "10/16/14, 11:55:07 PM", "NaN", "NaN"], ["1831", "Add support for variadic functions, and implement ROW type", "Christopher Berner", "cberner", "11/06/14, 07:32:38 PM", "NaN", "NaN"], ["1836", "Use ORC as the storage format for Raptor", "David Phillips", "electrum", "10/23/14, 05:21:11 AM", "NaN", "NaN"], ["1838", "Fix RcFilePageSource handling of offset and length", "Dain Sundstrom", "dain", "10/21/14, 07:11:15 PM", "NaN", "NaN"], ["1839", "Orc checkpoints", "Dain Sundstrom", "dain", "10/27/14, 11:03:15 PM", "This simplifies the ORC setup code and eliminates bugs with processing the position offsets.", "NaN"], ["1844", "Fix orc predicate", "Dain Sundstrom", "dain", "10/26/14, 09:43:31 PM", "Only the last 3 commits need to be reviewed", "NaN"], ["1845", "Handle nested maps correctly in map_keys", "David Phillips", "electrum", "10/21/14, 10:09:27 PM", "NaN", "NaN"], ["1846", "Add utility methods for reflection", "David Phillips", "electrum", "10/21/14, 11:34:24 PM", "NaN", "NaN"], ["1847", "Update window function API to supply partitionStartPosition on reset", "Eric Hwang", "erichwang", "10/24/14, 12:25:43 AM", "This reduces the amount of internal state that each window function needs to track and simplifies their logic", "NaN"], ["1849", "Perf fixes", "Dain Sundstrom", "dain", "10/22/14, 07:29:00 PM", "NaN", "NaN"], ["1850", "Update to Airbase 28 with Guava 18.0", "David Phillips", "electrum", "10/22/14, 07:03:07 PM", "NaN", "NaN"], ["1858", "Make test helper methods protected", "David Phillips", "electrum", "10/23/14, 10:43:26 PM", "NaN", "NaN"], ["1859", "Improve support for big queries (joins, group bys, distincts)", "Christopher Berner", "cberner", "10/25/14, 06:01:36 AM", "NaN", "NaN"], ["1860", "Precompute hash for nodes that need hash", "Nileema Shingte", "nileema", "11/11/14, 09:02:18 PM", "```\n1. Add abstraction for hash generation\n2. Add an optimizer that uses optimized hash generation for all nodes\nthat use groupByHash and lookup source\n3. Modify operator test to run for both with and without hash generation enabled\n4. Add a config option and session property to turn the feature on/off\n```\n\nMicrobenchmarks: \nGroupByHash https://gist.github.com/nileema/2010aa90023dc0cce6d8\nHashPagePartitioningFunction: https://gist.github.com/nileema/58e08ef66aed0c3209ef", "NaN"], ["1861", "Move ClassLoaderSafeConnectorPageSourceProvider to SPI", "David Phillips", "electrum", "10/25/14, 05:17:31 PM", "NaN", "NaN"], ["1862", "Fix class loader issues for Raptor ORC writer", "David Phillips", "electrum", "10/25/14, 05:17:18 PM", "NaN", "NaN"], ["1867", "Set context class loader for Hadoop FileSystem", "David Phillips", "electrum", "10/27/14, 05:24:22 AM", "NaN", "NaN"], ["1868", "Fix orc bugs", "Dain Sundstrom", "dain", "10/27/14, 09:31:14 PM", "NaN", "NaN"], ["1869", "Fix Hive partition pruning for null keys", "Dain Sundstrom", "dain", "10/28/14, 12:49:34 AM", "Fixes #1864", "NaN"], ["1870", "Raptor fixes", "David Phillips", "electrum", "10/29/14, 01:02:17 AM", "NaN", "NaN"], ["1872", "Fix typo in JMX connector documentation", "David Phillips", "electrum", "10/29/14, 09:43:04 PM", "NaN", "NaN"], ["1875", "Allow reading/writing sequences of pages with mixed block types", "Martin Traverso", "martint", "10/29/14, 10:20:21 PM", "The current implementation causes problems when a single remote request\ngets multiple pages that have different encodings for a given column. This\ncould happen, for example, if the stream is the result of a union of\nsubqueries or tables that produce different types of blocks for any given column.\n\nThe reason is that the identifier for the block encodings is written once per request\nbased on the encodings of the first page. This fix changes the wire serialization format\nto include the block encodings for each page in the response.\n\nFixes https://github.com/facebook/presto/issues/1874", "NaN"], ["1876", "Set context class loader when creating ORC writer", "David Phillips", "electrum", "10/30/14, 06:12:55 PM", "The ORC memory manager ends up loading VersionInfo which loads classes\nfrom the context class loader. The tests now set an empty class loader\nto verify that it uses the correct class loader.", "NaN"], ["1877", "Update release notes", "Christopher Berner", "cberner", "11/03/14, 07:26:27 PM", "NaN", "NaN"], ["1878", "Minor UI changes", "David Phillips", "electrum", "10/31/14, 04:22:11 PM", "NaN", "NaN"], ["1879", "0.80 release notes", "Dain Sundstrom", "dain", "11/01/14, 12:34:55 AM", "NaN", "NaN"], ["1883", "Update 0.80 release notes", "Martin Traverso", "martint", "10/31/14, 10:37:46 PM", "NaN", "NaN"], ["1884", "Change test to expect optimized readers enabled", "Dain Sundstrom", "dain", "11/01/14, 06:17:37 PM", "NaN", "NaN"], ["1887", "Fix correctness check skip in verifier", "Christopher Berner", "cberner", "11/03/14, 08:10:55 PM", "NaN", "NaN"], ["1888", "Update 0.80 release notes", "Martin Traverso", "martint", "11/03/14, 09:55:35 PM", "NaN", "NaN"], ["1889", "Make JsonType comparable", "Christopher Berner", "cberner", "11/11/14, 11:14:38 PM", "NaN", "NaN"], ["1890", "Update 0.80 release notes", "David Phillips", "electrum", "11/03/14, 11:05:36 PM", "NaN", "NaN"], ["1892", "Fix orc predicate", "Dain Sundstrom", "dain", "11/05/14, 03:17:02 AM", "NaN", "NaN"], ["1896", "Add 0.81 release notes", "Dain Sundstrom", "dain", "11/05/14, 05:04:25 AM", "NaN", "NaN"], ["1899", "Error code updates", "David Phillips", "electrum", "11/05/14, 07:52:19 PM", "NaN", "NaN"], ["1905", "Fix offset handling in lead, lag and nth_value", "David Phillips", "electrum", "11/06/14, 01:06:03 AM", "NaN", "NaN"], ["1909", "Fix orc double predicate pushdown", "Dain Sundstrom", "dain", "11/06/14, 12:19:15 AM", "NaN", "NaN"], ["1915", "Change internal serialization of TypeSignature back to a string", "Christopher Berner", "cberner", "11/07/14, 01:09:06 AM", "NaN", "NaN"], ["1918", "Release notes and an optimization", "Christopher Berner", "cberner", "11/07/14, 02:40:20 AM", "NaN", "NaN"], ["1919", "Partition performance", "Dain Sundstrom", "dain", "11/07/14, 04:07:27 AM", "NaN", "NaN"], ["1926", "Change typeSignature to not be required in client", "Dain Sundstrom", "dain", "11/07/14, 09:28:09 PM", "NaN", "NaN"], ["1927", "Raptor updates", "David Phillips", "electrum", "11/08/14, 07:16:52 PM", "NaN", "NaN"], ["1929", "Fix Hive TupleDomain compaction", "Dain Sundstrom", "dain", "11/07/14, 11:26:17 PM", "NaN", "NaN"], ["1930", "Add 0.82 release notes", "Dain Sundstrom", "dain", "11/08/14, 12:20:12 AM", "NaN", "NaN"], ["1933", "Add support for simple non-equi joins", "Christopher Berner", "cberner", "11/20/14, 12:53:05 AM", "NaN", "NaN"], ["1935", "Fix explain help text", "Dain Sundstrom", "dain", "11/13/14, 11:07:14 PM", "NaN", "NaN"], ["1942", "Add queries that fail before starting to expiration queue", "Martin Traverso", "martint", "11/11/14, 07:16:18 PM", "This was a bug introduced in https://github.com/facebook/presto/pull/1917", "NaN"], ["1945", "Fix handling of null collections in unnest", "Christopher Berner", "cberner", "11/11/14, 07:39:32 PM", "NaN", "NaN"], ["1946", "Expose at_timezone hidden function", "Eric Hwang", "erichwang", "11/12/14, 12:20:17 AM", "NaN", "NaN"], ["1951", "Fix resource leak in query queue", "Christopher Berner", "cberner", "11/12/14, 07:51:29 PM", "NaN", "NaN"], ["1952", "Add release notes for 0.83", "Christopher Berner", "cberner", "11/12/14, 09:09:40 PM", "NaN", "NaN"], ["1964", "Add developer guide to documentation", "Christopher Berner", "cberner", "11/25/14, 08:19:31 PM", "Currently, this includes documentation for types, scalar functions, and\naggregation functions", "NaN"], ["1966", "Add Xerial Snappy to Hive for Parquet", "Dain Sundstrom", "dain", "11/13/14, 11:13:42 PM", "NaN", "NaN"], ["1968", "Update to arlift 0.97", "Eric Hwang", "erichwang", "11/14/14, 02:31:53 AM", "- Remove redundant classes", "NaN"], ["1969", "Fix approximate joins", "Christopher Berner", "cberner", "11/14/14, 09:29:08 PM", "NaN", "NaN"], ["1973", "Fix test exchange client", "Dain Sundstrom", "dain", "11/15/14, 12:33:32 AM", "NaN", "NaN"], ["1978", "Reduce GC pressure on coordinator", "Christopher Berner", "cberner", "11/17/14, 09:43:19 PM", "NaN", "NaN"], ["1980", "Add release notes for 0.84", "Christopher Berner", "cberner", "11/18/14, 12:43:45 AM", "NaN", "NaN"], ["1986", "Only merge ORC reads that are close together", "Dain Sundstrom", "dain", "11/20/14, 07:31:11 PM", "NaN", "NaN"], ["1987", "Optimize partition pruning for Hive client", "Martin Traverso", "martint", "11/18/14, 11:15:07 PM", "- replace usage of Warehouse.makeSpecFromName with a more efficient implementation\n- build filters based on all partition keys, even if they are missing and call\n  getPartitionNamesByParts. Use empty string as placeholder for missing filters\n- short-circuit parsing partition key values and creating partition objects if\n  we can determine the partition doesn't match the predicate\n\nThis results in a speedup in planning time for a table with 500k partitions from 19s to 150ms.", "NaN"], ["1992", "Fix tracking of input rows/bytes for JOIN stages", "Martin Traverso", "martint", "11/19/14, 06:00:29 PM", "This is a little hacky, since we should really be tracking just the inputs due to the TableScan operator for any given stage. However, the current stats rollup system doesn't support this.\n\nFixes https://github.com/facebook/presto/issues/1991", "NaN"], ["1994", "Add ntile window function", "David Phillips", "electrum", "11/20/14, 07:21:50 AM", "NaN", "NaN"], ["1997", "name of WITH query needs to be normalized to be case-insensitive", "Sadayuki Furuhashi", "frsyuki", "11/19/14, 09:34:18 PM", "I found that this query fails:\n\n```\nwith AB as (select 1) select * from AB\n```\n\nwith this error message:\n\n```\n  Query 20141119_211516_04211_iz8i5 failed: Table td-presto.sfdb.ab does not exist\n```\n\nBut this query succeeds:\n\n```\nwith ab as (select 1) select * from ab\n```\n\nThis pull-request fixes the issue by normalizing name of WITH query so that TupleAnalyzer.visitTable can find the name using a part of QualifiedName.", "NaN"], ["1999", "Add statements for manipulating session properties", "Dain Sundstrom", "dain", "12/26/14, 12:41:28 AM", "NaN", "NaN"], ["2000", "Hive format benchmark cleanup", "Dain Sundstrom", "dain", "12/25/14, 09:02:54 PM", "NaN", "NaN"], ["2002", "Improve error message when using DISTINCT with non-comparable types", "Martin Traverso", "martint", "11/21/14, 07:01:53 PM", "NaN", "NaN"], ["2003", "Validate types for IN subquery expression", "Martin Traverso", "martint", "11/21/14, 07:46:16 PM", "NaN", "NaN"], ["2004", "Throw proper exception when memory size exceeded in TopN", "Martin Traverso", "martint", "11/21/14, 09:27:25 PM", "NaN", "NaN"], ["2006", "Change TPCH dates to be DATE type", "Dain Sundstrom", "dain", "12/03/14, 12:36:59 AM", "Fix date handling bugs in query tests, kafka tests, and jdbc connector\nMap DATE to TEXT in Cassandra connector", "NaN"], ["2009", "Add array constructor syntax to migration guide", "Christopher Berner", "cberner", "11/25/14, 05:40:10 PM", "NaN", "NaN"], ["2015", "Make ParametricOperator.getDescription() final", "Christopher Berner", "cberner", "11/26/14, 06:24:36 PM", "NaN", "NaN"], ["2018", "Add JMX counters for query bytes rate distribution", "Eric Hwang", "erichwang", "11/26/14, 04:09:47 AM", "NaN", "NaN"], ["2021", "Add full support for window functions", "David Phillips", "electrum", "12/08/14, 03:20:41 PM", "NaN", "NaN"], ["2022", "Temporarily disable string statistics in orc/drwf readers", "Martin Traverso", "martint", "11/27/14, 01:59:17 AM", "The writer performs comparisons using java Strings to determine the minimum and maximum\nvalues. This results in weird behaviors in the presence of surrogate pairs and special character (see http://icu-project.org/docs/papers/utf16_code_point_order.html)\n\nFor example, unicode codepoint 0x1D403 has the following representations:\n- UTF-16: [0xD835, 0xDC03]\n- UTF-8:  [0xF0, 0x9D, 0x90, 0x83]\n\nwhile codepoint 0xFFFD (the replacement character) has the following representations:\n- UTF-16: [0xFFFD]\n- UTF-8:  [0xEF, 0xBF, 0xBD]\n\nwhen comparisons between strings containing these characters are done with Java Strings (UTF-16),\n0x1D403 < 0xFFFD, but when comparisons are done using raw codepoints or UTF-8, 0x1D403 > 0xFFFD\n\nThe bottom-line is that UTF-16 lexicographic ordering is not equivalent to UTF-8 lexicographic ordering or codepoint ordering.", "NaN"], ["2023", "Update reference to floatingdecimal in docs", "Martin Traverso", "martint", "11/27/14, 10:43:17 PM", "NaN", "NaN"], ["2025", "Fix broken analyzer test", "Martin Traverso", "martint", "11/27/14, 02:09:40 AM", "Forgot to update the tests in df99f33ab82b20f6afc362c7eb1d165b30ef8441", "NaN"], ["2034", "Require Java 8", "Christopher Berner", "cberner", "12/02/14, 09:58:14 PM", "NaN", "NaN"], ["2038", "Respect includeCoordinator flag when scheduling randomly", "Martin Traverso", "martint", "12/02/14, 01:33:37 AM", "NaN", "NaN"], ["2041", "Flatten developer guide sections", "David Phillips", "electrum", "12/02/14, 06:42:34 PM", "NaN", "NaN"], ["2043", "Simplify example code for isNull function", "David Phillips", "electrum", "12/02/14, 08:41:00 PM", "NaN", "NaN"], ["2044", "Remove dependency on floatingdecimal library", "Martin Traverso", "martint", "12/02/14, 11:16:49 PM", "NaN", "NaN"], ["2045", "Change DATE memory representation to be days", "Dain Sundstrom", "dain", "12/16/14, 08:46:22 PM", "DATE is now stored as a 32-bit number of days since 1970-01-01", "NaN"], ["2046", "Fix caching of ROW type", "Christopher Berner", "cberner", "12/03/14, 03:22:58 AM", "NaN", "NaN"], ["2048", "Upgrade to Airlift 0.98-SNAPSHOT", "Dain Sundstrom", "dain", "12/03/14, 07:16:38 AM", "NaN", "NaN"], ["2053", "Change Travis config to use Java 8", "Christopher Berner", "cberner", "12/04/14, 09:41:11 PM", "NaN", "NaN"], ["2055", "Add a check for no nodes available in Raptor", "Nileema Shingte", "nileema", "12/22/14, 10:59:55 PM", "NaN", "NaN"], ["2056", "Test query fixes for TPCH DATE values", "Dain Sundstrom", "dain", "12/03/14, 11:43:12 PM", "NaN", "NaN"], ["2057", "Add utility for bridging Guava builders with Java 8 streams", "Martin Traverso", "martint", "12/04/14, 07:07:58 AM", "This allows us to do the following and get an ImmutableList or ImmutableSet:\n\n``` java\nList<Integer> list = collection.stream()\n    .map(...)\n    .filter(...)\n    .collect(ImmutableCollectors.immutableListCollector());\n\nSet<Integer> set = collection.stream()\n    .map(...)\n    .filter(...)\n    .collect(ImmutableCollectors.immutableSetCollector());\n```", "NaN"], ["2061", "Minor improvements to ImmutableCollectors", "Martin Traverso", "martint", "12/04/14, 07:14:51 PM", "- Rename methods to toImmutableList and toImmutableSet\n- Simplify method signatures\n- Remove unnecessary type specification for lambda args\n- Declare set collector as UNORDERED to give stream implementation\n  more room for optimization", "NaN"], ["2062", "Enable modernizer plugin", "Martin Traverso", "martint", "12/04/14, 07:53:29 PM", "For now, start with the stock set of violations and only warn.", "NaN"], ["2066", "Update required Java version", "Martin Traverso", "martint", "12/04/14, 10:51:32 PM", "NaN", "NaN"], ["2068", "Add docs for arbitrary() and release notes for 0.88", "Christopher Berner", "cberner", "12/05/14, 10:24:31 PM", "NaN", "NaN"], ["2071", "Cleanup", "Martin Traverso", "martint", "12/06/14, 08:58:08 AM", "Remove getter/predicate functions and replace them with method references or lambdas", "NaN"], ["2073", "Minor cleanup of Least", "Martin Traverso", "martint", "12/06/14, 08:37:16 AM", "Remove custom method in Signatures to create a specialized signature for least.\nThat's not necessary outside of the Least class and it's an implementation detail.", "NaN"], ["2079", "Update docs for arbitrary aggregation function", "David Phillips", "electrum", "12/10/14, 07:53:49 PM", "NaN", "NaN"], ["2082", "Fix excessive garbage creation", "Christopher Berner", "cberner", "12/09/14, 12:11:38 AM", "The ORC decoder was generating a large amount of garbage because it was\ninvoking a varargs function in the inner decoding loop. This reduces\nobject creation by about 80% on the workers", "NaN"], ["2083", "Parser cleanups for Java 8", "David Phillips", "electrum", "12/10/14, 12:49:47 AM", "NaN", "NaN"], ["2087", "Pass FunctionRegistry to specialize", "Martin Traverso", "martint", "12/09/14, 09:01:26 PM", "This is needed for functions that need to rely on other type-specific functions\n(e.g., map comparison, which requires a specific comparator based on the key type)", "NaN"], ["2088", "Categorize HDFS errors", "Christopher Berner", "cberner", "12/10/14, 01:20:08 AM", "NaN", "NaN"], ["2089", "Disable dependency checker for Travis", "Christopher Berner", "cberner", "12/09/14, 09:47:26 PM", "NaN", "NaN"], ["2090", "Add license header", "David Phillips", "electrum", "12/10/14, 01:03:09 AM", "NaN", "NaN"], ["2092", "Convert some usages of java.util.Optional", "David Phillips", "electrum", "12/12/14, 11:37:33 PM", "NaN", "NaN"], ["2094", "Couple minor fixes", "Christopher Berner", "cberner", "12/10/14, 11:06:12 PM", "NaN", "NaN"], ["2097", "Add CLI program to benchmark a Presto cluster", "Dain Sundstrom", "dain", "12/18/14, 02:08:25 AM", "NaN", "NaN"], ["2098", "Fix for row number window function optimization", "Nileema Shingte", "nileema", "12/11/14, 08:17:19 PM", "NaN", "NaN"], ["2101", "Upgrade approx_distinct", "Eric Hwang", "erichwang", "12/11/14, 10:40:44 PM", "NaN", "NaN"], ["2103", "Improvements to ML plugin", "Christopher Berner", "cberner", "12/19/14, 08:01:31 PM", "NaN", "NaN"], ["2105", "Categorize errors for Hive partition key parsing", "David Phillips", "electrum", "12/15/14, 08:30:43 PM", "NaN", "NaN"], ["2110", "Remove uses of IterableTransformer", "Martin Traverso", "martint", "12/13/14, 07:03:35 PM", "NaN", "NaN"], ["2113", "Add date and timestamp to TupleDomain", "David Phillips", "electrum", "12/15/14, 08:54:32 PM", "NaN", "NaN"], ["2114", "Add separate deterministic flag to SHOW FUNCTIONS", "David Phillips", "electrum", "12/14/14, 05:29:39 PM", "NaN", "NaN"], ["2117", "Allow configurable number of tablewriters per worker", "Nileema Shingte", "nileema", "12/22/14, 08:07:26 PM", "NaN", "NaN"], ["2119", "Upgrade to airlift tpch 0.4-SNAPSHOT", "Christopher Berner", "cberner", "12/16/14, 12:03:09 AM", "NaN", "NaN"], ["2121", "Split CANCELED state into ABORTED and CANCELED ", "Dain Sundstrom", "dain", "12/25/14, 08:27:21 PM", "Canceled now means that a user stopped the stage (and related tasks) execution\nAborted means a task failed somewhere, so the execution was stopped", "NaN"], ["2123", "Remove CodeCache workaround", "Martin Traverso", "martint", "12/17/14, 06:57:33 PM", "This is no longer an issue in Java 8", "NaN"], ["2124", "Switch to Docker based Travis builds", "Christopher Berner", "cberner", "12/17/14, 07:58:19 PM", "This should give us 4GB of memory instead of 3GB and also more CPU\nresources", "NaN"], ["2127", "Fix comparison of Type objects", "Christopher Berner", "cberner", "12/18/14, 01:39:25 AM", "NaN", "NaN"], ["2138", "Document DATE memory representation change", "Dain Sundstrom", "dain", "12/19/14, 09:52:39 PM", "NaN", "NaN"], ["2139", "Implement HASH_CODE operator for JSON type", "Christopher Berner", "cberner", "12/19/14, 09:31:58 PM", "NaN", "NaN"], ["2140", "Fail task when local planning fails", "Dain Sundstrom", "dain", "12/19/14, 11:20:23 PM", "NaN", "NaN"], ["2141", "Fix window functions with no input channels", "David Phillips", "electrum", "12/19/14, 11:03:52 PM", "NaN", "NaN"], ["2143", "Add release notes for 0.89", "Christopher Berner", "cberner", "12/20/14, 12:52:59 AM", "NaN", "NaN"], ["2144", "Raptor page sink", "Nileema Shingte", "nileema", "12/22/14, 10:36:24 PM", "NaN", "NaN"], ["2145", "Add release note for JOIN bug", "Dain Sundstrom", "dain", "12/20/14, 12:00:36 AM", "NaN", "NaN"], ["2147", "Minor refactoring for Raptor", "Nileema Shingte", "nileema", "12/22/14, 10:56:42 PM", "@martint  - Only the last commit needs to be reviewed. \nOther commits are a part of another PR that is not yet merged. ", "NaN"], ["2151", "Documentation updates", "David Phillips", "electrum", "12/22/14, 06:47:42 PM", "NaN", "NaN"], ["2152", "Use checkCondition for approx_distinct", "David Phillips", "electrum", "12/22/14, 06:45:40 PM", "NaN", "NaN"], ["2153", "Fix formatting for benchmark-driver documentation", "David Phillips", "electrum", "12/22/14, 07:12:14 PM", "NaN", "NaN"], ["2155", "Fix Hive unit test to use storage format", "David Phillips", "electrum", "12/22/14, 11:45:25 PM", "NaN", "NaN"], ["2156", "Fix bogus error due to IDEA-134059", "David Phillips", "electrum", "12/23/14, 01:07:56 AM", "NaN", "NaN"], ["2157", "Add json to array cast", "Christopher Berner", "cberner", "01/13/15, 11:14:54 PM", "Fixes #2109 and #2154 ", "NaN"], ["2159", "Use newConcurrentHashSet and newIdentityHashSet", "David Phillips", "electrum", "12/23/14, 09:02:24 PM", "NaN", "NaN"], ["2160", "Add a config to limit the number of rows per shard", "Nileema Shingte", "nileema", "12/29/14, 07:22:09 PM", "This imposes an approximate limit on the number of rows per shard.\nThis helps ensure that the shards are not too big, we do not care about\neach shard having exactly the same number of rows.", "NaN"], ["2162", "Add support for parsing array type specifications", "David Phillips", "electrum", "01/02/15, 07:24:34 AM", "This is a precursor to #2157 to support parsing of both standard array type specifications and Presto's more powerful nested collection types.", "NaN"], ["2163", "Fail plugin loading if artifact is missing", "David Phillips", "electrum", "01/02/15, 09:09:18 PM", "If an artifact is missing, the plugin will likely fail later\nwith a confusing NoClassDefFoundError. It is better to fail\nearly with a proper error message.", "NaN"], ["2164", "Allow Raptor files to be sorted on column(s)", "Nileema Shingte", "nileema", "01/07/15, 10:32:04 PM", "- Add  an interface in presto-main to sort a list of pages (this is marked as deprecated, we should identify a better way to expose this functionality)\n- Plug it in the Raptor page sink, cache pages in Raptor, and sort them before writing to disk ", "NaN"], ["2166", "Categorize errors for DateTime functions", "David Phillips", "electrum", "12/30/14, 10:59:21 PM", "NaN", "NaN"], ["2167", "Categorize errors when opening ORC splits", "David Phillips", "electrum", "12/30/14, 10:59:37 PM", "NaN", "NaN"], ["2171", "Replace boolean min/max with bool_and/bool_or", "David Phillips", "electrum", "01/02/15, 07:28:08 AM", "Also, add standard aggregation function 'every'.", "NaN"], ["2172", "Add test for HAVING without GROUP BY", "David Phillips", "electrum", "01/02/15, 07:52:54 AM", "NaN", "NaN"], ["2174", "Replace checkState with assert in TypeSignature", "David Phillips", "electrum", "01/05/15, 06:55:01 PM", "The assertions are verifying internal invariants of the parser.", "NaN"], ["2175", "Query tree cleanups", "David Phillips", "electrum", "01/05/15, 07:47:03 PM", "NaN", "NaN"], ["2176", "Minor code cleanup for Raptor", "David Phillips", "electrum", "01/05/15, 06:50:23 PM", "NaN", "NaN"], ["2179", "Categorize errors for string functions", "David Phillips", "electrum", "01/05/15, 07:43:35 PM", "NaN", "NaN"], ["2182", "Categorize errors for color functions", "David Phillips", "electrum", "01/05/15, 07:29:46 PM", "NaN", "NaN"], ["2183", "Categorize errors for map constructor", "David Phillips", "electrum", "01/05/15, 07:29:34 PM", "NaN", "NaN"], ["2184", "Update to Airbase 31", "David Phillips", "electrum", "01/05/15, 08:47:36 PM", "NaN", "NaN"], ["2186", "Fix query building for SHOW SESSION", "David Phillips", "electrum", "01/05/15, 11:59:14 PM", "NaN", "NaN"], ["2188", "Update to JLine 2.12", "David Phillips", "electrum", "01/06/15, 07:07:37 PM", "NaN", "NaN"], ["2190", "Ignore EOF exception while writing data to client", "Dain Sundstrom", "dain", "01/22/15, 10:50:22 PM", "EOF is triggered when client disconnects.  This is a client problem so we should not be logging this in the server.", "NaN"], ["2192", "Disable fallback to the interpreter", "David Phillips", "electrum", "01/07/15, 06:54:28 PM", "NaN", "NaN"], ["2195", "Cleanup logging for known remote task errors", "David Phillips", "electrum", "01/23/15, 02:01:19 AM", "NaN", "NaN"], ["2196", "Validate type of operand for unary '+'", "Martin Traverso", "martint", "01/08/15, 10:45:04 PM", "The operator is only valid for numeric types (currently DOUBLE and BIGINT)\nFor instance, the following query should fail with a type mismatch error:\n\n  SELECT +'a'\n\nFixes https://github.com/facebook/presto/issues/1412", "NaN"], ["2200", "Encode varbinary literals as a function call to from_base64(...)", "Martin Traverso", "martint", "01/08/15, 09:30:23 PM", "They were being encoded using the \"magic literal\" hack, but the value\nwas being encoded as a UTF-8 string. The magic literal evaluator doesn't\nknow it has to reverse the process, so this ends up corrupting varbinary\nliterals during constant folding or distributed evaluation.\n\nThis minor hack turns varbinary literals into function calls to:\n\n```\nfrom_base64(<base64-encoded varchar>)\n```\n\nFixes https://github.com/facebook/presto/issues/2197", "NaN"], ["2213", "Switch result bytes variable to long", "David Phillips", "electrum", "01/14/15, 05:52:44 PM", "NaN", "NaN"], ["2214", "Use Slice for table writer fragments", "David Phillips", "electrum", "01/21/15, 03:25:47 AM", "NaN", "NaN"], ["2220", "Refactor Hive connector to use standard structure", "David Phillips", "electrum", "01/21/15, 05:24:33 PM", "NaN", "NaN"], ["2222", "Add year of week function", "Joy Yao", "joy-yao", "01/14/15, 07:56:18 PM", "NaN", "NaN"], ["2225", "Improve JDBC driver conformance", "David Phillips", "electrum", "01/22/15, 08:58:13 PM", "NaN", "NaN"], ["2226", "Update modernizer plugin and run in default phase", "David Phillips", "electrum", "01/14/15, 09:39:32 PM", "NaN", "NaN"], ["2229", "Fix the approx_distinct(x, e) documented error bounds", "Eric Hwang", "erichwang", "01/15/15, 08:55:27 PM", "NaN", "NaN"], ["2230", "Add server version to query events", "Joy Yao", "joy-yao", "01/22/15, 07:21:10 PM", "NaN", "NaN"], ["2232", "Remove debug code from CompilerContext", "David Phillips", "electrum", "01/16/15, 11:33:48 PM", "NaN", "NaN"], ["2234", "Fix condition for no nodes available in RaptorSplitManager", "Nileema Shingte", "nileema", "01/17/15, 02:11:47 AM", "NaN", "NaN"], ["2235", "Add regexp_extract_all() function", "Christopher Berner", "cberner", "01/20/15, 09:40:21 PM", "NaN", "NaN"], ["2236", "Add API to close a writer sink", "Eric Hwang", "erichwang", "01/21/15, 01:51:18 AM", "NaN", "NaN"], ["2237", "Optimize imports", "David Phillips", "electrum", "01/21/15, 03:45:25 AM", "NaN", "NaN"], ["2243", "Fix some exception messages in Raptor", "Nileema Shingte", "nileema", "01/21/15, 12:32:37 AM", "NaN", "NaN"], ["2244", "Add config for metastore filtering", "Christopher Berner", "cberner", "01/23/15, 02:10:37 AM", "The metastore does not enforce canonical representation of values. For\nexample, a boolean column may represent the false value as \"0\", \"false\",\n\"False\" or possibly other representations as well. This adds a config\nflag \"hive.assume-canonical-partition-keys\" which can be set to true, if\nall partition values are known to use the canonical (Java)\nrepresentation. Doing so may provide a performance increase for tables\nwith a large number of partitions that have non-string partition keys.", "NaN"], ["2245", "Check file size in OrcReader", "Nileema Shingte", "nileema", "01/21/15, 08:04:13 PM", "NaN", "NaN"], ["2249", "Add regexp_extract_all() that takes a group number", "Christopher Berner", "cberner", "01/22/15, 01:56:46 AM", "NaN", "NaN"], ["2250", "Fix compiler error exception handling", "David Phillips", "electrum", "01/22/15, 01:17:59 AM", "NaN", "NaN"], ["2251", "Fix parsing stack overflow test", "David Phillips", "electrum", "01/22/15, 02:06:25 AM", "NaN", "NaN"], ["2252", "Clean up HttpRemoteTask error tracking", "Dain Sundstrom", "dain", "02/06/15, 07:35:16 PM", "The HttpRemoteTask mixes error tracking for updates and gets.  Since the gets virtually never fail, the gets cancel out failures from updates.  This PR splits the two into separate error trackers, so they can fail independently.", "NaN"], ["2254", "Add ClassLoaderSafeConnectorPageSinkProvider", "Eric Hwang", "erichwang", "01/22/15, 07:53:05 PM", "NaN", "NaN"], ["2260", "Update doc about presto version in query events", "Joy Yao", "joy-yao", "01/23/15, 01:38:20 AM", "NaN", "NaN"], ["2261", "Update 0.90 release notes", "Christopher Berner", "cberner", "01/23/15, 01:48:11 AM", "NaN", "NaN"], ["2265", "Add release note about partial table write fix", "Dain Sundstrom", "dain", "01/23/15, 07:59:36 PM", "NaN", "NaN"], ["2266", "Cleanup 0.90 release notes", "Martin Traverso", "martint", "01/23/15, 10:51:29 PM", "NaN", "NaN"], ["2267", "Restore min/max aggregations for booleans", "David Phillips", "electrum", "01/23/15, 10:59:17 PM", "NaN", "NaN"], ["2268", "Make server http threads configurable (increase defaults)", "Eric Hwang", "erichwang", "01/24/15, 08:26:10 AM", "Also export the executors as mbeans to get counters", "NaN"], ["2269", "Clear LazyBlockLoader reference after block load", "Dain Sundstrom", "dain", "01/26/15, 02:28:58 AM", "LazyBlockLoaders for ORC retains large memory allocations, which\nare not needed once the block is fully loaded.", "NaN"], ["2270", "Add 0.91 release notes", "Dain Sundstrom", "dain", "01/26/15, 03:10:15 AM", "NaN", "NaN"], ["2271", "Fix compilation for try_cast", "David Phillips", "electrum", "02/09/15, 11:01:32 PM", "Fixes #1696", "NaN"], ["2272", "Don't log stack trace for expected task failures", "David Phillips", "electrum", "02/11/15, 01:28:13 AM", "NaN", "NaN"], ["2274", "Add benchmark for structured types", "Christopher Berner", "cberner", "01/26/15, 10:32:51 PM", "NaN", "NaN"], ["2276", "Fix type parameter binding for unknown type", "Christopher Berner", "cberner", "02/03/15, 12:57:57 AM", "Fix case were type parameters could remain unbound during function\nresolution, if the argument type was the unknown type", "NaN"], ["2277", "Refactor Raptor StorageManager interfaces", "David Phillips", "electrum", "01/28/15, 11:36:08 PM", "NaN", "NaN"], ["2278", "Allow creating OrcRecordReader for entire file", "David Phillips", "electrum", "01/27/15, 09:43:23 PM", "NaN", "NaN"], ["2279", "Fix memory leak from aborted and failed tasks", "Dain Sundstrom", "dain", "01/28/15, 01:18:49 AM", "NaN", "NaN"], ["2281", "Add 0.92 release notes", "Dain Sundstrom", "dain", "01/28/15, 05:14:44 AM", "NaN", "NaN"], ["2282", "Set end time on tasks deleted before creation", "Dain Sundstrom", "dain", "01/28/15, 06:28:08 PM", "End time must be set for the task to be purged", "NaN"], ["2287", "Do not push down nondeterministic predicate", "Joy Yao", "joy-yao", "02/05/15, 10:57:45 PM", "NaN", "NaN"], ["2288", "Add error code for Hive JVM timezone mismatch", "David Phillips", "electrum", "01/29/15, 12:08:41 AM", "NaN", "NaN"], ["2293", "Allow ValuesNode with 0 rows", "Zhenxiao Luo", "zhenxiao", "01/29/15, 08:06:39 PM", "Fix https://github.com/facebook/presto/issues/2291", "NaN"], ["2296", "Fix incorrect check to disable string statistics in ORC", "Martin Traverso", "martint", "01/29/15, 08:22:20 PM", "The check was mistakenly added to the date statistics instead of to the string statistics code", "NaN"], ["2297", "Add file size checks to shard recovery", "Nileema Shingte", "nileema", "02/24/15, 11:13:05 PM", "There were instances where the shard was recovered as a zero byte file.\nWe do not know the root cause of this, so add a check and classify the\nerror so that we can monitor future occurances of this behavior.\n\nAdd unit tests for shard recovery", "NaN"], ["2299", "Remove Hive shading for libthrift", "David Phillips", "electrum", "01/30/15, 12:06:06 AM", "NaN", "NaN"], ["2302", "Fix analysis of window frames", "Christopher Berner", "cberner", "01/30/15, 09:02:43 PM", "Fix bug where comparable, non-orderable types could be used in the ORDER\nBY clause of a window frame", "NaN"], ["2303", "Have INSERT ignore hidden columns", "Eric Hwang", "erichwang", "01/30/15, 01:24:55 AM", "NaN", "NaN"], ["2304", "Fix TableWriterNode creation when table has hidden columns", "Eric Hwang", "erichwang", "01/30/15, 01:54:10 AM", "NaN", "NaN"], ["2306", "Allow COORDINATOR ONLY plan fragment to remain for the source", "Eric Hwang", "erichwang", "01/30/15, 03:30:36 AM", "This occurs when the LocalQueryRunner executes a query with a TableCommit.", "NaN"], ["2308", "Add method to create HiveMetastoreClient with a TProtocol", "Joy Yao", "joy-yao", "02/04/15, 05:23:31 PM", "NaN", "NaN"], ["2309", "Add shard pruning to Raptor", "David Phillips", "electrum", "02/26/15, 12:54:35 AM", "NaN", "NaN"], ["2310", "Update website links to use HTTPS", "David Phillips", "electrum", "01/31/15, 06:32:25 AM", "NaN", "NaN"], ["2312", "Synchronize on-disk data for Raptor", "David Phillips", "electrum", "02/12/15, 12:11:20 AM", "NaN", "NaN"], ["2313", "Cache specialized functions", "Christopher Berner", "cberner", "02/03/15, 06:18:11 PM", "This makes queries on relatively small amounts of data that involve\nparametric functions nearly 2x faster.\n\nFixes: https://github.com/facebook/presto/issues/2106", "NaN"], ["2315", "Add constraint violation standard error code", "Eric Hwang", "erichwang", "02/03/15, 06:35:49 AM", "NaN", "NaN"], ["2316", "Remove incorrect check when expiring queries", "Martin Traverso", "martint", "02/03/15, 08:32:19 PM", "A query that's in the expiration queue will necessarily have an associated\nquery execution, so checking whether queries.remove() returns non-null and\nlogging an error is incorrect.", "NaN"], ["2323", "CLI improvements", "David Phillips", "electrum", "02/05/15, 07:49:59 PM", "NaN", "NaN"], ["2327", "Fix update count fetch in StatementResource", "David Phillips", "electrum", "02/05/15, 08:19:44 PM", "NaN", "NaN"], ["2328", "Add Raptor tests for exceptional double values", "David Phillips", "electrum", "02/05/15, 08:57:25 PM", "NaN", "NaN"], ["2332", "Use individual slices for each value in ORC", "Dain Sundstrom", "dain", "02/06/15, 05:43:17 PM", "Instead of using a shared buffer for blocks and dictionaries use separate slice\nfor each value.  This reduces heap fragmentation and excessive memory retention\nat the expense of object creation.", "NaN"], ["2333", "Categorize Hive errors", "David Phillips", "electrum", "02/07/15, 08:24:44 PM", "NaN", "NaN"], ["2334", "Fix ORDER BY with LIMIT 0", "David Phillips", "electrum", "02/11/15, 12:47:12 AM", "NaN", "NaN"], ["2335", "Update to airlift 0.101-SNAPSHOT", "Martin Traverso", "martint", "02/07/15, 06:20:12 AM", "NaN", "NaN"], ["2336", "Add 0.93 release notes", "Dain Sundstrom", "dain", "02/06/15, 11:12:35 PM", "NaN", "NaN"], ["2337", "Validate method names in InvokeInstruction", "David Phillips", "electrum", "02/11/15, 02:24:25 AM", "NaN", "NaN"], ["2338", "Use scheduleWithFixedDelay instead of scheduleAtFixedRate for background tasks", "Dain Sundstrom", "dain", "02/08/15, 12:07:44 AM", "NaN", "NaN"], ["2339", "Implement try_cast as a parametric function", "Martin Traverso", "martint", "02/09/15, 11:29:46 PM", "NaN", "NaN"], ["2340", "Remove outdated JVM options", "David Phillips", "electrum", "02/09/15, 11:04:04 PM", "These options are not needed for Java 8:\n- CMSClassUnloadingEnabled is true by default\n- ReservedCodeCacheSize is larger than 150m by default", "NaN"], ["2343", "Add option to limit ORC read size", "Dain Sundstrom", "dain", "02/09/15, 09:03:06 PM", "NaN", "NaN"], ["2344", "Update thread group nameFormat to %s", "Dain Sundstrom", "dain", "02/11/15, 06:31:16 AM", "NaN", "NaN"], ["2347", "Reduce ORC reader memory usage", "Christopher Berner", "cberner", "02/11/15, 09:48:56 PM", "NaN", "NaN"], ["2348", "Upgrade to Airlift 0.102", "Dain Sundstrom", "dain", "02/11/15, 07:07:03 AM", "NaN", "NaN"], ["2353", "Update Hive CDH 4 plugin to CDH 4.7.1", "David Phillips", "electrum", "02/11/15, 08:17:50 PM", "This fixes the socket leak in DFSClient (HDFS-5671).", "NaN"], ["2355", "Fix native library for CDH 4", "David Phillips", "electrum", "02/11/15, 11:56:57 PM", "NaN", "NaN"], ["2357", "Enforce ORC max buffer size", "Dain Sundstrom", "dain", "02/17/15, 01:05:13 AM", "NaN", "NaN"], ["2358", "Fix modernize warnings", "David Phillips", "electrum", "02/12/15, 06:26:46 PM", "NaN", "NaN"], ["2359", "Only check operator.isBlocked if no pages were moved", "Dain Sundstrom", "dain", "04/21/15, 10:04:22 PM", "Unblock when any operator becomes unblocked.", "NaN"], ["2360", "Pack ORC direct slices into a single buffer", "Dain Sundstrom", "dain", "02/14/15, 08:53:01 PM", "NaN", "NaN"], ["2361", "Update to discovery-server 1.21", "David Phillips", "electrum", "02/12/15, 10:43:10 PM", "NaN", "NaN"], ["2363", "Increase flexibility of Hadoop configuration", "David Phillips", "electrum", "02/13/15, 07:12:36 PM", "NaN", "NaN"], ["2364", "Detect quick worker reboots and fail queries", "Dain Sundstrom", "dain", "02/14/15, 12:41:23 AM", "If a worker reboots quickly, the engine may think it is the same process and\nwill wait for results forever.  This change adds the process unique worker\ninstance id to the task info and if that changes, the query is failed\nimmediately.\n\nFixes #2330", "NaN"], ["2377", "Refactor query queueing", "Christopher Berner", "cberner", "03/06/15, 05:00:25 PM", "- Make the queuing policy injectable\n- Make default implementation more flexible", "NaN"], ["2378", "Improve field reference operator", "Christopher Berner", "cberner", "03/24/15, 12:03:38 AM", "Fixes #1944", "NaN"], ["2379", "Upgrade to dependencies", "Dain Sundstrom", "dain", "02/19/15, 06:53:20 AM", "Airbase 33\nAirlfit 0.103\nSlice 0.10\nJetty 9.2.8.v20150217\nDiscover 1.22", "NaN"], ["2383", "Fix documentation for aggregate functions", "David Phillips", "electrum", "02/19/15, 08:57:34 PM", "NaN", "NaN"], ["2386", "Test fixes from Josh@Travis-CI", "Christopher Berner", "cberner", "02/21/15, 03:47:46 PM", "NaN", "NaN"], ["2389", "Fix stage/task leak", "Christopher Berner", "cberner", "02/20/15, 08:10:58 PM", "Fixes #2387", "NaN"], ["2390", "Add support for hash-partitioned semijoins", "Haozhun Jin", "haozhun", "07/15/15, 06:11:28 PM", "Issue #2263 ", "NaN"], ["2391", "Optimize PurgeQueriesRunnable", "Christopher Berner", "cberner", "02/20/15, 09:42:13 PM", "NaN", "NaN"], ["2393", "Fix online shard recovery for Raptor", "David Phillips", "electrum", "02/21/15, 02:41:34 AM", "NaN", "NaN"], ["2394", "Fix race condition in TestExchangeClient", "Christopher Berner", "cberner", "02/21/15, 08:05:26 PM", "NaN", "NaN"], ["2395", "Show stack trace on query detail page in UI", "Haozhun Jin", "haozhun", "03/06/15, 08:21:41 PM", "Issue #2365 ", "NaN"], ["2396", "Fix race condition in TestPrioritizedFifoExecutor", "Christopher Berner", "cberner", "02/23/15, 03:04:17 AM", "Also make a couple improvements to the Travis build", "NaN"], ["2398", "Replace BackgroundCacheLoader with asyncReloading", "David Phillips", "electrum", "02/23/15, 05:42:39 PM", "NaN", "NaN"], ["2399", "Make SqlTask heartbeat update explicit", "Dain Sundstrom", "dain", "02/24/15, 05:22:14 AM", "SqlTask implicitly updates the heartbeat when getting TaskInfo.  Since the TaskInfo is fetched while checking for abandoned tasks, the tasks can never be abandoned.", "NaN"], ["2403", "Upgrade to Guice 4", "Christopher Berner", "cberner", "03/02/15, 08:17:03 PM", "NaN", "NaN"], ["2404", "Add session property for distributed joins", "Nileema Shingte", "nileema", "02/25/15, 06:18:45 PM", "NaN", "NaN"], ["2405", "Add support for Hive date partitions", "David Phillips", "electrum", "02/25/15, 07:05:21 PM", "NaN", "NaN"], ["2409", "Ensure symbols are not reused across plan nodes", "Martin Traverso", "martint", "02/25/15, 08:58:59 PM", "This fixes an aliasing issue that causes queries to return incorrect results.\nIt happens because symbol allocator can produce non-globally unique names when\ncolumns in the table share the same prefix and have the form xxx_nnn. E.g.,\n\n``` sql\nCREATE TABLE t AS SELECT * FROM (VALUES (1,2)) t (foo_1, foo_2_4);\nSELECT foo_1, foo_2_4 from t;\n```\n\nIncorrect plan:\n\n```\n - Output[foo_1, foo_2_4] => [foo:bigint, foo:bigint]\n         foo_1 := foo\n         foo_2_4 := foo\n     - Exchange[GATHER] => foo:bigint\n         - TableScan[t, original constraint=true] => [foo:bigint]\n                 foo := <column foo_1>\n```\n\nExpected plan:\n\n```\n - Output[foo_1, foo_2_4] => [foo:bigint, foo_2:bigint]\n         foo_1 := foo\n         foo_2_4 := foo_2\n     - Exchange[GATHER] => foo:bigint, foo_2:bigint\n         - TableScan[t, original constraint=true] => [foo:bigint, foo_2:bigint]\n                 foo := <column foo_1>\n                 foo_2 := <column foo_2_4>\n```", "NaN"], ["2410", "Fix counter leak", "Christopher Berner", "cberner", "02/25/15, 09:34:15 PM", "NaN", "NaN"], ["2412", "Add from_iso8601_* and to_iso8601", "Haozhun Jin", "haozhun", "06/08/15, 07:42:44 PM", "Issue #1323 \n\n@dain ", "NaN"], ["2413", "Mitigate jetty client leak", "Dain Sundstrom", "dain", "02/26/15, 06:24:24 PM", "NaN", "NaN"], ["2416", "Fix handling of structs in Parquet", "Christopher Berner", "cberner", "03/02/15, 11:10:24 PM", "NaN", "NaN"], ["2422", "Update release notes for 0.96", "Christopher Berner", "cberner", "02/27/15, 01:20:46 AM", "NaN", "NaN"], ["2423", "Update 0.96 release notes", "Martin Traverso", "martint", "02/27/15, 12:12:44 AM", "NaN", "NaN"], ["2424", "Fix Raptor index insertion", "David Phillips", "electrum", "02/26/15, 09:47:01 PM", "NaN", "NaN"], ["2425", "Update 0.96 release notes", "Dain Sundstrom", "dain", "02/27/15, 01:15:14 AM", "NaN", "NaN"], ["2434", "Fix memory usage accounting in MapAggregation", "Haozhun Jin", "haozhun", "03/04/15, 09:10:42 PM", "@cberner The issue we found when you were helping me understand MapAggregation this morning.", "NaN"], ["2436", "Speed up detection of ASCII strings in LIKE implementation", "Martin Traverso", "martint", "02/27/15, 05:22:06 PM", "Benchmark for a 100k pure ASCII string:\n\n```\nBenchmark  (inputSize)   Mode   Samples        Score  Score error    Units\nold             100000  thrpt        10    18110.697     1350.503    ops/s\nnew             100000  thrpt        10    53863.580     1914.092    ops/s\n```\n\nBenchmark for a 100k string with a non-ASCII character in the middle:\n\n```\nBenchmark  (inputSize)   Mode   Samples        Score  Score error    Units\nold             100000  thrpt        10    18632.550      740.718    ops/s\nnew             100000  thrpt        10   110526.867     3758.566    ops/s\n```", "NaN"], ["2438", "Add to_base and from_base UDF", "Haozhun Jin", "haozhun", "03/26/15, 05:31:43 PM", "NaN", "NaN"], ["2441", "Fix NPE in MetadataQueryOptimizer", "Martin Traverso", "martint", "02/27/15, 10:29:45 PM", "The PlanRewriter refactoring in b4eaa84b769c2ef7cc585977e59dc418d2c9035e missed a couple of changes\nto MetadataQueryOptimizer that result in NullPointerExceptions", "NaN"], ["2446", "Add parametric max and min function", "Haozhun Jin", "haozhun", "03/13/15, 05:12:12 PM", "Only long is implemented. Let me know if I can go ahead with the other 3 types. @martint ", "NaN"], ["2447", "Improve HttpRemoteTask update loop", "Dain Sundstrom", "dain", "03/11/15, 11:56:09 PM", "Always clear currentRequest when request is complete\nTreat more errors a fatal", "NaN"], ["2449", "Remove table_shards table from Raptor", "David Phillips", "electrum", "03/06/15, 02:09:47 AM", "NaN", "NaN"], ["2450", "New rcfile reader", "Dain Sundstrom", "dain", "10/04/16, 11:50:14 PM", "NaN", "NaN"], ["2455", "Move minColumn and maxColumn to DatabaseShardManager", "Nileema Shingte", "nileema", "03/02/15, 11:40:16 PM", "The function for creating the table name is in DatabaseShardManager, so it is more intuitive to look for the column name functions here. ", "NaN"], ["2457", "Fix unzip logic in ORC", "Martin Traverso", "martint", "03/04/15, 06:41:07 AM", "- Don't loop, since Inflater will decompress in one shot if all the necessary input data is available\n- Add a check to verify that the Inflater is \"finished\" after the input is decompressed\n- Don't assume offset/length of the output buffer are those of the underlying array (i.e., in case the\n  output buffer was a sub-slice of a bigger array.\n- Always call Inflater.end() to free up buffers", "NaN"], ["2460", "Don't ignore NOT in NOT BETWEEN predicate", "Martin Traverso", "martint", "03/04/15, 08:25:01 PM", "Fixes https://github.com/facebook/presto/issues/2459", "NaN"], ["2461", "Add array_distinct array function", "Haozhun Jin", "haozhun", "03/23/15, 06:17:31 PM", "@cberner please review", "NaN"], ["2462", "Make table writing always distributed", "Haozhun Jin", "haozhun", "06/30/15, 03:24:26 AM", "~~Depends on #2390~~", "NaN"], ["2464", "When scheduling only wait for RemoteSourceNodes", "Dain Sundstrom", "dain", "03/09/15, 07:58:16 PM", "Several parts of the scheduler system would use PlanFragment.getSources\nto find all nodes to schedule, and this method would look for all leaf nodes\nwhich are not in a black list. This black list was error prone and is missing\nValuesNode.  This missing entry causes the scheduler to wait for ValuesNodes\nto be scheduled which never happens since values nodes are not schduled.\n\nThe getSources is not really needed as all callers are searching for\nRemoteSourceNodes, so there is simply a method for that now.", "NaN"], ["2466", "Fix planner bug when mixing window functions and implicit coercions", "Martin Traverso", "martint", "03/06/15, 12:16:09 AM", "A query that has:\n- window functions\n- implicit coercions\n- wildcard field references (e.g., SELECT *)\n\nSuch as:\n\n```\nSELECT *, 1.0 * count(*) OVER () FROM (VALUES 1);\n```\n\nFails during planning with the following error:\n\n```\njava.lang.IllegalStateException: No mapping for field '0'\n    at com.google.common.base.Preconditions.checkState(Preconditions.java:197)\n    at com.facebook.presto.sql.planner.TranslationMap.rewrite(TranslationMap.java:114)\n    at com.facebook.presto.sql.planner.PlanBuilder.rewrite(PlanBuilder.java:72)\n    at com.facebook.presto.sql.planner.QueryPlanner.project(QueryPlanner.java:210)\n    at com.facebook.presto.sql.planner.QueryPlanner.visitQuerySpecification(QueryPlanner.java:128)\n    at com.facebook.presto.sql.planner.QueryPlanner.visitQuerySpecification(QueryPlanner.java:70)\n    at com.facebook.presto.sql.tree.QuerySpecification.accept(QuerySpecification.java:98)\n    at com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:22)\n    at com.facebook.presto.sql.planner.RelationPlanner.visitQuerySpecification(RelationPlanner.java:416)\n    at com.facebook.presto.sql.planner.RelationPlanner.visitQuerySpecification(RelationPlanner.java:97)\n    at com.facebook.presto.sql.tree.QuerySpecification.accept(QuerySpecification.java:98)\n    at com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:22)\n    at com.facebook.presto.sql.planner.QueryPlanner.planQueryBody(QueryPlanner.java:141)\n    at com.facebook.presto.sql.planner.QueryPlanner.visitQuery(QueryPlanner.java:97)\n    at com.facebook.presto.sql.planner.QueryPlanner.visitQuery(QueryPlanner.java:70)\n    at com.facebook.presto.sql.tree.Query.accept(Query.java:80)\n    at com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:22)\n    at com.facebook.presto.sql.planner.RelationPlanner.visitQuery(RelationPlanner.java:403)\n    at com.facebook.presto.sql.planner.RelationPlanner.visitQuery(RelationPlanner.java:97)\n    at com.facebook.presto.sql.tree.Query.accept(Query.java:80)\n    at com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:22)\n    at com.facebook.presto.sql.planner.LogicalPlanner.createRelationPlan(LogicalPlanner.java:176)\n    at com.facebook.presto.sql.planner.LogicalPlanner.plan(LogicalPlanner.java:80)\n```", "NaN"], ["2468", "Optimize memory allocations in ORC stream reader", "Christopher Berner", "cberner", "03/09/15, 08:48:46 PM", "NaN", "NaN"], ["2473", "Add split string function", "Haozhun Jin", "haozhun", "03/24/15, 03:04:50 AM", "NaN", "NaN"], ["2474", "Use default methods for optional Connector methods", "David Phillips", "electrum", "03/09/15, 04:39:03 PM", "NaN", "NaN"], ["2480", "Treat small ORC stripes as a single row group", "Dain Sundstrom", "dain", "03/09/15, 09:54:23 PM", "NaN", "NaN"], ["2481", "Refactor aggregation compiler to use MethodHandle", "Christopher Berner", "cberner", "03/09/15, 10:38:14 PM", "NaN", "NaN"], ["2482", "Disable ORC predicate pushdown for DATE", "Dain Sundstrom", "dain", "03/09/15, 10:36:34 PM", "NaN", "NaN"], ["2483", "Update 0.97 release notes", "Christopher Berner", "cberner", "03/10/15, 05:41:32 PM", "NaN", "NaN"], ["2484", "Rename NullableBigintState to NullableLongState", "Haozhun Jin", "haozhun", "03/10/15, 04:46:38 AM", "NaN", "NaN"], ["2485", "Update 0.97 release notes", "Martin Traverso", "martint", "03/10/15, 03:32:01 AM", "NaN", "NaN"], ["2486", "Add test for TupleDomainOrcPredicate", "Dain Sundstrom", "dain", "03/26/15, 11:53:58 PM", "NaN", "NaN"], ["2488", "Add shard size constraints to Raptor", "Nileema Shingte", "nileema", "03/12/15, 01:27:45 AM", "NaN", "NaN"], ["2489", "Select Hive reader by serde name not instance", "Dain Sundstrom", "dain", "03/11/15, 05:02:30 AM", "Instead of the expensive and error process of selecting the Hive reader by\ninstantiating a serde and then checking for a type, just check the class name\nof the serde.\n\nFixes #1734", "NaN"], ["2490", "Heap dump on out of memory error in tests", "Christopher Berner", "cberner", "03/13/15, 04:16:55 PM", "NaN", "NaN"], ["2491", "Add deadlock detection to queue manager", "Haozhun Jin", "haozhun", "03/23/15, 10:04:57 PM", "Issue #2470 ", "NaN"], ["2492", "Fix assertInvalidFunction so that test fails when no exception is thrown", "Haozhun Jin", "haozhun", "03/14/15, 12:00:43 AM", "NaN", "NaN"], ["2493", "Add test scope for testing-postgresql-server", "David Phillips", "electrum", "03/10/15, 10:14:33 PM", "NaN", "NaN"], ["2494", "Update queue configuration documentation", "David Phillips", "electrum", "03/10/15, 10:16:59 PM", "NaN", "NaN"], ["2495", "Handle ORC files with corrupt checkpoints", "Dain Sundstrom", "dain", "03/11/15, 12:04:14 AM", "When ORC switched to low memory mode it can create invalid checkpoints.  When\nreading the file if the checkpoints can not be decoded, treat the entire strip\nas a single row group.  DWRF files with a row group dictionary still fail\nbecause the row group dictionary length is contained in the checkpoint stream.", "NaN"], ["2496", "Update FAQ", "David Phillips", "electrum", "03/10/15, 11:23:45 PM", "NaN", "NaN"], ["2497", "More fixes for test failures in Travis", "Christopher Berner", "cberner", "03/11/15, 11:39:09 PM", "NaN", "NaN"], ["2499", "Clarify config property and queue entry in Queue Configuration", "Haozhun Jin", "haozhun", "03/11/15, 01:40:24 AM", "NaN", "NaN"], ["2506", "Merge new array/map/row memory format from Treasure Data", "Christopher Berner", "cberner", "03/13/15, 04:47:00 AM", "A few modifications on top of https://github.com/facebook/presto/pull/2240", "NaN"], ["2509", "Stop word wrap in stacktrace display in query.html UI", "Haozhun Jin", "haozhun", "03/12/15, 09:23:45 PM", "NaN", "NaN"], ["2512", "Fix Raptor storage config property name", "Nileema Shingte", "nileema", "03/12/15, 08:19:52 PM", "NaN", "NaN"], ["2516", "Preserve partitioning when planning non-decomposable aggregations", "Martin Traverso", "martint", "03/17/15, 07:50:33 PM", "NaN", "NaN"], ["2519", "Fix build broke by 9dab649", "Haozhun Jin", "haozhun", "03/13/15, 05:41:45 PM", "NaN", "NaN"], ["2520", "Fix build broken by f1bf4f1", "Haozhun Jin", "haozhun", "03/13/15, 06:03:37 PM", "NaN", "NaN"], ["2522", "Add support for various Parquet serde class names", "Dain Sundstrom", "dain", "03/13/15, 07:18:40 PM", "Fixes #2518", "NaN"], ["2524", "Improve memory allocation by block builders", "Christopher Berner", "cberner", "03/17/15, 01:33:54 AM", "NaN", "NaN"], ["2525", "Cache only certain directories for Travis", "Christopher Berner", "cberner", "03/16/15, 07:43:45 PM", "Builds are failing because the cache can't update within the 10min\ntimeout.", "NaN"], ["2529", "SPI cleanup", "David Phillips", "electrum", "03/17/15, 04:22:43 PM", "NaN", "NaN"], ["2533", "Evaluate arguments of lazy functions", "Haozhun Jin", "haozhun", "03/24/15, 03:17:58 AM", "#2346\n\ncc @martint ", "NaN"], ["2534", "Reduce Travis memory usage for build", "Christopher Berner", "cberner", "03/17/15, 12:17:26 AM", "This should reduce the chance of out of memory issues", "NaN"], ["2536", "Fix unreferenced-pruning bug in unnest", "Haozhun Jin", "haozhun", "03/17/15, 06:45:07 PM", "#2535", "NaN"], ["2537", "Remove keyType and valueType from MaxOrMinByState", "Haozhun Jin", "haozhun", "03/19/15, 08:56:07 PM", "NaN", "NaN"], ["2538", "Pre-partitioned window operator", "Dain Sundstrom", "dain", "04/03/15, 07:14:29 PM", "NaN", "NaN"], ["2545", "Allow opening query detail page in new tab", "Haozhun Jin", "haozhun", "03/18/15, 12:18:33 AM", "This is a quick hack to @electrum's #2515. Fixing this correctly would require using `<a>`, which would in turn require changing the table html to `<div>`s", "NaN"], ["2549", "Upgrade to Airbase 35, Airlift 0.105, Jetty 9.2.10.v20150310 and Discovery 1.23", "Dain Sundstrom", "dain", "03/17/15, 11:02:24 PM", "NaN", "NaN"], ["2550", "Add release notes for 0.98", "Christopher Berner", "cberner", "03/18/15, 08:05:27 PM", "NaN", "NaN"], ["2551", "Add release notes for 0.98", "Haozhun Jin", "haozhun", "03/18/15, 05:15:54 PM", "NaN", "NaN"], ["2552", "Optimize locking in TaskExecutor", "Christopher Berner", "cberner", "03/20/15, 10:51:44 PM", "taskHandle.destroy() can block for an arbitrary amount of time since it\ncloses operators, and this can involve calls into connectors. In\nparticular, this can call into HiveRecordSink.rollback() which triggers\nRPCs to HDFS.", "NaN"], ["2557", "Revamp system tables", "David Phillips", "electrum", "03/27/15, 05:54:35 PM", "NaN", "NaN"], ["2562", "Fix precomputed hash optimization", "Christopher Berner", "cberner", "03/20/15, 05:56:05 AM", "The precomputed hash optimization skips rows where any column is NULL.\nThis results in an uninitialized read in PrecomputedHashGenerator, which\ncauses all of those rows to hash to zero, which in turn causes\nGroupByHash to de-generate into a linked list resulting in O(n^2)\nrunning time.", "NaN"], ["2566", "Fix reading maps with NULL keys from ORC", "Christopher Berner", "cberner", "03/20/15, 07:53:18 PM", "NaN", "NaN"], ["2567", "Include node IDs in shard index table", "David Phillips", "electrum", "03/25/15, 01:02:56 AM", "NaN", "NaN"], ["2570", "Merge map subscript for UnknownType", "Christopher Berner", "cberner", "03/23/15, 07:29:26 PM", "NaN", "NaN"], ["2571", "Catch VerifyError when loading generated expression", "Dain Sundstrom", "dain", "03/25/15, 09:27:06 PM", "NaN", "NaN"], ["2572", "Do not over write last execution start", "Dain Sundstrom", "dain", "03/25/15, 07:54:18 PM", "NaN", "NaN"], ["2573", "Revert to older version of jetty", "Nileema Shingte", "nileema", "03/20/15, 11:25:21 PM", "NaN", "NaN"], ["2575", "Add release notes for 0.99", "Nileema Shingte", "nileema", "03/21/15, 03:03:37 AM", "NaN", "NaN"], ["2576", "Revert to previous Airbase, Airlift and Discovery", "David Phillips", "electrum", "03/21/15, 12:42:01 AM", "NaN", "NaN"], ["2577", "Rename task.shard.max-threads to task.split.max-threads", "David Phillips", "electrum", "03/24/15, 07:52:54 PM", "NaN", "NaN"], ["2584", "Various code cleanup", "Christopher Berner", "cberner", "03/25/15, 08:21:59 PM", "Also make the Travis tests more reliable", "NaN"], ["2585", "Fix queue name collisions", "Christopher Berner", "cberner", "03/26/15, 08:42:17 PM", "NaN", "NaN"], ["2587", "Improve performance of GroupByHash for single channel BIGINT", "Dain Sundstrom", "dain", "04/02/15, 11:32:03 PM", "Since BIGINT type is 64 bits the data can be place directly in the hashtable\nwhich speeds up performance by about 4x.\n\n```\nBenchmark                                             (channelCount)  (groupCount)  (hashEnabled)  Mode  Samples    Score  Score error  Units\nc.f.p.o.BenchmarkGroupByHash.groupByHashPreCompute                 1       3000000           true  avgt       20  383.543       30.581  ns/op\nc.f.p.o.BenchmarkGroupByHash.groupByHashPreCompute                 1       3000000          false  avgt       20  405.489       28.953  ns/op\nc.f.p.o.BenchmarkGroupByHash.addPagePreCompute                     1       3000000           true  avgt       20  288.792        2.623  ns/op\nc.f.p.o.BenchmarkGroupByHash.addPagePreCompute                     1       3000000          false  avgt       20  305.904        4.585  ns/op\nc.f.p.o.BenchmarkGroupByHash.bigintGroupByHash                     1       3000000           true  avgt       20  101.429        1.648  ns/op\nc.f.p.o.BenchmarkGroupByHash.bigintGroupByHash                     1       3000000          false  avgt       20   95.929        1.533  ns/op\n```", "NaN"], ["2588", "Fix referencing NULL values in ROWS", "Christopher Berner", "cberner", "03/25/15, 07:22:36 PM", "NaN", "NaN"], ["2589", "Free blocked splits when task is terminated", "Dain Sundstrom", "dain", "03/26/15, 04:42:50 AM", "NaN", "NaN"], ["2593", "Fix '%f' specifier for date_format and date_parse", "Haozhun Jin", "haozhun", "03/26/15, 10:09:30 PM", "#2590", "NaN"], ["2596", "Upgrade to Jetty 9.2.11.M0, Airlift 0.106, Airbase 36 and Discovery 1.24", "Dain Sundstrom", "dain", "03/26/15, 07:35:16 PM", "NaN", "NaN"], ["2597", "Extract AbstractTestFunctions", "David Phillips", "electrum", "04/08/15, 10:50:32 PM", "NaN", "NaN"], ["2598", "Improve javadoc for Type.createBlockBuilder()", "Christopher Berner", "cberner", "03/26/15, 10:25:34 PM", "NaN", "NaN"], ["2602", "Add hash partition count as a session property", "Nileema Shingte", "nileema", "03/31/15, 10:26:14 PM", "I need this for testing the Raptor queries. We can take it out later. ", "NaN"], ["2603", "Execute non-data parallel tasks in parallel", "Dain Sundstrom", "dain", "05/04/15, 09:35:27 PM", "NaN", "NaN"], ["2605", "Add missing HASH_CODE(map<K,V>)", "Christopher Berner", "cberner", "03/31/15, 04:41:28 PM", "NaN", "NaN"], ["2607", "Verify expression type in FunctionAssertions", "Dain Sundstrom", "dain", "04/02/15, 01:21:40 AM", "NaN", "NaN"], ["2612", "Metadata improvements", "David Phillips", "electrum", "03/30/15, 08:56:29 PM", "NaN", "NaN"], ["2614", "Fix coercions in joins", "Christopher Berner", "cberner", "03/30/15, 09:46:34 PM", "NaN", "NaN"], ["2615", "Speedup tests", "Christopher Berner", "cberner", "04/06/15, 09:17:34 PM", "Makes the tests about 10% faster on my laptop", "NaN"], ["2617", "Add release notes and doc for 0.100", "Haozhun Jin", "haozhun", "03/31/15, 07:10:31 PM", "NaN", "NaN"], ["2618", "Update 0.100 release notes", "Christopher Berner", "cberner", "03/31/15, 06:53:50 PM", "NaN", "NaN"], ["2619", "Categorize errors in Hive connector", "Christopher Berner", "cberner", "03/31/15, 11:51:26 PM", "NaN", "NaN"], ["2620", "Cleanup array documentation", "David Phillips", "electrum", "03/31/15, 07:07:36 PM", "NaN", "NaN"], ["2621", "Sort functions in array documentation", "David Phillips", "electrum", "03/31/15, 07:10:46 PM", "NaN", "NaN"], ["2622", "Add support for create table", "David Phillips", "electrum", "03/31/15, 11:13:59 PM", "NaN", "NaN"], ["2623", "Fix race condition in TestSqlStageExecution", "Christopher Berner", "cberner", "03/31/15, 11:52:25 PM", "NaN", "NaN"], ["2626", "Add shutdown support to Connector API", "David Phillips", "electrum", "04/01/15, 02:20:14 AM", "NaN", "NaN"], ["2628", "Grouping awareness and TableLayout abstraction", "Martin Traverso", "martint", "04/15/15, 05:37:30 PM", "NaN", "NaN"], ["2630", "Add array_agg function", "Haozhun Jin", "haozhun", "04/06/15, 04:43:25 PM", "#1975", "NaN"], ["2633", "Add array_position function", "Haozhun Jin", "haozhun", "04/21/15, 09:30:46 PM", "Documents will be added when you agree with:\n- the order of the parameters \n- the result when no match is found", "NaN"], ["2634", "Add Stats to StorageManager", "Nileema Shingte", "nileema", "04/06/15, 09:00:41 PM", "Add stats for all writes to backup ", "NaN"], ["2637", "Improve HiveSplitSource", "Christopher Berner", "cberner", "04/24/15, 01:26:44 AM", "NaN", "NaN"], ["2638", "Fix join precedence for cross and natural", "Haozhun Jin", "haozhun", "04/03/15, 10:00:15 PM", "Issue #2632 ", "NaN"], ["2641", "Byte code cleanup", "Dain Sundstrom", "dain", "04/08/15, 11:16:47 PM", "NaN", "NaN"], ["2642", "Categorize analysis errors", "Christopher Berner", "cberner", "04/06/15, 10:00:18 PM", "NaN", "NaN"], ["2643", "Fix imports to use non-shaded version of Guava", "Christopher Berner", "cberner", "04/07/15, 12:02:07 AM", "NaN", "NaN"], ["2648", "Add support for FULL OUTER JOIN", "Haozhun Jin", "haozhun", "04/29/15, 07:45:44 PM", "Fixes #2582 ", "NaN"], ["2649", "Categorize errors", "Christopher Berner", "cberner", "04/08/15, 06:10:26 PM", "NaN", "NaN"], ["2652", "Fix layout of main UI tables", "Haozhun Jin", "haozhun", "04/08/15, 05:32:55 PM", "#2513", "NaN"], ["2654", "Fix JDBC DatabaseMetaData.getIdentifierQuoteString", "David Phillips", "electrum", "04/07/15, 11:00:56 PM", "NaN", "NaN"], ["2655", "Treat ORC files as splittable", "David Phillips", "electrum", "04/08/15, 01:13:12 AM", "NaN", "NaN"], ["2656", "Wait for state change in stage instead of sleeping", "Dain Sundstrom", "dain", "04/13/15, 11:15:33 PM", "NaN", "NaN"], ["2657", "Add array_intersect to docs and release notes", "Christopher Berner", "cberner", "04/08/15, 05:48:55 PM", "NaN", "NaN"], ["2659", "Fix connector shutdown", "David Phillips", "electrum", "04/08/15, 04:32:41 PM", "NaN", "NaN"], ["2660", "Fix date/time handling for Kafka", "David Phillips", "electrum", "04/08/15, 08:18:03 PM", "NaN", "NaN"], ["2663", "Fix memory tracking in map_agg", "Christopher Berner", "cberner", "04/09/15, 04:45:24 PM", "Also reduce the memory usage by ~60%", "NaN"], ["2664", "Categorize stack overflow errors", "Christopher Berner", "cberner", "04/09/15, 01:24:36 AM", "NaN", "NaN"], ["2667", "Simplify memory tracking and fix several bugs", "Christopher Berner", "cberner", "04/09/15, 06:14:10 PM", "NaN", "NaN"], ["2670", "Fix compilation of While and DoWhile loops", "David Phillips", "electrum", "04/09/15, 08:56:51 PM", "NaN", "NaN"], ["2672", "Change web interface to use ReactJS and Bootstrap3", "Christopher Berner", "cberner", "04/16/15, 06:17:15 PM", "NaN", "NaN"], ["2673", "Improve handling of unsupported statements", "David Phillips", "electrum", "04/10/15, 12:22:40 AM", "NaN", "NaN"], ["2674", "Minor parser fixes", "David Phillips", "electrum", "04/10/15, 03:31:18 AM", "NaN", "NaN"], ["2679", "File compaction in Raptor", "Nileema Shingte", "nileema", "05/18/15, 04:44:45 PM", "- Compaction of files that are not sorted is simply appending files \n- Sorted files are merge sorted \n- Files that represent time ranges need to be merged across time ranges (not implemented in this PR) ", "NaN"], ["2680", "Fix memory leak in TestingPrestoServer", "Martin Traverso", "martint", "04/13/15, 04:52:04 PM", "InMemoryEventClient accumulates events but never cleans up. Replace it with a no-op event client", "NaN"], ["2683", "Only support per-host configuration for HDFS", "David Phillips", "electrum", "04/16/15, 06:18:18 PM", "NaN", "NaN"], ["2684", "Support multiple session properties in same header", "David Phillips", "electrum", "04/13/15, 10:57:14 PM", "Per RFC 7230 section 3.2.2, multiple header fields with the same name may\nbe combined into a single field with the values as a comma-separated list.", "NaN"], ["2685", "Make unions be partitioned if possible", "Nileema Shingte", "nileema", "04/27/15, 03:44:59 PM", "Depends on #2628 ", "NaN"], ["2689", "More cleanup of memory management", "Christopher Berner", "cberner", "04/15/15, 10:45:37 PM", "NaN", "NaN"], ["2690", "Return query error code to client", "Christopher Berner", "cberner", "04/15/15, 09:23:03 PM", "NaN", "NaN"], ["2691", "Minor fixes", "David Phillips", "electrum", "04/15/15, 06:01:53 PM", "NaN", "NaN"], ["2692", "Remove ExpressionAnalyzer dependency on Metadata\t", "David Phillips", "electrum", "04/16/15, 12:33:17 AM", "NaN", "NaN"], ["2693", "Fix negative memory reservation", "Christopher Berner", "cberner", "04/15/15, 11:09:40 PM", "NaN", "NaN"], ["2695", "Update to mysql-connector-java 5.1.35", "David Phillips", "electrum", "04/15/15, 08:40:29 PM", "NaN", "NaN"], ["2696", "Add Support for Parquet Column Rename", "Dain Sundstrom", "dain", "04/15/15, 09:15:54 PM", "NaN", "NaN"], ["2697", "Print column constraints based on layout predicate", "Martin Traverso", "martint", "04/15/15, 10:13:49 PM", "The layout predicate is the one that reflects the guarantees provided\nby the TableScan node, not the \"current constraint\", which is just a\nholder for temporary state during optimization.", "NaN"], ["2698", "Rename Cassandra CI profile", "David Phillips", "electrum", "04/15/15, 09:09:30 PM", "NaN", "NaN"], ["2699", "Reduce initial number of positions for topNRowNumber", "Martin Traverso", "martint", "04/15/15, 10:12:07 PM", "The operator allocates memory eagerly for that number of positions.\nThe current setting results in big allocations and unnecessary\nmemory usage for the common case.", "NaN"], ["2701", "Make WindowOperator support all streaming modes", "Eric Hwang", "erichwang", "04/16/15, 03:37:55 AM", "WindowOperator can now properly handle cases whereby the partitions are ungrouped, pre-grouped, fully-grouped, and if the sort columns are partial/fully sorted.\n\nAlso removes the existing PrePartitionedWindowOperator which only supported fully-grouped partitions.", "NaN"], ["2703", "Intersect pushed-down predicate with current constraint", "Martin Traverso", "martint", "04/16/15, 03:25:39 AM", "NaN", "NaN"], ["2705", "Implement effective predicate derivation & pushdown for exchange", "Martin Traverso", "martint", "04/16/15, 06:15:59 AM", "NaN", "NaN"], ["2714", "Remove flaky test", "Martin Traverso", "martint", "04/16/15, 05:55:32 PM", "This test fails intermittently due to how Antlr4 works (compared to Antlr3)", "NaN"], ["2715", "Remove unnecessary static qualifier", "Martin Traverso", "martint", "04/16/15, 05:37:32 PM", "NaN", "NaN"], ["2716", "Enable EqualityInference to extract equalities from single value IN value lists", "Eric Hwang", "erichwang", "04/16/15, 08:59:50 PM", "NaN", "NaN"], ["2717", "Minor code cleanup", "Christopher Berner", "cberner", "04/20/15, 04:27:32 PM", "NaN", "NaN"], ["2718", "Add column preferences to TableLayout API", "Martin Traverso", "martint", "04/16/15, 08:32:58 PM", "This enhancement allows the engine to provide a list of columns it's interested\nin. Connectors can use this information make decisions about which layouts are\nbetter than others.", "NaN"], ["2719", "Add session property to prefer streaming operators", "Martin Traverso", "martint", "04/17/15, 07:35:25 AM", "NaN", "NaN"], ["2720", "Add BlockEncodingSerde to MetadataManager", "Haozhun Jin", "haozhun", "04/20/15, 06:47:57 PM", "NaN", "NaN"], ["2721", "Update to Airlift 0.107-SNAPSHOT", "David Phillips", "electrum", "04/16/15, 08:51:15 PM", "NaN", "NaN"], ["2723", "Fix pruning unreferenced outputs for ExchangeNode", "Eric Hwang", "erichwang", "04/17/15, 01:46:54 AM", "-Also add some more sanity checks to ExchangeNode constructor", "NaN"], ["2724", "Add final UnaliasSymbolReferences optimizer run", "Eric Hwang", "erichwang", "04/16/15, 10:51:21 PM", "NaN", "NaN"], ["2727", "Refactor operator memory tracking to support blocking", "Christopher Berner", "cberner", "04/21/15, 01:29:41 AM", "Currently we don't do any blocking, but this is necessary to support the\nnew memory management system", "NaN"], ["2731", "Add TableCommitNode processing to PruneUnreferencedOutputs", "Eric Hwang", "erichwang", "04/17/15, 08:30:39 AM", "With the new way of planning exchanges, we were dropping all of the inputs\nbetween the TableCommitNode and the TableWriterNode, thereby failing all Presto\nwrite distributed write capability. This diff fixes and restores that before.", "NaN"], ["2733", "Add z-key shortcut and fix lag in web UI", "Haozhun Jin", "haozhun", "04/17/15, 06:02:06 PM", "NaN", "NaN"], ["2735", "Fix SqlStageExecution usage of Fragment layout", "Eric Hwang", "erichwang", "04/17/15, 07:57:10 PM", "Was previously referring to the unordered root.getOutputSymbols for the layout, when it should have been directly using the outputLayout.", "NaN"], ["2736", "Adjust plan optimizer order", "Eric Hwang", "erichwang", "04/17/15, 08:00:54 PM", "NaN", "NaN"], ["2737", "Make Block as InputChannel explicit in aggregation functions", "Haozhun Jin", "haozhun", "04/20/15, 07:40:04 PM", "NaN", "NaN"], ["2739", "Support query level memory limits", "Christopher Berner", "cberner", "04/20/15, 05:56:36 PM", "NaN", "NaN"], ["2741", "Show user and external error in different color in web UI", "Haozhun Jin", "haozhun", "04/20/15, 11:38:01 PM", "NaN", "NaN"], ["2742", "Few improvements to ActualProperties", "Eric Hwang", "erichwang", "04/18/15, 12:43:48 AM", "NaN", "NaN"], ["2743", "Add errorName and errorType to QueryError", "David Phillips", "electrum", "04/17/15, 11:40:28 PM", "NaN", "NaN"], ["2744", "Simplify Hive HDFS configuration", "David Phillips", "electrum", "04/18/15, 12:31:52 AM", "NaN", "NaN"], ["2745", "Fix Parquet handling of missing columns", "David Phillips", "electrum", "04/20/15, 02:45:00 PM", "The Parquet reader needs to handle the case where columns were added\nto the Hive metastore table after the table data was written.", "NaN"], ["2746", "Fix bugs in AddExchanges and constant optimizations", "Eric Hwang", "erichwang", "04/18/15, 02:40:52 AM", "NaN", "NaN"], ["2747", "Add PartitionedOutputOperator", "Dain Sundstrom", "dain", "05/08/15, 08:54:04 PM", "NaN", "NaN"], ["2749", "Fix unary operator PLUS", "Dain Sundstrom", "dain", "04/20/15, 06:00:34 PM", "Fixes #2594", "NaN"], ["2750", "Add ExchangeNode to Graphviz printer", "Nileema Shingte", "nileema", "04/23/15, 04:21:49 PM", "NaN", "NaN"], ["2751", "Fix orc structural tests", "Dain Sundstrom", "dain", "04/20/15, 06:00:07 PM", "Tests are also less brittle since equality is performed on structural types converted to normal Java collections.  Also, failures are easier to read since we are comparing with normal Java object (and I convert `Slice` to `String`)", "NaN"], ["2752", "Add cast ARRAY<F> to ARRAY<T>", "Dain Sundstrom", "dain", "04/29/15, 07:31:53 PM", "NaN", "NaN"], ["2753", "Add implicit coersions to VALUES", "Dain Sundstrom", "dain", "04/30/15, 11:21:40 PM", "NaN", "NaN"], ["2758", "Cleanup cardinality() implementation", "Christopher Berner", "cberner", "04/21/15, 11:00:02 PM", "NaN", "NaN"], ["2759", "Add static write method in BlockEncodingManager", "Haozhun Jin", "haozhun", "04/21/15, 07:36:41 PM", "NaN", "NaN"], ["2760", "Show loading message in UI", "David Phillips", "electrum", "04/21/15, 06:56:13 PM", "NaN", "NaN"], ["2761", "Add BlockEncodingSerde to FunctionRegistry", "Haozhun Jin", "haozhun", "04/22/15, 09:46:19 PM", "- So that BlockEncodingSerde is available for magic literal deserialization", "NaN"], ["2762", "Increase test timeout to fix Travis", "Christopher Berner", "cberner", "04/21/15, 10:22:04 PM", "NaN", "NaN"], ["2763", "Implement per node per query memory limits", "Christopher Berner", "cberner", "04/22/15, 07:00:58 PM", "NaN", "NaN"], ["2764", "Upgrade to Airbase 37", "David Phillips", "electrum", "04/22/15, 12:25:10 AM", "NaN", "NaN"], ["2765", "Fix NPE in BenchmarkQueryResult", "Dain Sundstrom", "dain", "04/21/15, 11:55:13 PM", "NaN", "NaN"], ["2766", "Change PrestoS3InputStream to use lazy seeks", "Dain Sundstrom", "dain", "04/22/15, 12:45:16 AM", "NaN", "NaN"], ["2767", "JDBC improvements", "David Phillips", "electrum", "04/22/15, 06:57:33 PM", "NaN", "NaN"], ["2768", "Update lastExecutionStartTime when any driver starts", "Dain Sundstrom", "dain", "04/29/15, 07:33:33 PM", "NaN", "NaN"], ["2769", "Fix CREATE TABLE for Raptor", "David Phillips", "electrum", "04/22/15, 04:08:30 PM", "NaN", "NaN"], ["2771", "Update to Airbase 38", "David Phillips", "electrum", "04/22/15, 06:38:51 PM", "NaN", "NaN"], ["2772", "Add COVAR_POP, COVAR_SAMP, CORR, REGR_SLOPE, REGR_INTERCEPT", "Haozhun Jin", "haozhun", "05/14/15, 09:24:31 PM", "Supersedes #2051", "NaN"], ["2774", "Fix drop table test to use correct table", "David Phillips", "electrum", "04/22/15, 07:55:41 PM", "NaN", "NaN"], ["2777", "Improve/fix property influenced plans", "Eric Hwang", "erichwang", "04/25/15, 08:05:12 PM", "NaN", "NaN"], ["2780", "Fix EncoderUtil.encodeNullsAsBits abstraction", "Haozhun Jin", "haozhun", "04/23/15, 06:04:18 PM", "NaN", "NaN"], ["2784", "Fix typo in SyntheticAddress comments", "Christopher Berner", "cberner", "04/24/15, 04:46:28 PM", "NaN", "NaN"], ["2785", "Make BlockEncodingManager always add built-in factories", "Haozhun Jin", "haozhun", "04/24/15, 09:01:09 PM", "- This way, tests can work by simply constructing a new `BlockEncodingManager`\n- This is how `TypeRegistry` works today", "NaN"], ["2786", "Fix NPE in BenchmarkQueryResult", "Dain Sundstrom", "dain", "04/25/15, 12:44:48 AM", "NaN", "NaN"], ["2787", "Implement memory pools on workers", "Christopher Berner", "cberner", "04/29/15, 05:10:11 PM", "NaN", "NaN"], ["2789", "Fix index join property propagation", "Eric Hwang", "erichwang", "04/25/15, 10:12:09 PM", "NaN", "NaN"], ["2791", "Correct handling of string functions for Unicode (non-ASCII) strings", "Dain Sundstrom", "dain", "04/30/15, 11:11:03 PM", "Currently, all of the string functions work incorrectly for Unicode (non-ASCII) strings (see  #2579)\nTo fix this, @fmeiser rewrote most of the methods to work correctly for Unicode. For the methods which already worked with non-ASCII, he added tests. All methods work directly with `Slice` without converting to `String`.\n\nI split original pull request (#2591) into two parts.  One part, I added to Slice directly (https://github.com/airlift/slice/pull/38), so it could access the internal of Slice, which result in a small speed boost.  The other part is here.  Additionally, I added more tests and a benchmark.\n\nBenchmark results\n\n```\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                          true         2  avgt        5      20.222       18.583  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                          true         5  avgt        5      31.776       14.151  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                          true        10  avgt        5      59.151       25.197  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                          true       100  avgt        5     445.298       44.887  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                          true      1000  avgt        5    4220.048      387.372  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                          true     10000  avgt        5   41436.997     6748.643  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                         false         2  avgt        5      36.761       19.102  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                         false         5  avgt        5      60.048       15.565  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                         false        10  avgt        5     100.922       24.394  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                         false       100  avgt        5     733.359       22.232  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                         false      1000  avgt        5    8330.696      177.062  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLeftTrim                         false     10000  avgt        5  105107.335     5595.990  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                            true         2  avgt        5       4.927        0.153  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                            true         5  avgt        5       4.568        0.140  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                            true        10  avgt        5       6.630        0.250  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                            true       100  avgt        5      33.386        0.911  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                            true      1000  avgt        5     224.277       10.458  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                            true     10000  avgt        5    2205.505      161.019  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                           false         2  avgt        5       4.080        0.297  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                           false         5  avgt        5      13.217        0.323  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                           false        10  avgt        5      14.578        0.711  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                           false       100  avgt        5      80.661        2.248  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                           false      1000  avgt        5     761.991       30.133  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLength                           false     10000  avgt        5    8226.699       81.321  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                             true         2  avgt        5      42.576        2.118  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                             true         5  avgt        5      65.601        3.301  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                             true        10  avgt        5     102.730       11.801  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                             true       100  avgt        5     769.916       17.984  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                             true      1000  avgt        5    8224.269      217.953  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                             true     10000  avgt        5   74965.387     4825.826  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                            false         2  avgt        5      52.088        0.915  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                            false         5  avgt        5     105.048        8.148  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                            false        10  avgt        5     193.123       13.632  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                            false       100  avgt        5    1569.459      127.311  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                            false      1000  avgt        5   17690.420     1098.144  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkLower                            false     10000  avgt        5  185126.292    10332.891  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                           true         2  avgt        5      25.245        9.516  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                           true         5  avgt        5      38.720       21.166  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                           true        10  avgt        5      57.678       30.383  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                           true       100  avgt        5     343.552        5.923  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                           true      1000  avgt        5    3676.776      540.327  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                           true     10000  avgt        5   46689.527    13522.092  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                          false         2  avgt        5      28.990       16.642  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                          false         5  avgt        5      46.178       27.107  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                          false        10  avgt        5      79.428       80.178  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                          false       100  avgt        5     508.880      298.995  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                          false      1000  avgt        5    5623.366     1715.816  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkReverse                          false     10000  avgt        5   75638.802    19926.038  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                         true         2  avgt        5      13.378        5.616  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                         true         5  avgt        5      22.950       19.878  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                         true        10  avgt        5      31.974       25.273  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                         true       100  avgt        5     212.902       12.214  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                         true      1000  avgt        5    1888.830       94.442  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                         true     10000  avgt        5   18814.020     1200.260  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                        false         2  avgt        5      36.954       20.222  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                        false         5  avgt        5      70.683       16.365  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                        false        10  avgt        5     122.992       15.054  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                        false       100  avgt        5     721.577      144.570  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                        false      1000  avgt        5    7427.438      583.645  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkRightTrim                        false     10000  avgt        5   98015.484     6136.921  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                    true         2  avgt        5       2.798        0.163  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                    true         5  avgt        5       5.267        0.144  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                    true        10  avgt        5      18.864       11.269  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                    true       100  avgt        5      29.027       19.471  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                    true      1000  avgt        5     157.819       27.130  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                    true     10000  avgt        5    1280.307      100.259  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                   false         2  avgt        5       2.803        0.090  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                   false         5  avgt        5       5.269        0.243  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                   false        10  avgt        5      33.788       16.033  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                   false       100  avgt        5      94.022       16.719  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                   false      1000  avgt        5     619.170       44.939  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStart                   false     10000  avgt        5    5476.401      266.483  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd             true         2  avgt        5       8.878        0.519  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd             true         5  avgt        5      20.183        7.732  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd             true        10  avgt        5      21.114        5.898  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd             true       100  avgt        5      73.295       28.279  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd             true      1000  avgt        5     383.032       26.316  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd             true     10000  avgt        5    3507.865      183.545  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd            false         2  avgt        5       7.596        0.398  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd            false         5  avgt        5      39.385       28.400  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd            false        10  avgt        5      52.014       22.878  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd            false       100  avgt        5     177.969       25.920  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd            false      1000  avgt        5    1325.836      137.824  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartFromEnd            false     10000  avgt        5   12168.301     1280.519  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength              true         2  avgt        5       2.779        0.179  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength              true         5  avgt        5      26.832       12.733  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength              true        10  avgt        5      32.499       22.123  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength              true       100  avgt        5      51.984        9.618  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength              true      1000  avgt        5     274.754       21.501  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength              true     10000  avgt        5    2513.272      160.565  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength             false         2  avgt        5       2.808        0.121  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength             false         5  avgt        5      47.729       23.736  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength             false        10  avgt        5      65.283       23.358  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength             false       100  avgt        5     160.441       17.009  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength             false      1000  avgt        5     965.220       58.410  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLength             false     10000  avgt        5    9237.194      497.736  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd       true         2  avgt        5      26.134       10.500  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd       true         5  avgt        5      36.046       17.350  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd       true        10  avgt        5      38.347       14.512  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd       true       100  avgt        5      90.389       25.483  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd       true      1000  avgt        5     511.703       27.230  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd       true     10000  avgt        5    4750.136      440.145  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd      false         2  avgt        5      35.049       20.179  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd      false         5  avgt        5      67.110       24.906  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd      false        10  avgt        5      97.434       22.444  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd      false       100  avgt        5     267.049       27.025  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd      false      1000  avgt        5    1842.101      140.554  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkSubstringStartLengthFromEnd      false     10000  avgt        5   17421.955      525.860  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                              true         2  avgt        5      14.986        0.743  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                              true         5  avgt        5      27.355        0.339  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                              true        10  avgt        5      44.679        1.630  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                              true       100  avgt        5     415.864       23.629  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                              true      1000  avgt        5    3735.030      163.791  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                              true     10000  avgt        5   42947.000    21308.508  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                             false         2  avgt        5      23.956        1.923  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                             false         5  avgt        5      41.364        3.133  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                             false        10  avgt        5      69.545        3.745  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                             false       100  avgt        5     609.266       16.541  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                             false      1000  avgt        5    7155.778      334.445  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkTrim                             false     10000  avgt        5  101488.696     5323.441  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                             true         2  avgt        5      46.959       37.490  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                             true         5  avgt        5      67.280       52.553  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                             true        10  avgt        5     103.150       58.757  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                             true       100  avgt        5     704.528       38.133  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                             true      1000  avgt        5    7471.537      437.504  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                             true     10000  avgt        5   69990.883     4131.438  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                            false         2  avgt        5      62.522        2.174  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                            false         5  avgt        5      93.060       56.668  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                            false        10  avgt        5     148.329       69.036  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                            false       100  avgt        5    1662.402       84.428  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                            false      1000  avgt        5   17539.205      495.578  ns/op\nc.f.p.o.s.StringFunctionsBenchmark.benchmarkUpper                            false     10000  avgt        5  186298.605    10171.332  ns/op\n```", "NaN"], ["2796", "Fix recording of blocked operators and drivers", "Dain Sundstrom", "dain", "04/27/15, 07:10:18 PM", "NaN", "NaN"], ["2799", "Update 0.101 release notes", "David Phillips", "electrum", "04/27/15, 09:32:56 PM", "NaN", "NaN"], ["2800", "Update release notes for 0.101", "Christopher Berner", "cberner", "04/27/15, 10:49:53 PM", "NaN", "NaN"], ["2801", "Add 0.101 release notes", "Dain Sundstrom", "dain", "04/27/15, 10:06:43 PM", "NaN", "NaN"], ["2802", "Don't insert redundant filter after index source", "Martin Traverso", "martint", "04/27/15, 11:46:49 PM", "NaN", "NaN"], ["2803", "Update release notes", "Martin Traverso", "martint", "04/28/15, 04:18:54 PM", "NaN", "NaN"], ["2805", "Process right outer joins without flipping operands", "Haozhun Jin", "haozhun", "04/29/15, 07:45:45 PM", "Depend on #2648 ", "NaN"], ["2808", "Update 0.101 release notes", "Christopher Berner", "cberner", "04/28/15, 05:37:42 PM", "NaN", "NaN"], ["2809", "Use Slice to encode null flags in blocks", "Martin Traverso", "martint", "04/28/15, 08:32:38 PM", "The getRegion method copies chunks of the underlying boolean array on every call.\nBy switching it to a slice, we can create a view, instead.", "NaN"], ["2811", "Encode offsets using a Slice in VariableWidthBlock", "Martin Traverso", "martint", "04/28/15, 10:24:45 PM", "The getRegion method copies chunks of the underlying array on every call.\nBy switching it to a slice we can create a view, instead.", "NaN"], ["2815", "Use JONI to replace java.regex for regex sql functions", "Haozhun Jin", "haozhun", "05/01/15, 12:14:03 AM", "java.regex\n\n```\n(patternString)  (sourceLength)  Mode  Samples           Score    Score error  Units\n          .*x.*            1024  avgt       10     3330533.930     287806.854  ns/op\n          .*x.*           32768  avgt       10  3396127115.000  242129758.239  ns/op\n      .*(x|y).*            1024  avgt       10     6847263.706     630131.295  ns/op\n      .*(x|y).*           32768  avgt       10  7405041415.600  481333551.769  ns/op\n    longdotstar            1024  avgt       10     4386599.874     269038.787  ns/op\n    longdotstar           32768  avgt       10  5272199253.700  563624383.505  ns/op\n          phone            1024  avgt       10       13611.496       1390.554  ns/op\n          phone           32768  avgt       10       22267.639       3061.627  ns/op\n        literal            1024  avgt       10        2225.823        285.027  ns/op\n        literal           32768  avgt       10       65688.858       4443.133  ns/op\n```\n\njoni\n\n```\n(patternString)  (sourceLength)  Mode  Samples       Score  Score error  Units\n          .*x.*            1024  avgt       10    5929.790      103.427  ns/op\n          .*x.*           32768  avgt       10  189781.419     4770.151  ns/op\n      .*(x|y).*            1024  avgt       10    2764.053      143.002  ns/op\n      .*(x|y).*           32768  avgt       10   94982.894     4222.652  ns/op\n    longdotstar            1024  avgt       10     693.856       26.088  ns/op\n    longdotstar           32768  avgt       10   20160.877      720.529  ns/op\n          phone            1024  avgt       10    6841.006      274.788  ns/op\n          phone           32768  avgt       10   12165.194      191.484  ns/op\n        literal            1024  avgt       10    1183.754       70.801  ns/op\n        literal           32768  avgt       10   36016.862      880.373  ns/op\n```", "NaN"], ["2818", "Improve code formatting", "Christopher Berner", "cberner", "04/29/15, 05:52:18 PM", "NaN", "NaN"], ["2819", "Improve Presto JVM requirement checks", "David Phillips", "electrum", "04/29/15, 06:48:54 PM", "NaN", "NaN"], ["2820", "Track all memory pools in coordinator", "Christopher Berner", "cberner", "04/30/15, 08:27:36 PM", "NaN", "NaN"], ["2821", "Fix deadlock in SqlStageExecution.waitForNewExchangesOrBuffers", "Dain Sundstrom", "dain", "04/30/15, 04:53:00 AM", "NaN", "NaN"], ["2823", "Change color of INSUFFICIENT_RESOURCES and QUEUED", "Haozhun Jin", "haozhun", "04/30/15, 10:55:17 PM", "NaN", "NaN"], ["2827", "Remove unused parameter", "Martin Traverso", "martint", "04/30/15, 08:14:02 PM", "NaN", "NaN"], ["2828", "Update documented maven requirement", "Christopher Berner", "cberner", "04/30/15, 09:52:52 PM", "NaN", "NaN"], ["2829", "Refactor memory manager config and tests", "Christopher Berner", "cberner", "04/30/15, 10:54:13 PM", "This should make the tests pass deterministically", "NaN"], ["2831", "Add timeline visualization to query info ui", "Haozhun Jin", "haozhun", "05/14/15, 04:56:04 PM", "e.g. screenshot showing hash skew:\n\n![screen shot 2015-04-30 at 4 20 31 pm](https://cloud.githubusercontent.com/assets/2020503/7424394/61fef074-ef55-11e4-8d21-bb115847a338.png)", "NaN"], ["2832", "Fix handling of NULLs in contains()", "Christopher Berner", "cberner", "05/01/15, 12:15:29 AM", "NaN", "NaN"], ["2833", "Fix JDBC connector tests", "David Phillips", "electrum", "05/01/15, 02:20:27 AM", "NaN", "NaN"], ["2835", "Cache final query info and free output stage", "Dain Sundstrom", "dain", "05/01/15, 03:05:31 AM", "NaN", "NaN"], ["2839", "Identify common partitioning columns for preferred properties", "Nileema Shingte", "nileema", "05/01/15, 08:11:09 PM", "NaN", "NaN"], ["2841", "Add documentation and release notes for regexp/joni", "Haozhun Jin", "haozhun", "05/01/15, 10:35:43 PM", "NaN", "NaN"], ["2842", "Add release notes for full/right outer join", "Haozhun Jin", "haozhun", "05/01/15, 09:45:52 PM", "NaN", "NaN"], ["2843", "Update release notes for 0.102", "Christopher Berner", "cberner", "05/01/15, 09:17:41 PM", "NaN", "NaN"], ["2844", "0.102 release changes", "Dain Sundstrom", "dain", "05/01/15, 09:44:21 PM", "NaN", "NaN"], ["2845", "Improve HIVE_PARTITION_SCHEMA_MISMATCH error message", "Dain Sundstrom", "dain", "05/01/15, 11:40:11 PM", "NaN", "NaN"], ["2846", "Fix test for partition schema mismatch", "David Phillips", "electrum", "05/02/15, 12:22:08 AM", "NaN", "NaN"], ["2847", "Refactor PreferredProperty derivations", "Nileema Shingte", "nileema", "05/11/15, 11:20:02 PM", "NaN", "NaN"], ["2848", "Cleanup cardinality() implementation for map", "Haozhun Jin", "haozhun", "05/04/15, 05:11:21 PM", "Similar to #2758, but for maps.", "NaN"], ["2850", "Allow invalid UTF-8 sequence in string functions", "Dain Sundstrom", "dain", "05/04/15, 06:31:36 PM", "Change upper, lower, and reverse to skip invalid UTF-8 sequences.\nChange trim functions to stop at an invalid UTF-8 sequence.", "NaN"], ["2853", "Fix variable name in ArrayDistinctFunction", "David Phillips", "electrum", "05/04/15, 05:00:37 PM", "NaN", "NaN"], ["2856", "Close ORC HDFS input stream", "David Phillips", "electrum", "05/04/15, 04:58:02 PM", "NaN", "NaN"], ["2858", "Upgrade to Slice 0.12", "Dain Sundstrom", "dain", "05/04/15, 06:47:06 PM", "NaN", "NaN"], ["2859", "Add debug logging for shard iterator", "Nileema Shingte", "nileema", "05/04/15, 10:27:23 PM", "NaN", "NaN"], ["2865", "Fix Scheduler IntelliJ warnings", "Dain Sundstrom", "dain", "05/05/15, 08:23:12 PM", "NaN", "NaN"], ["2870", "Fix map comparison bug in TestRowOperators", "Haozhun Jin", "haozhun", "05/06/15, 08:05:02 PM", "NaN", "NaN"], ["2871", "Add replaceShards to ShardManager", "Nileema Shingte", "nileema", "05/05/15, 11:16:18 PM", "NaN", "NaN"], ["2873", "Finish memory pool implementation", "Christopher Berner", "cberner", "05/08/15, 09:59:22 PM", "The coordinator now automatically moves queries into the reserved pool\nwhen necessary, and keeps a global view of the cluster's memory pools.", "NaN"], ["2874", "Remove unused imports", "Dain Sundstrom", "dain", "05/06/15, 02:05:15 AM", "NaN", "NaN"], ["2876", "Prune StageInfo objects from expired queries", "Christopher Berner", "cberner", "05/07/15, 03:02:43 AM", "The StageInfo object graph can be very large, as it contains the entire\nplan, all the tasks, and all the buffers involved in the tasks. This\nchange prunes StageInfo objects for all queries past the max query\nhistory limit", "NaN"], ["2887", "Fix eager loading of Hive partitions", "Christopher Berner", "cberner", "05/08/15, 08:47:14 PM", "Remove defensive copy of Hive partitions to avoid eagerly loading all\nthe partitions", "NaN"], ["2890", "Handle conflicts when replacing Raptor shards", "David Phillips", "electrum", "05/09/15, 01:13:02 AM", "NaN", "NaN"], ["2891", "Add Type parameter to createStreamReader in orc", "Haozhun Jin", "haozhun", "05/11/15, 08:34:19 PM", "So that BlockStreamReader can access it", "NaN"], ["2893", "Implement DELETE", "David Phillips", "electrum", "06/04/15, 01:30:27 AM", "NaN", "NaN"], ["2897", "SharedBuffer support for pre-partitioned pages", "Nileema Shingte", "nileema", "05/15/15, 10:00:16 PM", "NaN", "NaN"], ["2901", "Fix race between TaskContext and Driver", "Christopher Berner", "cberner", "05/12/15, 01:54:51 AM", "NaN", "NaN"], ["2906", "Increase CLI history file size", "David Phillips", "electrum", "05/12/15, 08:00:18 PM", "NaN", "NaN"], ["2908", "Update 0.103 release notes", "Christopher Berner", "cberner", "05/12/15, 11:07:43 PM", "NaN", "NaN"], ["2909", "Add 0.103 release notes", "Dain Sundstrom", "dain", "05/13/15, 02:32:33 AM", "NaN", "NaN"], ["2910", "Refactor BlockBuilderStatus for usability", "Haozhun Jin", "haozhun", "05/15/15, 06:57:10 PM", "BlockBuilderStatus currently contains functionality for both block building and page building tracking. It also requires caller's close interaction to make the block building tracking work.\n\nBy splitting it into two, the two tracking functionalities are split, and close interaction is removed.", "NaN"], ["2911", "Do not create directories that already exist", "David Phillips", "electrum", "05/13/15, 12:10:01 AM", "Certain remote file systems have locks around directory creation,\nso having many processes attempting to re-create the same shared\nparent directory can cause problems.", "NaN"], ["2912", "Add new Blocks for structural types", "Haozhun Jin", "haozhun", "07/09/15, 06:19:09 PM", "Depends on ~~#2910~~ ~~#3148~~\n\nAdd ArrayBlock, InterleavedBlock, ArrayBlockBuilder, InterleavedBlockBuilder, ArrayElementBlockWriter,\nLazyArrayBlock", "NaN"], ["2913", "Add ProjectionPushDown optimizer", "Nileema Shingte", "nileema", "05/20/15, 06:19:38 PM", "NaN", "NaN"], ["2915", "Update 0.103 release notes", "David Phillips", "electrum", "05/13/15, 07:35:46 PM", "NaN", "NaN"], ["2916", "Revert broken upgrade to Parquet 1.6.0", "David Phillips", "electrum", "05/13/15, 07:55:20 PM", "NaN", "NaN"], ["2918", "Handle thread interruption in StatementClient", "David Phillips", "electrum", "05/14/15, 12:45:36 AM", "NaN", "NaN"], ["2919", "Fix Raptor writer leak on query failure", "David Phillips", "electrum", "05/14/15, 05:02:49 PM", "NaN", "NaN"], ["2925", "Add support for cross join", "Joy Yao", "joy-yao", "10/13/15, 08:40:24 PM", "NaN", "NaN"], ["2928", "Memory manager improvements", "Christopher Berner", "cberner", "05/19/15, 04:54:12 AM", "NaN", "NaN"], ["2930", "Upgrade to Airlift 0.110-SNAPSHOT", "David Phillips", "electrum", "05/15/15, 11:41:16 PM", "NaN", "NaN"], ["2931", "Cleanup dependencies for plugins", "David Phillips", "electrum", "05/18/15, 06:30:20 PM", "Remove `provided` scope for all dependencies except SPI dependencies, and move those to their own section.", "NaN"], ["2933", "Refactor ActualProperties", "Eric Hwang", "erichwang", "06/06/15, 08:31:38 AM", "This cleans up ActualProperties so that the inherent class structure enforces the actual relationships between properties. It also adds includes a bunch of related API cleanup and bug fixes.", "NaN"], ["2936", "Fix broken no more buffers check", "Dain Sundstrom", "dain", "05/18/15, 06:18:31 PM", "NaN", "NaN"], ["2937", "Make SplitSource.getNextBatch async", "Dain Sundstrom", "dain", "05/19/15, 05:05:17 AM", "NaN", "NaN"], ["2939", "Use string formatting for compiler comments", "David Phillips", "electrum", "05/18/15, 11:20:21 PM", "NaN", "NaN"], ["2943", "Clean up query and stage state machines", "Dain Sundstrom", "dain", "05/31/15, 12:43:24 AM", "This adds a StateStateMachine and updates the QueryStateMachine to match the style.  Additionally, I added tests for the state transitions.\n\nI also fixed the failure tracking between these two state machines, and cleaned up race conditions in the the query completion code.", "NaN"], ["2946", "Move OrcUtil to hive-apache", "David Phillips", "electrum", "05/19/15, 06:58:05 PM", "NaN", "NaN"], ["2949", "Track overall memory usage of PartitionBuffers", "Nileema Shingte", "nileema", "05/20/15, 11:06:32 PM", "NaN", "NaN"], ["2955", "Optimize performance of selectRandomNodes()", "Christopher Berner", "cberner", "05/20/15, 06:53:08 PM", "Results in a 10-20% cpu reduction during load testing", "NaN"], ["2958", "Update test for ProjectionPushDown", "Nileema Shingte", "nileema", "05/20/15, 11:04:36 PM", "NaN", "NaN"], ["2959", "Fix excessive SettableFuture retention", "Christopher Berner", "cberner", "05/20/15, 10:57:13 PM", "NaN", "NaN"], ["2961", "Compaction for shards with temporal information", "Nileema Shingte", "nileema", "06/20/15, 12:52:07 AM", "NaN", "NaN"], ["2962", "Update to Hive 1.2.0", "David Phillips", "electrum", "05/22/15, 12:36:05 AM", "NaN", "NaN"], ["2963", "Fix fetching temporal column id from Raptor metadata", "Nileema Shingte", "nileema", "05/22/15, 07:48:17 AM", "```\nDue to the following JDBI issue: https://github.com/jdbi/jdbi/issues/154\nwe get a 0 as the column_id when the temporal_column_id is set to NULL.\nThis fix works around that issue.\n```", "NaN"], ["2965", "Fix spurious change", "David Phillips", "electrum", "05/21/15, 04:09:13 PM", "NaN", "NaN"], ["2966", "Add timeouts for Raptor backup operations", "David Phillips", "electrum", "05/28/15, 06:38:07 PM", "NaN", "NaN"], ["2967", "Update release notes", "Martin Traverso", "martint", "05/21/15, 06:29:24 PM", "NaN", "NaN"], ["2968", "Update release notes", "Christopher Berner", "cberner", "05/21/15, 07:19:13 PM", "NaN", "NaN"], ["2970", "Increase timeouts for S3 retry tests", "David Phillips", "electrum", "05/21/15, 08:52:12 PM", "NaN", "NaN"], ["2971", "Update release notes for 0.104", "David Phillips", "electrum", "05/21/15, 09:00:15 PM", "NaN", "NaN"], ["2972", "Improve memory accounting in JOIN and some aggregations", "Christopher Berner", "cberner", "05/29/15, 07:39:16 PM", "NaN", "NaN"], ["2976", "Update documentation for json_size", "David Phillips", "electrum", "05/22/15, 07:53:18 PM", "NaN", "NaN"], ["2977", "Remove ordinal position from ColumnMetadata", "David Phillips", "electrum", "05/22/15, 11:44:34 PM", "NaN", "NaN"], ["2983", "Categorize errors", "Christopher Berner", "cberner", "05/27/15, 02:48:02 AM", "NaN", "NaN"], ["2984", "Fix concurrent modification ", "Christopher Berner", "cberner", "05/26/15, 07:30:55 PM", "NaN", "NaN"], ["2985", "Dequeue pages from all partition buffers when any master buffer advances", "Nileema Shingte", "nileema", "05/26/15, 08:06:41 PM", "Fixes #2975 ", "NaN"], ["2986", "Add release notes for 0.105", "Nileema Shingte", "nileema", "05/26/15, 09:09:30 PM", "NaN", "NaN"], ["2987", "Remove unnecessary serialized fields from Raptor splits", "David Phillips", "electrum", "05/27/15, 07:58:24 PM", "NaN", "NaN"], ["2988", "Fix warnings", "David Phillips", "electrum", "05/27/15, 09:53:40 PM", "NaN", "NaN"], ["2989", "Add TableLayout to explain plans", "Eric Hwang", "erichwang", "05/27/15, 08:51:01 PM", "NaN", "NaN"], ["2990", "Fix task leak due to race condition", "Martin Traverso", "martint", "05/28/15, 09:47:25 PM", "If the query is aborted before a stage begins executing, it\nunconditionally sets the state of the stage to SCHEDULED\neven if the stage was already in a done state and the scheduling\nloop never finishes.", "NaN"], ["2992", "Parallelize startup of table scan task splits", "David Phillips", "electrum", "05/29/15, 12:44:55 AM", "NaN", "NaN"], ["2993", "Exclude modernizer plugin in eclipse", "Henning Schmiedehausen", "hgschmie", "05/29/15, 07:36:48 PM", "otherwise eclipse complains about a plugin without coverage in m2e.", "NaN"], ["2996", "Performance optimizations for coordinator", "Christopher Berner", "cberner", "06/01/15, 10:41:52 PM", "NaN", "NaN"], ["2997", "Fix index resource leak when switching index strategies", "Eric Hwang", "erichwang", "05/29/15, 10:00:49 PM", "NaN", "NaN"], ["2998", "Make Raptor backup providers extensible", "David Phillips", "electrum", "05/31/15, 02:01:53 AM", "NaN", "NaN"], ["3003", "Reduce granularity of set thread name", "Christopher Berner", "cberner", "06/02/15, 12:38:20 AM", "This should save 5-10% of coordinator CPU in large clusters", "NaN"], ["3007", "Move MemoryPool JMX export out of LocalMemoryManager", "Christopher Berner", "cberner", "06/04/15, 09:52:12 PM", "NaN", "NaN"], ["3008", "Upgrade to slice 0.13", "Christopher Berner", "cberner", "06/02/15, 09:07:26 PM", "NaN", "NaN"], ["3010", "Update 0.106 release notes", "Christopher Berner", "cberner", "06/02/15, 09:06:02 PM", "NaN", "NaN"], ["3011", "Fix AsyncQueue to actually wait", "Dain Sundstrom", "dain", "06/02/15, 08:33:02 PM", "AsyncQueue was only blocking for the first element.", "NaN"], ["3012", "Add 0.106 release notes", "Dain Sundstrom", "dain", "06/02/15, 08:43:03 PM", "NaN", "NaN"], ["3013", "Fix Raptor backup timeouts", "David Phillips", "electrum", "06/02/15, 09:56:59 PM", "NaN", "NaN"], ["3014", "Fix race condition in AsyncQueue", "Dain Sundstrom", "dain", "06/02/15, 11:35:09 PM", "NaN", "NaN"], ["3016", "Add pluggable query execution policies and new phased policy ", "Dain Sundstrom", "dain", "08/18/15, 03:41:12 AM", "depends on airlift/airlift#322", "NaN"], ["3021", "Add cluster OOM killer", "Christopher Berner", "cberner", "09/01/15, 12:32:19 AM", "NaN", "NaN"], ["3025", "HttpRemoteTask improvements", "Christopher Berner", "cberner", "06/04/15, 12:49:41 AM", "NaN", "NaN"], ["3026", "Make CreateViewTask method private", "David Phillips", "electrum", "06/04/15, 12:31:23 AM", "NaN", "NaN"], ["3028", "Fix plan printing of table layouts", "David Phillips", "electrum", "06/12/15, 08:05:20 PM", "NaN", "NaN"], ["3031", "Fix leak in SqlStageExecution", "Christopher Berner", "cberner", "06/04/15, 09:51:29 PM", "Task scheduling and stage cancellation race against each other. In\nparticular, scheduleSourcePartitionedNodes() may take a long time if it\nhas to wait for free nodes, and a LIMIT query could finish after just\none of those tasks starts running. In this case the stage will be\ncancelled while scheduling is still in progress, which will cause tasks\nto be leaked, as they're created after the stage already has\ntransitioned to the CANCELLED state.", "NaN"], ["3033", "Update 0.107 release notes", "Christopher Berner", "cberner", "06/05/15, 12:35:05 AM", "NaN", "NaN"], ["3034", "Reenable error classification code for syntax errors", "Dain Sundstrom", "dain", "06/05/15, 12:59:36 AM", "NaN", "NaN"], ["3041", "Upgrade to Slice 0.14", "Dain Sundstrom", "dain", "06/05/15, 07:13:11 PM", "Fixes #3006", "NaN"], ["3042", "Update to ASM 5.0.4", "David Phillips", "electrum", "06/08/15, 10:37:40 PM", "NaN", "NaN"], ["3049", "Improve lexer matching of invalid characters", "Martin Traverso", "martint", "06/06/15, 06:01:11 PM", "This variant is functionally equivalent but faster than the non-greedy match.", "NaN"], ["3055", "Update to airbase 40", "Martin Traverso", "martint", "06/08/15, 09:50:29 PM", "Fixes a race condition in Jetty that can cause http requests to hang.\n\nAlso, use jmh version from airbase.", "NaN"], ["3058", "Cleanup PagePartitionFunction", "Haozhun Jin", "haozhun", "06/15/15, 10:54:21 PM", "Remove partition() as it is no longer used. Relevant logic has been moved to PartitionedOutputOperator in previous changes.", "NaN"], ["3059", "Fix a typo in SliceArrayBlock.copyRegion", "Haozhun Jin", "haozhun", "06/10/15, 04:07:58 PM", "https://github.com/facebook/presto/pull/2972#issuecomment-110384276\n\ncc @cberner", "NaN"], ["3064", "Account for RowNumberOperator memory usage", "Eric Hwang", "erichwang", "06/10/15, 04:42:03 PM", "NaN", "NaN"], ["3065", "Rename dfs.domain-socket-path", "David Phillips", "electrum", "06/10/15, 07:10:39 PM", "The new name is hive.dfs.domain-socket-path.", "NaN"], ["3066", "Hive metastore improvements", "David Phillips", "electrum", "06/16/15, 07:02:13 PM", "NaN", "NaN"], ["3069", "Fix typo in TupleDescriptor comments", "Xin Yao", "yaoxin226", "06/11/15, 02:14:32 AM", "NaN", "NaN"], ["3082", "Fix memory accounting error in OperatorContext", "Eric Hwang", "erichwang", "06/12/15, 07:31:11 PM", "NaN", "NaN"], ["3083", "Make ctrl-C in CLI cancel the query", "David Phillips", "electrum", "06/13/15, 11:48:01 PM", "NaN", "NaN"], ["3084", "Update documentation for Black Hole connector", "David Phillips", "electrum", "06/16/15, 04:21:31 PM", "NaN", "NaN"], ["3085", "Improve UI query task table", "Dain Sundstrom", "dain", "06/13/15, 05:57:58 PM", "NaN", "NaN"], ["3101", "Support Connection.setReadOnly in JDBC driver", "David Phillips", "electrum", "06/15/15, 06:43:35 PM", "NaN", "NaN"], ["3102", "Add cast VARCHAR as TIMESTAMP WITH TIME ZONE", "Dain Sundstrom", "dain", "06/15/15, 09:21:50 PM", "NaN", "NaN"], ["3103", "Fix formatting for release notes", "David Phillips", "electrum", "06/16/15, 04:20:37 PM", "NaN", "NaN"], ["3106", "Fix bug in property derivations for delete", "Haozhun Jin", "haozhun", "06/15/15, 11:22:29 PM", "`visitDelete` in property derivations pass on its child's property unchanged. This is incorrect because none of the columns are passed on in delete operation. Now none of the symbols in the child's property references a valid column after the delete operation. This change removes all symbol from the property.", "NaN"], ["3109", "Improve cast VARCHAR to date time type", "Dain Sundstrom", "dain", "06/16/15, 05:39:31 AM", "NaN", "NaN"], ["3115", "Add missing newline to RPM files", "David Phillips", "electrum", "06/16/15, 04:17:20 PM", "NaN", "NaN"], ["3116", "Fix Raptor delete test", "David Phillips", "electrum", "06/16/15, 05:44:19 PM", "NaN", "NaN"], ["3118", "Fix auto-refresh timing bug in web ui", "Haozhun Jin", "haozhun", "06/25/15, 12:59:27 AM", "@cberner, this is low-priority. This pr can wait until you come back.", "NaN"], ["3119", "Skip checks for presto-server-rpm module", "David Phillips", "electrum", "06/16/15, 07:17:21 PM", "NaN", "NaN"], ["3121", "Remove debug mode logging for CLI", "David Phillips", "electrum", "06/17/15, 12:55:15 AM", "NaN", "NaN"], ["3124", "Do not set no more splits when a stage fails", "Dain Sundstrom", "dain", "06/17/15, 07:21:06 PM", "Setting no more splits causes a normal completion, and should not be\nused for FAILED or ABORTED stages.  The patch changes the, incorrect,\nblack list of stage states to a white list.", "NaN"], ["3129", "Update to airlift 0.111-SNAPSHOT", "Martin Traverso", "martint", "06/22/15, 06:06:46 PM", "NaN", "NaN"], ["3131", "Fix writing Unicode data in Raptor", "David Phillips", "electrum", "06/18/15, 11:04:22 PM", "NaN", "NaN"], ["3132", "Don't push predicates through non-grouped aggregations", "Martin Traverso", "martint", "06/19/15, 04:45:55 PM", "NaN", "NaN"], ["3136", "Fix aggregation test utilities", "Haozhun Jin", "haozhun", "06/30/15, 03:57:41 AM", "Currently in `partialAggregation`, all page are aggregated to produce one intermediate result. And it gets combined with an empty intermediate result to produce the final result.  Instead, it should produce one intermediate result for each page.\n\nThis bug gives false confidence that combine() is correctly implemented for an aggregation function. When in fact, combine() is hardly tested.\n\n`assertAggregation` is also fixed to produce 2 pages when possible.", "NaN"], ["3137", "Add option to CLI to set logging levels", "Martin Traverso", "martint", "06/24/15, 01:58:06 AM", "NaN", "NaN"], ["3148", "Add getObject, writeObject to Block and BlockBuilder", "Haozhun Jin", "haozhun", "06/25/15, 01:18:27 AM", "NaN", "NaN"], ["3149", "Hive test updates", "David Phillips", "electrum", "06/22/15, 10:53:09 PM", "NaN", "NaN"], ["3150", "Make Hive table creation test helpers protected", "David Phillips", "electrum", "06/22/15, 11:14:32 PM", "NaN", "NaN"], ["3151", "Upgrade to Airbase 41", "David Phillips", "electrum", "06/23/15, 03:35:43 AM", "NaN", "NaN"], ["3153", "Fix dependencies for Example HTTP connector", "David Phillips", "electrum", "06/23/15, 08:22:16 PM", "NaN", "NaN"], ["3154", "Add release notes for memory accounting fixes", "Eric Hwang", "erichwang", "06/23/15, 11:41:44 PM", "NaN", "NaN"], ["3155", "UI fixes", "David Phillips", "electrum", "06/23/15, 11:53:22 PM", "NaN", "NaN"], ["3156", "Update release notes", "David Phillips", "electrum", "06/24/15, 12:53:48 AM", "NaN", "NaN"], ["3159", "Add SPNEGO auth to server", "Martin Traverso", "martint", "06/24/15, 05:21:18 AM", "NaN", "NaN"], ["3160", "Update 0.108 release notes", "Martin Traverso", "martint", "06/24/15, 07:01:21 AM", "NaN", "NaN"], ["3166", "Make POSITION non-reserved keyword", "Martin Traverso", "martint", "06/24/15, 06:42:35 PM", "Otherwise, references to columns named 'position' need to be quoted.", "NaN"], ["3167", "Fix property derivations issue for UNNEST", "Martin Traverso", "martint", "06/24/15, 07:29:28 PM", "Fixes https://github.com/facebook/presto/issues/3165", "NaN"], ["3174", "Validate timestamp literals during analysis", "Martin Traverso", "martint", "06/26/15, 01:06:51 AM", "NaN", "NaN"], ["3175", "Remove examples for SHOW SCHEMAS and DESCRIBE", "David Phillips", "electrum", "06/25/15, 07:05:04 PM", "NaN", "NaN"], ["3177", "Session property types", "Dain Sundstrom", "dain", "07/31/15, 01:07:14 AM", "All session properties now have a SQL type, default value and description.  The value `SET SESSION` can now be any constant expression, and the `SHOW SESSION` command prints the current effective value and default value for all session properties.", "NaN"], ["3180", "Adjust AddExchanges streaming behavior ", "Eric Hwang", "erichwang", "06/26/15, 08:54:51 PM", "NaN", "NaN"], ["3181", "Properly propagate exceptions in index joins", "Eric Hwang", "erichwang", "06/26/15, 06:08:00 PM", "NaN", "NaN"], ["3183", "Fix ORC reader position handling for skipped row groups", "David Phillips", "electrum", "06/27/15, 03:50:05 AM", "NaN", "NaN"], ["3185", "Add coercion for union", "Haozhun Jin", "haozhun", "07/14/15, 10:18:41 PM", "Fixes #2532 ", "NaN"], ["3187", "Remove big query support", "Christopher Berner", "cberner", "06/30/15, 12:55:11 AM", "Remove the big query session property, and all related configs", "NaN"], ["3190", "Use two hex digits for Raptor directory structure", "David Phillips", "electrum", "06/29/15, 04:37:55 AM", "NaN", "NaN"], ["3194", "Allow partial json cast", "Haozhun Jin", "haozhun", "07/14/15, 01:42:56 AM", "Issue #2756 ", "NaN"], ["3196", "Support legacy directory hashing for Raptor backup", "David Phillips", "electrum", "06/29/15, 11:02:18 PM", "NaN", "NaN"], ["3200", "Add config and session property force_distributed_writing", "Haozhun Jin", "haozhun", "07/01/15, 06:04:33 PM", "NaN", "NaN"], ["3204", "Fix missed FAILED signal", "Christopher Berner", "cberner", "07/01/15, 05:24:51 AM", "Queries with a stage that fails while the query is still in the STARTING\nstate will never complete, due to the transition being ignored", "NaN"], ["3205", "Add release notes for 0.109", "Christopher Berner", "cberner", "07/01/15, 05:55:23 PM", "NaN", "NaN"], ["3206", "Update deprecated properties in development config", "David Phillips", "electrum", "07/01/15, 04:18:52 PM", "NaN", "NaN"], ["3207", "Update 0.109 release notes", "Martin Traverso", "martint", "07/01/15, 06:10:13 PM", "NaN", "NaN"], ["3209", "Update 0.109 release notes", "Christopher Berner", "cberner", "07/01/15, 06:46:37 PM", "NaN", "NaN"], ["3210", "Change force distributed writing to redistribute writes", "Haozhun Jin", "haozhun", "07/01/15, 08:23:43 PM", "Change the feature so that it does redistribution regardless of whether the source plan of TableWriter is distributed. Rename config and session property accordingly.", "NaN"], ["3211", "Update to airlift 0.112", "Martin Traverso", "martint", "07/01/15, 11:00:44 PM", "Fixes a bug with server-to-server kerberos auth setup", "NaN"], ["3212", "Revert \"Finish inner join if build is empty\"", "Christopher Berner", "cberner", "07/01/15, 11:32:25 PM", "This change caused queries to hang, because a task could finish its join\nbefore receiving splits for all the remote sources that the probe side\nshould pull from. Those would then never be closed.", "NaN"], ["3214", "Fix RPM build", "Christopher Berner", "cberner", "07/02/15, 06:35:47 PM", "NaN", "NaN"], ["3219", "Remove redundant note for lower function", "David Phillips", "electrum", "07/10/15, 08:21:59 PM", "NaN", "NaN"], ["3223", "Categorize errors", "Christopher Berner", "cberner", "07/08/15, 12:27:53 AM", "NaN", "NaN"], ["3225", "Print class definition when class writing fails in CompilerUtils", "Haozhun Jin", "haozhun", "07/07/15, 05:37:28 PM", "NaN", "NaN"], ["3226", "Change getJavaType of array, map, and row to Block", "Haozhun Jin", "haozhun", "07/23/15, 05:00:53 PM", "Depends on ~~#2912~~", "NaN"], ["3228", "Fix TopNRowNumber output trunction bug", "Eric Hwang", "erichwang", "07/07/15, 09:44:12 PM", "NaN", "NaN"], ["3229", "Increase test timeouts to avoid Travis failures", "Christopher Berner", "cberner", "07/08/15, 01:03:48 AM", "NaN", "NaN"], ["3248", "Do type-aware equality comparison in FieldSetFilteringRecordSet", "Haozhun Jin", "haozhun", "07/17/15, 07:58:33 PM", "NaN", "NaN"], ["3259", "Use separate config class for Hive metastore URI", "David Phillips", "electrum", "07/14/15, 07:03:26 PM", "This is required for connector extensions that use a different\nmethod to locate the metastore.", "NaN"], ["3261", "Add max_n and min_n", "Haozhun Jin", "haozhun", "08/21/15, 03:21:23 AM", "This is a precursor of adding max_by_n and min_by_n", "NaN"], ["3266", "Fix row number bug", "Christopher Berner", "cberner", "07/14/15, 06:25:19 PM", "NaN", "NaN"], ["3267", "Enable cluster memory manager by default", "Christopher Berner", "cberner", "07/23/15, 06:38:20 PM", "Also add documentation", "NaN"], ["3268", "Expose stats in Presto JDBC driver", "Eric Hwang", "erichwang", "07/15/15, 09:22:35 PM", "NaN", "NaN"], ["3275", "Fix race condition in SHOW TABLES test", "David Phillips", "electrum", "07/15/15, 01:05:29 AM", "NaN", "NaN"], ["3279", "Add dictionary encoded block", "Nileema Shingte", "nileema", "07/31/15, 07:00:45 PM", "NaN", "NaN"], ["3283", "Require explicit output functions for aggregations", "David Phillips", "electrum", "07/17/15, 02:54:29 PM", "Relying on the state serializer to write to the output builder\nis wrong because blocks should always be written using the type.", "NaN"], ["3285", "Fix compiling primitive class constants", "David Phillips", "electrum", "07/16/15, 01:03:31 AM", "NaN", "NaN"], ["3286", "Update to ANTLR 4.5.1", "David Phillips", "electrum", "07/16/15, 04:20:01 AM", "NaN", "NaN"], ["3287", "Fix constant subscript analysis", "David Phillips", "electrum", "07/16/15, 05:27:41 AM", "The index check should only occur for arrays, not maps.", "NaN"], ["3288", "Remove duplicate word in comment of TupleDescriptor", "Xin Yao", "yaoxin226", "07/16/15, 05:24:29 PM", "NaN", "NaN"], ["3289", "Add release notes for 0.111", "Christopher Berner", "cberner", "07/16/15, 06:10:26 PM", "NaN", "NaN"], ["3290", "Add release notes for 0.111", "Haozhun Jin", "haozhun", "07/16/15, 08:48:04 PM", "Also added `IF NOT EXISTS` as requested by @joy-yao ", "NaN"], ["3291", "Add release notes for JDBC driver query stats", "Eric Hwang", "erichwang", "07/16/15, 09:04:37 PM", "NaN", "NaN"], ["3292", "Add documentation and release notes for histogram function", null, "jacobgao", "07/16/15, 09:46:56 PM", "NaN", "NaN"], ["3293", "Update histogram documentation and release notes", "David Phillips", "electrum", "07/16/15, 10:25:32 PM", "NaN", "NaN"], ["3296", "Refactor aggregation code", "David Phillips", "electrum", "08/21/15, 03:01:20 AM", "NaN", "NaN"], ["3302", "Add false test case to TestArrayOperator.testComparison", "Haozhun Jin", "haozhun", "07/18/15, 12:19:49 AM", "NaN", "NaN"], ["3303", "Extract plugin for JMX connector", "David Phillips", "electrum", "07/20/15, 04:54:37 PM", "NaN", "NaN"], ["3307", "Minor PlanPrinter improvements", "David Phillips", "electrum", "07/20/15, 04:21:49 PM", "NaN", "NaN"], ["3308", "Update to airlift.slice 0.15", "Haozhun Jin", "haozhun", "07/20/15, 07:34:08 PM", "NaN", "NaN"], ["3309", "Discover plugins at runtime for development", "David Phillips", "electrum", "10/05/15, 08:15:49 PM", "Plugins require a `META-INF/services` resource which is normally built by the `presto-plugin` Maven packaging. But this requires a command line build and thus makes development more difficult. This change generates the service descriptor files automatically just like the Maven packaging when the plugin is loaded from a POM, allowing things to mostly \"just work\" when run in the IDE.", "NaN"], ["3310", "Replace TupleDomain.Function with JDK Function", "David Phillips", "electrum", "07/21/15, 10:14:36 PM", "NaN", "NaN"], ["3311", "Add outputColumns to ConnectorIndexResolver", "Joy Yao", "joy-yao", "07/22/15, 04:23:42 AM", "NaN", "NaN"], ["3316", "Do not print redundant layout", "David Phillips", "electrum", "07/21/15, 05:18:28 PM", "NaN", "NaN"], ["3317", "Add UnknownOperator", "Haozhun Jin", "haozhun", "07/23/15, 10:14:40 PM", "Fixes #3184 \n1. If fixes query SELECT NULL UNION SELECT NULL\n2. It avoids future failure when ambiguity check is added to coercion-enabled resolving in FunctionRegistry", "NaN"], ["3318", "Convert Hive TimeZone config to a string", "David Phillips", "electrum", "07/21/15, 10:55:02 PM", "The JDK toString() method for TimeZone is extremely long and\nmakes the Bootstrap config printing hard to read.", "NaN"], ["3319", "Fix table creation tests for Cassandra and Kafka", "David Phillips", "electrum", "07/21/15, 10:55:43 PM", "NaN", "NaN"], ["3320", "Add json_parse, json_format; remove cast between json and varchar", "Haozhun Jin", "haozhun", "08/17/15, 10:31:01 PM", "Issue #3197 ", "NaN"], ["3321", "Fix table creation for Hive connector", "David Phillips", "electrum", "07/21/15, 11:15:19 PM", "NaN", "NaN"], ["3324", "Add additional test and comment for partial json cast", "Haozhun Jin", "haozhun", "07/23/15, 10:16:57 PM", "NaN", "NaN"], ["3325", "Remove getType method from WindowFunction", "David Phillips", "electrum", "07/23/15, 11:23:22 PM", "NaN", "NaN"], ["3326", "Add threshold for Hive tuple domain compaction", "David Phillips", "electrum", "07/22/15, 06:50:51 PM", "NaN", "NaN"], ["3327", "Add cast between json and primitive casts", "Haozhun Jin", "haozhun", "01/26/16, 07:37:34 PM", "Depends on ~~#3320~~(merged)", "NaN"], ["3330", "Port system connector to TableLayout API", "Nileema Shingte", "nileema", "07/22/15, 10:42:57 PM", "NaN", "NaN"], ["3331", "Fix symbol unaliasing", "Christopher Berner", "cberner", "07/23/15, 10:13:03 PM", "NaN", "NaN"], ["3332", "Update release notes for 0.112", "David Phillips", "electrum", "07/22/15, 11:48:54 PM", "NaN", "NaN"], ["3336", "Implement array and map types in Raptor", "Christopher Berner", "cberner", "08/16/15, 08:53:36 PM", "NaN", "NaN"], ["3339", "Fix unaliasing of symbols for ExchangeNode inputs", "Christopher Berner", "cberner", "07/24/15, 01:13:06 AM", "NaN", "NaN"], ["3340", "Verify that all required functions are registered for types", "Christopher Berner", "cberner", "08/29/15, 05:05:02 PM", "NaN", "NaN"], ["3341", "Fix JMX connector", "David Phillips", "electrum", "07/24/15, 01:30:46 AM", "NaN", "NaN"], ["3342", "Allow using any type with value window functions", "David Phillips", "electrum", "07/29/15, 09:52:55 PM", "NaN", "NaN"], ["3348", "Use JDBC batches for inserting Raptor shard metadata", "David Phillips", "electrum", "07/26/15, 04:29:03 AM", "This improves performance, especially when there is substantial latency\nbetween Presto and the metadata database.", "NaN"], ["3349", "Add session properties to queries", "Joy Yao", "joy-yao", "08/03/15, 11:52:12 PM", "NaN", "NaN"], ["3350", "Refactor session APIs", "Dain Sundstrom", "dain", "07/25/15, 07:23:41 PM", "NaN", "NaN"], ["3353", "Enforce minimum Java version for building", "David Phillips", "electrum", "07/27/15, 06:38:17 PM", "This prints a failure like this (with property changed to `-51` for testing):\n\n```\n[INFO] --- maven-enforcer-plugin:1.2:enforce (default) @ presto-root ---\n[WARNING] Rule 2: org.apache.maven.plugins.enforcer.RequireJavaVersion failed with message:\nDetected JDK Version: 1.8.0-45 is not in the allowed range 1.8.0-51.\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 3.256 s\n[INFO] Finished at: 2015-07-25T22:26:59-07:00\n[INFO] Final Memory: 36M/339M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.2:enforce (default) on project presto-root: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]\n```", "NaN"], ["3358", "Fix negative caching for Hive metadata", "David Phillips", "electrum", "07/28/15, 06:26:09 PM", "Fixes #1540 ", "NaN"], ["3360", "Fix regression for repartition exchange", "Haozhun Jin", "haozhun", "07/29/15, 06:33:58 PM", "This pull request blocks release.\n\nThis pull request slows down test.\n\n@cberner @martint Can you think of other cases where deduplication might have broken things?\n\n---\n\nFixes #3351 \n\nCommits e64daa6 and ba43b9a introduces deduplication to partitionKeys in `UnaliasSymbolReferences.visitExchange`. This breaks the following query\n\n```\nSELECT COUNT(*) FROM lineitem FULL JOIN orders ON lineitem.orderkey = orders.orderkey AND orders.orderkey = 2\n```\n\nBefore deduplication, left partitionKey is `(orderkey, expr=2)`, right partitionKey is `(orderkey_0, orderkey_0)`. After deduplication, right partitionKey becomes `(orderkey_0)`. As a result, left and right partitionKey no longer matches. Incorrect result is produces because matching rows hashes to different partitions.", "NaN"], ["3361", "Fix race condition in SHOW TABLES smoke test", "David Phillips", "electrum", "07/27/15, 10:48:07 PM", "NaN", "NaN"], ["3365", "Fix extra META-INF/services resources in JDBC", "David Phillips", "electrum", "07/28/15, 11:27:03 PM", "NaN", "NaN"], ["3367", "Code cleanup", "David Phillips", "electrum", "08/06/15, 01:04:39 AM", "NaN", "NaN"], ["3368", "Fix Hive metastore socket leak on SOCKS connect failure", "David Phillips", "electrum", "07/29/15, 06:40:18 PM", "NaN", "NaN"], ["3369", "Update documentation and tests for URL functions", "David Phillips", "electrum", "07/29/15, 06:24:15 PM", "NaN", "NaN"], ["3370", "Remove round-trip between string and byte[] for like", "Haozhun Jin", "haozhun", "07/30/15, 11:00:36 PM", "By using NonStrictUTF8Encoding, joni does not suffer infinite loop problem with invalid utf8. As a result, the roundtrip between byte[] and string can be removed.", "NaN"], ["3372", "Add metadata tables to Raptor", "Nileema Shingte", "nileema", "09/23/15, 01:07:43 AM", "NaN", "NaN"], ["3373", "Cap the number of worker threads for tests", "David Phillips", "electrum", "07/29/15, 09:14:56 PM", "NaN", "NaN"], ["3375", "Handle lower case in column renames", "Nileema Shingte", "nileema", "07/30/15, 08:09:43 PM", "NaN", "NaN"], ["3377", "Implement INSERT for Hive with partition support", "Dain Sundstrom", "dain", "10/09/15, 04:05:11 AM", "NaN", "NaN"], ["3383", "Use metadata to transform properties in TestingConnectorSession", "Dain Sundstrom", "dain", "07/31/15, 02:27:59 AM", "NaN", "NaN"], ["3386", "Allow disabling catalogs from being loaded", "David Phillips", "electrum", "07/31/15, 10:21:53 PM", "This allows skipping bad catalogs without deleting the catalog\nproperties file, which is often useful during development.", "NaN"], ["3387", "Add 0.113 release notes", "Dain Sundstrom", "dain", "07/31/15, 10:47:52 PM", "NaN", "NaN"], ["3388", "Add table properties to Raptor", "Nileema Shingte", "nileema", "08/11/15, 06:37:05 PM", "NaN", "NaN"], ["3389", "Remove TestTpchDistributedQueriesWithoutHashGeneration", "Dain Sundstrom", "dain", "07/31/15, 11:11:21 PM", "This test when combined with the others requires too many resources", "NaN"], ["3393", "Handle RowGroupDictionaries correctly in dictionary reader", "Nileema Shingte", "nileema", "08/03/15, 09:58:35 PM", "NaN", "NaN"], ["3394", "Allow non-comparable type for LOWER/UPPER UNBOUNDED", "Haozhun Jin", "haozhun", "08/03/15, 08:41:50 PM", "At the time I introduce Block as native container for structural types, I ran into the problem that Block is not self comparable, and cannot be supplied to `TupleDomain`/`Marker`. I had 2 options for this problem.\n1. Allow non-comparable type in Marker as long as the value is UPPER UNBOUNDED or LOWER UNBOUNDED.\n2. Use BogusComparable instead of the real type whenever Block is about to be passed to constructor of Marker.\n\nI argued for option 1 because\n- The theory of tuple domain does not require values to be comparable. And Eric is working toward removing this restriction in Presto.\n- Current code works for non-comparable type as long as the value is UPPER UNBOUNDED or LOWER UNBOUNDED.\n\nEric argued for option 2 because\n- Changing Marker (which is in SPI) means we are committed to support this use case, and people may depend on it.\n\nI decided to go for option 2.\n\nNow Nileema brought to my attention that this query failed in the most recent verifier run:\n\n```\nSELECT * FROM t WHERE ds='2015-07-15' AND array_column IS NOT NULL AND userid=1 LIMIT 100;\n```\n\nI looked into this, and this is the cause:\n- At `HttpRemoteTask.scheduleUpdate` line 397, `updateRequest` is json serialized. Through a few other calls, it will end up in `SerializableNativeValue.Serializer.serialize` line 87. When `arrayColumn is not null` is present in the query, `BogusComparable.class.getCanonicalName` will be serialized into json because it's in `updateRequest.fragment.partitionedSourceNode.currentConstraint.domains[x].value.ranges.type`.\n- At  `SerializableNativeValue.Deserializer.extractClassType` line 181, it fails because ~~`BogusComparable` is not in SPI~~ `forName` is not a pair with `getCanonicalName`. `forName` is a pair with `getName`.\n\nHowever, fixing the above serialization code to use `getName` instead of `getCanonicalName` results in another failure:\n- At line 226 in `Domain`. IllegalArgumentException: Mismatched Domain types: class `com.facebook.presto.sql.planner.DomainTranslator$Visitor$BogusComparable` vs class `com.facebook.presto.orc.TupleDomainOrcPredicate$BogusComparable`\n\nNow, even if I have only one `BogusComparable`, the above failure won't go away unless that one `BogusComparable` is in `presto-spi` because of how `PluginClassLoader` works.\n\nNow, it means either way, the solution to the problem will end up in SPI. In this case, I prefer option 1.\n\nI talked with @martint and @cberner, they support going for option 1.\n\ncc @dain because you are doing the release.\ncc @erichwang because TupleDomain is your territory.\ncc @nileema because you found the bug above.", "NaN"], ["3402", "Fix HostAddress string serialization round trip", "Eric Hwang", "erichwang", "08/04/15, 11:01:16 PM", "NaN", "NaN"], ["3405", "Remove use of deprecated @NotNull annotation", "Martin Traverso", "martint", "08/05/15, 07:17:56 PM", "It's no longer supported in Antlr 4.5+", "NaN"], ["3410", "Use correct table name in AbstractTestHiveClient.doCreateTable", "Dain Sundstrom", "dain", "08/05/15, 11:04:16 PM", "Test was creting a table using a name in a class field and then\nverifying the created table using a name from a method parameter.", "NaN"], ["3411", "Hive split discovery fix", "Christopher Berner", "cberner", "08/06/15, 04:24:30 AM", "NaN", "NaN"], ["3412", "Update to airlift 0.113", "Martin Traverso", "martint", "08/06/15, 10:30:19 PM", "Fixes an issue with how kerberos principals are formatted", "NaN"], ["3418", "Fix compilation error due to airlift change", "Martin Traverso", "martint", "08/06/15, 10:49:48 PM", "Airlift 0.113 changed some interfaces to use Java's Optional\ninstead of Guava's, which broke some usages when we updated to\nthat version", "NaN"], ["3419", "Fix starvation and more races in BackgroundHiveSplitLoader", "Haozhun Jin", "haozhun", "08/15/15, 01:44:27 AM", "The race was not solved in #3411 once `resume()` invocation is taken into consideration.\n#3411 also introduced starvation.", "NaN"], ["3425", "Add table property documentataion and release notes", "Dain Sundstrom", "dain", "10/10/15, 06:44:23 PM", "NaN", "NaN"], ["3439", "Reduce allocation of hadoop configuration objects", "Christopher Berner", "cberner", "08/11/15, 05:08:43 PM", "NaN", "NaN"], ["3442", "Add missing Raptor connector table properties", "Dongmin Yu", "miniway", "08/12/15, 02:47:51 AM", "NaN", "NaN"], ["3445", "Improve docs", "Christopher Berner", "cberner", "09/01/15, 06:03:00 PM", "NaN", "NaN"], ["3446", "Add missing JsonCreator annotation at PageBufferInfo", "Dongmin Yu", "miniway", "08/13/15, 04:12:49 PM", "NaN", "NaN"], ["3451", "Update 0.115 release notes", "Christopher Berner", "cberner", "08/14/15, 05:31:22 PM", "NaN", "NaN"], ["3455", "Update to airlift 0.114", "Christopher Berner", "cberner", "08/14/15, 10:22:07 PM", "NaN", "NaN"], ["3456", "Update release notes", "Martin Traverso", "martint", "08/14/15, 10:42:30 PM", "NaN", "NaN"], ["3466", "Allow temporal column of Date type", "Nileema Shingte", "nileema", "08/19/15, 08:03:18 PM", "NaN", "NaN"], ["3467", "Cleanup helper functions for ArrayType and MapType", "Christopher Berner", "cberner", "08/19/15, 02:14:22 AM", "NaN", "NaN"], ["3468", "Remove RecordSet from SystemTable interface", "David Phillips", "electrum", "08/19/15, 08:03:36 PM", "NaN", "NaN"], ["3472", "Fix Raptor metadata tests", "Nileema Shingte", "nileema", "08/19/15, 11:34:25 PM", "NaN", "NaN"], ["3478", "Add permission checks to analyzer", "Dain Sundstrom", "dain", "09/07/15, 06:04:49 PM", "The design and implementation of this is very much a work in progress, so please let me know if you have any feedback.\n\nThis adds basic authorization checks to Presto.  During analysis of a statement we check if the identity attached to the session is authorized to perform the specified actions.  For statements that skip analysis, the checks are performed directly in the task execution.  The implementation allows for plugins to provide a system wide access control implementation or a specific access control for connector instance.", "NaN"], ["3479", "Remove duplicate test block creation methods", "Haozhun Jin", "haozhun", "08/24/15, 11:54:43 PM", "A simple test-only change", "NaN"], ["3483", "Add release notes for cast between JSON and VARCHAR", "Haozhun Jin", "haozhun", "08/25/15, 12:27:04 AM", "NaN", "NaN"], ["3484", "Fixes for Raptor compaction", "Nileema Shingte", "nileema", "09/02/15, 05:12:05 AM", "NaN", "NaN"], ["3486", "Interrupt rewrites if the thread is interrupted", "Nileema Shingte", "nileema", "08/21/15, 11:16:34 PM", "NaN", "NaN"], ["3489", "Copy Raptor shards to backup in background threads", "Nileema Shingte", "nileema", "09/02/15, 05:26:15 PM", "Fixes #3271", "NaN"], ["3491", "Use basic I/O for copying Raptor backup files", "David Phillips", "electrum", "08/25/15, 03:47:32 PM", "Interrupting a thread that is stuck in an NIO system call can block\nthe interrupting thread until the system call returns. Additionally,\nusing FileChannel.transferTo() will invoke the sendfile() system call\nwhich has unclear performance semantics for remote file systems.", "NaN"], ["3494", "Make deletion commit asynchronous", "David Phillips", "electrum", "08/26/15, 03:01:26 PM", "NaN", "NaN"], ["3496", "Rename Block to ByteCodeBlock and some other code cleanup", "Christopher Berner", "cberner", "08/25/15, 09:47:19 PM", "NaN", "NaN"], ["3497", "Remove outdated memory manager configs", "Christopher Berner", "cberner", "08/26/15, 12:19:07 AM", "NaN", "NaN"], ["3498", "Make current node in Discovery immutable", "Eric Hwang", "erichwang", "08/26/15, 02:58:27 AM", "It should never change, and this callsite is highly contended.", "NaN"], ["3500", "Replace TableHandle for deletion queries", "David Phillips", "electrum", "08/27/15, 04:18:41 AM", "This also has some minor cleanup and refactoring for Raptor.", "NaN"], ["3501", "Update to airbase 43", "Martin Traverso", "martint", "08/26/15, 06:09:23 PM", "Fixes a compatibility issue with Java 8u60", "NaN"], ["3502", "Fix bug in PlanSanityChecker for exchange node", "Haozhun Jin", "haozhun", "08/26/15, 11:25:32 PM", "NaN", "NaN"], ["3504", "Cancel queries before shutting down DistributedQueryRunner", "Haozhun Jin", "haozhun", "08/26/15, 11:53:35 PM", "Travis complains that more than 4MB is generated in log file. In Travis, TestQueues generate 1k-5k lines of log warning of \"Server refused connection\". The same test generates around 250 lines of warning on my laptop. These warnings are generated because worker is closed before coordinator while queries are still running.", "NaN"], ["3505", "Move BeginTableWrite to the end of optimizers", "Haozhun Jin", "haozhun", "08/27/15, 12:31:33 AM", "TableLayout is chosen in AddExchanges, which is after BeginTableWrite in today's list optimizers. In order to implement an optimizer to optimize out the delete node (as needed in metadata delete support), BeginTableWrite need to be moved down in the list of optimizers.", "NaN"], ["3508", "Fix leak in StateMachine listeners caused by the query scheduler", "Dain Sundstrom", "dain", "08/27/15, 09:05:28 PM", "NaN", "NaN"], ["3509", "Add checksum() aggregation function", "Christopher Berner", "cberner", "08/28/15, 11:34:20 PM", "NaN", "NaN"], ["3513", "Allow Stage state transition from SCHEDULING_SPLITS to SCHEDULED", "Dain Sundstrom", "dain", "08/28/15, 09:00:50 PM", "NaN", "NaN"], ["3518", "Fix 3 bugs in CLI related to user aborting query", "Haozhun Jin", "haozhun", "09/16/15, 01:03:34 AM", "NaN", "NaN"], ["3519", "Optimize expression interpreter for simple case statement", "Nileema Shingte", "nileema", "09/02/15, 07:56:11 AM", "Optimizations for simple case, searched case and coalesce ", "NaN"], ["3520", "Reuse random node selector across splits in selectCandidateNodes", "Dain Sundstrom", "dain", "08/30/15, 05:46:17 AM", "Improver performance of NodeScheduling by reusing random node scheduler\nfor all splits.", "NaN"], ["3523", "Update to airline 0.7", "David Phillips", "electrum", "08/31/15, 11:26:53 PM", "NaN", "NaN"], ["3524", "Add support for EXPLAIN of DDL tasks", "Dain Sundstrom", "dain", "09/02/15, 12:38:43 AM", "NaN", "NaN"], ["3526", "Remove unnecessary generics in DataDefinitionExecution", "David Phillips", "electrum", "09/02/15, 12:26:09 AM", "NaN", "NaN"], ["3528", "Update 0.116 release notes", "Christopher Berner", "cberner", "09/02/15, 01:29:23 AM", "NaN", "NaN"], ["3529", "Reduce contention in scheduler", "Dain Sundstrom", "dain", "09/08/15, 11:29:18 PM", "NaN", "NaN"], ["3530", "Update 0.116 release notes", "Haozhun Jin", "haozhun", "09/02/15, 04:32:48 PM", "NaN", "NaN"], ["3536", "Fix arbitrary function for structural type", "Haozhun Jin", "haozhun", "09/15/15, 07:55:57 PM", "NaN", "NaN"], ["3537", "Optimize updateState() to be O(n) instead of O(n^2)", "Christopher Berner", "cberner", "09/06/15, 07:31:06 PM", "NaN", "NaN"], ["3539", "Add access control check for RENAME COLUMN", "Dain Sundstrom", "dain", "09/12/15, 02:31:01 AM", "NaN", "NaN"], ["3540", "Add connector ID to all Raptor handles", "David Phillips", "electrum", "09/02/15, 09:12:46 PM", "NaN", "NaN"], ["3543", "Organize imports", "Christopher Berner", "cberner", "09/03/15, 07:57:57 PM", "NaN", "NaN"], ["3544", "Copy correct shard to backup", "Nileema Shingte", "nileema", "09/03/15, 08:45:59 PM", "NaN", "NaN"], ["3545", "Update airlift to 0.115", "Nileema Shingte", "nileema", "09/03/15, 08:16:02 PM", "NaN", "NaN"], ["3547", "Add back cast between JSON and VARCHAR to ease migration", "Haozhun Jin", "haozhun", "09/04/15, 07:56:42 PM", "NaN", "NaN"], ["3551", "Fix bug in BigintGroupByHash", "Christopher Berner", "cberner", "09/05/15, 12:00:08 AM", "When set is rehash'ed and contains a null, the value zero will appear to\nbe contained in the set in addition to null", "NaN"], ["3552", "Add release notes for 0.117", "Haozhun Jin", "haozhun", "09/04/15, 11:57:43 PM", "NaN", "NaN"], ["3554", "Replace checkNotNull with requireNonNull", "Christopher Berner", "cberner", "09/14/15, 06:37:48 PM", "I'll squash the commits together before pushing, but left them separate to make review easier. Each commit was creating using the \"inline\" action in IntelliJ to inline that specific signature", "NaN"], ["3555", "Check view access control using view owner", "Dain Sundstrom", "dain", "09/16/15, 12:53:29 AM", "NaN", "NaN"], ["3556", "Add simple shard balancer", "David Phillips", "electrum", "09/09/15, 08:29:55 PM", "The shard ejector runs on every node. If a node is too full relative\nto other nodes, it ejects shards onto other less full nodes until the\nnode is at average capacity.", "NaN"], ["3559", "Improve verifier when underlying data may change", "Christopher Berner", "cberner", "09/16/15, 12:33:32 AM", "NaN", "NaN"], ["3560", "Categorize HDFS read error for ORC", "Dain Sundstrom", "dain", "09/08/15, 07:39:26 PM", "NaN", "NaN"], ["3562", "Fix condition for SharedBuffer completion", "Nileema Shingte", "nileema", "09/10/15, 04:10:41 AM", "SharedBuffer should be marked complete when both conditions are satisfied:\n- client has acknowledged all the pages\n- client knows that there will be no more pages\n\nOnce the client sees all the pages and the no more pages signal,\nexplicitly send a delete to the SharedBuffer to indicate completion.", "NaN"], ["3573", "Fix null pointer exception caused by checksum()", "Christopher Berner", "cberner", "09/09/15, 07:27:49 PM", "NaN", "NaN"], ["3574", "Add session properties for reader in Raptor", "Nileema Shingte", "nileema", "09/10/15, 04:35:28 AM", "NaN", "NaN"], ["3576", "Use assertEquals instead of assertTrue", "Christopher Berner", "cberner", "09/10/15, 12:07:40 AM", "NaN", "NaN"], ["3577", "Fix bug in union coercion", "Haozhun Jin", "haozhun", "09/09/15, 10:06:42 PM", "Fixes #3572 \n\nCode originally assumes that for a RelationPlan plan, plan.outputSymbols\nis always the same as plan.root.outputs. This fix removes that assumption.", "NaN"], ["3580", "Fix concurrency issue in RaptorSplitSource", "David Phillips", "electrum", "09/10/15, 04:37:21 AM", "NaN", "NaN"], ["3581", "Add shard_uuid hidden column to Raptor", "Nileema Shingte", "nileema", "09/10/15, 11:31:54 PM", "NaN", "NaN"], ["3585", "Upgrade to Airlift 0.116", "Dain Sundstrom", "dain", "09/10/15, 10:25:07 PM", "NaN", "NaN"], ["3586", "Add Presto 0.118 release notes", "Dain Sundstrom", "dain", "09/10/15, 11:21:11 PM", "NaN", "NaN"], ["3587", "Create a new dictionary when we read a row group", "Nileema Shingte", "nileema", "09/11/15, 03:41:20 PM", "We try to reuse the already allocated dictionary (if possible) for\nmultiple reads. This forces us to make a copy of the dictionary for\nevery batch. Allocate a new dictionary for every read, to avoid copying\nthe dictionary each time.", "NaN"], ["3590", "Add release notes", "Nileema Shingte", "nileema", "09/11/15, 05:24:36 PM", "NaN", "NaN"], ["3591", "Clarify concurrency semantics of RaptorSplitSource", "David Phillips", "electrum", "09/14/15, 05:37:46 PM", "The previous version had unclear semantics and relied on an implicit\ncontract with the caller to use it correctly. The new version guards\nthese invariants explicitly.", "NaN"], ["3593", "Fix and cleanup unit tests for aggregation functions", "Haozhun Jin", "haozhun", "09/15/15, 08:11:12 PM", "There were 2 assertAggregation functions. One of them can do automatic splitting up to exercise and verify the correctness of combine functions. The other can't. This commit removes all external calls to the latter.", "NaN"], ["3594", "Fix min_by and max_by for structural type", "Haozhun Jin", "haozhun", "09/15/15, 08:15:19 PM", "The functions max_by and min_by was not spotted and updated to use the new native containers and methods when structural type implementation was changed to Block.\n\nDepends on #3593 ", "NaN"], ["3596", "Fix handling of full width characters in CLI", "David Phillips", "electrum", "09/14/15, 04:14:01 PM", "NaN", "NaN"], ["3598", "CLI improvements", "David Phillips", "electrum", "09/14/15, 07:17:13 PM", "NaN", "NaN"], ["3601", "Add documentation for max_n and min_n", "Haozhun Jin", "haozhun", "09/15/15, 07:51:04 PM", "NaN", "NaN"], ["3604", "Close Raptor ShardIterator resources in correct order", "David Phillips", "electrum", "09/14/15, 11:04:49 PM", "NaN", "NaN"], ["3605", "Change OrcReader to return Blocks instead of Vectors", "Dain Sundstrom", "dain", "09/16/15, 01:08:20 AM", "NaN", "NaN"], ["3606", "Fix scheduler handling of partial cancel", "Dain Sundstrom", "dain", "09/15/15, 05:20:27 PM", "NaN", "NaN"], ["3613", "Simplify return value in Validator", "Christopher Berner", "cberner", "09/16/15, 06:19:06 PM", "NaN", "NaN"], ["3614", "Add 0.119 release notes", "Dain Sundstrom", "dain", "09/16/15, 01:27:40 AM", "NaN", "NaN"], ["3616", "Handle ctrl-C as keyboard input in CLI", "David Phillips", "electrum", "09/16/15, 05:30:41 PM", "After the recent changes to read keyboard input during the status\nupdate loop, the handler for SIGINT sporadically stops working.\nWhen this occurs, ctrl-C is read as normal keyboard input.", "NaN"], ["3618", "Create new ORC dictionary even when dictionary is empty", "Dain Sundstrom", "dain", "09/16/15, 11:34:25 PM", "A new dictionary array must be create each time the dicitonary is opened.\nOtherwise, the null slot may inherit a value from the previous dictionary.", "NaN"], ["3619", "Remove invalid release note about ORC null read bug", "Dain Sundstrom", "dain", "09/17/15, 01:34:33 AM", "NaN", "NaN"], ["3625", "Remove location aware scheduling", "Nileema Shingte", "nileema", "10/14/15, 01:20:21 AM", "NaN", "NaN"], ["3627", "Remove redundant call to setEndpoint", "David Phillips", "electrum", "09/18/15, 04:58:41 AM", "NaN", "NaN"], ["3630", "Move tuning and web interface to admin section", "David Phillips", "electrum", "09/18/15, 06:14:49 PM", "NaN", "NaN"], ["3631", "Make double comparison precision configurable", "Christopher Berner", "cberner", "09/20/15, 05:52:14 AM", "NaN", "NaN"], ["3633", "Make session catalog and schema optional", "David Phillips", "electrum", "09/20/15, 11:52:24 PM", "NaN", "NaN"], ["3635", "Reorder HiveMetadata allow flags", "Dain Sundstrom", "dain", "09/20/15, 06:19:50 PM", "NaN", "NaN"], ["3636", "Add nested rowtype reference support", "Joy Yao", "joy-yao", "10/20/15, 07:58:26 PM", "NaN", "NaN"], ["3638", "Fix race condition in query queues", "Grzegorz Kokosi\u0144ski", "kokosing", "10/09/15, 04:07:02 PM", "Lets consider an example from #3637:\n \"user.${USER}\": {\n \"maxConcurrent\": 2,\n \"maxQueued\": 2\n },\n \"dashboard.${USER}\": {\n \"maxConcurrent\": 1,\n \"maxQueued\": 1\n },\nWhen two queries are scheduled at the same time, both of them get permit\nand most likely both of them will try to get enqueued.\nThis situation lead to an error that one of them was not able\nto enqueue - \"Entering secondary queue failed\".\nIn order to solve that, I removed the check of the of size of queued queries.\nIt is not needed as it is already covered by checking the permits in reserve method.\nOnce query got permit it is assumed that it can be enqueued. As a side effect query queue can\nfor the moment hold more queries in state QUEUED than it is set in configuration.\n\nTask: #3637\n\nTest Plan: Running unit tests", "NaN"], ["3639", "Make TestStateMachine more reliable in Travis", "Dain Sundstrom", "dain", "09/21/15, 07:42:32 PM", "In low CPU test environments it can take longer than 50ms for a simple future\nto run completely", "NaN"], ["3640", "Fix ArrayType.equalTo to work with comparable non-orderable types", "Raghav Sethi", "raghavsethi", "09/29/15, 09:00:16 PM", "Attempt to fix #3542. Still having some trouble passing the new test, because it's not clear how comparable non-orderable types should be handled at lower layers.", "NaN"], ["3641", "Add Redis plugin to server distribution", "Dain Sundstrom", "dain", "09/21/15, 07:53:54 PM", "NaN", "NaN"], ["3642", "Update 0.119 release notes", "Christopher Berner", "cberner", "09/21/15, 09:31:03 PM", "NaN", "NaN"], ["3643", "Update to H2 1.4.189", "David Phillips", "electrum", "09/21/15, 10:24:44 PM", "NaN", "NaN"], ["3644", "Add 0.119 release notes", "Dain Sundstrom", "dain", "09/21/15, 09:29:48 PM", "NaN", "NaN"], ["3645", "Separate TaskResource timeout and response thread pools", "Eric Hwang", "erichwang", "09/22/15, 01:08:03 AM", "NaN", "NaN"], ["3649", "Update 0.119 release notes", "Christopher Berner", "cberner", "09/22/15, 05:57:45 PM", "NaN", "NaN"], ["3650", "Turn compaction off at table level", "Nileema Shingte", "nileema", "09/23/15, 12:59:07 AM", "NaN", "NaN"], ["3651", "Update release notes for 0.119", "Haozhun Jin", "haozhun", "09/22/15, 06:40:15 PM", "NaN", "NaN"], ["3652", "Update 0.119 release notes", "David Phillips", "electrum", "09/22/15, 07:31:04 PM", "NaN", "NaN"], ["3655", "Update release notes for 0.119", "Christopher Berner", "cberner", "09/22/15, 07:44:18 PM", "NaN", "NaN"], ["3657", "Make \"zone\" keyword non-reserved", "Martin Traverso", "martint", "09/23/15, 04:41:41 AM", "NaN", "NaN"], ["3658", "Fix planning of huge IN lists", "David Phillips", "electrum", "09/25/15, 07:01:22 PM", "NaN", "NaN"], ["3660", "Update to JDBI 2.63.1", "David Phillips", "electrum", "09/23/15, 07:43:03 PM", "NaN", "NaN"], ["3664", "Rewrite a very slow test for cross join", "Joy Yao", "joy-yao", "09/24/15, 07:29:11 PM", "NaN", "NaN"], ["3665", "Fix column name for RaptorMetadataTables", "Nileema Shingte", "nileema", "09/24/15, 12:57:57 AM", "NaN", "NaN"], ["3667", "Remove JSON plan printer and domains from Input columns", "Eric Hwang", "erichwang", "09/24/15, 09:33:19 PM", "NaN", "NaN"], ["3671", "Remove development config for Redis", "David Phillips", "electrum", "09/24/15, 06:06:33 PM", "NaN", "NaN"], ["3673", "Fix and improve failure backoff in HttpRemoteTask", "Dain Sundstrom", "dain", "09/25/15, 12:50:30 AM", "NaN", "NaN"], ["3675", "Add release notes for HttpRemoteTask throttling changes", "Dain Sundstrom", "dain", "09/25/15, 01:06:20 AM", "NaN", "NaN"], ["3676", "Implement CopyPositions for ArrayBlock and InterleavedBlock", "Haozhun Jin", "haozhun", "09/25/15, 10:34:59 PM", "Fixes #3670 \n\ncc @nileema ", "NaN"], ["3683", "Refactor FunctionRegistry", "Christopher Berner", "cberner", "10/05/15, 06:38:56 PM", "NaN", "NaN"], ["3684", "Fix ConcurrentModificationException in StateMachine listener management", "Dain Sundstrom", "dain", "10/10/15, 06:49:22 PM", "When cleaning up listener future acquire lock on FutureStateChange", "NaN"], ["3685", "Update partitionedSplitCount in NodeTasks eagerly", "Joy Yao", "joy-yao", "10/03/15, 06:41:30 PM", "NaN", "NaN"], ["3686", "Add TableScan, ScanFilterProject memory tracking", "Haozhun Jin", "haozhun", "12/03/15, 07:03:55 PM", "Also added system memory tracking to PageSource. Tracking for RecordCursor has not been added.", "NaN"], ["3687", "Implement metadata delete for Hive", "Haozhun Jin", "haozhun", "10/14/15, 11:52:01 PM", "Depend on #3377", "NaN"], ["3688", "Update release notes for 0.120", "Haozhun Jin", "haozhun", "09/29/15, 10:37:12 PM", "NaN", "NaN"], ["3689", "Make TupleDomain type generic", "Eric Hwang", "erichwang", "11/04/15, 08:28:26 PM", "NaN", "NaN"], ["3691", "Added authenticated icon and principal info to web UI", "Raghav Sethi", "raghavsethi", "10/07/15, 12:12:58 AM", "Completes feature request #3674. Required adding bootstrap fonts directory to repo.", "NaN"], ["3692", "Optimize ORC reading when file contains many tiny stripes", "Haozhun Jin", "haozhun", "10/25/15, 06:04:15 AM", "Fixes #3663 ", "NaN"], ["3695", "Fail casts between JSON and VARCHAR", "Haozhun Jin", "haozhun", "10/07/15, 11:34:32 PM", "NaN", "NaN"], ["3696", "Remove unused ordinalPosition from HiveColumnHandle", "David Phillips", "electrum", "10/13/15, 04:28:15 PM", "NaN", "NaN"], ["3698", "Update docs for release", "Nileema Shingte", "nileema", "10/01/15, 07:12:14 PM", "NaN", "NaN"], ["3702", "Add projection pushdown through exchanges", "Eric Hwang", "erichwang", "10/07/15, 04:57:29 AM", "Fixes issue #3600", "NaN"], ["3703", "Fix reversed condition in InCodeGenerator", "Haozhun Jin", "haozhun", "10/02/15, 04:53:28 PM", "NaN", "NaN"], ["3705", "Categorize Raptor metadata database errors", "David Phillips", "electrum", "10/05/15, 07:27:22 PM", "This also uses explicit connection management for JDBI SQL objects.", "NaN"], ["3706", "Update Checkstyle for imports", "David Phillips", "electrum", "10/05/15, 05:14:32 AM", "NaN", "NaN"], ["3707", "Fixes for Raptor metadata tables", "Nileema Shingte", "nileema", "10/05/15, 11:21:22 PM", "NaN", "NaN"], ["3709", "Fixed 'IN' functionality for large sets of overloaded raw types", "Raghav Sethi", "raghavsethi", "10/08/15, 12:24:36 AM", "NaN", "NaN"], ["3710", "Remove FunctionInfo", "Christopher Berner", "cberner", "11/05/15, 03:47:21 AM", "NaN", "NaN"], ["3711", "Fix TypeUtils.hashPosition for structural types", "Raghav Sethi", "raghavsethi", "10/07/15, 12:14:41 AM", "- Fixes hashcode generation for maps that contain complex types, and enables GROUP BY on them", "NaN"], ["3712", "TestingConnectorSession improvements", "David Phillips", "electrum", "10/07/15, 12:47:24 AM", "NaN", "NaN"], ["3717", "Minor fixes", "David Phillips", "electrum", "10/06/15, 10:55:51 PM", "NaN", "NaN"], ["3719", "Use shard UUID instead of ID", "David Phillips", "electrum", "10/07/15, 01:01:23 AM", "NaN", "NaN"], ["3720", "Increase default failure detector tolerance", "Christopher Berner", "cberner", "10/09/15, 01:24:41 AM", "Because failure-detector.threshold is used to compare the ratio of two\nexponentially decayed counters, the current default only tolerates\n~600ms of network outage. This changes the default to ~6s.", "NaN"], ["3724", "Fix projection pushdown", "Eric Hwang", "erichwang", "10/07/15, 11:26:29 PM", "NaN", "NaN"], ["3725", "Rename blackhole splits_count to split_count", "David Phillips", "electrum", "10/07/15, 11:41:49 PM", "NaN", "NaN"], ["3726", "Add release 0.122 notes", "Eric Hwang", "erichwang", "10/08/15, 12:10:56 AM", "NaN", "NaN"], ["3727", "Blackhole improvements", "David Phillips", "electrum", "10/08/15, 01:04:37 AM", "NaN", "NaN"], ["3728", "Updated release notes", "Raghav Sethi", "raghavsethi", "10/08/15, 12:59:34 AM", "NaN", "NaN"], ["3729", "Update docs to work with newer versions of Sphinx", "David Phillips", "electrum", "10/08/15, 01:21:17 AM", "NaN", "NaN"], ["3730", "Update to airlift 0.117", "Eric Hwang", "erichwang", "10/08/15, 01:36:33 AM", "NaN", "NaN"], ["3732", "Replace isNullLiteral with stream operators", "Raghav Sethi", "raghavsethi", "10/20/15, 11:42:18 PM", "As discussed [here](https://github.com/facebook/presto/pull/3709#discussion_r41449474)", "NaN"], ["3742", "Increase the timeout for a response in HttpRemoteTask", "Nileema Shingte", "nileema", "10/09/15, 11:13:21 PM", "NaN", "NaN"], ["3744", "Update 0.122 release notes with optimize hash generation bug", "Eric Hwang", "erichwang", "10/10/15, 12:13:25 AM", "NaN", "NaN"], ["3746", "Make projection push down properly translate exchange partition keys", "Eric Hwang", "erichwang", "10/13/15, 03:30:02 AM", "NaN", "NaN"], ["3747", "Add column support for Raptor", "Joy Yao", "joy-yao", "10/14/15, 03:39:22 AM", "NaN", "NaN"], ["3748", "Add slack integration for travis", "Martin Traverso", "martint", "10/11/15, 07:09:34 AM", "NaN", "NaN"], ["3750", "Assure sample weight column is last in HivePageSink", "Dain Sundstrom", "dain", "10/13/15, 06:01:30 PM", "NaN", "NaN"], ["3753", "Fix rehash bug in TypedHistogram and added test", "Raghav Sethi", "raghavsethi", "10/14/15, 09:37:00 PM", "NaN", "NaN"], ["3754", "Enable modernizer plugin (for everything except presto-main and presto-redis)", "Christopher Berner", "cberner", "10/13/15, 04:50:12 PM", "NaN", "NaN"], ["3756", "Add intermediate aggregations", "Nileema Shingte", "nileema", "10/23/15, 07:09:02 PM", "NaN", "NaN"], ["3759", "Make fields final", "Martin Traverso", "martint", "10/14/15, 05:33:20 PM", "They are effectively immutable, so there's no reason for them to be non-final", "NaN"], ["3760", "Fix DatabaseMetaData.getColumns method in JDBC driver", "David Phillips", "electrum", "10/15/15, 07:37:00 PM", "NaN", "NaN"], ["3761", "Use world writable permissions for Hive directories", "David Phillips", "electrum", "10/14/15, 09:23:28 PM", "The metastore needs to be able to remove files and directories when\ndropping partitions. This does not work if the metastore is running\nas a different user than the file owner unless the directories are\nworld writable. There is probably a better way to handle this.", "NaN"], ["3765", "Update 0.123 release notes", "Christopher Berner", "cberner", "10/16/15, 09:29:49 PM", "NaN", "NaN"], ["3768", "Add release notes for hash generation fix", "Eric Hwang", "erichwang", "10/15/15, 06:42:32 PM", "NaN", "NaN"], ["3769", "Add storage reclamation for Raptor", "David Phillips", "electrum", "11/09/15, 05:44:29 PM", "NaN", "NaN"], ["3772", "Fix incorrect handling of aggregation arguments", "Raghav Sethi", "raghavsethi", "10/20/15, 11:39:31 PM", "NaN", "NaN"], ["3773", "Fix scheduling when only coordinator is eligible", "Haozhun Jin", "haozhun", "10/16/15, 12:38:24 AM", "NaN", "NaN"], ["3775", "Remove duplicate DistinctLimitNode from query plan", "Raghav Sethi", "raghavsethi", "10/20/15, 11:22:25 PM", "Fixes #3646 ", "NaN"], ["3777", "Add release notes for Hive INSERT and DELETE", "Dain Sundstrom", "dain", "10/19/15, 07:08:00 PM", "NaN", "NaN"], ["3783", "Throw exception for delete from unpartitioned Hive table", "Haozhun Jin", "haozhun", "10/19/15, 05:43:24 PM", "Related to #3776 ", "NaN"], ["3784", "Improve Travis config", "Christopher Berner", "cberner", "10/20/15, 07:09:28 PM", "NaN", "NaN"], ["3788", "Update release notes for 0.123", "Haozhun Jin", "haozhun", "10/19/15, 10:35:17 PM", "NaN", "NaN"], ["3789", "Delete staging file when shard recovery fails", "David Phillips", "electrum", "10/19/15, 08:09:27 PM", "NaN", "NaN"], ["3795", "Add optimizer to remove projections with duplicate derivations", "Eric Hwang", "erichwang", "11/04/15, 08:33:14 PM", "NaN", "NaN"], ["3797", "Support Create Table If not exists As Select", "Zhenxiao Luo", "zhenxiao", "03/04/16, 04:06:18 AM", "Fix https://github.com/facebook/presto/issues/3796", "NaN"], ["3798", "Added 0.124 notes to toctree", "Raghav Sethi", "raghavsethi", "10/21/15, 01:56:04 AM", "NaN", "NaN"], ["3799", "Add configuration support for system access control plugin", "Dain Sundstrom", "dain", "10/22/15, 06:15:26 AM", "NaN", "NaN"], ["3804", "Fix Hive metastore client API stats", "David Phillips", "electrum", "10/23/15, 03:23:51 PM", "NaN", "NaN"], ["3806", "Avoid over-creation of initial splits", "Christopher Berner", "cberner", "11/05/15, 12:17:27 AM", "Fix a race in BackgroundHiveSplitLoader which caused it to create more\ninitial splits than specified in config.", "NaN"], ["3813", "Raptor changes in support of storage reclamation", "David Phillips", "electrum", "10/23/15, 08:15:16 PM", "These smaller changes are pulled out of #3769", "NaN"], ["3816", "Avoid duplicate computation when buildPartitions", "Joy Yao", "joy-yao", "10/26/15, 08:17:10 PM", "NaN", "NaN"], ["3817", "Improve session/table property validation", "Raghav Sethi", "raghavsethi", "11/04/15, 11:11:12 PM", "Fixes #3793 \n\nPresto now validates and produces (more) useful error messages for:\n- Invalid property names:\n\n```\npresto> CREATE TABLE test_raghavsethi_presto_5 (x varchar) WITH (foo = bar);\nQuery 20151023_211129_00000_kidpb failed: Table property 'foo' unsupported for catalog 'catalog'\n```\n- Incorrect types:\n\n```\npresto> CREATE TABLE test_raghavsethi_presto_5 (x varchar) WITH (format = true);\nQuery 20151023_223656_00001_nur8f failed: Unsupported value for table property 'format': Cannot convert 'true' to varchar\n```\n- Incorrect values:\n\n```\npresto> CREATE TABLE test_raghavsethi_presto_5 (x varchar) WITH (format = 'FOOBAR');\nQuery 20151023_223623_00000_nur8f failed: Unable to set table property 'format' to ''FOOBAR'': No enum constant com.facebook.presto.hive.HiveStorageFormat.FOOBAR\n```", "NaN"], ["3818", "Fix formatting for SchemaDao", "David Phillips", "electrum", "10/23/15, 11:13:46 PM", "NaN", "NaN"], ["3819", "Skip precomputed hash for bigint grouped aggregation", "Nileema Shingte", "nileema", "10/26/15, 05:03:49 PM", "Bigint grouped aggregation has special operators that do not use precomputed hash, so we can skip hash generation to save network and memory. ", "NaN"], ["3820", "Reduce number of threads used by testing server", "Christopher Berner", "cberner", "10/23/15, 11:52:53 PM", "NaN", "NaN"], ["3821", "Add verification support for byte array", null, "suyucs", "10/28/15, 05:11:10 PM", "If the verify query contains checksum, the object we get is actually a byte array, add support for checking results over byte array.", "NaN"], ["3822", "Improve error message for Hive tables where partition column is also in table", "Raghav Sethi", "raghavsethi", "11/04/15, 11:13:11 PM", "NaN", "NaN"], ["3830", "Fix issues for Dereference", "Joy Yao", "joy-yao", "10/27/15, 01:05:22 AM", "NaN", "NaN"], ["3831", "Fix IndexOutOfBounds when split contains no stripes", "Haozhun Jin", "haozhun", "10/26/15, 07:15:16 PM", "In normal circumstance, a stripe is ~200MB and split is ~60MB. As a result, around 2/3 of splits are empty. Processing them causes `IndexOutOfBoundsException` after merging #3692 because `mergeAdjacentDiskRanges` doesn't handle empty stripe list.\n\nThis PR also fixes a comment in #3692 that should have been addressed earlier.", "NaN"], ["3832", "Add query ID to Session and ConnectorSession", "Haozhun Jin", "haozhun", "10/27/15, 05:12:13 PM", "NaN", "NaN"], ["3833", "Fix race in hash build memory tracking", "Christopher Berner", "cberner", "10/27/15, 04:37:25 PM", "The transfer of memory reservation ownership from the operator context\nto the task context was not atomic. Therefore other tasks will observe a\nfree memory when there actually isn't any, while the hash build is\nfinishing.", "NaN"], ["3835", "Add session property to toggle distributed index joins", "Eric Hwang", "erichwang", "10/27/15, 01:25:43 AM", "NaN", "NaN"], ["3841", "Allow select * in aggregate if all columns are also in group by", "Joy Yao", "joy-yao", "10/30/15, 09:01:09 PM", "NaN", "NaN"], ["3843", "Update release notes", "Joy Yao", "joy-yao", "10/30/15, 07:37:31 PM", "NaN", "NaN"], ["3845", "Update release notes", "Raghav Sethi", "raghavsethi", "10/30/15, 11:11:16 PM", "NaN", "NaN"], ["3853", "Update 0.124 release notes", "Christopher Berner", "cberner", "10/28/15, 10:28:15 PM", "NaN", "NaN"], ["3857", "Fix correctness for certain types of IN lists", "Haozhun Jin", "haozhun", "10/29/15, 05:53:42 PM", "In a query of shape `x in (1, 3, 101)`. If all the constant values in the list\nis within the range of 32-bit signed integer, high-bits in `x` will be\ntruncated. As a result, `true` can potentially be returned when the correct\nresult is `false`.", "NaN"], ["3867", "Add 0.124 release notes", "Dain Sundstrom", "dain", "10/30/15, 09:26:52 PM", "NaN", "NaN"], ["3870", "Add release notes", "Joy Yao", "joy-yao", "10/30/15, 10:58:21 PM", "NaN", "NaN"], ["3871", "airlift", "Joy Yao", "joy-yao", "10/30/15, 11:32:05 PM", "NaN", "NaN"], ["3873", "Update release notes", "Raghav Sethi", "raghavsethi", "10/31/15, 12:04:55 AM", "NaN", "NaN"], ["3875", "Add flexibility to Hive connector related to storage decisions", "Haozhun Jin", "haozhun", "11/06/15, 02:36:58 AM", "NaN", "NaN"], ["3877", "Add Parquet to supported file formats", "Anton", "petroav", "11/03/15, 06:38:41 AM", "NaN", "NaN"], ["3879", "Fix ambiguity when parsing POSITION(x IN (y))", "Martin Traverso", "martint", "11/04/15, 06:28:48 PM", "The argument `x IN (y)` should be parsed as part of the special POSITION syntax but it's being\ninterpreted as an `IN` predicate.\n\nMove the rule up above the `functionCall` rule to give it higher precedence.", "NaN"], ["3882", "Fix property derivations for partitioned right or full joins", "Martin Traverso", "martint", "11/03/15, 04:42:48 AM", "The derived properties of a hash-partitioned right or full join are currently\nderived incorrectly from the properties of the probe side. Since this kind of join\ncan produce nulls from any of the partitions in case of a lack of matches,\nthe partitioning properties are violated for nulls.\n\nThis is problematic for queries that can benefit from the partitioning properties\nafter the join to avoid a reshuffle (e.g., an group by aggregation on the join key),\nand may result in wrong results if the join produces any nulls on the probe side.\n\nE.g.,\n\n``` sql\nSELECT a.orderkey, count(*)\nFROM (\n   SELECT CASE WHEN orderkey > 2 THEN orderkey END orderkey\n   FROM orders\n   WHERE orderkey < 20\n) a RIGHT OUTER JOIN (\n   SELECT orderkey\n   FROM orders\n   WHERE orderkey < 20) b\nON a.orderkey = b.orderkey\nGROUP BY 1;\n```\n\nProduces:\n\n```\n orderkey | _col1\n----------+-------\n NULL     |     1    <<<<<\n NULL     |     1    <<<<<\n        4 |     1\n        3 |     1\n        7 |     1\n        6 |     1\n        5 |     1\n(7 rows)\n```", "NaN"], ["3883", "Add function type", "Christopher Berner", "cberner", "11/06/15, 12:45:12 AM", "This is needed for implementing lambdas", "NaN"], ["3884", "Add parser for lambda", "Haozhun Jin", "haozhun", "11/05/15, 08:41:34 PM", "NaN", "NaN"], ["3886", "Retain local properties that guarantee the window pre-partitioned inputs", "Eric Hwang", "erichwang", "11/04/15, 08:31:26 PM", "NaN", "NaN"], ["3894", "Fix 0.126 release notes", "Eric Hwang", "erichwang", "11/04/15, 09:19:30 PM", "NaN", "NaN"], ["3895", "Relax access denied message test assertion in smoke tests", "Dain Sundstrom", "dain", "11/04/15, 11:14:30 PM", "NaN", "NaN"], ["3896", "Fix BlockUtils merge issue", "Raghav Sethi", "raghavsethi", "11/04/15, 11:29:23 PM", "NaN", "NaN"], ["3897", "Use assertContains instead of assertTrue", "David Phillips", "electrum", "11/06/15, 11:43:53 PM", "NaN", "NaN"], ["3899", "Minor cleanup", "Martin Traverso", "martint", "11/05/15, 05:24:35 PM", "NaN", "NaN"], ["3900", "Analyzer cleanups", "Martin Traverso", "martint", "11/05/15, 07:08:02 PM", "NaN", "NaN"], ["3904", "Add test for metadata delete with non-varchar partition column", "Haozhun Jin", "haozhun", "11/05/15, 08:37:26 PM", "And clean up code", "NaN"], ["3905", "Fix setup of hive client tests", "Martin Traverso", "martint", "11/05/15, 08:55:19 PM", "The latest version of TestNG does not call overriden methods in subclasses when they are\nannotated with @BeforeClass, etc.", "NaN"], ["3906", "Refactor PageProcessor to use ByteCodeExpressions", "Nileema Shingte", "nileema", "11/09/15, 06:40:28 PM", "Example of generate byte code tree\nhttps://gist.github.com/nileema/21678edd182daa30517b", "NaN"], ["3907", "Give better error message when a column is not found", "Joy Yao", "joy-yao", "11/09/15, 09:12:40 PM", "NaN", "NaN"], ["3910", "Fix 2 bugs related to unpartitioned table", "Haozhun Jin", "haozhun", "11/06/15, 10:47:08 PM", "NaN", "NaN"], ["3915", "Do not include any blocks for TableScanOperator output if types are empty", "Joy Yao", "joy-yao", "11/10/15, 04:24:04 AM", "Some ConnectorRecordSetProvider provides 1 block even when no block is expected.\nThis causes IndexOutOfBoundException during cross join.", "NaN"], ["3916", "Use an older testng version for hive tests", "Martin Traverso", "martint", "11/09/15, 05:57:46 PM", "Versions newer than 6.9 seem to be hitting a bug where the @BeforeClass\nin a subclass doesn't always get invoked", "NaN"], ["3924", "Categorize errors in JDBC connector", "Christopher Berner", "cberner", "11/09/15, 09:29:50 PM", "NaN", "NaN"], ["3926", "Improved framework for writing parametric functions", "Christopher Berner", "cberner", "02/23/16, 08:45:06 PM", "NaN", "NaN"], ["3927", "Don't derive global constants from local constants", "Eric Hwang", "erichwang", "11/09/15, 09:30:03 PM", "NaN", "NaN"], ["3929", "Reduce data size for test query", "Martin Traverso", "martint", "11/09/15, 11:49:35 PM", "H2 doesn't seem to be able to handle OUTER JOINs efficiently,\nwhich was causing these tests to take 1-2 minutes each.", "NaN"], ["3930", "Fix race in AbstractTestHiveClient", "Haozhun Jin", "haozhun", "11/10/15, 12:44:26 AM", "NaN", "NaN"], ["3931", "Use row count for filtering shards to compact", "David Phillips", "electrum", "11/10/15, 01:03:02 AM", "NaN", "NaN"], ["3932", "Refactor getCommonSuperType and canCoerce", "Haozhun Jin", "haozhun", "12/02/15, 07:56:07 PM", "Prerequisite for type unification code for lambda\n- Now that they take both Type and TypeSignature\n- Coercion rules were duplicated in the original code (and diverged). The rules appear only once in the code after this commit.\n- Coercion between types with type parameters (e.g. array<T> and array<U>)  could be arbitrary and potentially complex. After this commit,  coercibility between types is decoupled from compatibility between  type parameters.\n- Add tests", "NaN"], ["3933", "Cleanup staging files in Raptor", "David Phillips", "electrum", "11/12/15, 05:07:32 PM", "NaN", "NaN"], ["3934", "Update tempto from 1.0.38 to 1.0.39.", "Grzegorz Kokosi\u0144ski", "kokosing", "11/10/15, 06:55:28 AM", "Update tempto from 1.0.38 to 1.0.39.", "NaN"], ["3941", "Migrate SPI Constraint predicate to use NullableValue bindings", "Eric Hwang", "erichwang", "11/11/15, 09:13:24 PM", "NaN", "NaN"], ["3943", "Improve Tableau web connector documentation", "David Phillips", "electrum", "11/12/15, 04:21:23 PM", "NaN", "NaN"], ["3944", "Ensure matching inputs and outputs for Exchanges in ProjectionPushDown optimizer", "Eric Hwang", "erichwang", "11/11/15, 09:18:15 PM", "Fixes issue #3937 ", "NaN"], ["3948", "Improve pending splits accounting", "Dain Sundstrom", "dain", "11/20/15, 08:09:31 PM", "Insead of updating by deltas, which is error prone, set the full current\nvalue.  Also, add a finalizer to detect leaked slots, and remove the leaked\nslots (and log an error).  Finally, change HttpRemoteTask get count methods\nto return zero when the task is in a done state.", "NaN"], ["3949", "Fixed a typo", "Joy Yao", "joy-yao", "11/11/15, 10:44:19 PM", "NaN", "NaN"], ["3950", "Move RLE block to spi", "Nileema Shingte", "nileema", "11/16/15, 08:01:24 PM", "NaN", "NaN"], ["3952", "Add capability to share or separate index join lookups and caching", "Eric Hwang", "erichwang", "11/12/15, 07:40:47 PM", "NaN", "NaN"], ["3956", "Add 0.126 release notes", "Dain Sundstrom", "dain", "11/12/15, 11:28:00 PM", "NaN", "NaN"], ["3957", "Fixes for compaction to use correct types", "Nileema Shingte", "nileema", "11/13/15, 12:04:13 AM", "NaN", "NaN"], ["3962", "Add local parallel to build side outer join", "Dain Sundstrom", "dain", "12/01/15, 06:45:49 PM", "Change join operator to support to support local parallel for right and\nfull outer joins.", "NaN"], ["3963", "Add execution support for CUBE, ROLLUP and GROUPING SETS", "Raghav Sethi", "raghavsethi", "03/04/16, 10:44:36 PM", "closes #1851 ", "NaN"], ["3966", "Update 0.126 release notes", "Martin Traverso", "martint", "11/13/15, 10:49:46 PM", "- Add a few missing entries\n- Organize main section, roughly, in order of: bug fixes, features and perf improvements.", "NaN"], ["3968", "Enable DomainTranslator to fold constants when extracting TupleDomains", "Eric Hwang", "erichwang", "11/14/15, 01:19:47 AM", "NaN", "NaN"], ["3970", "Add release notes for new TupleDomain modifications", "Eric Hwang", "erichwang", "11/17/15, 01:22:18 AM", "NaN", "NaN"], ["3972", "Parser module code cleanup", "David Phillips", "electrum", "12/22/15, 09:52:28 PM", "NaN", "NaN"], ["3978", "Set the max response size for exchange client", "Nileema Shingte", "nileema", "11/18/15, 12:15:48 AM", "Currently the exchange max response size is set to the max content\nlength of the http client. When we extract pages from the SharedBuffer\nwe do not account for the additional bytes that are included in the\nblock encodings. In some cases this can cause the response size to go\nover the max allowed response size. Mitigate this by asking for only\nhalf response size. This leaves enough room for additional bytes added\nby the block encodings.", "NaN"], ["3980", "Improve error message for unresolved operators", "Raghav Sethi", "raghavsethi", "12/29/15, 09:29:28 PM", "Fixes #3203", "NaN"], ["3981", "Remove trailing whitepace in .gitignore", "Raghav Sethi", "raghavsethi", "11/17/15, 08:18:28 PM", "This is required because older versions of git fail to match lines with trailing whitespace in .gitignore", "NaN"], ["3982", "Update release notes for 0.126", "Haozhun Jin", "haozhun", "11/18/15, 07:39:47 PM", "NaN", "NaN"], ["3983", "Update 0.126 release notes", "Raghav Sethi", "raghavsethi", "11/18/15, 01:57:25 AM", "NaN", "NaN"], ["3984", "Fix property preference push down for CROSS JOIN UNNEST", "Eric Hwang", "erichwang", "11/17/15, 10:57:03 PM", "NaN", "NaN"], ["3985", "Fix memory leak in coordinator (and another minor change)", "Christopher Berner", "cberner", "11/18/15, 12:40:03 AM", "NaN", "NaN"], ["3986", "Add limited support for cube, rollup and grouping set to grammar", "Raghav Sethi", "raghavsethi", "11/19/15, 09:40:54 PM", "- Add CUBE, ROLLUP and simple GROUPING SETS (i.e. one-level of nesting)\n  to grammar.\n- Add analyzer support for simple grouping sets.\n- Add planning and execution support for queries with a single\n  grouping set (this is essentially a simple GROUP BY).", "NaN"], ["3987", "Update logic to compute request timeout in HttpRemoteTask", "Nileema Shingte", "nileema", "11/18/15, 10:34:15 PM", "NaN", "NaN"], ["3995", "Clean up Block.read and BlockBuilder.write functions", "Haozhun Jin", "haozhun", "11/20/15, 11:04:33 PM", "This pull request is short and straightforward. Discussed with @martint about the change.", "NaN"], ["3998", "Use injected ExecutorService instead of creating a new one", "Christopher Berner", "cberner", "11/20/15, 05:48:24 PM", "NaN", "NaN"], ["3999", "Fix bug in CountConstantOptimizer and add test", "Raghav Sethi", "raghavsethi", "12/11/15, 03:14:24 AM", "Fixes #3839", "NaN"], ["4001", "Disable index join repartitioning when it would disrupt streaming execution", "Eric Hwang", "erichwang", "11/20/15, 08:48:19 PM", "NaN", "NaN"], ["4009", "Add getHiveBucket that takes Block and position", "Haozhun Jin", "haozhun", "11/23/15, 05:20:40 PM", "NaN", "NaN"], ["4011", "Simplify output buffer coordination during execution", "Dain Sundstrom", "dain", "12/02/15, 02:33:07 AM", "Move partition function configuration to plan fragment, so task always knows\nhow to partition output.", "NaN"], ["4012", "Add support for constructing large constant arrays", "Raghav Sethi", "raghavsethi", "12/29/15, 08:56:33 PM", "Generating bytecode to constant fold array constructors fails when the number of elements in the array exceeds 255 (i.e. the maximum number of arguments to a Java function). We now simply generate an array block instead of generating bytecode.\n\nFixes #4008.", "NaN"], ["4020", "Update 0.127 release notes", "Christopher Berner", "cberner", "11/23/15, 06:36:44 PM", "NaN", "NaN"], ["4023", "Some fixes related to Block", "Haozhun Jin", "haozhun", "11/24/15, 09:19:59 PM", "3 commits logically separate but all related to Block. They are all very simple.\n- Fix cast from json to nested row or map\n- Remove duplicate code from RowPageBuilder test class\n- Avoid allocating BlockBuilder memory indiscriminately", "NaN"], ["4025", "Disable parallel with Smart Builder by default", "Jason van Zyl", "jvanzyl", "11/24/15, 08:37:37 PM", "There are issues with being aggressively parallel running tests for Presto. There are\ngenerally not enough resources on a typical laptop to use the Smart Builder with 8 threads.\nWe are also seeing issues with the maven-release-plugin.", "NaN"], ["4037", "Make table properties case insensitive", "Nileema Shingte", "nileema", "12/02/15, 12:52:59 AM", "Fixes #4024 ", "NaN"], ["4038", "Force combine_hash to produce int values", "Eric Hwang", "erichwang", "11/26/15, 06:54:57 AM", "NaN", "NaN"], ["4039", "Hide release notes from main table of contents", "David Phillips", "electrum", "12/02/15, 07:30:13 PM", "The ever-growing list of release notes makes the table of contents harder\nread, such as when scrolling to the end to see the SQL statement syntax.", "NaN"], ["4045", "Make node_id in deleted_shard_nodes not nullable", "David Phillips", "electrum", "12/01/15, 02:11:24 AM", "Fixes #4043", "NaN"], ["4046", "Fix local shard cleanup for shards on multiple nodes", "David Phillips", "electrum", "12/01/15, 04:00:51 AM", "The shard cleaner was marking the deleted shard as cleaned on all\nnodes it was deleted from, not just the current node being cleaned.\n\nFixes #4042", "NaN"], ["4049", "Fix contention on Raptor created shard tables", "David Phillips", "electrum", "12/01/15, 02:13:08 AM", "Deleting old created shards using a transaction effectively locks\nthe table and either blocks or is blocked by concurrent inserts.\nUsing a temporary table with autocommit does not need locking.", "NaN"], ["4050", "Add checkState to HttpRemoteTask.cancel()", "Christopher Berner", "cberner", "12/01/15, 07:52:12 PM", "NaN", "NaN"], ["4052", "Fix blank line before closing brace", "David Phillips", "electrum", "12/01/15, 02:09:51 AM", "NaN", "NaN"], ["4053", "Minor code cleanup for Redis connector", "David Phillips", "electrum", "12/05/15, 12:04:10 AM", "NaN", "NaN"], ["4056", "Cleanup handling of tests for update queries", "David Phillips", "electrum", "12/04/15, 02:31:26 AM", "NaN", "NaN"], ["4059", "Update 0.128 release notes", "Christopher Berner", "cberner", "12/01/15, 01:23:18 AM", "NaN", "NaN"], ["4060", "Add support for network topology aware scheduling", "Christopher Berner", "cberner", "12/03/15, 08:27:10 PM", "NaN", "NaN"], ["4065", "Disable column addition test in TestRedisDistributed", "Haozhun Jin", "haozhun", "12/01/15, 09:19:02 PM", "NaN", "NaN"], ["4070", "Rename QualifiedTableName to QualifiedObjectName", "David Phillips", "electrum", "12/02/15, 06:54:49 PM", "NaN", "NaN"], ["4077", "Remove invalid check for broadcast partition id", "Dain Sundstrom", "dain", "12/03/15, 01:02:43 AM", "Do not use the partition id assigned to the task for broadcast distributions.\nInstead all tasks read from the same partition.", "NaN"], ["4083", "Some changes to query tests", "Haozhun Jin", "haozhun", "12/04/15, 02:53:00 AM", "NaN", "NaN"], ["4086", "Add documentation for VALUES", "Dain Sundstrom", "dain", "12/04/15, 01:37:19 AM", "Fixes #2122", "NaN"], ["4087", "Fix duplicate position counts in paritioned PagesIndexes", "Dain Sundstrom", "dain", "12/04/15, 01:38:15 AM", "Fixes #3265", "NaN"], ["4089", "Refactor Validator constructor to make it reusable", null, "suyucs", "12/05/15, 01:04:59 AM", "Made several minor changes to presto-verifier so that the hql validation service can directly invoke several key functions here. The changes include:\n(1) Make QueryPair json serializable. \n(2) Add a new constructor\n(3) Record query results for both preQueries and postQueries", "NaN"], ["4092", "Some fixes related to unknown type", "Haozhun Jin", "haozhun", "12/07/15, 08:09:18 PM", "1. Add debug flag for fake source file and line number in byte code\n   - This is extremely useful in helping me debug issues in 2\n   - When ADD_FAKE_LINE_NUMBER flag is turned on, every instruction in generated byte code will have a fake line number associated with them. This way, stack trace will have line numbers for generated byte code. When combined with DUMP_BYTE_CODE_RAW, they become a powerful tool to debug byte code.\n2. Reimplement cast from unknown to any type\n   - Cast from unknown used to be implemented as a set of special handlings here and there. This pull request reimplements them as an ordinary parametric scalar function (operator).\n   - Also fix a Void handling bug in ByteCodeUtils.unboxPrimitiveIfNecessary.\n   - Prerequisite for 3 and 4\n3. Fix cast from array<unknown> to array<T>\n   - Fixes #2852 \n4. Revert \"Do not constant fold failures in conditional expressions\"\n   - That original commit removes fail function because it was believed at the time that fixing a correctness bug in fail function was not possible without major changes. It turns out that the belief was wrong.\n5. Add current stacktrace to stored failure in fail function\n   - Also add test that verifies the code works during query execution time.   The old test only verifies constant folding works for this function.", "NaN"], ["4093", "Add columnar processing for dictionary encoded blocks", "Nileema Shingte", "nileema", "12/11/15, 09:23:52 PM", "Microbenchmark results: https://gist.github.com/nileema/070bcd81e3f107a41734\n\nhttps://cloud.githubusercontent.com/assets/1034143/11601338/4ca8bae2-9a87-11e5-9098-17308791a599.png", "NaN"], ["4095", "Replace DynamicSliceOutput with Slices.allocate in copyPositions", "Nileema Shingte", "nileema", "12/08/15, 08:00:05 PM", "Several implementations of copy copyPositions use DynamicSliceOutput\neven though we know the exact size of the slice that we need. Replace\nthese usages with Slices.allocate.", "NaN"], ["4097", "Cleanup work for Raptor bucketing", "David Phillips", "electrum", "12/05/15, 12:01:50 AM", "NaN", "NaN"], ["4098", "Make QueryPair and Query class json serializable", null, "suyucs", "12/05/15, 12:31:15 AM", "Make QueryPair and Query class json serializable and used them as input of validation executor service. ", "NaN"], ["4100", "Fix table layout for Blackhole connector", "David Phillips", "electrum", "12/05/15, 12:17:38 AM", "It uses TupleDomain.none(), not all()", "NaN"], ["4101", "Revert table layout change to Hive tests", "David Phillips", "electrum", "12/05/15, 02:07:41 PM", "NaN", "NaN"], ["4103", "Record Pre and Post Query Results", null, "suyucs", "12/05/15, 03:03:36 AM", "Record Pre and Post Query Results", "NaN"], ["4105", "Use UTF-8 helper methods for Slice", "David Phillips", "electrum", "12/07/15, 04:03:38 PM", "NaN", "NaN"], ["4106", "Optimize query plan using when table is physically distributed", "Dain Sundstrom", "dain", "01/30/16, 10:37:57 PM", "Add support for connectors to declare distribution of table across nodes\nOnly redistribute tables across nodes if necessary", "NaN"], ["4110", "Add one constructor to AllColumns for hqltranslator usage", null, "suyucs", "12/07/15, 06:48:20 PM", "Add a constructor to AllColumns which only takes prefix as input. This was original supported in previous presto version. ", "NaN"], ["4115", "Move kill button to bottom in UI", "David Phillips", "electrum", "12/07/15, 08:42:12 PM", "This makes it harder to click by accident.", "NaN"], ["4117", "Improve accuracy of getRetainedSizeInBytes()", "Christopher Berner", "cberner", "12/08/15, 08:04:53 PM", "NaN", "NaN"], ["4118", "Add bucket support for Raptor", "David Phillips", "electrum", "02/03/16, 05:45:47 AM", "NaN", "NaN"], ["4121", "Fix flaky ORC test by using NullMemoryManager", "Haozhun Jin", "haozhun", "12/11/15, 07:50:24 PM", "NaN", "NaN"], ["4124", "Add support for calling stored procedures", "David Phillips", "electrum", "01/08/16, 01:31:01 AM", "NaN", "NaN"], ["4128", "Use task instance ID to identify remote task uniquely", "Nileema Shingte", "nileema", "12/12/15, 06:01:57 PM", "NaN", "NaN"], ["4134", "Remove PruneRedundantProjections optimization", "Martin Traverso", "martint", "12/09/15, 08:59:34 PM", "The code makes incorrect assumptions about the scope of \"symbols\" and\nproduces plans that are not equivalent. We'll revisit later.", "NaN"], ["4135", "Make GroupByHash faster", "Raghav Sethi", "raghavsethi", "12/11/15, 11:25:29 PM", "Partially addresses #4019.", "NaN"], ["4137", "Clean up Presto ConnectorManager", "Eric Hwang", "erichwang", "12/11/15, 12:48:28 AM", "NaN", "NaN"], ["4138", "Fix running non-SELECT queries with verifier", null, "suyucs", "12/11/15, 12:12:47 AM", "PR/3097 fixed executeQuery to only accept SELECT queries, so this restores the previous functionality of being able to run non-SELECT queries.The presto-verifier is using executeQuery to run all query, pre-queries and post-queries.  This will fail those DDL or update queries within pre-queries and post-queries. In this commit, I use execute(sql) to replace executeQuery(sql). ", "NaN"], ["4143", "Add release notes for TopN hang fix", "Dain Sundstrom", "dain", "12/10/15, 06:36:30 PM", "NaN", "NaN"], ["4144", "Update 0.129 release notes", "Martin Traverso", "martint", "12/10/15, 08:31:51 PM", "NaN", "NaN"], ["4146", "Update to Hadoop 2.7.1", "David Phillips", "electrum", "12/10/15, 11:57:39 PM", "NaN", "NaN"], ["4149", "Fix distributed tests for table creation", "David Phillips", "electrum", "12/10/15, 11:45:59 PM", "Combine test methods to make it easier for connectors to override.", "NaN"], ["4152", "Add GROUPING SETS, CUBE and ROLLUP documentation", "Raghav Sethi", "raghavsethi", "03/05/16, 01:34:55 AM", "NaN", "NaN"], ["4155", "Add positionEqualsRow method that works with a page", "Nileema Shingte", "nileema", "12/12/15, 06:00:07 PM", "NaN", "NaN"], ["4157", "Upgrade to Airlift 0.120", "Dain Sundstrom", "dain", "12/14/15, 08:51:31 PM", "NaN", "NaN"], ["4158", "Update release notes for 0.130", "Raghav Sethi", "raghavsethi", "12/13/15, 07:25:08 PM", "NaN", "NaN"], ["4162", "Various performance optimizations", "Christopher Berner", "cberner", "12/14/15, 11:54:51 PM", "NaN", "NaN"], ["4164", "Various cleanup", "David Phillips", "electrum", "12/14/15, 07:20:13 PM", "NaN", "NaN"], ["4166", "Use correct size in PageBuilder.reset()", "Christopher Berner", "cberner", "12/14/15, 10:40:35 PM", "Currently we allocate one block worth of memory per row", "NaN"], ["4168", "Fix premature close in DistributedQueryRunner", "David Phillips", "electrum", "12/14/15, 09:56:03 PM", "NaN", "NaN"], ["4169", "Refactor the generated methods in PageProcessorCompiler", "Nileema Shingte", "nileema", "12/15/15, 08:15:01 PM", "Some of the methods become too large when there are a lot of\nprojections.\n\nExample of generated code: https://gist.github.com/nileema/8c7d72240364a5532145", "NaN"], ["4175", "Fix performance regression when building hash tables", "Martin Traverso", "martint", "12/15/15, 07:52:16 PM", "This is due to a bug in the latest version of XXHash64\nthat causes strings of the same length to hash to the same value\nfor lengths between 16 and 31.\n\nSee https://github.com/airlift/slice/pull/48", "NaN"], ["4177", "Turn off optimizer for pushing table write through UNION ALL", "Haozhun Jin", "haozhun", "12/15/15, 08:38:32 PM", "Also\n- Add configuration and session property for turning it on\n- Add a test that will fail when the optimizer is turned on\n\ncc @raghavsethi ", "NaN"], ["4180", "Various performance optimizations and improvements to memory allocation and tracking", "Christopher Berner", "cberner", "12/22/15, 11:18:32 PM", "NaN", "NaN"], ["4181", "Create dictionary block once and reuse", "Nileema Shingte", "nileema", "12/16/15, 11:17:33 PM", "In the SliceDictionaryStreamReader, the same dictionary is shared for\nall data in a stripe. All blocks created from the same stripe should\nreference the same dictionary block.", "NaN"], ["4183", "Optimize aggregations for dictionary encoded data", "Nileema Shingte", "nileema", "01/08/16, 12:16:34 AM", "```\nOptimize aggregations for dictionary encoded data\n\nThis optimization is limited to aggregations on a single column that is\ndictionary encoded. The optimization does the following:\n\n1. If aggregation is on a dictionary encoded column, extract the\n   dictionary\n2. Every position of the dictionary that is referenced is added to the\n   GroupByHash\n3. Hold on to the last seen dictionary to keep track of positions that\n   we have been processed.\n```", "NaN"], ["4187", "Add initial transaction support", "Eric Hwang", "erichwang", "12/28/15, 08:19:39 PM", "NaN", "NaN"], ["4189", "Add main method to test query runners", "Dain Sundstrom", "dain", "12/17/15, 07:02:23 AM", "The *QueryRunner classes know how to create a small Presto cluster for unit\ntests, and since they are used for testing, they always work.  Thes mini\nclusters are useful for debugging issues with connector since they start\nquickly and don't suffer from class loader and build issuses when starting a\nfull Presto server.", "NaN"], ["4191", "Fix getSizeInBytes call for DictionaryBlocks", "Nileema Shingte", "nileema", "12/17/15, 07:58:02 PM", "NaN", "NaN"], ["4193", "Fix test failure in TestFeaturesConfig", "Haozhun Jin", "haozhun", "12/16/15, 11:32:21 PM", "NaN", "NaN"], ["4195", "Return compact block in copyRegion for VariableWidthBlock", "Nileema Shingte", "nileema", "12/17/15, 10:27:18 PM", "NaN", "NaN"], ["4196", "Refactor the generated code for PageProcessorCompiler", "Nileema Shingte", "nileema", "12/17/15, 09:01:41 PM", "We saw method too large exception for some huge queries. Refactor the\ncode so that we generate smallar methods.", "NaN"], ["4197", "Fix invalid bucket calculation HashPartitionMaskOperator", "Dain Sundstrom", "dain", "12/17/15, 08:25:22 PM", "Math.abs returns a negative value for MIN_INT, so when this hash value\nis later modded to choose a bucket, the bucket is also negative.  In the\nHashPartitionMaskOperator this means the value is a member of no buckets\nand the value is skipped.\n\nThe HashPartitionMaskOperator is only used for the experimental local\nparallel aggregations optimization so this not a high impact bug.", "NaN"], ["4202", "Update airlift to 0.121", "Haozhun Jin", "haozhun", "12/17/15, 08:08:11 PM", "NaN", "NaN"], ["4204", "Add 0.131 release notes", "Dain Sundstrom", "dain", "12/18/15, 03:39:11 AM", "NaN", "NaN"], ["4205", "Change default builder to multithreaded", "Christopher Berner", "cberner", "12/18/15, 04:47:51 PM", "This is the default in Maven, and reduces build time for \"mvn clean\ninstall -DskipTests -T4\" from 6min 31sec to 4min 15secs on my laptop", "NaN"], ["4210", "Update native libraries for hive-hadoop2", "David Phillips", "electrum", "12/18/15, 05:59:10 PM", "This allows running on older Linux distributions.", "NaN"], ["4212", "Add negative cache to topology aware scheduler", "Christopher Berner", "cberner", "12/21/15, 05:00:39 PM", "NaN", "NaN"], ["4220", "Change unit of namenode latency stats to millisecond", "Haozhun Jin", "haozhun", "12/22/15, 01:08:06 AM", "NaN", "NaN"], ["4223", "Increase default http timeout threads", "Christopher Berner", "cberner", "12/23/15, 05:13:18 AM", "With a default of one thread, it's impossible to tell if there's a\nbacklog of http requests that need to be timed out.", "NaN"], ["4225", "Simplify connector handle serialization", "David Phillips", "electrum", "01/07/16, 05:57:33 AM", "NaN", "NaN"], ["4237", "Make equality checks for standard type classes faster", "Martin Traverso", "martint", "12/30/15, 11:23:19 PM", "Treat the standard types as singletons, which allows usto implement equals() with reference equality.\n\nThis is especially important for HivePageSink, which performs this operations multiple times per row and per column.\n\nA test writing a table containing 16 columns showed a reduction in CPU utilization of ~5%.", "NaN"], ["4238", "Fix incorrect hashcode implementation for AbstractType", "Martin Traverso", "martint", "12/28/15, 06:33:09 PM", "It's inconsistent with the implementation of equals(), which is based on comparing the type signature.", "NaN"], ["4239", "Fix analysis of queries with GROUP BY ()", "Raghav Sethi", "raghavsethi", "12/28/15, 07:54:44 PM", "NaN", "NaN"], ["4243", "Remove redundant check for aggregation functions", "Raghav Sethi", "raghavsethi", "12/30/15, 11:46:09 PM", "- We already create a grand total grouping set when implicit\n  aggregations are present, and therefore can simply check if\n  groupingSets is non-empty\n- Add test to verify error for select items not in GROUP BY ()", "NaN"], ["4245", "Optimize writing Hive tables", "Martin Traverso", "martint", "12/29/15, 04:48:17 AM", "- Avoid unnessary encoding/decoding when writing VARCHAR columns\n\nCurrently, the flow in HivePageSink converts VARCHAR columns from\nSlice to String to Text before they get written. We now bypass the\nconversion to String by dealing directly with Text objects.\n\nAdditionally, we include an optimized version of Hive's columnar SerDe\nthat avoids a call to get the String value in serialize(), which is only\nused to check whether the string is empty.\n- Avoid unnecessary type equality checks to decide how values should be\n  extracted from column Blocks.\n\nInstead, build field \"setters\" and rely on the JVM to do the dispatch.\nAlso, avoid creating boxed objects for each value that's written.\n\nA test with a table containing 13 varchar and 3 bigint columns showed\na reduction in CPU utilization of ~34%.", "NaN"], ["4248", "Add resource_over_commit session property", "Christopher Berner", "cberner", "01/04/16, 07:17:56 PM", "This session property indicates that the query would like to opt-out of\nthe normal memory management system. This disables all memory limits for\nthe query, but means that it may be killed at any time, even if its\nmemory usage is zero, if the coordinator needs to reclaim memory.", "NaN"], ["4249", "Use correct context class loader for loading connectors", "David Phillips", "electrum", "12/29/15, 09:13:24 PM", "NaN", "NaN"], ["4251", "Update release notes for 0.132", "Raghav Sethi", "raghavsethi", "12/31/15, 12:02:46 AM", "NaN", "NaN"], ["4252", "Fix effective predicate extraction for outer joins", "Eric Hwang", "erichwang", "12/30/15, 02:34:23 AM", "This fixes a bug where outer joins could report an effective predicate of\nFALSE when the inner side of the outer join had a FALSE predicate.", "NaN"], ["4254", "Move bytecode library to separate module", "David Phillips", "electrum", "12/30/15, 07:44:32 PM", "NaN", "NaN"], ["4255", "Rename remaining ByteCode and byteCode references", "David Phillips", "electrum", "12/30/15, 08:15:58 PM", "NaN", "NaN"], ["4257", "Add transaction docs and more unit tests", "Eric Hwang", "erichwang", "01/04/16, 10:47:35 PM", "NaN", "NaN"], ["4261", "Categorize error when creating Hive record writer", "David Phillips", "electrum", "01/02/16, 11:19:29 PM", "Fixes #4260 ", "NaN"], ["4263", "Allow mapping all Hive metastore exceptions", "David Phillips", "electrum", "01/04/16, 05:47:53 PM", "NaN", "NaN"], ["4265", "Add newline between error code groups", "David Phillips", "electrum", "01/04/16, 05:46:44 PM", "NaN", "NaN"], ["4268", "Categorize class compilation errors for PageProcessor", "Nileema Shingte", "nileema", "01/05/16, 09:38:35 PM", "NaN", "NaN"], ["4270", "Update 0.132 release notes", "Martin Traverso", "martint", "01/05/16, 04:37:52 AM", "NaN", "NaN"], ["4272", "Improve retained size accounting of BlockBuilders", "Christopher Berner", "cberner", "01/05/16, 01:57:22 AM", "NaN", "NaN"], ["4273", "Make new transaction-specific tokens non-reserved", "Eric Hwang", "erichwang", "01/04/16, 11:12:38 PM", "Fixes #4262", "NaN"], ["4276", "Reduce the size of generated code for PageProcessor", "Nileema Shingte", "nileema", "01/05/16, 10:11:13 PM", "We saw Method too large for some really large queries. Refactor the code\nso that we generate smallar methods.", "NaN"], ["4277", "Remove null and partition key from SHOW COLUMNS", "David Phillips", "electrum", "01/21/16, 02:08:26 AM", "Knowledge of nullability is not implemented at all.\n\nPartition keys are only used by the Hive connector, so we can handle that\nby adding the information to the comment column.", "NaN"], ["4278", "IsolationLevel improvements", "David Phillips", "electrum", "01/06/16, 12:55:51 AM", "NaN", "NaN"], ["4280", "Rename resource_over_commit to resource_overcommit", "David Phillips", "electrum", "01/05/16, 08:26:05 PM", "NaN", "NaN"], ["4281", "Rename abort to rollback in Connector", "David Phillips", "electrum", "01/05/16, 08:19:14 PM", "NaN", "NaN"], ["4282", "Update 0.132 release notes", "Christopher Berner", "cberner", "01/05/16, 11:36:50 PM", "NaN", "NaN"], ["4284", "Fix typo in READ UNCOMMITED isolation level", "Eric Hwang", "erichwang", "01/06/16, 01:04:55 AM", "NaN", "NaN"], ["4285", "Separate multiple-tasks-per-node tests", "Christopher Berner", "cberner", "01/05/16, 11:19:02 PM", "This reduces run time of tests in presto-tests by ~5%, and should reduce\nhttp stress, which will hopefully help Travis", "NaN"], ["4286", "Lookup network locations in background", "David Phillips", "electrum", "01/11/16, 08:50:59 PM", "NaN", "NaN"], ["4288", "Hide at_timezone function", "Martin Traverso", "martint", "01/06/16, 08:22:24 PM", "It's an internal function that's used for mapping the AT TIME ZONE language\nconstruct, and it's not meant to be used directly.", "NaN"], ["4290", "Update to Airlift 0.122", "David Phillips", "electrum", "01/06/16, 11:01:56 PM", "NaN", "NaN"], ["4291", "Optimize page compaction for dictionary blocks", "Nileema Shingte", "nileema", "01/08/16, 08:05:44 PM", "```\nWhen a page is compacted, if there are dictionary blocks with the same\nsource id, this property should be retained even after the page is\ncompacted. Group all the dictionary blocks in a page with the same\nsource id and compact them together. This also allows the result blocks\nto use the same ids block.\n```", "NaN"], ["4294", "Remove title from detected Presto server version", "David Phillips", "electrum", "01/07/16, 04:58:44 AM", "NaN", "NaN"], ["4295", "Simplify server version detection", "David Phillips", "electrum", "01/07/16, 07:44:14 PM", "NaN", "NaN"], ["4298", "Fix connector creation", "David Phillips", "electrum", "01/07/16, 09:19:07 PM", "NaN", "NaN"], ["4300", "Add PlanNodeId to OperatorStats", "Christopher Berner", "cberner", "01/09/16, 01:44:13 AM", "NaN", "NaN"], ["4301", "Add query CPU time in verifier result", null, "suyucs", "01/08/16, 07:50:13 PM", "Fetch the job execution CPU time from Stats and record it in verifier result", "NaN"], ["4302", "Add optimization for RunLengthEncoded blocks in PageProcessor", "Nileema Shingte", "nileema", "01/26/16, 12:07:57 AM", "For columnar processing RunLengthEncoded blocks can be highly optimized\nby only processing the value block and outputting an RLE block.\n\nFixes #4211 ", "NaN"], ["4304", "Categorize exceptions in from_iso_8601 functions", "Christopher Berner", "cberner", "01/09/16, 12:24:45 AM", "NaN", "NaN"], ["4306", "Update to Jetty 9.3.7.RC0", "David Phillips", "electrum", "01/08/16, 11:23:12 PM", "NaN", "NaN"], ["4307", "Add dictionary optimizations to FilterPage in Page Processor", "Nileema Shingte", "nileema", "02/01/16, 04:57:45 PM", "Example generated code: \n\nhttps://gist.github.com/nileema/b2fcb7e194e84f450bf1", "NaN"], ["4308", "Properly expire started transactions that go idle", "Eric Hwang", "erichwang", "01/09/16, 07:17:00 AM", "This fixes a bug whereby transactions that consist of just the START TRANSACTION\nstatement will not be expired.", "NaN"], ["4309", "Update Raptor to use transactional API", "David Phillips", "electrum", "01/20/16, 07:11:53 AM", "NaN", "NaN"], ["4313", "Transactional API fixes", "David Phillips", "electrum", "01/11/16, 08:48:16 PM", "NaN", "NaN"], ["4315", "Update BlackHole connector to transactional API", "David Phillips", "electrum", "01/20/16, 05:41:56 AM", "NaN", "NaN"], ["4318", "Add release notes for dictionary aggregations", "Nileema Shingte", "nileema", "01/12/16, 11:03:00 PM", "NaN", "NaN"], ["4319", "Fix theoretical memory tracking leak", "Christopher Berner", "cberner", "01/16/16, 12:38:48 AM", "DriverFactory is not guaranteed to be closed. It's only closed when all\nsplits have been received and drivers created for them. However,\nseveral OperatorFactories rely on close() being called.\nFor example, LookupJoinOperatorFactory will \"leak\" memory in the memory\ntracking system, if close() is not called.", "NaN"], ["4320", "Improve error message when trying to use a \"raw\" type", "Christopher Berner", "cberner", "05/04/16, 10:38:44 PM", "Improve error message when a parametric type is used without type\nparameters. For example, SELECT CAST(1 AS MAP)", "NaN"], ["4323", "Fix deadlock between HttpPageBufferClient and ExchangeClient", "Haozhun Jin", "haozhun", "01/13/16, 12:02:55 AM", "- By design, HttpPageBufferClient must not delegate to clientCallback when  it's holding a lock to `this`. And `checkNotHoldsLock` is used to enforce this  requirement and help reason about this invariant locally.\n- `checkNotHoldsLock` is missing in `handleFailure`. And `PagesResponse` `onSuccess`  callback delegates to `handleFailure` while holding the lock to this.\n- As a result, `handleFailure` delegates to `ExchangeClient.clientFailed`, which  tries to acquires a lock on the parent `ExchangeClient` while still holding a  lock to `HttpPageBufferClient`.\n- This creates a deadlock because both `ExchangeClient.getStatus` and  `HttpPageBufferClient.getStatus` acquires a lock on their respective this.  And the former delegates to the latter.", "NaN"], ["4324", "Update tests for TestingHttpClient changes", "David Phillips", "electrum", "01/13/16, 12:54:14 AM", "NaN", "NaN"], ["4325", "Use StandardTypes.MAP instead of \"map\"", "Christopher Berner", "cberner", "01/16/16, 12:37:33 AM", "NaN", "NaN"], ["4326", "Remove outdated reference to sys schema in TPCH docs", "David Phillips", "electrum", "01/13/16, 02:26:21 AM", "NaN", "NaN"], ["4327", "Update to Airlift 0.123", "David Phillips", "electrum", "01/13/16, 02:31:08 AM", "NaN", "NaN"], ["4332", "Minor BlackHole test improvements", "David Phillips", "electrum", "01/15/16, 11:35:56 PM", "NaN", "NaN"], ["4336", "Update JMX connector", "David Phillips", "electrum", "01/20/16, 03:23:27 AM", "NaN", "NaN"], ["4337", "Remove support for legacy directory hashing", "David Phillips", "electrum", "01/13/16, 08:45:32 PM", "NaN", "NaN"], ["4340", "Update InformationSchema connector", "David Phillips", "electrum", "01/20/16, 03:02:13 AM", "NaN", "NaN"], ["4341", "Update airbase and airlift", "Nileema Shingte", "nileema", "01/14/16, 08:50:53 PM", "NaN", "NaN"], ["4342", "Fix race in PageProcessor", "Christopher Berner", "cberner", "01/14/16, 09:57:10 PM", "Fix race which could cause queries to fail when using\nconcat(array, array), or when enabling columnar processing", "NaN"], ["4345", "Update Kafka connector", "David Phillips", "electrum", "01/20/16, 07:27:05 AM", "NaN", "NaN"], ["4349", "Add sticky headers, sorting to task table in web UI", "Raghav Sethi", "raghavsethi", "01/15/16, 08:26:17 PM", "![screen shot 2016-01-15 at 11 52 40 am](https://cloud.githubusercontent.com/assets/715788/12363636/b898443a-bb7e-11e5-88f3-f98edb09c6f0.png)", "NaN"], ["4351", "Fix compatibility issue in ClientTypeSignature", "Christopher Berner", "cberner", "01/15/16, 09:15:28 PM", "A series of commits, starting with\n3dbb3ba83dcbeed5faf490e34ebaed1398989f77 broke compatibility between the\nJDBC driver and Presto servers version < 0.133, this fixes the JDBC\ndriver to be compatible with all versions of the server", "NaN"], ["4352", "Update Cassandra connector", "David Phillips", "electrum", "01/20/16, 07:12:55 AM", "NaN", "NaN"], ["4353", "Avoid relying on iteration order of Map.keySet and Map.values", "Christopher Berner", "cberner", "01/16/16, 01:40:01 AM", "Many parts of planning rely on ProjectNode.getExpressions() and\nProjectNode.getOutputSymbols() being in corresponding order. However,\nthis is only true if the iteration order of Map.keySet and Map.values\nfor the Map passed to the constructor of ProjectNode are corresponding.", "NaN"], ["4354", "Update 0.133 release notes", "Raghav Sethi", "raghavsethi", "01/20/16, 05:22:05 PM", "NaN", "NaN"], ["4356", "Fix signature parsing issues", "Martin Traverso", "martint", "01/17/16, 09:42:48 PM", "NaN", "NaN"], ["4362", "Move 0.132 release note to correct section", "David Phillips", "electrum", "01/20/16, 07:26:20 AM", "NaN", "NaN"], ["4366", "Add release notes for 0.134", "Raghav Sethi", "raghavsethi", "01/19/16, 05:39:47 PM", "NaN", "NaN"], ["4368", "Add missing argument to checkCondition call", "Christopher Berner", "cberner", "01/19/16, 11:43:30 PM", "NaN", "NaN"], ["4375", "Fix minor bugs in Web UI", "Raghav Sethi", "raghavsethi", "01/19/16, 11:30:37 PM", "The functionality to convert task IDs to sortable strings generated\nnumbers that were too large for TinySort to deal with. Changed\nzero-padding to 5 digits and also fixed a missing parenthesis error.", "NaN"], ["4377", "Update TPCH connector", "David Phillips", "electrum", "01/20/16, 06:46:10 AM", "NaN", "NaN"], ["4379", "Make NetworkLocationCache load asynchronously at startup", "Christopher Berner", "cberner", "01/20/16, 07:18:08 PM", "NaN", "NaN"], ["4380", "Remove legacy partition API", "David Phillips", "electrum", "01/20/16, 08:10:26 PM", "NaN", "NaN"], ["4381", "Add checkConnectorSupports to IsolationLevel", "David Phillips", "electrum", "01/20/16, 07:01:37 PM", "NaN", "NaN"], ["4383", "Reduce number of threads needed by tests", "Christopher Berner", "cberner", "01/21/16, 04:34:53 AM", "NaN", "NaN"], ["4384", "Cleanup usages of SplitManager", "David Phillips", "electrum", "01/20/16, 09:43:48 PM", "NaN", "NaN"], ["4387", "Use a custom identifier for dictionary blocks", "Nileema Shingte", "nileema", "01/21/16, 08:56:48 PM", "We used UUID.randomUUID() to get a dictionary identifier, which can have a\nlot of contention and slow down queries. Change it to use a custom dictionary identifier.", "NaN"], ["4388", "Update to GCE Travis", "Christopher Berner", "cberner", "01/21/16, 07:13:15 AM", "I ran this 10 times in Travis, and it passed every time", "NaN"], ["4390", "Fix IndexOutOfBoundsException in TopologyAwareScheduler", "Christopher Berner", "cberner", "01/21/16, 08:31:55 PM", "This correctly handles the case when the server is first starting up and\nthe NetworkLocationCache is empty. Also, improve the test coverage for\nthe NetworkLocationCache.", "NaN"], ["4394", "Fix bad error message", "Christopher Berner", "cberner", "01/21/16, 09:20:44 PM", "NaN", "NaN"], ["4396", "Fix flaky test in TestSerDeUtils", "Haozhun Jin", "haozhun", "01/22/16, 01:42:51 AM", "caused by race condition in ObjectInspectorFactory.getReflectionObjectInspector", "NaN"], ["4397", "Update 0.134 release notes", "Christopher Berner", "cberner", "01/22/16, 05:01:57 PM", "NaN", "NaN"], ["4398", "Add release notes for 0.134", "Nileema Shingte", "nileema", "01/22/16, 12:17:04 AM", "NaN", "NaN"], ["4400", "Fix integration tests for Hive", "David Phillips", "electrum", "01/22/16, 05:21:00 AM", "NaN", "NaN"], ["4410", "Rename PartitionCommitter close to flush", "David Phillips", "electrum", "01/25/16, 07:48:22 PM", "NaN", "NaN"], ["4411", "Update release notes for partition API removal", "David Phillips", "electrum", "01/22/16, 05:19:02 PM", "NaN", "NaN"], ["4416", "Change javascript in query.html to support Safari", "Raghav Sethi", "raghavsethi", "01/22/16, 10:48:49 PM", "Safari's lack of support for arrow function breaks the tasks table in the\nquery page on the Web UI. Changed the arrow function to an anonymous function.\n#4414", "NaN"], ["4418", "Print info for queries with large CPU regressions", "Dain Sundstrom", "dain", "01/26/16, 08:21:57 PM", "Also print summary of CPU regressions and speedups", "NaN"], ["4423", "Categorize errors in varbinary decoding", "Christopher Berner", "cberner", "01/26/16, 08:18:51 PM", "NaN", "NaN"], ["4424", "Add TableParameterCodec, remove retention table property", "Haozhun Jin", "haozhun", "02/06/16, 12:07:08 AM", "NaN", "NaN"], ["4427", "Improve error message for LIMIT > max int", "Christopher Berner", "cberner", "01/29/16, 12:24:04 AM", "Currently this fails in TopNOperator constructor, because there's an\ninteger overflow in the LocalExecutionPlanner when it casts a long to an\nint.", "NaN"], ["4429", "Fix Raptor shards system table", "David Phillips", "electrum", "01/27/16, 01:20:41 AM", "NaN", "NaN"], ["4430", "Generalize type check in the client", "Martin Traverso", "martint", "01/27/16, 08:46:26 PM", "The client currently expects an exact match for basic types (VARCHAR, etc).\nIn this commit, we relax the check to allow matching on the base name of a type\nin preparation for upcoming changes to implement parametric VARCHAR.\n\nAdding the change now allows us to deploy it to clients now to avoid complicated\ncoordinated rollouts when parametric VARCHAR is rolled out on the server side.\n\nThe upcoming changes to implement parametric VARCHAR", "NaN"], ["4432", "Allow filtering verifier queries by type", "Christopher Berner", "cberner", "01/29/16, 12:43:18 AM", "NaN", "NaN"], ["4436", "Refine usage of HDFS error codes", "Christopher Berner", "cberner", "01/29/16, 01:09:22 AM", "NaN", "NaN"], ["4439", "Update release notes for 0.135", "Haozhun Jin", "haozhun", "01/28/16, 02:49:34 AM", "NaN", "NaN"], ["4443", "Disable dictionary processing for non-deterministic expressions", "Nileema Shingte", "nileema", "02/24/16, 08:00:05 PM", "If a projection is non-deterministic fall back to simple columnar\nprocessing instead of dictionary processing.", "NaN"], ["4444", "Make TestHiveIntegrationSmokeTest extensible", "Haozhun Jin", "haozhun", "03/21/16, 06:46:37 PM", "NaN", "NaN"], ["4445", "Add query progress monitor to Presto JDBC client", "Haozhun Jin", "haozhun", "02/06/16, 12:28:28 AM", "Originally, only progress of SELECT queries can be monitored. Additionally,\nprogress monitor is only possible after the list of output columns become\nknown.\n\nA callback mechanism is implemented in this commit so that users who want\naccess to query progress can register a listener before issuing the query.\nThe listener will periodically receive stats as the query makes progress.\nThis works for any query.", "NaN"], ["4446", "Fix stage/task leak", "Christopher Berner", "cberner", "02/18/16, 01:02:19 AM", "NaN", "NaN"], ["4450", "Add visual query plan to UI with live stats", "Martin Traverso", "martint", "01/30/16, 01:13:02 AM", "![screen shot 2016-01-29 at 9 59 31 am](https://cloud.githubusercontent.com/assets/14387/12683629/1e929862-c66f-11e5-98a1-c574fd58da2f.png)", "NaN"], ["4451", "Only disallow ORDER BY LIMIT > int max", "Christopher Berner", "cberner", "01/29/16, 08:56:50 PM", "LIMIT > int max works fine, if there's no ORDER BY, so only disallow it\nfor queries with ORDER BY.\nThis fixes an overly aggressive assertion added by\nf6733cb1197f05d1791f95aeca49eae9c810057b", "NaN"], ["4452", "Add release notes for 0.136", "Christopher Berner", "cberner", "01/29/16, 11:48:05 PM", "NaN", "NaN"], ["4453", "Add bit_count, bitwise_not/and/or/xor functions", "Haozhun Jin", "haozhun", "02/01/16, 11:24:49 PM", "@martint has looked at the interface.\n\nRelated issue: #4028 ", "NaN"], ["4454", "Add exception handling using TRY keyword", "Raghav Sethi", "raghavsethi", "02/23/16, 10:11:08 PM", "Added the TRY keyword to Presto to allow users to catch common errors - division by zero, invalid function arguments, failure to cast/parse etc. When one of these errors is caught while processing an expression inside a TRY, a NULL value is produced. This is done by moving expressions inside TRYs into separate functions within the PageProcessor class. These functions returned boxed primitives, wrappers or nulls. Nulls are returned when the specific exceptions outlined above are caught.", "NaN"], ["4455", "Fix typo in node classification", "Martin Traverso", "martint", "02/01/16, 07:32:10 PM", "This was causing plans containing index joins and semi joins to\nnot render correctly", "NaN"], ["4461", "Remove unused surefire exclusion in presto-main", "David Phillips", "electrum", "02/01/16, 11:59:32 PM", "NaN", "NaN"], ["4463", "Move TypeCalculation to parser module", "David Phillips", "electrum", "02/02/16, 12:02:10 AM", "NaN", "NaN"], ["4465", "Add retries to loadRoles and loadTablePrivileges", "Raghav Sethi", "raghavsethi", "04/12/16, 01:09:45 AM", "Fixes #4457.", "NaN"], ["4467", "Add support for varbinary input to approx_distinct", "Eric Hwang", "erichwang", "02/02/16, 09:49:45 PM", "NaN", "NaN"], ["4469", "Add validation for partition key ordering in Hive", "Raghav Sethi", "raghavsethi", "02/03/16, 09:31:08 PM", "Hive requires that partition keys be the last columns in the table and\nthat they be specified in the same order as they are in the table\nproperties. Added validation to check that the CREATE TABLE statements\nwith Hive targets contain columns in the right order. Previously, the\ncolumns would appear to be arbitrarily re-ordered, which could cause\nconfusion.\n\nAlso fixed tests that specified columns in the wrong order.\n\nCloses #4421 ", "NaN"], ["4470", "Add disjoint sets data structure", "Haozhun Jin", "haozhun", "02/05/16, 01:25:53 AM", "NaN", "NaN"], ["4472", "Update release notes for 0.137", "Raghav Sethi", "raghavsethi", "02/05/16, 04:21:08 AM", "NaN", "NaN"], ["4473", "Fix NPE in domain translator", "Martin Traverso", "martint", "02/04/16, 01:50:40 AM", "Fixes https://github.com/facebook/presto/issues/4471", "NaN"], ["4474", "Fix unregistered double input in approx_percentile", "Raghav Sethi", "raghavsethi", "02/03/16, 11:40:42 PM", "ApproximateDoublePercentileArrayAggregations had not been added to FunctionRegistry.\n\nFixes #4395 ", "NaN"], ["4475", "Use unique names for Hive integration test tables", "David Phillips", "electrum", "02/04/16, 12:41:23 AM", "The tests can run in parallel and thus each test needs its own name.", "NaN"], ["4476", "Fix planning failure in JOIN when distributed on a constant", "Dain Sundstrom", "dain", "02/05/16, 11:04:08 PM", "NaN", "NaN"], ["4478", "Dynamically adjust running splits per task", "Christopher Berner", "cberner", "02/18/16, 06:41:44 AM", "NaN", "NaN"], ["4480", "Add tests for generic page processor", "Nileema Shingte", "nileema", "02/09/16, 06:41:54 PM", "NaN", "NaN"], ["4486", "Enable EqualityInference to determine more equalities", "Eric Hwang", "erichwang", "02/09/16, 11:47:25 PM", "EqualityInference will now attempt a single pass at rewriting\nsubexpressions in terms of other known equalities. This increases\nthe potential for finding additional expression relationships.", "NaN"], ["4487", "Fix typo in AddExchanges.visitJoin variables", "Dain Sundstrom", "dain", "02/06/16, 12:32:56 AM", "NaN", "NaN"], ["4488", "Fix partitioning when writing sampled table", "Dain Sundstrom", "dain", "02/06/16, 01:24:44 AM", "Partitioning function must handle sample symbol directly when writing a\nsampled table.", "NaN"], ["4489", "Enable sampled queries for Raptor bucketed tests", "David Phillips", "electrum", "02/06/16, 06:43:00 PM", "NaN", "NaN"], ["4490", "Allow null constants in PartitionFunctionBinding", "Dain Sundstrom", "dain", "02/06/16, 10:45:04 PM", "NaN", "NaN"], ["4491", "Remove broken stage distribution from plan visualizer", "Dain Sundstrom", "dain", "02/08/16, 06:48:18 PM", "NaN", "NaN"], ["4492", "Fix schema name in SHOW TABLES error message", "David Phillips", "electrum", "02/07/16, 06:14:26 AM", "It was previously surrounded by \"Optional[]\".", "NaN"], ["4493", "Use unique table name for Hive unsupported type test", "David Phillips", "electrum", "02/07/16, 06:15:09 AM", "NaN", "NaN"], ["4507", "Switch AbstractTestQueries test to use varchar instead of date", "Eric Hwang", "erichwang", "02/08/16, 08:35:07 PM", "Some connectors do not have date type implemented yet.", "NaN"], ["4508", "Allow type-compatible changes to view definitions", "Martin Traverso", "martint", "02/09/16, 01:15:22 AM", "If the output of a view changes in a type-compatible way (e.g.,\ndue to changes to underlying tables or inferred expression types), don't\nconsider it stale.\n\nA type-compatible change is any change where the actual type of running\nthe view query is implicitly coercible to the declared type of the view.\n\nThe planner now adds coercions if necessary to produce the expected output.\n\nFixes https://github.com/facebook/presto/issues/4501", "NaN"], ["4509", "Use static imports for Assert methods", "David Phillips", "electrum", "02/09/16, 03:21:28 AM", "NaN", "NaN"], ["4511", "Fix Raptor ShardIterator for MySQL", "David Phillips", "electrum", "02/08/16, 11:16:24 PM", "MySQL does not support getRow() for streaming result sets.", "NaN"], ["4512", "Record CPU and walltime from progress monitor in Verifier", null, "suyucs", "02/09/16, 07:27:24 PM", "Record CPU and walltime from progress monitor in Verifier. This way, we are able to know walltime and cpu time for both select and non-select queries", "NaN"], ["4514", "Fix invalid plan with precomputed hash optimization in scalar subquery", "Martin Traverso", "martint", "02/09/16, 02:52:32 AM", "A query that includes an aggregation, join or other operation at the top-level\nof a scalar subquery breaks when the pre-computed hash optimization kicks in\ndue to the additional column containing the hash.\n\nWhen processing EnforceSingleRow, add a projection to restore schema\nexpected by that node.", "NaN"], ["4515", "Use unique view and table name in test", "Martin Traverso", "martint", "02/09/16, 04:45:35 AM", "Otherwise, it causes tests to fail when run in parallel", "NaN"], ["4521", "Disable views tests for connectors that don't support them", "Martin Traverso", "martint", "02/09/16, 08:37:49 PM", "NaN", "NaN"], ["4524", "Improve message for mismatched insert column types", "David Phillips", "electrum", "02/10/16, 08:40:09 PM", "NaN", "NaN"], ["4526", "Avoid calling seekToRow on the ORC reader", "Nileema Shingte", "nileema", "02/10/16, 07:50:54 PM", "Calling seekToRow on the ORC reader tries to recompute the offsets each\ntime which in this implementation can be an expensive operation. Avoid\ncalling seekToRow instead iterate through the file sequentially.", "NaN"], ["4530", "Fix Hive partition column validation", "Raghav Sethi", "raghavsethi", "02/11/16, 02:17:54 AM", "The previous approach to validation presented the user with a\ndifficult to understand error message and made one code path\nunreachable.", "NaN"], ["4533", "Update release notes for 0.137", "Raghav Sethi", "raghavsethi", "02/11/16, 09:01:55 PM", "NaN", "NaN"], ["4535", "Add 0.137 release notes for expression optimizations", "Eric Hwang", "erichwang", "02/11/16, 02:38:43 AM", "NaN", "NaN"], ["4537", "Fix type signature serialization for varchar", "David Phillips", "electrum", "02/11/16, 09:05:18 PM", "Varchar without length should serialize without a length\nrather than using the internal MAX_INT representation.\n\nFixes #4532 ", "NaN"], ["4539", "Fix SPI tests for varchar change", "David Phillips", "electrum", "02/11/16, 10:59:41 PM", "NaN", "NaN"], ["4540", "Fix varchar type in test error messages", "David Phillips", "electrum", "02/11/16, 11:35:43 PM", "NaN", "NaN"], ["4541", "Fix testing MetadataUtil for varchar", "David Phillips", "electrum", "02/12/16, 12:37:15 AM", "NaN", "NaN"], ["4543", "Add create time to query detail page in UI", "David Phillips", "electrum", "02/12/16, 10:20:38 PM", "NaN", "NaN"], ["4545", "Revert \"Add predicates simplification to SimplifyExpressions\"", "Haozhun Jin", "haozhun", "02/13/16, 12:44:18 AM", "This reverts commit b2ef00c02e78f5e233fe972c7841036cde6a0615.\n\nBacks out #4431 \n\nWe found a correctness issue introduced in this pull request. The query below\nproduces two rows when it should only produce one.\n\n```\n  SELECT\n    *\n  FROM (\n    values\n      ('1234', 'a', '2016-01-01'),\n      ('1235', 'b', '2016-01-01'),\n  ) z(app, s, ds)\n  WHERE\n    ds > '2015-01-01' AND\n    (app like '%234' OR app like '%235' OR\n    app like '234%' OR app like '235%')\n    AND (app like '%235' OR app like '235%')\n```", "NaN"], ["4547", "Update 0.137 release notes", "Martin Traverso", "martint", "02/13/16, 10:33:30 PM", "Re-organize entries so that bug fixes are all grouped together\nbefore new features.\n\nAdded some missing entries and removed the entry related to the\nexpression optimizer change that was reverted before the release.", "NaN"], ["4548", "Reduce lock contention in getStageInfo", "Dain Sundstrom", "dain", "02/14/16, 03:47:30 AM", "Remove unnecessary lock on getAllTasks which is used by getStageInfo.  This\ncan cause long pauses when a client fetches query info, which is sent with\neach StatementResource response.", "NaN"], ["4563", "Add 0.138 release notes", "Dain Sundstrom", "dain", "02/17/16, 02:22:31 AM", "NaN", "NaN"], ["4564", "Add release notes for new Parquet reader", "Dain Sundstrom", "dain", "02/22/16, 11:14:13 PM", "NaN", "NaN"], ["4565", "Optimize some TupleDomains to produce fewer expressions", "Raghav Sethi", "raghavsethi", "02/29/16, 11:40:57 PM", "Optimizations convert NOT IN queries to discontinuous ranges. For\nexample, NOT IN (1, 2, 3) turns into <1 || (>1 && <2) || >3. Queries\nwith a large number of values in the IN list become slow (or in some\ncases generated classes that were too large) as a result of this.\n\nAdded an optimization to convert ranges into expressions with NOT INs\nif doing so would produce fewer expressions.\n\nFixes #4434.", "NaN"], ["4573", "Add config for join probe parallelism", "Christopher Berner", "cberner", "02/18/16, 12:20:34 AM", "NaN", "NaN"], ["4574", "Minor fix in javadoc for StateMachine", "Nileema Shingte", "nileema", "02/18/16, 10:15:49 PM", "NaN", "NaN"], ["4582", "Update state change logic to match method name", "Nileema Shingte", "nileema", "02/18/16, 10:14:45 PM", "The method isPossibleStateChange did exactly opposite of what it\nsuggests. Invert the logic to match the name of the method and the logic\nusing this method.", "NaN"], ["4584", "Remove OrcRecordCursor and DwrfRecordCursor", "Raghav Sethi", "raghavsethi", "02/18/16, 11:22:58 PM", "We now use PageSource instead of record cursors for these file formats, so this was dead code.", "NaN"], ["4586", "Add grammar and analysis for EXPLAIN ANALYZE", "Christopher Berner", "cberner", "03/03/16, 10:35:43 PM", "NaN", "NaN"], ["4588", "Remove partition key from ColumnMetadata", "David Phillips", "electrum", "02/26/16, 06:23:24 AM", "NaN", "NaN"], ["4591", "Reduce extra symbols generated in predicate push down", "Dain Sundstrom", "dain", "02/25/16, 01:24:17 AM", "During predicate push down the join condition may be mixed up, which\ncauses the push down code to consider the condition to be new, and\nthus the generation of many unnecessary symbols.  These symbols are\npropagated through the plan and cause extra data to be stored in hashes\nand sent over the network.\n\nThis problem can be reduced by canonicalizing the old and new join\ncriteria expression when checking if the criteria has changed.", "NaN"], ["4594", "Fix bug in CTAS when temporary directory is not used", "Haozhun Jin", "haozhun", "02/22/16, 08:27:40 PM", "When writing to a new table in Hive connector, it asserts that the target directory doesn't already exist for every partition (or once in the case of unpartitioned table), which is doomed to fail when a temporary directory is not used. This check is now removed when it is detected that temporary directory is\nnot used.\n\nTest is not added because Hive integration test doesn't simulate multiple workers, and Hive integration smoke test doesn't exist for S3.\n\nFixes #4531 ", "NaN"], ["4598", "Check data size in FixedWidthBlock constructor", "Dain Sundstrom", "dain", "02/23/16, 10:25:26 PM", "The data slice must be at least positionCount \\* fixedSize, or the code\nfails during use.", "NaN"], ["4599", "Improve precomputed hash optimizer", "Dain Sundstrom", "dain", "03/22/16, 06:41:19 PM", "Optimizers can change the plan structure, adding or removing the need for\nprecomputed hashes, so only add precomputed hashes after plan is stable.\n\nRemove unnecessary hash symbols before a hash build and drop them at\nexchanges.", "NaN"], ["4605", "Make closing OrcFileWriter idempotent", "David Phillips", "electrum", "02/22/16, 06:32:19 PM", "NaN", "NaN"], ["4606", "Move Hive connector to transactional API", "Haozhun Jin", "haozhun", "03/08/16, 12:03:13 AM", "NaN", "NaN"], ["4611", "Force redistribute single-node left when distributed joins are enabled", "Dain Sundstrom", "dain", "02/22/16, 09:44:16 PM", "NaN", "NaN"], ["4612", "Add bucket number to Raptor shards system table", "David Phillips", "electrum", "02/22/16, 09:12:18 PM", "NaN", "NaN"], ["4613", "Update release notes for 0.139", "Raghav Sethi", "raghavsethi", "02/22/16, 09:58:47 PM", "NaN", "NaN"], ["4615", "Update release notes", "Christopher Berner", "cberner", "02/22/16, 10:49:03 PM", "NaN", "NaN"], ["4617", "Fix TestShardMetadataRecordCursor", "David Phillips", "electrum", "02/23/16, 12:13:36 AM", "NaN", "NaN"], ["4618", "Implement EXPLAIN ANALYZE", "Christopher Berner", "cberner", "03/31/16, 10:14:38 PM", "NaN", "NaN"], ["4619", "Use MySQL specific queries for DELETE", "David Phillips", "electrum", "02/23/16, 06:03:50 AM", "NaN", "NaN"], ["4620", "Add config for Raptor transaction cleaner interval", "David Phillips", "electrum", "02/23/16, 06:26:28 AM", "NaN", "NaN"], ["4621", "Add grace period for bucket and shard reassignment", "David Phillips", "electrum", "02/23/16, 05:30:25 PM", "NaN", "NaN"], ["4627", "Classify DBIExceptions as Raptor metadata errors", "Raghav Sethi", "raghavsethi", "03/01/16, 11:52:54 PM", "Add DBIExceptions to the catch blocks in ShardMetadataRecordCursor and\ncategorize them as Raptor metadata errors.\n\nFixes #4585 ", "NaN"], ["4628", "Move isOptimizedReaderEnabled to DefunctConfig", "Raghav Sethi", "raghavsethi", "03/04/16, 06:39:33 AM", "This flag is no longer used\n\nFixes #4589 ", "NaN"], ["4631", "Add documentation for TRY + release notes", "Raghav Sethi", "raghavsethi", "03/02/16, 04:57:32 PM", "NaN", "NaN"], ["4632", " Properly handle normalization of IS DISTINCT FROM comparison", "Eric Hwang", "erichwang", "02/23/16, 10:54:22 PM", "NaN", "NaN"], ["4634", "Avoid potentially expensive call to getQueryInfo", "Nileema Shingte", "nileema", "02/24/16, 08:00:59 PM", "getQueryInfo can be quite expensive as it needs to get the stageInfo for\nall stages. If we already have the information in QueryExecution, avoid\ncalls to getQueryInfo.", "NaN"], ["4635", "Consider invalid Raptor $shard_uuid value as false predicate", "Haozhun Jin", "haozhun", "02/25/16, 07:53:13 PM", "Before this fix, it throws an uncategorized exception when invalid Raptor $shard_uuid is encountered.\n\nFixes #3951 ", "NaN"], ["4636", "Fix wrong latency in presto-verifier", null, "suyucs", "02/24/16, 05:32:35 PM", "I used a getWallTimeMillis from JobStatus as the latency, which turns out\nto be a different metrics. ", "NaN"], ["4638", "Improve concurrency for processing old created shards", "David Phillips", "electrum", "02/24/16, 08:46:08 PM", "NaN", "NaN"], ["4640", "Only merge two projections if source is deterministic", "Dain Sundstrom", "dain", "02/24/16, 10:41:07 PM", "Fixes #4639", "NaN"], ["4641", "Set maximum threads for backup timeout", "David Phillips", "electrum", "02/24/16, 10:51:35 PM", "NaN", "NaN"], ["4644", "Add more comments to Connector interface", "Eric Hwang", "erichwang", "02/25/16, 01:23:47 AM", "NaN", "NaN"], ["4648", "Add Raptor system table for table property", "Haozhun Jin", "haozhun", "05/18/16, 09:01:45 PM", "Fixes #4344 ", "NaN"], ["4652", "Update Raptor ShardCleaner", "David Phillips", "electrum", "02/27/16, 07:04:05 AM", "NaN", "NaN"], ["4657", "Upgrade to Airbase 49", "Christopher Berner", "cberner", "02/27/16, 12:26:08 AM", "NaN", "NaN"], ["4658", "Add environment name to UI", "David Phillips", "electrum", "03/01/16, 12:33:13 AM", "NaN", "NaN"], ["4659", "Remove bogus \"from deserializer\" Hive column comments", "David Phillips", "electrum", "02/27/16, 10:14:11 PM", "NaN", "NaN"], ["4660", "Retry Raptor commit database operations", "David Phillips", "electrum", "02/27/16, 10:15:06 PM", "NaN", "NaN"], ["4670", "Make TestTableWriterOperator safe for parallel testing", "Dain Sundstrom", "dain", "02/29/16, 08:51:48 PM", "Fixes #4669", "NaN"], ["4671", "Fix failed task logging", "Christopher Berner", "cberner", "03/01/16, 01:41:12 AM", "Previously we would only record the failed task for a query, if it was\nin the output stage", "NaN"], ["4672", "Add workaround for CodeCache not being collected effectively", "Nileema Shingte", "nileema", "03/01/16, 06:25:48 PM", "This problem is seen in jdk8u45+ where the CodeCache becomes full\ncausing the JIT compiler to be disabled. We'd seen this problem earlier\nwith Java 7 and implemented this as a workaround. Reviving the old\ncommit with minor changes.\n\nThis reverts commit da9df1eb7f797f8e287fe8e51dcdcbd983d2a85e.", "NaN"], ["4673", "Update 0.140 release notes", "Christopher Berner", "cberner", "03/01/16, 02:39:02 AM", "NaN", "NaN"], ["4677", "Add release 0.140 notes", "Dain Sundstrom", "dain", "03/01/16, 05:37:45 PM", "NaN", "NaN"], ["4682", "Invalidate cached data for new name when renaming a table", "Dain Sundstrom", "dain", "03/05/16, 09:42:06 PM", "Fixes #4662", "NaN"], ["4684", "Change HIVE_WRITER_ERROR to be more specific", "Raghav Sethi", "raghavsethi", "03/01/16, 08:42:12 PM", "We now break down the error codes into more specific categories -\nHIVE_WRITER_DATA_ERROR and HIVE_WRITER_METADATA_ERROR to better\nunderstand what fraction of errors come from the metastore vs HDFS.", "NaN"], ["4685", "Prevent test for large IN running on JDBC", "Raghav Sethi", "raghavsethi", "03/01/16, 08:23:51 PM", "H2 is unable to handle the large NOT IN queries\n\ncc @electrum ", "NaN"], ["4688", "Fix HIVE_WRITER error codes", "Raghav Sethi", "raghavsethi", "03/01/16, 10:48:47 PM", "Missed changing references when the names were changed.\n\nFixes #4668.", "NaN"], ["4689", "Fix import order issue caused by refactor", "Raghav Sethi", "raghavsethi", "03/01/16, 11:48:14 PM", "NaN", "NaN"], ["4690", "Reduce IN list size for tests", "Raghav Sethi", "raghavsethi", "03/02/16, 02:29:38 AM", "Some connectors are unable to deal with large NOT IN lists, reduce the\nsize such that all connectors pass tests.", "NaN"], ["4691", "Migrate NOT_EQUAL operator for arrays to new framework", "Christopher Berner", "cberner", "03/10/16, 05:03:23 PM", "NaN", "NaN"], ["4692", "Optimize JsonExtract.extract", "Christopher Berner", "cberner", "03/03/16, 09:20:36 PM", "Avoid creating unnecessary Exception objects. This accounts for > 5% of\nworker CPU time for my query", "NaN"], ["4696", "Implement left and right pad for strings", "Phillip Cloud", "cpcloud", "03/16/16, 03:26:40 AM", "Implements left and right pad.\n\nTODOs:\n- [x] Add truncation behavior\n- [x] Possibly abstract away all of the setup code for left and right pad\n- [x] Handle the empty string and `<= 0` desired pad length", "NaN"], ["4699", "Move local parallel to planner", "Dain Sundstrom", "dain", "04/27/16, 04:47:23 PM", "- Local exchanges are added during planning and now appear in explain.\n- Rewrote local exchange implementation\n- Rewrote parallel hash build to fix memory tracking and to support\n  build outer joins.\n- Added specialized function to enforce layout in local execution planner\n  instead of adding extra projection", "NaN"], ["4700", "Add ConnectorNodePartitioningProvider to SampledTpchConnectorFactory", "Dain Sundstrom", "dain", "03/03/16, 02:01:36 AM", "NaN", "NaN"], ["4702", "Fix NPE when query fails during parsing/planning", "Martin Traverso", "martint", "03/03/16, 04:47:43 AM", "Commit 135626b793b42841c77f6b8c8b7fe80fac8ffc46 introduced a bug\nwhere queries that fail during parsing/analysis/planning\nthrow an NPE when the completion event is processed.\n\nThis is due to an incorrect assumption that QueryInfos always\nhave an output stage, which is not true for queries that fail\nbefore execution starts.\n\nFixes https://github.com/facebook/presto/issues/4701", "NaN"], ["4704", "Add 0.141 release notes", "David Phillips", "electrum", "03/03/16, 05:32:58 AM", "NaN", "NaN"], ["4705", "Add parsing error test to distributed queries", "Martin Traverso", "martint", "03/03/16, 05:31:47 AM", "This reproduces the issue fixed in 0498c0df5aff14c9f7f88db1fe2e43ed3470aa5b\n\nNone of the existing tests catch it because it only propagates\nto the caller if they happen during parsing (which causes the\nQueryMonitor to be called inline). Errors that occur during\nplanning and analysis call the QueryMonitor in a background thread,\nso they go to the logs, instead.", "NaN"], ["4707", "Increase H2 threads for Raptor distributed tests", "David Phillips", "electrum", "03/11/16, 08:11:00 PM", "NaN", "NaN"], ["4711", "Add INTEGER type to Presto", "Raghav Sethi", "raghavsethi", "04/08/16, 09:09:53 PM", "The last two commits will be squashed together before merge.", "NaN"], ["4713", "Remove references to \"dual\" table", "David Phillips", "electrum", "03/07/16, 04:51:02 PM", "NaN", "NaN"], ["4716", "\tEnable PredicatePushDown to handle FALSE join criterias", "Eric Hwang", "erichwang", "03/04/16, 09:52:01 PM", "NaN", "NaN"], ["4721", "Add configuration for write compression to Hive connector", "Christopher Berner", "cberner", "03/07/16, 06:37:10 PM", "Compression now defaults to GZIP for all formats (previously it was GZIP\nfor some, but not all formats). It can be changed via the\nhive.compression-codec config", "NaN"], ["4722", "Fix broken \"respect format\" flag", "Martin Traverso", "martint", "03/08/16, 11:00:08 PM", "The check was being performed in the wrong place (HivePageSink).\nBy the time the execution got to that point, the choice of\nwhich storage format to use had been made (by extracting it from\nthe table metadata), so both branches were performing the same\noperation.", "NaN"], ["4723", "Use garbage collection for cleaning local shards", "David Phillips", "electrum", "03/08/16, 05:22:43 PM", "NaN", "NaN"], ["4730", "Add checkstyle rule for illegal imports", "David Phillips", "electrum", "03/07/16, 08:53:40 PM", "NaN", "NaN"], ["4731", "Use correct imports in Hive connector", "David Phillips", "electrum", "03/07/16, 09:05:39 PM", "NaN", "NaN"], ["4734", "Show create table support", "Joy Yao", "joy-yao", "05/02/16, 05:29:05 AM", "NaN", "NaN"], ["4738", "Remove grouping set test from some connectors", "Raghav Sethi", "raghavsethi", "03/09/16, 12:59:14 AM", "Tests added to AbstractTestQueries that relied on the type of the\nTPC-H shipdate column being DATE fail when run against the Redis and\nCassandra connectors. Prevented these tests from running against these\nby overriding them in connector-specific test classes.", "NaN"], ["4739", "Allow configuring the interval for checking code cache size", "Nileema Shingte", "nileema", "03/14/16, 05:56:48 PM", "NaN", "NaN"], ["4740", "Add new JMH-based HiveFileFormatBenchmark", "Dain Sundstrom", "dain", "03/09/16, 02:36:57 AM", "NaN", "NaN"], ["4741", "Remove grouping set test from TestRedisDistributed", "Raghav Sethi", "raghavsethi", "03/09/16, 02:35:06 AM", "The distributed Redis connector does not support the DATE type in the\ngrouping set tests.", "NaN"], ["4743", "Remove JetBrains NotNull annotation", "David Phillips", "electrum", "03/09/16, 06:39:53 PM", "NaN", "NaN"], ["4744", "Writing to Hive bucket table", "Haozhun Jin", "haozhun", "04/02/16, 01:57:53 AM", "NaN", "NaN"], ["4746", "Fix build", "Christopher Berner", "cberner", "03/09/16, 10:21:28 PM", "Remove banned @NotNull annotation", "NaN"], ["4747", "Update 0.142 release notes", "Raghav Sethi", "raghavsethi", "03/15/16, 07:56:57 PM", "Also added Sphinx label in SELECT documentation to make referencing\nthe complex grouping operations section more easily.", "NaN"], ["4749", "Simplify ORC tests", "Dain Sundstrom", "dain", "04/14/16, 05:38:47 PM", "NaN", "NaN"], ["4753", "Raptor backup cleaner improvements", "David Phillips", "electrum", "03/11/16, 06:57:32 PM", "NaN", "NaN"], ["4754", "Enable index joins to probe through certain window functions", "Eric Hwang", "erichwang", "03/11/16, 10:35:38 PM", "For example, the following query should be able to be index joined if\nthe TPCH orders table has an index on orderkey:\n\nSELECT *\nFROM lineitem\nJOIN (SELECT _, COUNT(_) OVER (PARTITION BY orderkey) FROM orders) o\nON l.orderkey = o.orderkey\n\nThis change constrains the allowable index join probes to window functions\nconsisting of aggregation functions that have windows partitioned by\nsome superset of the join keys, without an order by clause, and\nwith only range frame specification.", "NaN"], ["4759", "Add more instrumentation for Raptor", "Nileema Shingte", "nileema", "03/14/16, 08:47:10 PM", "NaN", "NaN"], ["4760", "Migrate slice() to new scalar framework", "Mark", "geraint0923", "04/07/16, 01:18:15 AM", "Migrate array_slice to new framework", "NaN"], ["4761", "Migrate array_intersect() to new scalar framework", "Mark", "geraint0923", "05/04/16, 12:00:33 AM", "Migrate array_intersect() to new scalar framework and do some small optimizations.", "NaN"], ["4764", "Support ConfigurationAwareModule for backup providers", "David Phillips", "electrum", "03/12/16, 06:06:46 PM", "NaN", "NaN"], ["4766", "Only cleanup Raptor backup shards once\t", "David Phillips", "electrum", "03/13/16, 03:43:18 AM", "NaN", "NaN"], ["4770", "Do not allow duplicate entries in deleted shards", "David Phillips", "electrum", "03/14/16, 09:22:25 PM", "NaN", "NaN"], ["4772", "Include full response object for null content type", "David Phillips", "electrum", "03/14/16, 05:43:51 PM", "NaN", "NaN"], ["4776", "Cancel incomplete backup jobs on rollback", "Nileema Shingte", "nileema", "03/15/16, 12:19:54 AM", "NaN", "NaN"], ["4777", "Migrate array_sort() to new scalar framework", "Mark", "geraint0923", "05/18/16, 09:45:36 PM", "Migrate array_sort() to new scalar framework, and a benchmark to show the performance improvement:\n\n```\nBenchmark                             (name)    Mode  Cnt    Score   Error Units\nBenchmarkArraySort.arraySort      array_sort    avgt   20  108.867 \u00b1 4.888 ns/op\nBenchmarkArraySort.arraySort  old_array_sort    avgt   20  120.720 \u00b1 6.268 ns/op\n```", "NaN"], ["4782", "Update 0.142 release notes", "Christopher Berner", "cberner", "03/15/16, 07:41:40 PM", "NaN", "NaN"], ["4783", "Add release 0.142 release notes", "Dain Sundstrom", "dain", "03/15/16, 05:57:38 PM", "NaN", "NaN"], ["4784", "Add 0.142 release notes", "Dain Sundstrom", "dain", "03/15/16, 06:33:05 PM", "NaN", "NaN"], ["4786", "Update 0.142 release notes", "Martin Traverso", "martint", "03/15/16, 08:36:25 PM", "NaN", "NaN"], ["4787", "Migrate array_greater_than_or_equal operator to new scalar framework", "Mark", "geraint0923", "04/19/16, 04:14:03 AM", "Migrate array_greater_than_or_equal operator to new scalar framework", "NaN"], ["4790", "Improve scheduler latency", "Dain Sundstrom", "dain", "03/17/16, 04:32:16 AM", "NaN", "NaN"], ["4802", "Delete created shards at end of transaction commit", "David Phillips", "electrum", "03/16/16, 10:05:49 PM", "This should reduce contention on the table.", "NaN"], ["4803", "Improve error message for At Time Zone type mismatch", "Eric Hwang", "erichwang", "03/16/16, 09:27:37 PM", "NaN", "NaN"], ["4804", "Migrate map_equal operator to new scalar framework", "Mark", "geraint0923", "03/29/16, 05:20:40 PM", "(1) make @TypeParameter annotation repeatable\n(2) add @Nullable checking to new scalar framework\n(3) migrate map_equal operator to new scalar framework", "NaN"], ["4806", "Only rebalance shards to worker nodes", "David Phillips", "electrum", "03/17/16, 04:14:50 PM", "NaN", "NaN"], ["4807", "Migrate array_equal operator to new scalar framework", "Mark", "geraint0923", "03/29/16, 06:53:06 PM", "Migrate array_equal operator to new scalar framework", "NaN"], ["4808", "Improve ordering of all-at-once scheduler", "Dain Sundstrom", "dain", "03/19/16, 12:06:32 AM", "The scheduler is single threaded per query so the actual order stages are\nprocessed in the all-at-once scheduler can effect short queries on fast\nconnectors.", "NaN"], ["4815", "Add support for scalar subqueries in delete queries", "David Phillips", "electrum", "03/28/16, 09:15:49 PM", "NaN", "NaN"], ["4819", "Add missing synchronization for HttpRemoteTask.addSplits", "David Phillips", "electrum", "03/17/16, 11:34:11 PM", "NaN", "NaN"], ["4822", "Fix travis link in README.md", "\u0141ukasz Osipiuk", "losipiuk", "03/18/16, 04:19:15 PM", "NaN", "NaN"], ["4825", "Add stats for Raptor backup store", "David Phillips", "electrum", "03/18/16, 09:08:49 PM", "NaN", "NaN"], ["4830", "Fix CTAS in Hive when respectTableFormat is false", "Haozhun Jin", "haozhun", "03/21/16, 06:43:25 PM", "NaN", "NaN"], ["4840", "Pipeline deletion of backup shards", "David Phillips", "electrum", "03/21/16, 06:40:19 PM", "Deletion times can have high variablility, so waiting for an entire batch\nto complete before starting a new batch greatly decreases throughput.", "NaN"], ["4841", "JDBC QueryBuilder improvements", "David Phillips", "electrum", "03/21/16, 05:55:49 PM", "NaN", "NaN"], ["4842", "Improve QueryBuilder to support VARCHAR, DATE, TIME, TIMESTAMP condition", "Mark", "geraint0923", "05/18/16, 09:46:18 PM", "Improve QueryBuilder to support VARCHAR condition and add corresponding test.", "NaN"], ["4843", "Apply non-TupleDomain predicates to Hive partition list", "Haozhun Jin", "haozhun", "03/26/16, 06:42:03 AM", "When Hive connectors look up all partitions that is potentially needed, it\ndidn't taken advantage of the additional predicate available in Constraint to\napply the predicates that are not representable as TupleDomain.\n\nWhen Hive was migrated to the current TableLayout API, this additional filter\nwas not added. This is most likely caused by not thinking thoroughly enough and\ntrying to retain whatever the code did with the old API.", "NaN"], ["4844", "Add retry for inserting created shards", "David Phillips", "electrum", "03/22/16, 06:15:52 PM", "NaN", "NaN"], ["4845", "Add width bucket implementation for array bin specification", "Phillip Cloud", "cpcloud", "03/24/16, 08:03:44 PM", "Adds `width_bucket(double operand, array<double> bins)` implementation. This is a follow up to https://github.com/prestodb/presto/pull/4791", "NaN"], ["4846", "Upgrade to hive-apache 0.17", "Christopher Berner", "cberner", "03/22/16, 07:29:19 PM", "This fixes a potential native memory leak when writing (and possibly reading) gzip'ed files", "NaN"], ["4849", "Cleanup logging for ShardCleaner", "David Phillips", "electrum", "03/23/16, 04:43:26 PM", "NaN", "NaN"], ["4853", "Add 0.143 release notes", "Christopher Berner", "cberner", "03/23/16, 06:49:57 PM", "NaN", "NaN"], ["4854", "Fix Hive error codes", "Christopher Berner", "cberner", "03/23/16, 06:48:32 PM", "Several error codes used the wrong range (they were using the USER\nrange)", "NaN"], ["4855", "Upgrade to airlift 0.125", "Christopher Berner", "cberner", "03/23/16, 06:42:03 PM", "NaN", "NaN"], ["4856", "Add loading indicator to query detail page", "David Phillips", "electrum", "03/23/16, 08:07:35 PM", "NaN", "NaN"], ["4857", "Enforce minimum file descriptor limit on startup", "Dain Sundstrom", "dain", "04/22/16, 05:04:20 PM", "NaN", "NaN"], ["4859", "Add 0.143 release notes", "Dain Sundstrom", "dain", "03/23/16, 08:08:57 PM", "NaN", "NaN"], ["4860", "Update string padding function documentation", "David Phillips", "electrum", "03/23/16, 10:06:40 PM", "NaN", "NaN"], ["4861", "Fix document for Queue", "Mark", "geraint0923", "03/24/16, 12:20:34 AM", "NaN", "NaN"], ["4862", "Update 0.143 release notes", "Martin Traverso", "martint", "03/23/16, 11:32:50 PM", "NaN", "NaN"], ["4863", "Update release notes for 0.143", "Haozhun Jin", "haozhun", "03/23/16, 11:35:47 PM", "NaN", "NaN"], ["4865", "Fix commit to delete created shards within transaction", "David Phillips", "electrum", "03/24/16, 12:00:30 AM", "NaN", "NaN"], ["4866", "Split preference from requirement in hash optimizer", "Dain Sundstrom", "dain", "03/24/16, 11:14:56 PM", "The precomputed hash optimizer adds projections earlier than needed\ndue to the mixing of preferences and requirements.  Once split the\noptimizer can delay creation of hash symbols until needed.", "NaN"], ["4870", "Fix NullPointerException from non-read queries", "Raghav Sethi", "raghavsethi", "03/25/16, 11:00:34 PM", "Queries that do not read (SELECT) generate null results. We would\nattempt to create a multiset out of these, causing a\nNullPointerException. This fixes the resultsMatch function to deal\nwith this case properly.", "NaN"], ["4871", "Fix potential deadlock in phased scheduler", "Martin Traverso", "martint", "03/24/16, 11:11:44 PM", "The code was incorrectly ignoring the fragment that contains\nthe TableScan or Values nodes as a source and missed an edge\nin the scheduling dependency graph if there was a broadcast\njoin (probe + join in in the same fragment).\n\nThis could cause the join-probe stage to be scheduled independently\n(and before) the build stage in a contented cluster, which\nwould result in the query deadlocking.\n\nFixes https://github.com/prestodb/presto/issues/4869", "NaN"], ["4872", "Remove unused import", "Christopher Berner", "cberner", "03/24/16, 09:59:47 PM", "NaN", "NaN"], ["4873", "Exchange client cleanup", "Dain Sundstrom", "dain", "03/28/16, 09:30:48 PM", "NaN", "NaN"], ["4878", "Use new classes to represent Hive metastore entities", "Haozhun Jin", "haozhun", "07/20/16, 01:45:30 AM", "NaN", "NaN"], ["4880", "Release data sources properly in optimized Parquet reader", "Nezih Yigitbasi", "nezihyigitbasi", "03/26/16, 05:04:45 AM", "The optimized Parquet reader does not properly close the file system connections that it uses to read dictionary pages (which exhausts eventually the http client connection pool when using the AWS sdk and queries start failing etc.). This PR fixes that. Related to #4879. \\cc @zhenxiao @dain ", "NaN"], ["4881", "Factor out logic to instantiate thrift transport", "Martin Traverso", "martint", "03/28/16, 05:49:54 PM", "This is so that implementations that derive from the hive connector\ncan reuse and customize the thrift transport used for talking\nto the metastore", "NaN"], ["4882", "Fix phased scheduling policy for right join", "Martin Traverso", "martint", "03/27/16, 04:38:52 AM", "It was mistakenly considering the left branch as the build side.", "NaN"], ["4885", "Fix delete for predicates that optimize to false", "David Phillips", "electrum", "03/28/16, 05:56:15 PM", "NaN", "NaN"], ["4888", "Move width_bucket documentation to correct section", "David Phillips", "electrum", "03/28/16, 05:55:57 PM", "NaN", "NaN"], ["4889", "Fix assertUpdate to verify update count", "David Phillips", "electrum", "03/28/16, 08:06:27 PM", "NaN", "NaN"], ["4890", "Add response pages to exchange buffer in one batch", "Dain Sundstrom", "dain", "03/29/16, 06:30:49 PM", "NaN", "NaN"], ["4891", "Add stats to RemoteTask", "Nileema Shingte", "nileema", "04/11/16, 11:13:56 PM", "NaN", "NaN"], ["4892", "Include more debug information in page transport error", "Christopher Berner", "cberner", "03/29/16, 04:26:05 PM", "NaN", "NaN"], ["4893", "Update release notes for 0.143", "Haozhun Jin", "haozhun", "03/29/16, 01:24:59 AM", "NaN", "NaN"], ["4896", "Remove supported type check in HiveType factories", "Haozhun Jin", "haozhun", "04/11/16, 10:52:11 PM", "- Add supported type check where necessary after surveying callers\n- Improve supported type check in HiveMetadata to catch unsupported type\n  (Originally, only supported type that can't be written is caught.\n  Unsupported types are caught downstream by HiveRecordWriter.)\n- Improve error message to provide user more context on unsupported type\n- Pre-requisite for upcoming refactoring of metastore classes", "NaN"], ["4898", "Make queueing rules pluggable", "Christopher Berner", "cberner", "03/31/16, 07:33:47 PM", "First step toward: https://github.com/prestodb/presto/issues/4742", "NaN"], ["4905", "Fix AggregationAnalyzer failure caused by AtTimeZone", "Mark", "geraint0923", "03/30/16, 10:26:18 PM", "Before this commit, visitAtTimeZone in AggregationAnalyzer would be\ndirected to visitExpression which throws UnsupportedOperationException\ncausing the failure in AggregationAnalyzer.\n\nThis commit implements visitAtTimeZone which will process the value\nfield in AtTimeZone.", "NaN"], ["4907", "Upgrade to airlift 0.126 and Jetty 9.3.8 final", "Christopher Berner", "cberner", "03/30/16, 11:03:44 PM", "NaN", "NaN"], ["4910", "Add SMALLINT and TINYINT to Presto", "Raghav Sethi", "raghavsethi", "05/20/16, 10:23:15 PM", "Closes #4513.", "NaN"], ["4912", "Document option for debugging GC problems", "Christopher Berner", "cberner", "03/31/16, 06:57:56 PM", "NaN", "NaN"], ["4916", "Add retries for teardown queries in Verifier", null, "suyucs", "04/02/16, 01:43:15 AM", "NaN", "NaN"], ["4922", "Get rid of unnecessary boxing", "Nezih Yigitbasi", "nezihyigitbasi", "04/01/16, 07:59:46 PM", "NaN", "NaN"], ["4927", "Add delay and throttle for reassigning buckets", "David Phillips", "electrum", "04/02/16, 05:53:13 AM", "NaN", "NaN"], ["4930", "Various changes to diagnose communication problems", "Dain Sundstrom", "dain", "04/04/16, 05:56:23 PM", "NaN", "NaN"], ["4934", "Upgrade to airlift 0.126", "Christopher Berner", "cberner", "04/04/16, 07:33:46 PM", "NaN", "NaN"], ["4936", "Use placeholder when output stage json is long", "Haozhun Jin", "haozhun", "04/05/16, 07:50:31 PM", "NaN", "NaN"], ["4937", "Fix Hive client integration test", "Haozhun Jin", "haozhun", "04/05/16, 12:03:47 AM", "NaN", "NaN"], ["4940", "Add configuration for task notification thead pool", "Dain Sundstrom", "dain", "04/05/16, 06:44:48 PM", "NaN", "NaN"], ["4943", "Change ParquetPrimitiveConverter to use type.write", "Raghav Sethi", "raghavsethi", "04/20/16, 04:38:37 AM", "Not sure why it was any other way - presumably because this code\npredates the current version of the type system.\n\ncc @zhenxiao @nezihyigitbasi ", "NaN"], ["4944", "Log query failures at debug level", "Martin Traverso", "martint", "04/05/16, 08:21:58 PM", "Every query failure (including syntax or semantic errors) were\nbeing logged at ERROR level.", "NaN"], ["4945", "Minor fixes to query detail page in web UI", "Raghav Sethi", "raghavsethi", "04/05/16, 11:01:37 PM", "- Upgrade to highlight.js 9.3\n- Increase body width on large screens", "NaN"], ["4946", "Fix starvation caused by resource_overcommit", "Christopher Berner", "cberner", "04/06/16, 01:18:45 AM", "NaN", "NaN"], ["4948", "Add fetch task instance id directly from SqlTaskManager", "Dain Sundstrom", "dain", "04/06/16, 06:32:59 AM", "Currently when the task instance id is needed the caller asks for the\nentire task info, which is very expensive to compute, and then the caller\nextracts the single id field.", "NaN"], ["4949", "Fix grouping set bug with partitioned source", "Raghav Sethi", "raghavsethi", "04/06/16, 05:44:08 PM", "To determine whether a partitioned exchange is required, we would look at the union of all columns in the grouping set, when in fact a grouping set requires partitioning on the intersection of the columns of every set. Added partitioningRequirement field to AggregationNode to store this, and made necessary changes to AddExchanges.", "NaN"], ["4953", "Update release notes for 0.144", "Haozhun Jin", "haozhun", "04/06/16, 07:18:22 PM", "NaN", "NaN"], ["4956", "Migrate more map functions/operators to new scalar framework", "Mark", "geraint0923", "04/11/16, 05:12:52 PM", "NaN", "NaN"], ["4957", "Fix checking result for array functions in product tests", "Mark", "geraint0923", "04/07/16, 08:23:31 PM", "NaN", "NaN"], ["4961", "revert \"Add decimal product tests in presto-hive\"", "Wojciech Biela", "ilfrin", "04/07/16, 04:05:32 PM", "This reverts commit 65dda94e4d198ef080723a48274455229caf895f. This\ncommit was not supposed to be merged, it was an element of a larger part\nremoved and edited before the merge of the DECIMAL works. This makes\nthe integration tests fail, due to presto-hive/src/test/sql/create-test.sql\nhaving an 8 column table tmp_presto_test and inserting 10 columns into\nit.\n\n@martint, to make the tests pass we could just fix the create-test.sql and remove the two additional columns from the insert into, but in fact the whole commit doesn't make sense since we decided to drop it for now. \n\nTo prevent this from happening (breaking integration tests) we're working on including them in the Travis build (currently Travis runs the units test and product tests), we should have that ready in a couple days.", "NaN"], ["4970", "Remove awareness of bucketed Hive table", "Haozhun Jin", "haozhun", "04/08/16, 06:47:06 PM", "because of known correctness issue in planning and scheduling when reading\nfrom such table. Being aware of Hive table bucketing, Presto uses a\npotentially more efficient query execution plan. But the new plan had bugs\nthat are only exposed when combined with the specifics of Hive.", "NaN"], ["4974", "Use correct executor in Raptor BackupManager", "David Phillips", "electrum", "04/08/16, 07:39:21 PM", "NaN", "NaN"], ["4975", "Release notes", "Christopher Berner", "cberner", "04/08/16, 08:02:48 PM", "NaN", "NaN"], ["4978", "Update release notes for 0.145", "Raghav Sethi", "raghavsethi", "04/12/16, 12:36:06 AM", "NaN", "NaN"], ["4979", "Remove unused methods from CassandraType", "David Phillips", "electrum", "04/09/16, 12:15:27 AM", "NaN", "NaN"], ["4988", "Remove extra comma in string documentation", "David Phillips", "electrum", "04/11/16, 05:45:23 PM", "NaN", "NaN"], ["4990", "Add GraphViz visitor for GroupIdNode", "Raghav Sethi", "raghavsethi", "04/12/16, 02:01:06 AM", "NaN", "NaN"], ["4994", "Fix aggregations to check for coerced types", "Raghav Sethi", "raghavsethi", "04/12/16, 12:33:35 AM", "The project function in QueryPlanner did not check for existing\ncoercions when determining the type of a symbol. Added another method\nin Analysis to check for coercions.", "NaN"], ["4995", "Fix missed TPC-H column type changes", "Raghav Sethi", "raghavsethi", "04/12/16, 12:52:34 AM", "Missed some changes in earlier commits.", "NaN"], ["4996", "Update Airlift and enable HTTP/2 for tests", "David Phillips", "electrum", "04/12/16, 01:01:35 AM", "NaN", "NaN"], ["4997", "Clean up row field reference implementation", "Mark", "geraint0923", "05/04/16, 12:02:48 AM", "Fix #4965 ", "NaN"], ["5002", "New output buffer implementation", "Dain Sundstrom", "dain", "07/07/16, 03:11:26 AM", "NaN", "NaN"], ["5004", "Update test classes", "David Phillips", "electrum", "04/12/16, 10:19:55 PM", "NaN", "NaN"], ["5006", "Disable HTTP/2 client for tests", "David Phillips", "electrum", "04/12/16, 10:25:48 PM", "Enabling this breaks the tests when run with many threads.", "NaN"], ["5007", "Use shorter HTTP client timeout for CLI", "David Phillips", "electrum", "04/12/16, 10:34:48 PM", "NaN", "NaN"], ["5015", "Fix joins to check for coerced types", "Raghav Sethi", "raghavsethi", "04/14/16, 07:44:26 PM", "Can't find a test that breaks, unfortunately. The plans produced were wrong but most queries will still actually work.", "NaN"], ["5016", "Fix incorrect SQL examples in GROUP BY docs", "Anton", "petroav", "04/13/16, 09:38:15 PM", "NaN", "NaN"], ["5022", "Fix hashcode operator for varbinary", "Martin Traverso", "martint", "04/14/16, 05:38:41 AM", "Due to legacy reasons, we have code for computing the hash code\nof a type in two places: hash_code operator and block.\n\nThe update to 64-bit hashes missed a change to varbinary's\nHASH_CODE operator, so the operator and the block produced\ndifferent hashes for the same data.\n\nEventually, we should converge to having just the hash_code\noperator implementations and get rid of the block methods.", "NaN"], ["5024", "Fix verifier to deal with list comparisons", "Raghav Sethi", "raghavsethi", "04/15/16, 11:09:40 PM", "NaN", "NaN"], ["5029", "Remove empty file", "David Phillips", "electrum", "04/14/16, 09:34:41 PM", "NaN", "NaN"], ["5031", "Track system memory in output operator and reduce memory usage", "Christopher Berner", "cberner", "04/19/16, 09:28:11 PM", "NaN", "NaN"], ["5034", "Assure queries are always expired", "Dain Sundstrom", "dain", "04/15/16, 05:04:40 AM", "NaN", "NaN"], ["5037", "Replace queues with resource group implementation", "Christopher Berner", "cberner", "04/27/16, 09:25:36 PM", "NaN", "NaN"], ["5039", "Fix function resolution with ambiguous coercions", "Raghav Sethi", "raghavsethi", "04/19/16, 01:30:44 AM", "This code will be exercised by the upcoming narrowing coercions PR.", "NaN"], ["5042", "Change verifier to only run read-only queries by default", "Christopher Berner", "cberner", "04/18/16, 04:22:43 PM", "NaN", "NaN"], ["5043", "Add ROW definition and CAST syntax", "Mark", "geraint0923", "04/27/16, 05:11:22 PM", "The row type definition syntax will be like:\n    `ROW(x BIGINT, y DOUBLE)`\n\nAnd the CAST for row type syntax will be like:\n    `CAST(ROW(1, 2) AS ROW(x BIGINT, y DOUBLE))`\n\nNote that for now the field names could only contains alphanumeric\ncharacters (there should not be special characters like `\"`, `'`,\nand space)", "NaN"], ["5047", "Cleanup error code allocation handling", "David Phillips", "electrum", "04/18/16, 06:02:33 PM", "The error code range is intended to be a 16-bit space per connector,\nso this change makes that clear and simplifies error code allocation\nwithin connectors.\n\nAdditionally, this splits the error code space between the decoder\nlibrary and the Kafka connector, which changes the existing codes.\n\nSee https://github.com/prestodb/presto/wiki/Error-Codes", "NaN"], ["5048", "Cleanup usages for USER_ERROR", "David Phillips", "electrum", "04/18/16, 06:10:49 PM", "NaN", "NaN"], ["5049", "Categorize abandoned task errors correctly", "Christopher Berner", "cberner", "04/18/16, 09:24:57 PM", "These are not user error, as they're caused by Presto failing to poll\ndata from another task.", "NaN"], ["5051", "Fix default idle timeout for exchange and scheduler http clients", "Dain Sundstrom", "dain", "04/19/16, 06:06:11 PM", "Exchange and scheduler clients do long polling so idle and request\ntimeout should be the same since the connection will be idle for the\nlength of the request", "NaN"], ["5053", "Change integral literals to fit in narrowest type", "Raghav Sethi", "raghavsethi", "04/20/16, 06:44:36 PM", "NaN", "NaN"], ["5054", "Remove empty file", "David Phillips", "electrum", "04/19/16, 12:46:56 AM", "NaN", "NaN"], ["5055", "Fix TRY compilation when part of a sub-expression", "Raghav Sethi", "raghavsethi", "04/20/16, 08:40:46 PM", "TRY method calls expect only the subset of parameters required to\nevaluate the inner expression. Howver, due to the way the parameter\nlist was constructed, method calls would instead be passed a superset\nof parameters when the TRY call was a sub-expression. This caused\ncompilation to fail.\n\nFixes #4901 ", "NaN"], ["5059", "Fix typo", "Christopher Berner", "cberner", "04/20/16, 12:18:47 AM", "NaN", "NaN"], ["5060", "Add 0.145 release notes", "Dain Sundstrom", "dain", "04/20/16, 01:18:58 AM", "NaN", "NaN"], ["5061", "Prevent TRY expression from being merged", "Raghav Sethi", "raghavsethi", "04/21/16, 12:38:19 AM", "This makes situations where TRY has non-CallExpression arguments more common. Unfortunately this means more users are shown unhelpful error messages about compiler failure. Now we also optimize TRY away when input is not CallExpression to prevent this.", "NaN"], ["5062", "Hotfix 0.144.2", "Dain Sundstrom", "dain", "04/20/16, 05:44:18 AM", "NaN", "NaN"], ["5066", "Update Provisio and Presto Maven Plugin", "Jason van Zyl", "jvanzyl", "04/20/16, 09:11:00 PM", "This updates Provisio to 0.1.27 in both the `provisio` packaging and the `presto-plugin`\npackaging which adds support for the posixLongFileName option in commons-compress which\nwill prevent the issue with long files names that result in the\n\"X is too long ( > 100 bytes)\" type of exceptions.", "NaN"], ["5067", "Add MAP cast support", "Mark", "geraint0923", "04/23/16, 10:36:18 PM", "Fix #4821 ", "NaN"], ["5069", "Update release notes", "Christopher Berner", "cberner", "04/20/16, 11:00:42 PM", "NaN", "NaN"], ["5071", "Sort array functions in documentation", "David Phillips", "electrum", "04/21/16, 01:20:07 AM", "NaN", "NaN"], ["5073", "Use Java 8 Time API for sql datetime types", "Mark", "geraint0923", "04/29/16, 01:42:39 AM", "Use Java 8 Time APT for SqlDate, SqlTime, SqlTimeWithTimeZone,\nSqlTimestamp, and SqlTimestampWithTimeZone.\n\nThis will fix #5057 ", "NaN"], ["5074", "Fix TRY switch case fallthrough", "Raghav Sethi", "raghavsethi", "04/21/16, 04:51:32 AM", "NaN", "NaN"], ["5077", "Fix bad type info in window function and aggregate", "Raghav Sethi", "raghavsethi", "04/21/16, 04:54:17 AM", "We should not have been using getTypeWithCoercions on those\ncallsites.\n\nFixes #5075 ", "NaN"], ["5081", "Update 0.145 release notes", "Martin Traverso", "martint", "04/21/16, 07:43:03 PM", "NaN", "NaN"], ["5083", "Fix broken plan when underlying view type changes", "Martin Traverso", "martint", "04/21/16, 09:12:22 PM", "The fix in 6acc22b7eecb06c39718ffce0d809a0cde4a8347 was incomplete and\ndid not properly return the declare type of the view. As a result,\nthe planner would omit necessary implicit coercions.", "NaN"], ["5084", "Add 0.145 release notes", "Dain Sundstrom", "dain", "04/21/16, 11:42:22 PM", "NaN", "NaN"], ["5087", "Exclude test from incompatible connectors", "Raghav Sethi", "raghavsethi", "04/22/16, 05:25:16 AM", "testCompatibleTypeChangeForView2 is not compatible with Redis, Kafka,\nand Cassandra connectors.", "NaN"], ["5094", "Fix mongodb connector pom", "Nezih Yigitbasi", "nezihyigitbasi", "04/23/16, 05:36:58 AM", "Master doesn't build due to this issue.", "NaN"], ["5095", "Fix the description for `regexp_like` in product tests", "Mark", "geraint0923", "04/23/16, 06:25:52 AM", "NaN", "NaN"], ["5098", "Minor cleanup of QueryPlanner", "Martin Traverso", "martint", "04/23/16, 07:31:48 PM", "NaN", "NaN"], ["5101", "Various analyzer/planner cleanups", "Martin Traverso", "martint", "04/27/16, 05:58:07 AM", "NaN", "NaN"], ["5103", "Overhaul Web UI", "Raghav Sethi", "raghavsethi", "06/21/16, 08:44:30 PM", "Here's the overhaul of the web UI I worked on over the hackathon and the last couple of weeks.\n### Design Goals\n- To make it easy to see how a cluster is doing\n- To make it easier to find queries in the query list\n- To help figure out query performance, stragglers and skew\n- To bring the Web UI into 2016\n### Non-goals\n- Looking decent at window sizes < 1024px\n- Supporting browsers that aren't Chrome, Safari or Firefox of a vintage > 3 months ago\n- Premature performance optimizations\n### New Features\n- Cluster heads up display: Now it's easy to see how your cluster is doing, and what sort of load it's under:\n  - Running, blocked and queued queries\n  - Aggregate data input rates (rows and bytes)\n  - Aggregate parallelism (drivers and CPU time)\n  - All of these also have sparklines for easy trend analysis\n- More powerful query list\n  - Finer-grained stats (execution, CPU and elapsed time)\n  - Powerful sorting and filtering options\n  - Limit number of displayed queries to improve page performance\n  - Regex search on query ID, source, user and query text\n- Completely re-written query detail page\n  - Query resource utilization statistics, and sparklines for parallelism, input data and memory\n  - All new stage-level statistics\n    - See timings, memory and task info\n    - Find stragglers and outliers with bar charts of CPU time and buffers for all tasks\n  - And best of all - everything on this page is live - you can see stats change as a query progresses (can also be paused)\n- All UI code has been either re-written or cleaned up dramatically. Everything is now in React.\n### Regressions\n- The new table framework doesn't support sticky headers out of the box (will add it back eventually)\n- Sorting the tasks table by multiple columns sequentially does not maintain previous sort orders\n- Removed the stage view in the query detail page (it seemed to be universally despised)\n- The density of information on the cluster overview page seems like a potential regression to me, but the demand for more information won the battle.\n- There are about 5x more bells and whistles than before - which means that I'm going to be fixing edge cases for weeks.\n### Feedback Required\n\ncc @martint @dain @haozhun @electrum @nileema \n- I've made dozens of copy and design decisions, please let me know if (a) you feel strongly about something or (b) it's easy to change.\n- I need to figure out how to roll this out in a way that we're not stuck with a broken UI in between releases. My current idea is to have a banner at the top of the old UI with a link to the new one first. Then I would change it to the new UI and put a link to the old one. Then I would remove the old one completely. Is this too complex? Can I just bite the bullet and replace the old UI?\n\nAs an aside, I've tried to avoid adding dependencies, especially into the build process. This means that I rejected all libraries that really wanted me to have Webpack or Browserify, or ones that the only way to get a piece of JS out of was to install a CoffeeScript-JSX Compiler (!!). This also means that we're using JSX, which is not great for performance, but I'm going to punt on compiling stuff until we know there's a perf problem.", "NaN"], ["5109", "Update tempto to 1.7", "Raghav Sethi", "raghavsethi", "04/27/16, 05:31:05 PM", "NaN", "NaN"], ["5111", "Add 0.146 release notes", "Dain Sundstrom", "dain", "04/25/16, 08:46:40 PM", "NaN", "NaN"], ["5112", "Add link to 0.146 release notes", "Dain Sundstrom", "dain", "04/25/16, 08:50:11 PM", "NaN", "NaN"], ["5117", "Fix leak during task cleanup", "Martin Traverso", "martint", "04/26/16, 12:47:30 AM", "There was a bug in TaskInfoFetcher that caused stop()\nnot getting called if the final task info had already\nbeen fetched.", "NaN"], ["5120", "Add new config file format for resource groups", "Christopher Berner", "cberner", "04/27/16, 11:45:55 PM", "The first 3 commits have already been reviewed", "NaN"], ["5123", "Fix race in CursorProcessor when invoking stateful functions", "Mark", "geraint0923", "04/27/16, 05:11:49 PM", "CursorProcessor should also be instantiated for each Driver just as PageProcessor, which could avoid the race condition in CursorProcessor. \n\nFix #5110 ", "NaN"], ["5125", "Replace BoundedExecutor with BoundedThreadPool", "David Phillips", "electrum", "05/01/16, 06:13:21 PM", "NaN", "NaN"], ["5126", "Minor documentation updates", "David Phillips", "electrum", "04/27/16, 12:26:12 AM", "NaN", "NaN"], ["5134", "Add 0.144.3 release notes", "Martin Traverso", "martint", "04/28/16, 12:12:25 AM", "NaN", "NaN"], ["5136", "Add query prioritization to resource groups", "Christopher Berner", "cberner", "04/28/16, 11:10:48 PM", "NaN", "NaN"], ["5137", "Return result of BackupStore shard deletion", "David Phillips", "electrum", "04/28/16, 12:31:10 AM", "NaN", "NaN"], ["5138", "Fix race in TestTaskExecutor.testTaskHandle", "Mark", "geraint0923", "04/29/16, 01:40:23 AM", "Fix #5116 ", "NaN"], ["5139", "Update existing Presto views in Hive metastore", "David Phillips", "electrum", "04/28/16, 04:54:30 AM", "When replacing an existing Presto view, update the view data\nin the Hive metastore rather than dropping and recreating it.", "NaN"], ["5146", "Fix NoClassDefFoundError for HDFS client", "David Phillips", "electrum", "04/28/16, 07:53:21 PM", "NaN", "NaN"], ["5148", "Various refactoring for Raptor", "David Phillips", "electrum", "04/29/16, 12:21:18 AM", "NaN", "NaN"], ["5149", "Allow multiple sources in a task", "Dain Sundstrom", "dain", "05/03/16, 12:56:28 AM", "NaN", "NaN"], ["5153", "Support old style row type with angle brackets", "Mark", "geraint0923", "05/03/16, 12:27:44 AM", "Put back the support of the old style row type like:\n    `row<bigint,double>('col0','col1')`\n\nSo that the new server could correctly parse the data from the old\nversion Presto server.\n\nI will also run it on verifier to see if it really solve the problem.", "NaN"], ["5154", "Fix testTryNoMergeProjections", "Raghav Sethi", "raghavsethi", "04/29/16, 01:42:14 AM", "NaN", "NaN"], ["5158", "Update partition split count when task status changes", "Dain Sundstrom", "dain", "04/29/16, 04:41:16 PM", "The count must be updated whenever the task status changes or\nthe cluster may deadlock since split slots marked as free.", "NaN"], ["5159", "Fix EXPLAIN tests", "David Phillips", "electrum", "04/29/16, 04:55:39 PM", "The tests were broken by db56e93d87a5ab15cea265ab76553c3283a1cd1d\ndue to an additional exchange added for local parallism.", "NaN"], ["5160", "Fix unsynchronized read", "Christopher Berner", "cberner", "04/29/16, 09:45:07 PM", "NaN", "NaN"], ["5161", "Add docs for resource groups", "Christopher Berner", "cberner", "04/30/16, 08:47:17 PM", "Also clean up the selector code", "NaN"], ["5163", "Update to Airlift 0.128-SNAPSHOT", "David Phillips", "electrum", "04/29/16, 08:02:00 PM", "NaN", "NaN"], ["5165", "Fix cli crash when trying to use `extract` with an invalid field", "Mark", "geraint0923", "05/03/16, 11:59:34 PM", "Fix #5156 ", "NaN"], ["5166", "Fix reading symlink file format", "Christopher Berner", "cberner", "04/30/16, 06:58:52 PM", "Symlinks may cross HDFS instances. Previously we were using the current\nfilesystem for reading the sizes of the targets of the symlink. We now\nalways create a new filesystem for each symlink target, that matches its\npath.", "NaN"], ["5167", "Generalize ShowCreate AST node", "Martin Traverso", "martint", "04/30/16, 06:51:57 PM", "This is to support other entity types without\nhaving to create one node for each.", "NaN"], ["5168", "Update release notes", "Christopher Berner", "cberner", "04/30/16, 07:18:05 PM", "NaN", "NaN"], ["5171", "Remove incompatible varchar Hive integration tests", "David Phillips", "electrum", "05/02/16, 06:36:41 AM", "Not all supported versions of Hive support varchar, so these tests\nneed to be written the same way as other non-universal types.", "NaN"], ["5173", "Update table property encoder to use strict types", "David Phillips", "electrum", "05/02/16, 07:37:17 PM", "NaN", "NaN"], ["5175", "Add maven plugin profiling extension", "Christopher Berner", "cberner", "05/03/16, 04:51:04 PM", "This can be enabled with `-Dbuildtime.output.log=true` and profiles the\ntime spent in each Maven plugin", "NaN"], ["5179", "Cleanup raptor one_split_per_bucket_threshold handling", "Dain Sundstrom", "dain", "05/03/16, 06:11:53 PM", "NaN", "NaN"], ["5180", "Allow only one SourceOperator in DriverFactory", "Dain Sundstrom", "dain", "05/03/16, 10:29:27 PM", "NaN", "NaN"], ["5183", "Implement IndexLookupSourceSupplier.destroy", "Dain Sundstrom", "dain", "05/03/16, 07:55:01 PM", "This method is alwasy called and currently throws UnsupportedOperationException, which\ndoes not fail queries, but causes a stacktrace in the logs.", "NaN"], ["5185", "Rename Hive table property clustered_by to bucketed_by", "David Phillips", "electrum", "05/04/16, 04:43:40 AM", "NaN", "NaN"], ["5186", "Fix Hive integration tests", "David Phillips", "electrum", "05/04/16, 04:04:57 PM", "NaN", "NaN"], ["5187", "Fix grouping set bug with predicate pushdown", "Raghav Sethi", "raghavsethi", "05/05/16, 12:14:50 AM", "Predicates should not be pushed down through aggregation nodes unless\nthe predicate can be applied to the intersection of the grouping set\ncolumns.", "NaN"], ["5197", "Add support for IntersectNode to plan printer", "Martin Traverso", "martint", "05/09/16, 11:32:43 PM", "Generally not needed since Intersect is never part\nof an optimized plan, but it's useful when debugging.", "NaN"], ["5198", "Add documentation for local file connector", "Nileema Shingte", "nileema", "05/05/16, 10:04:17 PM", "NaN", "NaN"], ["5200", "[WIP] Update 0.147 release notes", "Martin Traverso", "martint", "05/06/16, 04:33:31 PM", "NaN", "NaN"], ["5201", "AddExchanges cleanup + semijoin improvements", "Eric Hwang", "erichwang", "05/14/16, 02:14:53 AM", "NaN", "NaN"], ["5203", "Add docs for flatten function", "Christopher Berner", "cberner", "05/05/16, 07:55:09 PM", "NaN", "NaN"], ["5204", "Revert to BoundedExecutor", "David Phillips", "electrum", "05/05/16, 04:39:07 PM", "We are seeing weird scheduling latencies with BoundedThreadPool.", "NaN"], ["5205", "Add Raptor one split per bucket threshold configuration", "Dain Sundstrom", "dain", "05/05/16, 04:51:26 PM", "NaN", "NaN"], ["5207", "Fix formatting for docs", "Nileema Shingte", "nileema", "05/05/16, 10:34:09 PM", "NaN", "NaN"], ["5208", "Update tuning guide", "Christopher Berner", "cberner", "05/09/16, 07:29:42 PM", "Explain when it's appropriate to add more worker threads.", "NaN"], ["5210", "Add element_at for map", "Christopher Berner", "cberner", "05/10/16, 03:27:30 PM", "This is in preparation for changing the behavior of map subscript to\nthrow an exception when the key is not found.", "NaN"], ["5211", "Fix tests for raptor storage manager", "Nileema Shingte", "nileema", "05/06/16, 12:38:24 AM", "NaN", "NaN"], ["5212", "Change subscript to throw on missing key", "Christopher Berner", "cberner", "01/09/17, 05:09:31 PM", "Change map subscript to throw an exception for keys that do not exist in\nthe map. The old behavior can be acheived with the element_at function.", "NaN"], ["5214", "Reduce fixed per-group cost of array_agg", "Christopher Berner", "cberner", "05/06/16, 07:48:57 PM", "NaN", "NaN"], ["5218", "Add logging for Raptor backup operations", "David Phillips", "electrum", "05/06/16, 09:06:48 PM", "NaN", "NaN"], ["5220", "Improve Raptor error codes", "David Phillips", "electrum", "05/06/16, 10:16:15 PM", "NaN", "NaN"], ["5222", "Avoid logging exception when canceling a query", "Dain Sundstrom", "dain", "05/10/16, 04:37:56 PM", "A query is canceled by failing the query with a user canceled error code.  When\na query is failed the exception is logged, but this is not needed for a canceled\nquery.", "NaN"], ["5224", "Update default config values", "Christopher Berner", "cberner", "05/09/16, 04:00:15 PM", "NaN", "NaN"], ["5225", "Clean array documentation", "David Phillips", "electrum", "05/09/16, 04:23:57 PM", "NaN", "NaN"], ["5226", "Fix NoClassDefFoundError for KMSClientProvider", "David Phillips", "electrum", "05/09/16, 06:09:15 PM", "NaN", "NaN"], ["5230", "Update minimum Maven version to 3.3.9", "David Phillips", "electrum", "05/09/16, 08:11:18 PM", "NaN", "NaN"], ["5231", "Fail all tasks with correct error message before shutdown", "Nileema Shingte", "nileema", "05/11/16, 08:27:37 PM", "NaN", "NaN"], ["5232", "Add configuration to enable colocated joins", "Dain Sundstrom", "dain", "05/09/16, 09:22:48 PM", "NaN", "NaN"], ["5235", "Fix Hive distributed tests for alter table", "David Phillips", "electrum", "05/10/16, 01:55:31 AM", "Fixes #5234", "NaN"], ["5239", "Sort list of connectors in documentation", "David Phillips", "electrum", "05/10/16, 03:32:21 PM", "NaN", "NaN"], ["5242", "Only abort transaction if query actually failed", "Dain Sundstrom", "dain", "05/11/16, 03:42:37 AM", "A failure after the query completes is not a true query failure.  This\ncan happen when a limit races with a failure on a node.", "NaN"], ["5247", "Install latest JDK in Travis build", "David Phillips", "electrum", "05/11/16, 03:30:28 AM", "NaN", "NaN"], ["5250", "Remove unused PagesIndex method", "Dain Sundstrom", "dain", "05/11/16, 06:05:29 AM", "NaN", "NaN"], ["5251", "Add warning to all development config files", "Dain Sundstrom", "dain", "05/12/16, 01:33:51 AM", "NaN", "NaN"], ["5252", "Warn if server is not using G1 garbage collector", "Dain Sundstrom", "dain", "05/12/16, 01:28:02 AM", "NaN", "NaN"], ["5253", "Revert query state machine set failure cause", "Dain Sundstrom", "dain", "05/11/16, 06:01:22 AM", "The failure cause must be set before the state is changed, so listeners\ncan be observe the exception object", "NaN"], ["5258", "Add atop connector", "Christopher Berner", "cberner", "05/14/16, 03:14:07 AM", "NaN", "NaN"], ["5260", "Remove TestingRowConstructor", "Mark", "geraint0923", "05/19/16, 10:55:01 PM", "Fix #5193 ", "NaN"], ["5261", "Set finalQueryInfo only when we have the final task info", "Nileema Shingte", "nileema", "05/11/16, 10:01:09 PM", "Currently, we set the finalQueryInfo when the query transitions to\nfinished state. However, we might not have the final task info yet.\nThis change sets the finalQueryInfo when we get the final taskInfo for\nall tasks.", "NaN"], ["5264", "Make ExplainAnalyzeOperator wait for final stats from all stages", "Nileema Shingte", "nileema", "05/19/16, 11:48:54 PM", "NaN", "NaN"], ["5265", "Fix bugs and misleading names related to scheduler", "Haozhun Jin", "haozhun", "05/12/16, 01:39:05 AM", "cc @dain \n\nSee commit message body for details", "NaN"], ["5266", "Allow tables to have different layouts for read and write", "Dain Sundstrom", "dain", "05/14/16, 05:07:07 AM", "NaN", "NaN"], ["5270", "Fix tests to avoid OOMs", "Nileema Shingte", "nileema", "05/13/16, 06:13:21 PM", "NaN", "NaN"], ["5271", "Update TaskManagerConfig test to latest defaults", "Dain Sundstrom", "dain", "05/13/16, 06:54:59 AM", "NaN", "NaN"], ["5273", "Name threads properly", "David Phillips", "electrum", "05/13/16, 07:24:19 PM", "NaN", "NaN"], ["5274", "Update TaskManagerConfig test to latest defaults", "Dain Sundstrom", "dain", "05/13/16, 06:58:47 PM", "NaN", "NaN"], ["5277", "Add 0.148 release notes", "Dain Sundstrom", "dain", "05/13/16, 10:23:20 PM", "NaN", "NaN"], ["5282", "Add map_union aggregate function", "Haozhun Jin", "haozhun", "05/19/16, 07:05:08 PM", "This supersedes #3378 (contribution from @AshwinJay)\n\nI made a lot of changes to the original pull request. @martint mentioned that the best practice is to have someone take a quick look in such circumstances. Please take a quick look.", "NaN"], ["5284", "Minor fixes in Raptor", "Nileema Shingte", "nileema", "05/17/16, 07:20:17 PM", "- toString for BucketShards didn't render correctly for unbucketed tables \n- SQLState returned bu getSQLState can be null, so add a check for that. ", "NaN"], ["5286", "Add sanity check that RemoteSource is not locally parallelized", "Haozhun Jin", "haozhun", "05/17/16, 01:41:56 AM", "This commit mitigates a bug in trunk that can produce invalid local execution\nplans that can result incorrect results. With this commit, known reproduction\nof the bug will fail at local execution planning time.\n\nThe bug involves having a task.writer-count config set to 1 on coordinator and\nat least 2 on worker. Under such circumstance, an invalid plan will be\ngenerated. Executing such a plan can result in writing duplicated data to\ntables.", "NaN"], ["5293", "Remove Jackson dependency from parser", "David Phillips", "electrum", "05/17/16, 07:18:47 PM", "NaN", "NaN"], ["5295", "Change return type of ConnectorMetadata.getInsertLayout", "Haozhun Jin", "haozhun", "05/18/16, 07:04:29 PM", "As I try to write an implementation for ConnectorMetadata.getInsertLayout, I realized the original return type chosen in #5266 can be improved. Specifically, \n- `ConnectorTableLayout` (original return type) contains a lot of unnecessary information which is useless for INSERT, making the implementation more complex/expensive than necessary\n- `ConnectorTableLayout` could potentially be turned into a `ConnectorTableLayoutHandle`, which is a valid input for `ConnectorMetadata.getLayout`\n  - If `getLayout` can take table layout for both SELECT and INSERT, it essentially defeats the purpose of #5266.\n  - Although it seems this doesn't actually happen, it wasn't apparent. This causes unnecessary confusion for connector implementer.\n\ncc @dain ", "NaN"], ["5297", "Fix incorrect schema for KafkaQueryRunner", "Haozhun Jin", "haozhun", "05/17/16, 11:36:39 PM", "NaN", "NaN"], ["5299", "Remove redundant check for constant block from OrcPageSource", "Nileema Shingte", "nileema", "05/18/16, 12:24:58 AM", "NaN", "NaN"], ["5305", "Update 0.148 release notes", "Christopher Berner", "cberner", "05/18/16, 08:33:58 PM", "NaN", "NaN"], ["5306", "Fix formatting for SHOW CREATE TABLE documentation", "David Phillips", "electrum", "05/18/16, 11:39:19 PM", "NaN", "NaN"], ["5307", "Minor fixes for zip function", "David Phillips", "electrum", "05/18/16, 11:08:31 PM", "NaN", "NaN"], ["5308", "Fix grouping on aggregation arguments", "Raghav Sethi", "raghavsethi", "05/19/16, 10:52:23 PM", "Fixes #5267.", "NaN"], ["5309", "MySQL and PostgreSQL connector tests", "Brian Rickman", "brian-rickman", "05/24/16, 01:56:45 AM", "These SQL (or convention) based Tempto tests exercise the basic functionality of the MySQL and PostgreSQL connectors.  \n\nWhile adding MySQL and PostgreSQL Docker containers to the test environment I realized that the existing docker-compose.yml files contained many redundancies.  I refactored the docker-compose files to ease future maintenance.  \n\nI tested the docker-compose refactor by executing \"run_on_docker.sh [configuration] -x quarantine,big_query,profile_specific_tests\" for all five configurations.", "NaN"], ["5313", "Use varchar instead of date for Redis hash tests", "David Phillips", "electrum", "05/19/16, 01:31:39 AM", "We should make dates work, but this fixes the tests for now.", "NaN"], ["5325", "Add bucket number as hidden column to Raptor tables", "Nileema Shingte", "nileema", "06/20/16, 11:09:34 PM", "NaN", "NaN"], ["5333", "Fix running queries JMX stat when resource groups are enabled", "Christopher Berner", "cberner", "05/21/16, 12:46:16 AM", "NaN", "NaN"], ["5334", "Fix the failure when chaining `AT TIME ZONE`", "Mark", "geraint0923", "05/20/16, 10:24:02 PM", "Fix #5332 .\n\nChaining `AT TIME ZONE` results in the value part of the outer `AT TIME ZONE` being rewritten, while the map of types is not updated, then `getType` with the rewritten value expression will return a `null`,\nwhich causes the query failure.\n\nIn this commit, we invoke `getType` the old value part of the outer `AT TIME ZONE`, which should fix the problem without updating the map of types.", "NaN"], ["5335", "Table identity", "Joy Yao", "joy-yao", "10/24/16, 09:51:56 PM", "NaN", "NaN"], ["5336", "Remove unused annotation ForStorageManager", "David Phillips", "electrum", "05/21/16, 05:28:30 AM", "NaN", "NaN"], ["5339", "Avoid creating TaskInfo objects when we need only TaskStatus", "Nileema Shingte", "nileema", "06/20/16, 11:10:56 PM", "NaN", "NaN"], ["5345", "Remove checkstyle exclude for PagesIndexOrdering", "David Phillips", "electrum", "05/24/16, 08:14:41 PM", "NaN", "NaN"], ["5351", "Write different day's data to different shards for tables with temporal column", "Joy Yao", "joy-yao", "07/25/16, 09:29:20 PM", "NaN", "NaN"], ["5359", "Fix multiple complex aggregations with same arguments", "Raghav Sethi", "raghavsethi", "05/25/16, 06:50:21 PM", "NaN", "NaN"], ["5360", "Remove unused NodeIdUserAgentRequestFilter", "David Phillips", "electrum", "05/26/16, 01:57:07 AM", "NaN", "NaN"], ["5372", "Add event listener plugin", "Raghav Sethi", "raghavsethi", "08/17/16, 02:17:12 AM", "Closes #5254.", "NaN"], ["5374", "Fix the name for TableFinish in plan", "Joy Yao", "joy-yao", "05/28/16, 12:42:16 AM", "TableCommitNode has been renamed to TableFinishNode. Changing the name in plan to reflect this.", "NaN"], ["5383", "Optimize bit packed long decoding in ORC reader", "Christopher Berner", "cberner", "06/03/16, 11:14:09 PM", "This makes the bit unpacking ~5x faster in synthetic benchmarks and\nimproves overall CPU efficiency by 25% for the query I'm running (a\nscan-filter-aggregate)", "NaN"], ["5389", "Disable outer join->index join optimization for non-equi criteria", "Martin Traverso", "martint", "06/01/16, 07:54:24 PM", "NaN", "NaN"], ["5390", "Do not store node assignments for bucketed shards", "David Phillips", "electrum", "06/21/16, 12:53:02 AM", "NaN", "NaN"], ["5391", "Log shard when raptor backup times out", "Greg", "ggreg", "04/03/17, 04:39:55 PM", "Before this change, the error was only \"Shard existence check timed out\" without indication about the shard or the file that backs it.\nThese changes add the shard UUID. The error will then be \"Shard 139575c3-6ef8-48f5-a7ad-e4d739342382 existence check timeout\".", "NaN"], ["5392", "Release notes for 0.148", "Christopher Berner", "cberner", "06/02/16, 09:25:39 PM", "NaN", "NaN"], ["5393", "Add type matching checker for PlanSanityChecker", "Mark", "geraint0923", "06/22/16, 07:58:35 PM", "Fix #5076 \n\nIn order to use `getExpressionTypes` in `PlanSanityChecker.Checker` to calculate the type of expressions, we need to pass `SqlParser` into `LogicalPlanner`.", "NaN"], ["5396", "Reenable bucket writing and execution in Hive with bug fixes", "Haozhun Jin", "haozhun", "06/30/16, 07:58:02 PM", "NaN", "NaN"], ["5398", "Fix query rejection to fail query", "Christopher Berner", "cberner", "06/20/16, 02:31:03 PM", "Previously a 500 error was returned to the client, but the query was\nreported in the QUEUED state. However, it wasn't actually queued and\nwould never execute.", "NaN"], ["5400", "Fix RowNumberOperator ouput copy logic", "Dain Sundstrom", "dain", "06/03/16, 05:50:09 PM", "Copy code was using wrong SQL type to copy data.", "NaN"], ["5405", "Use primitive arrays in blocks to improve performance", "Dain Sundstrom", "dain", "06/20/16, 07:07:19 PM", "NaN", "NaN"], ["5415", "Add JMX counter for cpu time in shard compaction", "Nileema Shingte", "nileema", "07/22/16, 05:20:27 PM", "NaN", "NaN"], ["5417", "Fix force local scheduling for Hive", "David Phillips", "electrum", "06/20/16, 06:42:26 PM", "Only force local scheduling for splits when at least one address is\navailable for scheduling. This prevents a \"No nodes available\" error\nfor file systems like S3 that require remote access.", "NaN"], ["5419", "Fix formatting", "David Phillips", "electrum", "06/13/16, 07:32:01 PM", "NaN", "NaN"], ["5422", "Support CREATE TABLE LIKE", "Mark", "geraint0923", "10/12/16, 06:35:09 PM", "Fix #3771 \n\nThe syntax is like:\n\n```\n    CREATE TABLE test_table (\n        id bigint,\n        LIKE other_table)\n```", "NaN"], ["5428", "Only install CoordinatorModule in coordinator", "David Phillips", "electrum", "06/20/16, 10:31:20 PM", "This removes things like the statement resource from workers.", "NaN"], ["5430", "Make round() return NaN when input is NaN", "Martin Traverso", "martint", "06/10/16, 03:26:08 AM", "This was a regression in a recent change: 54eeef93\n\nFixes https://github.com/prestodb/presto/issues/5429", "NaN"], ["5431", "Fix interval type handling in REST API", "Christopher Berner", "cberner", "06/10/16, 09:42:37 PM", "Previously, intervals greater than 24 days returned the wrong value due\nto integer overflow.", "NaN"], ["5438", "Improve error messages for math operations", "David Phillips", "electrum", "06/20/16, 07:42:26 PM", "NaN", "NaN"], ["5440", "Fix PreferredProperties translation when partitioning on empty set", "Eric Hwang", "erichwang", "06/14/16, 11:19:15 PM", "Fixes #5439 ", "NaN"], ["5441", "Update release notes for 0.147", "Raghav Sethi", "raghavsethi", "06/13/16, 11:01:11 PM", "NaN", "NaN"], ["5444", "Fix race condition when task is aborted before it is started", "Nileema Shingte", "nileema", "06/14/16, 04:08:56 AM", "When a task is aborted before calling start on the remote task, the\nstate changes were not propogated correctly in the\nContinuousTaskInfoFetcher.", "NaN"], ["5447", "Fix aggregation partitioning preferences for local exchanges", "Raghav Sethi", "raghavsethi", "06/14/16, 09:45:39 PM", "cc @erichwang.\n\nTests are going to be in another PR that will be merged shortly.", "NaN"], ["5450", "Add shard organization in Raptor", "Nileema Shingte", "nileema", "07/25/16, 09:53:47 PM", "NaN", "NaN"], ["5452", "Fix handling of negative intervals", "David Phillips", "electrum", "06/15/16, 09:04:59 PM", "NaN", "NaN"], ["5457", "Respect nulls in ARRAY_AGG function", "Mark", "geraint0923", "06/21/16, 05:13:48 PM", "Fix #5449 ", "NaN"], ["5460", "Add resource group id to failure message when queue is full", "Christopher Berner", "cberner", "06/21/16, 07:00:52 PM", "NaN", "NaN"], ["5461", "Add HTTP backup store for Raptor\t", "David Phillips", "electrum", "06/21/16, 09:40:35 PM", "NaN", "NaN"], ["5464", "Use utility method in FileBackupStore", "David Phillips", "electrum", "06/17/16, 09:10:54 PM", "NaN", "NaN"], ["5467", "Make round() return -0.0 for small negative numbers", "Martin Traverso", "martint", "06/17/16, 09:40:16 PM", "The implementation calls Math.round, which returns a Java long.\nThe trick of negating the value does not work in that case,\nso make sure we negate the value after it has been converted\nback to double.", "NaN"], ["5468", "Add CPU limits to resource groups", "Christopher Berner", "cberner", "06/24/16, 09:34:45 PM", "NaN", "NaN"], ["5469", "Update 0.148 release notes for interval fixes", "David Phillips", "electrum", "06/19/16, 07:07:38 PM", "NaN", "NaN"], ["5475", "Remove expected type argument from assertInvalidFunction", "David Phillips", "electrum", "06/20/16, 07:51:36 PM", "NaN", "NaN"], ["5476", "Fix parsing of intervals in TestingPrestoClient", "David Phillips", "electrum", "06/20/16, 09:44:38 PM", "NaN", "NaN"], ["5477", "Refactor shard compaction in Raptor ", "Nileema Shingte", "nileema", "06/29/16, 06:54:49 PM", "NaN", "NaN"], ["5478", "Rename ResourceGroupConfig to FileResourceGroupConfig", "David Phillips", "electrum", "06/21/16, 12:57:54 AM", "NaN", "NaN"], ["5479", "Rename ForGracefulShutdown to ForNodeManager", "David Phillips", "electrum", "06/21/16, 08:35:45 PM", "NaN", "NaN"], ["5480", "Remove unused class MockQueryManager", "David Phillips", "electrum", "06/21/16, 08:35:30 PM", "NaN", "NaN"], ["5481", "Log background shard recovery failures", "David Phillips", "electrum", "06/20/16, 11:56:38 PM", "NaN", "NaN"], ["5483", "Update remote task stats to be less expensive ", "Nileema Shingte", "nileema", "06/21/16, 06:10:30 PM", "For busy clusters remote task stats can take up ~3% of the coordinator CPU, replace these stats with less expensive stats. ", "NaN"], ["5484", "Capture partition count in Plan", "Dain Sundstrom", "dain", "07/07/16, 05:15:07 AM", "When local parallel is enabled, the coordinator creates a query plan that\nrequires parallel execution, but parallelism is set on each working using\nthe task concurrency property.  If the coordinator and worker have\ndifferent settings for this property, the execution can fail.\n\nInstead of dividing the parallelism planning between the coordinator and\nthe worker, the coordinator makes the decision and records this in the\nplan.", "NaN"], ["5485", "Ensure the symbol reference is valid", "Martin Traverso", "martint", "06/21/16, 01:21:21 AM", "A equivalent check to this one was lost in the recent\nrefactoring to introduce symbol references", "NaN"], ["5489", "Improve system memory accounting for exchanges", "Christopher Berner", "cberner", "06/22/16, 12:33:09 AM", "NaN", "NaN"], ["5490", "Update HDFS timeout configs", "Christopher Berner", "cberner", "06/21/16, 07:01:47 PM", "NaN", "NaN"], ["5491", "Fix map subscript operator failure when using complex type as map key", "Mark", "geraint0923", "06/22/16, 05:58:18 PM", "Fix #5482 ", "NaN"], ["5493", "Record Presto version for Hive writes", "Mark", "geraint0923", "08/17/16, 10:33:44 PM", "Fix #5010 \n\nThis is PR is only for Hive connector, will add support for other connectors in separated PRs.", "NaN"], ["5494", "Minor cleanup", "Martin Traverso", "martint", "06/21/16, 08:37:24 PM", "NaN", "NaN"], ["5495", "Add 0.144.6 release notes", "Martin Traverso", "martint", "06/22/16, 04:03:50 PM", "NaN", "NaN"], ["5497", "Fix RowToRowCast failure when casting an UNKNOWN type", "Mark", "geraint0923", "06/22/16, 05:53:47 PM", "Fix #5496 ", "NaN"], ["5499", "Feature explain analyze v2", "Karol Sobczak", "sopel39", "01/27/17, 05:11:41 PM", "NaN", "NaN"], ["5503", "Improve accounting of memory used by local exchanges", "Christopher Berner", "cberner", "06/22/16, 06:44:41 PM", "NaN", "NaN"], ["5504", "Remove unavailable partitioning preferences from GroupId", "Raghav Sethi", "raghavsethi", "06/22/16, 10:16:21 PM", "Default behavior for GroupId was to pass down parent partitioning\npreferences unchanged and also pass up child actual partitioning\nunchanged. However:\n- Children of the GroupId plan node do not have access to new symbols\n  (i.e. passthrough symbols) that are generated by the GroupId. These\n  must be translated.\n- We cannot pass through preferences for symbols that are modified by\n  GroupId (i.e. the set difference of the distinct and common grouping\n  columns). These must be removed.\n\nFixes #5498.", "NaN"], ["5506", "Update JDBC connector for transaction API", "David Phillips", "electrum", "07/07/16, 07:43:48 PM", "NaN", "NaN"], ["5507", "Remove getDataSourceName from ConnectorSplitSource", "David Phillips", "electrum", "07/07/16, 08:33:13 PM", "NaN", "NaN"], ["5508", "Minor cleanups", "Martin Traverso", "martint", "06/22/16, 11:08:12 PM", "NaN", "NaN"], ["5510", "Update to airlift.slice 0.22", "Haozhun Jin", "haozhun", "06/23/16, 01:30:09 AM", "NaN", "NaN"], ["5511", "Update 0.149 release notes", "Martin Traverso", "martint", "06/24/16, 02:27:45 AM", "NaN", "NaN"], ["5517", "Reimplement getRange and copyRange for InterleavedBlock", "Haozhun Jin", "haozhun", "06/23/16, 08:18:14 PM", "Fixes #5516 \n- The new implementation does not support non-aligned getRange and copyRange.\n- Special handling for zero-column InterleavedBlock is removed because they\n  are no longer constructed.\n- This fixes bugs related handling of zero-column InterleavedBlock\n  and zero-position InterleavedBlock. Specifically,\n  - Before this commit, calling copyRegion on a zero-column InterleavedBlock\n    causes div-by-zero exception\n  - Before this commit, calling getRegion and semiCompact subsequently on a\n    zero-length block can produce a zero-column InterleavedBlock", "NaN"], ["5518", "Rename method in AbstractInterleavedBlock", "Haozhun Jin", "haozhun", "06/24/16, 07:28:51 PM", "NaN", "NaN"], ["5520", "Rename hive.ipc-ping-interval to hive.dfs.ipc-ping-interval", "Martin Traverso", "martint", "06/24/16, 02:27:17 AM", "NaN", "NaN"], ["5524", "Fix shading for JDBC driver", "David Phillips", "electrum", "06/25/16, 02:54:03 AM", "Remove some extra files which should not be included.", "NaN"], ["5527", "Remove unused BlockEncodingSerde from Raptor", "David Phillips", "electrum", "06/30/16, 12:16:29 AM", "NaN", "NaN"], ["5528", "Remove unused getEstimatedSize method from BlockEncoding", "David Phillips", "electrum", "06/27/16, 06:53:01 PM", "NaN", "NaN"], ["5529", "Fix dependencies for atop connector", "David Phillips", "electrum", "06/27/16, 05:30:01 PM", "Remove provided scope from dependencies not part of the SPI.", "NaN"], ["5536", "Fix SQL injection in Raptor ShardMetadataRecordCursor", "David Phillips", "electrum", "07/12/16, 01:03:25 AM", "NaN", "NaN"], ["5539", "Fix race in task results API", "David Phillips", "electrum", "07/06/16, 01:27:57 AM", "A race in Jetty causes the results API to randomly fail with\ngarbage results (most frequently a 500 status code).", "NaN"], ["5542", "Remove old broken rcfile reader", "Dain Sundstrom", "dain", "07/02/16, 01:29:45 AM", "A new optimized rcfile reader is in #2450", "NaN"], ["5551", "Add hidden $PATH column to Hive connector", "Mark", "geraint0923", "08/10/16, 07:06:28 PM", "Fix #5486 \nThis hidden column is named $PATH in Hive.", "NaN"], ["5552", "Filter non-existing shards before organizing them", "Nileema Shingte", "nileema", "07/19/16, 09:58:59 PM", "NaN", "NaN"], ["5553", "Verifier shadow", "Christopher Berner", "cberner", "07/08/16, 07:27:48 PM", "NaN", "NaN"], ["5554", "Fix typo", "Christopher Berner", "cberner", "07/05/16, 04:05:51 PM", "NaN", "NaN"], ["5556", "Improve documentation for MySQL date time functions", "Haozhun Jin", "haozhun", "07/11/16, 05:24:36 PM", "I changed the text slightly after submitting the pull request. Use this screenshot to understand the format only:\n\n![screen shot 2016-07-01 at 11 21 56 am](https://cloud.githubusercontent.com/assets/2020503/16530730/560c4c1a-3f7e-11e6-9462-257176484eaf.png)", "NaN"], ["5558", "Use succinct size for Raptor recovery log message", "David Phillips", "electrum", "07/02/16, 01:11:37 AM", "NaN", "NaN"], ["5559", "Use succinctBytes helper method", "David Phillips", "electrum", "07/03/16, 04:10:22 AM", "NaN", "NaN"], ["5573", "Externalize error types", "Martin Traverso", "martint", "07/05/16, 11:29:04 PM", "This decouples the error category from the error code, so it allows\nplugins to define error codes that are classified as anything other\nthan EXTERNAL.", "NaN"], ["5587", "Minor UI fixes", "Raghav Sethi", "raghavsethi", "07/06/16, 10:31:08 PM", "Fixes #5562, #5561, and #5560 ", "NaN"], ["5589", "Add 0.150 release notes", "Eric Hwang", "erichwang", "07/06/16, 10:15:05 PM", "NaN", "NaN"], ["5591", "Add additional test for TypedSet", "David Phillips", "electrum", "07/20/16, 05:49:50 PM", "This test makes the API usage more clear.", "NaN"], ["5597", "Move static aggregation instances out of implementation classes", "Mark", "geraint0923", "07/12/16, 10:18:52 PM", "Fix #5544 ", "NaN"], ["5598", "Fix build for JDBC connector", "David Phillips", "electrum", "07/07/16, 08:32:12 PM", "NaN", "NaN"], ["5605", "Support more types in BlackHole connector", "Andrii Rosa", "arhimondr", "08/08/16, 03:54:37 PM", "Support TINYINT, SMALLINT, INTEGER and DECIMAL type in BlackholePageSource", "NaN"], ["5607", "Native implementation of aggregate functions for FLOAT", "Maciej 'mac' Grzybek", "maciejgrzybek", "07/13/16, 03:49:52 PM", "It is to avoid confusion when coercions to DOUBLE are used and e.g. sum(float) = double.\nE.g. when single value is given to sum(float) it would return double value which would not equal input when coercions are used.\nSmall commits added for review convenience, thus commit-by-commit approach is preferred.\n\nInternal TD review: https://github.com/Teradata/presto/pull/242\n\nCC: @cberner ", "NaN"], ["5608", "Restore missing validation for scalar operators", "David Phillips", "electrum", "07/08/16, 11:34:28 PM", "NaN", "NaN"], ["5610", "Add Hive bucketing warning to 0.150 release notes", "David Phillips", "electrum", "07/08/16, 11:45:57 PM", "<img width=\"1090\" alt=\"screen shot 2016-07-08 at 4 41 09 pm\" src=\"https://cloud.githubusercontent.com/assets/9230/16704355/d09c9f82-452a-11e6-855c-f6b4a07dea79.png\">", "NaN"], ["5611", "Use SQL comments for example output in docs", "David Phillips", "electrum", "07/09/16, 12:00:32 AM", "NaN", "NaN"], ["5614", "Add annotations for registering window functions", "David Phillips", "electrum", "07/12/16, 04:08:03 PM", "NaN", "NaN"], ["5616", "Remove unused TypeManager from ML plugin", "David Phillips", "electrum", "07/12/16, 04:13:27 PM", "NaN", "NaN"], ["5623", "Remove unnecessary calls to parameterizedTypeName", "David Phillips", "electrum", "07/12/16, 04:31:16 PM", "NaN", "NaN"], ["5624", "Add functions to SPI", "David Phillips", "electrum", "07/19/16, 09:32:18 PM", "NaN", "NaN"], ["5625", "Add flag to restore legacy array_agg behavior", "Mark", "geraint0923", "07/12/16, 10:20:53 PM", "The flag is named experimental.legacy-array-agg, which is default to be\nfalse. When it is set to true, array_agg will ignore the null values.\n\nFix #5622 \n\n@cberner Could you please suggest what default value shoud be used for this new flag? Currently I use `false` as default.", "NaN"], ["5626", "Force single writer when partitioning scheme exists", "Haozhun Jin", "haozhun", "07/11/16, 10:19:07 PM", "This fixes a regression introduced by 4ed06d97", "NaN"], ["5633", "Set VALUES driver instance count to 1", "Dain Sundstrom", "dain", "07/12/16, 10:14:48 PM", "NaN", "NaN"], ["5638", "Update TypeValidator to allow type only coercion", "Mark", "geraint0923", "07/15/16, 11:38:05 PM", "Fix #5630 ", "NaN"], ["5647", "Add reverse function for array", "Mark", "geraint0923", "07/19/16, 07:03:48 PM", "Implement ARRAY_REVERSE function\n\nCloses #5636 ", "NaN"], ["5650", "Add JDBC driver test for setting time zone", "David Phillips", "electrum", "08/01/16, 07:25:21 PM", "NaN", "NaN"], ["5652", "Fix aggregation split in AddLocalExchanges", "Haozhun Jin", "haozhun", "07/14/16, 12:30:59 AM", "This fixes a correctness issue where the plan has duplicate partial aggregations when\n- task_concurrency is set to 1, and\n- aggregation does not require remote exchange, but require local exchange\n  (node partitioned but not stream partitioned).", "NaN"], ["5653", "Revert \"Capture partition count in Plan\"", "Dain Sundstrom", "dain", "07/13/16, 11:01:49 PM", "This reverts:\n6c44dfbaf789d4ebc47f3e5615399a8849347e68\n4ed06d97c22d67a685fc716597f33b1767748bd5\n5d3f5cf7b68e9ad3c2cf3d9e56de7057e247c9c8\n408766219ac5d6c5f151065f66e53b0d26aa2a7d", "NaN"], ["5655", "Add TypeTranslator to Hive connector", "Mark", "geraint0923", "07/19/16, 07:02:04 PM", "The specific Hive connector could change their type-to-HiveType\nconversion rules by implementing a new TypeInfoResolver.", "NaN"], ["5658", "Remove unnecessary inspection suppression", "Christopher Berner", "cberner", "07/20/16, 05:13:47 PM", "This inspection was fixed in 2016.2", "NaN"], ["5669", "Add documentation for cosine_similarity function", "Christopher Berner", "cberner", "07/15/16, 08:06:35 PM", "NaN", "NaN"], ["5674", "Add getInt method to LongArrayBlock", "Raghav Sethi", "raghavsethi", "07/18/16, 06:08:39 PM", "Some aggregations on narrower integral types fail due to mismatches\nbetween intermediate state and input/output state. This is a temporary\nhack to get around the problem. This will be fixed by getting rid of\nimplicit combine functions.\n\n@haozhun mentioned @cberner is planning to do this. This became a problem after @dain added the IntArrayBlock/LongArrayBlock performance optimizations.", "NaN"], ["5676", "Remove redundant field in CreateViewTask", "David Phillips", "electrum", "07/19/16, 07:50:17 PM", "NaN", "NaN"], ["5677", "Remove redundant methods from MetadataUtil", "David Phillips", "electrum", "07/19/16, 07:49:45 PM", "NaN", "NaN"], ["5678", "Add cosine_similarity function", "Raghav Sethi", "raghavsethi", "07/19/16, 07:21:36 PM", "NaN", "NaN"], ["5679", "Support Block type for FieldReference in ExpressionInterpreter", "Haozhun Jin", "haozhun", "07/16/16, 01:49:36 AM", "NaN", "NaN"], ["5680", "Add request URI to more exceptions in HttpPageBufferClient", "Haozhun Jin", "haozhun", "07/16/16, 03:39:18 AM", "NaN", "NaN"], ["5692", "Fail query when non-equi conjuncts exist for OUTER JOINs", "Haozhun Jin", "haozhun", "07/18/16, 10:38:19 PM", "This will be merged into hotfix-0.144, not master. This fixes regression in https://github.com/prestodb/presto/pull/3954. The fix is no longer relevant in master branch because non-equi join support for OUTER JOIN has since been added.\n\nThis commit fixes a regression in hotfix-0.144 branch where queries that contain non-equi conjuncts for OUTER JOINs were not properly rejected. Instead, such conjuncts were ignored.", "NaN"], ["5693", "Add release notes for 0.151", "Christopher Berner", "cberner", "07/18/16, 09:29:34 PM", "NaN", "NaN"], ["5695", "Add release notes for 0.144.7", "Haozhun Jin", "haozhun", "07/19/16, 07:36:12 PM", "NaN", "NaN"], ["5696", "Fix handling of failure class without canonical name", "David Phillips", "electrum", "07/19/16, 07:49:07 PM", "This fixes an NPE for things like anonymous classes. However, I'm not sure why `getCanonicalName()` is used at all rather than `getName()`. @dain?", "NaN"], ["5697", "Add Raptor system table for table stats", "David Phillips", "electrum", "07/20/16, 12:04:51 AM", "NaN", "NaN"], ["5704", "Add test for TypeTranslator in Hive connector", "Mark", "geraint0923", "07/20/16, 07:31:51 PM", "Also update the integration test to fit TypeTranslator.", "NaN"], ["5705", "Clarify guarantees for deserialize and @CombineFunction", "Haozhun Jin", "haozhun", "07/21/16, 05:54:03 PM", "Additionally, this commit removes unncessary checks in deserialize whose\nresult is guaranteed.", "NaN"], ["5708", "Remove owner from ConnectorTableMetadata", "David Phillips", "electrum", "08/02/16, 04:05:19 PM", "This field is only used for table creation and is not needed now that we have `ConnectorSession`.", "NaN"], ["5717", "Fix function resolution for varchar(x)", "Christopher Berner", "cberner", "08/04/16, 10:26:46 PM", "NaN", "NaN"], ["5718", "Fix cosine_similiarity tests", "Christopher Berner", "cberner", "07/21/16, 12:10:17 AM", "NaN", "NaN"], ["5720", "3 minor metastore related change in Hive connector", "Haozhun Jin", "haozhun", "07/21/16, 05:58:56 PM", "NaN", "NaN"], ["5728", "Add config property to read Hive byte, short and integer as BIGINT", "Raghav Sethi", "raghavsethi", "07/25/16, 09:24:44 PM", "NaN", "NaN"], ["5731", "Update release notes", "Christopher Berner", "cberner", "07/27/16, 05:03:16 PM", "NaN", "NaN"], ["5740", "Add catalog-based access control to SystemAccessControl", "Raghav Sethi", "raghavsethi", "07/28/16, 11:49:29 PM", "NaN", "NaN"], ["5741", "Replace getServices in Plugin with explicit getters", "David Phillips", "electrum", "08/10/16, 01:52:29 AM", "NaN", "NaN"], ["5745", "Correctly handle errors from url_decode", "David Phillips", "electrum", "08/12/16, 07:20:47 PM", "This makes the function work with try().", "NaN"], ["5746", "Fix failure to read Hive tables with null owner", "Haozhun Jin", "haozhun", "07/26/16, 12:44:06 AM", "NaN", "NaN"], ["5747", "Update default task info refresh interval", "Nileema Shingte", "nileema", "07/25/16, 07:28:27 PM", "NaN", "NaN"], ["5748", "Fix query failure when referencing a field of a NULL row", "Mark", "geraint0923", "07/26/16, 10:50:55 PM", "We try to pre-calculate the row field value if possible when planning.\nBefore this commit, we fail to handle the case when the row itself is\nevaluated to be null, which will lead to a NullPointerException.\n\nIn this commit, the row field will be pre-calculated as a null if the\nrow is evaluated to be a null.", "NaN"], ["5749", "Revert change to the default value of task info refresh interval", "Nileema Shingte", "nileema", "07/25/16, 10:04:08 PM", "This can add up to 1 second latency to the wall time of queries.\nReset this to the previous value of 200ms until we can decouple query\nstats from query completion.", "NaN"], ["5750", "Explicitly set wasNull flag in DereferenceCodeGenerator", "Mark", "geraint0923", "08/03/16, 10:28:43 PM", "There is no guarntee that all generators set wasNull, some unexpected\nwasNull flag might fall through, which could leads to the issue like https://github.com/prestodb/presto/issues/5367.\n\nTo avoid this issue, wasNull flag needs to be set explicitly in\nDereferenceCodeGenerator.", "NaN"], ["5755", "Assert SQL formatting in TestSqlParser", "David Phillips", "electrum", "08/09/16, 12:20:07 AM", "NaN", "NaN"], ["5760", "Add security to atop connector", "Christopher Berner", "cberner", "08/06/16, 01:06:22 AM", "NaN", "NaN"], ["5763", "Add port to task list on query detail page", "Raghav Sethi", "raghavsethi", "08/11/16, 03:53:11 AM", "Required reducing the font size, and converting some headers to icons. Supersedes #5513.", "NaN"], ["5766", "Document result order for MaxByN and MinByN", "Haozhun Jin", "haozhun", "07/29/16, 05:59:01 PM", "NaN", "NaN"], ["5767", "Fixup some exception handling", "Dain Sundstrom", "dain", "07/29/16, 09:06:26 PM", "NaN", "NaN"], ["5768", "Fix verifier write shadowing to deal with LIMIT queries", "Raghav Sethi", "raghavsethi", "07/29/16, 11:51:20 PM", "The verifier introduces a LIMIT 0 while determining column\ntypes. However, if the query being shadowed already has a LIMIT\nclause, then parsing fails. This removes the limit from the query\nbefore introducing the zero limit.", "NaN"], ["5778", "Reimplement SignatureBinder to support repeated literal variable", "Haozhun Jin", "haozhun", "09/26/16, 10:25:59 PM", "NaN", "NaN"], ["5780", "Fix flaky test in TestShardOrganizationManager", "David Phillips", "electrum", "08/02/16, 03:26:33 PM", "NaN", "NaN"], ["5781", "Cleanup varchar(x) handling", "Christopher Berner", "cberner", "11/23/16, 11:04:59 PM", "NaN", "NaN"], ["5783", "Fix bug where hive.bucket-execution is not honored", "Haozhun Jin", "haozhun", "08/02/16, 08:24:10 PM", "Before this fix,\n- If bucketExecutionEnabled is true, insert into bucketed table would work\n  (as if bucketExecutionWriting is enabled);\n- If bucketExecutionEnabled is false, insert would either fail or create a\n  corrupted partition in an existing table.", "NaN"], ["5784", "Fix naming of JMX stat for ThriftHiveMetastore", "David Phillips", "electrum", "08/09/16, 04:51:57 AM", "This renames AllViews to GetAllViews.", "NaN"], ["5789", "Initial Java 9 support", "Martin Traverso", "martint", "08/04/16, 07:00:32 PM", "NaN", "NaN"], ["5793", "Improve mechanism for getting final task stats", "Nileema Shingte", "nileema", "08/29/16, 09:17:17 PM", "NaN", "NaN"], ["5794", "Update to airbase 56", "Martin Traverso", "martint", "08/05/16, 12:55:43 AM", "NaN", "NaN"], ["5813", "Clarify distributed joins in tuning doc", "Nezih Yigitbasi", "nezihyigitbasi", "08/08/16, 06:50:21 PM", "@cberner Based on our discussion, this PR is for clarifying that only the right side of a join has to fit in the cluster's memory with distributed joins.", "NaN"], ["5818", "Transaction-ish delete/insert support for Hive", "Haozhun Jin", "haozhun", "09/03/16, 12:33:00 AM", "NaN", "NaN"], ["5826", "Spilling to disk for aggregation v2", "Piotr Nowojski", "pnowojski", "11/24/16, 06:48:41 PM", "Supersedes https://github.com/prestodb/presto/pull/5142. Notable changes:\n- reordered commits\n- removed `max-entries-before-spill` property", "NaN"], ["5827", "Add file based security to Hive connector", "Christopher Berner", "cberner", "08/15/16, 04:17:14 PM", "NaN", "NaN"], ["5829", "Fix potential connection leak in Raptor ShardIterator", "David Phillips", "electrum", "08/10/16, 08:59:08 PM", "Fixes #5762", "NaN"], ["5830", "Removed unused import", "Martin Traverso", "martint", "08/10/16, 12:52:08 AM", "NaN", "NaN"], ["5831", "Fix broken analyzer unit test", "Martin Traverso", "martint", "08/10/16, 01:49:23 AM", "NaN", "NaN"], ["5837", "Fix SQL formatting and view creation for grouping sets", "David Phillips", "electrum", "08/10/16, 11:30:53 PM", "NaN", "NaN"], ["5841", "Fix test failure in TestStatementBuilder", "David Phillips", "electrum", "08/11/16, 12:27:24 AM", "NaN", "NaN"], ["5851", "Add throwable cause to Table/PartitionAlreadyExistsException", "Haozhun Jin", "haozhun", "08/15/16, 06:49:57 PM", "NaN", "NaN"], ["5852", "Update documentation for CALL", "David Phillips", "electrum", "08/28/16, 01:21:13 AM", "NaN", "NaN"], ["5862", "Add local disk quota for Raptor", "David Phillips", "electrum", "08/15/16, 11:13:35 PM", "NaN", "NaN"], ["5863", "Handle partition key and hidden columns in wrapper in Hive", "Mark", "geraint0923", "09/23/16, 04:53:04 PM", "NaN", "NaN"], ["5866", "Export JMX counters for NodeManager node counts", "Eric Hwang", "erichwang", "08/16/16, 08:42:13 PM", "NaN", "NaN"], ["5870", "Update to airlift 0.134", "Martin Traverso", "martint", "08/16/16, 05:30:59 PM", "Fixes an where tools.jar needed to be available in the classpath\nin order for airlift's JMX agent code to work.\n\nFixes #5854", "NaN"], ["5871", "Revert \"Add support for Char in presto-orc\"", "Christopher Berner", "cberner", "08/16/16, 09:27:20 PM", "This reverts commit 64747a03682fbe4abf5e18dc7f2d04cf1edd2ce5.", "NaN"], ["5872", "Update slice to 0.24", "Nileema Shingte", "nileema", "08/16/16, 07:31:16 PM", "NaN", "NaN"], ["5875", "Add release notes and docs", "Christopher Berner", "cberner", "08/18/16, 09:43:12 PM", "NaN", "NaN"], ["5876", "Optimize TestFullOrcReader", "Christopher Berner", "cberner", "08/17/16, 12:21:31 AM", "This reduces the running time by ~30%", "NaN"], ["5880", "Fix injection error for EventListenerManager", "Raghav Sethi", "raghavsethi", "08/17/16, 05:36:13 PM", "NaN", "NaN"], ["5881", "Optimize dictionary blocks and orc tests", "Christopher Berner", "cberner", "08/30/16, 07:47:11 PM", "This reduces the time for the full orc tests by ~20%", "NaN"], ["5884", "Add release notes for 0.152", "Nileema Shingte", "nileema", "08/19/16, 07:04:33 PM", "NaN", "NaN"], ["5885", "Add tinyint, smallint and integer pushdown for JDBC connectors", "Mark", "geraint0923", "08/19/16, 01:42:51 AM", "NaN", "NaN"], ["5886", "Record Presto query id for Hive writes", "Mark", "geraint0923", "08/26/16, 08:01:32 PM", "NaN", "NaN"], ["5890", "Add documentation for event listener plugin", "Raghav Sethi", "raghavsethi", "09/28/16, 01:02:00 AM", "NaN", "NaN"], ["5895", "Update release notes for 0.152", "David Phillips", "electrum", "08/19/16, 07:21:37 PM", "NaN", "NaN"], ["5899", "Fix product tests", "Maciej 'mac' Grzybek", "maciejgrzybek", "08/19/16, 06:34:33 PM", "Fixes product tests, regressions introduced by i.e.:\n- Fix column sizes of types for JDBC driver\n- Add product tests covering complex use of window functions\n- Change to only jackson.annotations being provided\n- Fix JDBC metadata for floating point\n## \n\nTD internal PR: https://github.com/Teradata/presto/pull/308", "NaN"], ["5901", "Hide all non-annotation Jackson classes from plugins", "David Phillips", "electrum", "08/19/16, 10:02:57 PM", "This hides classes in other packages like \"datatype\".", "NaN"], ["5902", "Add setup/teardown queries to verifier events", "Christopher Berner", "cberner", "08/19/16, 08:01:49 PM", "Also print the setup/teardown queries when a query fails", "NaN"], ["5903", "Remove non-annotation Jackson dependencies from SPI", "David Phillips", "electrum", "08/19/16, 07:40:28 PM", "NaN", "NaN"], ["5904", "Remove unused assembly descriptor for ML plugin", "David Phillips", "electrum", "08/19/16, 10:24:10 PM", "NaN", "NaN"], ["5905", "Remove javax.inject from SPI classes", "David Phillips", "electrum", "08/19/16, 11:10:22 PM", "NaN", "NaN"], ["5906", "Remove provided scope from record-decoder dependencies", "David Phillips", "electrum", "08/19/16, 11:21:48 PM", "NaN", "NaN"], ["5908", "Do not include provided scope dependencies in plugins", "David Phillips", "electrum", "08/19/16, 11:44:25 PM", "NaN", "NaN"], ["5909", "Use index for inner join with filter", "Joy Yao", "joy-yao", "08/19/16, 11:50:41 PM", "NaN", "NaN"], ["5914", "Swallow exception on cleanup to expose the underlying cause of failure", "Maciej 'mac' Grzybek", "maciejgrzybek", "08/22/16, 06:32:19 PM", "Failure on tear down is logged and exception is swallowed. This allows propagating real cause of test failure.\nSimilar mechanism is used in AbstractTestHiveClient.\n\n---\n\nThis is to address https://github.com/prestodb/presto/pull/4985#discussion_r59222727.", "NaN"], ["5917", "Add missing dependency for presto-product-tests", "David Phillips", "electrum", "08/22/16, 07:25:47 PM", "NaN", "NaN"], ["5920", "Eliminate Raptor database queries for column metadata", "David Phillips", "electrum", "08/22/16, 08:47:33 PM", "NaN", "NaN"], ["5921", "Fix SHOW CREATE TABLE for Raptor", "Raghav Sethi", "raghavsethi", "09/01/16, 01:45:33 AM", "- Add ordering, bucketing and temporal properties to TableColumn\n- Add distribution name and bucket count to RaptorTableHandle\n\nFixes #5797 ", "NaN"], ["5923", "Fix race in event listener test", "Raghav Sethi", "raghavsethi", "08/24/16, 07:44:09 PM", "Changed TestEventListener to be single threaded. Also improved test to\nre-use query runner and session.", "NaN"], ["5927", "Fix getSizeInBytes for VariableWidth- and Interleaved-Block", "Haozhun Jin", "haozhun", "08/23/16, 07:13:11 PM", "NaN", "NaN"], ["5931", "Remove provided scope from jackson-databind", "Raghav Sethi", "raghavsethi", "08/23/16, 07:13:41 PM", "NaN", "NaN"], ["5933", "Fix dependencies", "David Phillips", "electrum", "08/23/16, 08:51:42 PM", "NaN", "NaN"], ["5936", "Raptor cleanup", "David Phillips", "electrum", "08/24/16, 01:24:16 AM", "NaN", "NaN"], ["5945", "Upgrade to units 1.0", "Christopher Berner", "cberner", "08/25/16, 06:46:11 PM", "NaN", "NaN"], ["5948", "Remove teradata-functions dependency on presto-main", "David Phillips", "electrum", "08/25/16, 11:45:25 PM", "NaN", "NaN"], ["5956", "Resource group refactoring", "Christopher Berner", "cberner", "08/26/16, 06:25:49 PM", "refactoring to support making resource group management pluggable.", "NaN"], ["5957", "Fix test for event listener", "Nileema Shingte", "nileema", "08/29/16, 06:17:18 PM", "Run the tests with task concurrency of 1 so that number of\nsplit completion events is simpler to reason about.", "NaN"], ["5959", "Update to Jackson 2.8.1", "David Phillips", "electrum", "08/25/16, 11:36:17 PM", "NaN", "NaN"], ["5960", "Use whitelist model for plugin class loading", "David Phillips", "electrum", "08/28/16, 01:22:25 AM", "NaN", "NaN"], ["5974", "Move QueryId to SPI", "Christopher Berner", "cberner", "08/26/16, 10:19:08 PM", "NaN", "NaN"], ["5976", "Fix race condition in Query canncel state change", "Dain Sundstrom", "dain", "08/26/16, 11:06:57 PM", "The failure cause must be recorded before the query state\nis changed or a user can observe the FAILED state without\nany failure information.\n\nFixes #5975", "NaN"], ["5978", "Move resource group configuration classes to SPI", "Christopher Berner", "cberner", "08/27/16, 04:44:48 AM", "NaN", "NaN"], ["5981", "Add optimizer to push down aggregation through Exchange", "Mark", "geraint0923", "09/06/16, 06:20:51 PM", "Supersedes #4955 ", "NaN"], ["5986", "Make resource group configuration pluggable", "Christopher Berner", "cberner", "08/31/16, 09:28:22 PM", "NaN", "NaN"], ["5987", "Use @SqlNullable for functions instead of @Nullable", "David Phillips", "electrum", "08/30/16, 06:45:23 PM", "NaN", "NaN"], ["5992", "Use columnValue to handle all prefilled column value", "Mark", "geraint0923", "08/30/16, 07:17:42 PM", "`partitionKey` could be null if the current column is a hidden column,\nwhich might cause NPE and query failure.", "NaN"], ["5999", "Add JOL to list of allowed SPI classes", "David Phillips", "electrum", "08/31/16, 05:46:01 PM", "NaN", "NaN"], ["6003", "Add malformed JSON checks to Web UI query detail page", "Raghav Sethi", "raghavsethi", "09/01/16, 12:09:48 AM", "NaN", "NaN"], ["6004", "Simplify HivePageSink", "Dain Sundstrom", "dain", "10/29/16, 03:28:33 AM", "NaN", "NaN"], ["6008", "Release notes, docs, and cleanup for resource groups", "Christopher Berner", "cberner", "09/01/16, 09:57:20 PM", "NaN", "NaN"], ["6014", "Add more malformed JSON checks to Web UI query detail page", "Raghav Sethi", "raghavsethi", "09/02/16, 03:21:53 PM", "Fixes #5977", "NaN"], ["6016", "Fix Hive S3 tests", "David Phillips", "electrum", "09/02/16, 01:32:28 AM", "NaN", "NaN"], ["6017", "Fix aggregation functions from plugins", "David Phillips", "electrum", "09/04/16, 12:52:24 AM", "NaN", "NaN"], ["6027", "Hotfix 0.152.1", "Haozhun Jin", "haozhun", "09/03/16, 04:15:42 AM", "NaN", "NaN"], ["6029", "Add new RcFile writer", "Dain Sundstrom", "dain", "02/26/17, 10:17:46 PM", "NaN", "NaN"], ["6030", "Fix warnings in StatementResource", "David Phillips", "electrum", "09/04/16, 01:05:23 AM", "NaN", "NaN"], ["6031", "Use ConditionalModule from Airlift", "David Phillips", "electrum", "09/04/16, 04:08:45 PM", "NaN", "NaN"], ["6033", "Remove outdated INSERT limitation from documentation", "David Phillips", "electrum", "09/05/16, 06:17:36 PM", "NaN", "NaN"], ["6034", "Minor Raptor cleanup", "David Phillips", "electrum", "09/05/16, 06:17:20 PM", "NaN", "NaN"], ["6038", "Allow configuring Raptor to use MySQL", "David Phillips", "electrum", "09/06/16, 02:56:48 PM", "NaN", "NaN"], ["6039", "Add support for CREATE SCHEMA, ALTER SCHEMA, DROP SCHEMA", "David Phillips", "electrum", "09/27/16, 10:10:29 PM", "NaN", "NaN"], ["6040", "Fix doc", "Joy Yao", "joy-yao", "09/05/16, 10:04:49 PM", "NaN", "NaN"], ["6041", "Fix documentation", "Joy Yao", "joy-yao", "09/07/16, 05:50:49 PM", "NaN", "NaN"], ["6044", "Add ConnectorId to enforce separation from catalog name", "Dain Sundstrom", "dain", "09/19/16, 06:11:28 PM", "NaN", "NaN"], ["6046", "Remove setOptionalConfig from Plugin", "David Phillips", "electrum", "09/07/16, 05:03:12 AM", "NaN", "NaN"], ["6051", "Fix stats to not include queued time in planning time", "Igor Demura", "idemura", "09/20/16, 09:38:24 PM", "Fixes #5980 ", "NaN"], ["6057", "Split out InternalNodeManager from NodeManager", "David Phillips", "electrum", "09/12/16, 04:16:25 AM", "NaN", "NaN"], ["6062", "Make PlanOptimizers not implement Provider", "Martin Traverso", "martint", "09/07/16, 07:38:24 PM", "We were previously relying on that for injection, which makes it\nhard to track down where the list of optimizers comes from\nwhen debugging.", "NaN"], ["6064", "Fix incorrect effective predicate derivation for GROUP BY ()", "Martin Traverso", "martint", "09/07/16, 08:08:46 PM", "It was incorrectly deriving \"false\" if the source expression\nproduces no rows. A GROUP BY () always produces one group.\n\nFixes https://github.com/prestodb/presto/issues/6059", "NaN"], ["6065", "Update airlift discovery-server to 1.27", "Haozhun Jin", "haozhun", "09/07/16, 08:46:56 PM", "NaN", "NaN"], ["6067", "Add per-transaction Hive metastore cache", "Mark", "geraint0923", "09/09/16, 07:25:04 PM", "NaN", "NaN"], ["6069", "Include path name in error when directory listing fails", "David Phillips", "electrum", "09/07/16, 11:21:07 PM", "NaN", "NaN"], ["6070", "Upgrade airlift to 0.136", "Haozhun Jin", "haozhun", "09/08/16, 01:33:55 AM", "This downgrades Jetty version to 9.3.9.M1", "NaN"], ["6075", "Fix race condition when running cassandra tests in parallel", "Martin Traverso", "martint", "09/08/16, 07:12:18 PM", "NaN", "NaN"], ["6076", "Upgrade airlift to 0.137", "Raghav Sethi", "raghavsethi", "09/14/16, 09:50:30 PM", "NaN", "NaN"], ["6081", "Fix missing schema error message for SHOW TABLES", "David Phillips", "electrum", "09/09/16, 04:11:59 AM", "NaN", "NaN"], ["6082", "Constraint annotation fixups", "\u0141ukasz Osipiuk", "losipiuk", "09/30/16, 05:44:51 PM", "NaN", "NaN"], ["6086", "Fix modification during iteration in InMemoryHiveMetastore", "David Phillips", "electrum", "09/09/16, 08:57:04 PM", "NaN", "NaN"], ["6089", "Fix TestOrcBloomFilters", "Dain Sundstrom", "dain", "09/09/16, 11:29:59 PM", "NaN", "NaN"], ["6090", "Verify task writer count and task concurrency are a power of two", "Dain Sundstrom", "dain", "09/14/16, 03:03:49 AM", "NaN", "NaN"], ["6091", "IS_DISTINCT_FROM operator.", "Igor Demura", "idemura", "10/19/16, 11:22:25 PM", "Fixes #5969 ", "NaN"], ["6092", "Add bucket balancer to Raptor", "Raghav Sethi", "raghavsethi", "12/30/16, 06:42:22 PM", "NaN", "NaN"], ["6093", "Improve rollback when add_partitions fails in Hive connector", "Haozhun Jin", "haozhun", "09/20/16, 08:13:47 PM", "NaN", "NaN"], ["6096", "Make NodeManager specific to a connector instance", "David Phillips", "electrum", "09/14/16, 12:01:13 AM", "NaN", "NaN"], ["6098", "Handle Long.MIN_VALUE in json_array_get", "David Phillips", "electrum", "09/12/16, 04:40:24 PM", "Fixes #6088 ", "NaN"], ["6099", "Replace SLF4J with Airlift logger", "David Phillips", "electrum", "09/12/16, 04:55:40 PM", "NaN", "NaN"], ["6100", "Remove broken %w specifier for MySQL date functions", "David Phillips", "electrum", "09/12/16, 09:48:55 PM", "NaN", "NaN"], ["6104", "Fix atop connector to work with new classloader setup", "Christopher Berner", "cberner", "09/17/16, 04:24:53 PM", "NaN", "NaN"], ["6107", "Cleanup plugin dependencies", "David Phillips", "electrum", "09/14/16, 01:36:08 AM", "NaN", "NaN"], ["6109", "Replace ConnectorFactoryContext with ConnectorContext", "David Phillips", "electrum", "09/14/16, 03:00:24 PM", "NaN", "NaN"], ["6110", "Remove CurrentNodeId class from Raptor", "David Phillips", "electrum", "09/14/16, 04:03:23 PM", "This makes all the usages consistent by using NodeManager directly.", "NaN"], ["6111", "Update presto-maven-plugin to 0.1.12", "David Phillips", "electrum", "09/14/16, 06:07:59 AM", "The new version validates SPI dependencies.", "NaN"], ["6117", "Fix NPE in OrganizationJob", "Aleksei Statkevich", "AlekseiS", "09/14/16, 08:32:44 PM", "\"null\" is passed for Optional insteal of empty().", "NaN"], ["6119", "Set initial capacity to avoid array copy operations for each row", "Joy Yao", "joy-yao", "09/16/16, 04:24:18 AM", "NaN", "NaN"], ["6124", "Fix kafka plugin dependency scope", "Nezih Yigitbasi", "nezihyigitbasi", "09/15/16, 07:43:29 PM", "Before this fix queries were failing with the error below. /cc @electrum \n\n```\njava.lang.NoClassDefFoundError: org/apache/log4j/Logger\n        at kafka.utils.Logging$class.logger(Logging.scala:24)\n        at kafka.consumer.SimpleConsumer.logger$lzycompute(SimpleConsumer.scala:30)\n        at kafka.consumer.SimpleConsumer.logger(SimpleConsumer.scala:30)\n        at kafka.utils.Logging$class.info(Logging.scala:67)\n        at kafka.consumer.SimpleConsumer.info(SimpleConsumer.scala:30)\n        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:74)\n        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:68)\n        at kafka.consumer.SimpleConsumer.send(SimpleConsumer.scala:91)\n        at kafka.javaapi.consumer.SimpleConsumer.send(SimpleConsumer.scala:68)\n        at com.facebook.presto.kafka.KafkaSplitManager.getSplits(KafkaSplitManager.java:83)\n        at com.facebook.presto.split.SplitManager.getSplits(SplitManager.java:45)\n        at com.facebook.presto.sql.planner.DistributedExecutionPlanner$Visitor.visitTableScan(DistributedExecutionPlanner.java:112)\n        at com.facebook.presto.sql.planner.DistributedExecutionPlanner$Visitor.visitTableScan(DistributedExecutionPlanner.java:92)\n        at com.facebook.presto.sql.planner.plan.TableScanNode.accept(TableScanNode.java:135)\n        at com.facebook.presto.sql.planner.DistributedExecutionPlanner$Visitor.visitLimit(DistributedExecutionPlanner.java:258)\n        at com.facebook.presto.sql.planner.DistributedExecutionPlanner$Visitor.visitLimit(DistributedExecutionPlanner.java:92)\n        at com.facebook.presto.sql.planner.plan.LimitNode.accept(LimitNode.java:86)\n        at com.facebook.presto.sql.planner.DistributedExecutionPlanner.plan(DistributedExecutionPlanner.java:78)\n        at com.facebook.presto.sql.planner.DistributedExecutionPlanner.plan(DistributedExecutionPlanner.java:83)\n        at com.facebook.presto.execution.SqlQueryExecution.planDistribution(SqlQueryExecution.java:318)\n        at com.facebook.presto.execution.SqlQueryExecution.start(SqlQueryExecution.java:231)\n        at com.facebook.presto.execution.QueuedExecution.lambda$start$1(QueuedExecution.java:62)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```", "NaN"], ["6125", "Update legacy config in TestingPrestoServer", "David Phillips", "electrum", "09/19/16, 06:03:20 PM", "NaN", "NaN"], ["6126", "Fix aggregations with multiple grouping sets and no input", "Raghav Sethi", "raghavsethi", "09/22/16, 11:54:58 PM", "When an aggregation with an empty grouping set, cube or rollup\nprocessed no input rows, no output was produced. This is incorrect, as\nempty grouping sets are considered to be global aggregations and must\ngenerate a single row even if no input was processed. This commit adds\nthe following:\n- Detecting when there is an empty grouping set, and changing the plan\n  to collapse to a single final aggregration operator.\n- Changing HashAggregationOperator to generate default outputs when\n  there are global aggregations and no input has been processed.\n- Tests for several cases when no input is received.\n\nFixes #6066 ", "NaN"], ["6128", "Avoid the unnecessary ImmutableMap copy", "Mark", "geraint0923", "09/16/16, 01:10:33 AM", "Fix #6121 ", "NaN"], ["6130", "Remove unused variable", "Joy Yao", "joy-yao", "09/16/16, 10:06:22 PM", "NaN", "NaN"], ["6131", "Fix argument ordering in QueryContext constructor", "Raghav Sethi", "raghavsethi", "09/21/16, 08:47:29 PM", "NaN", "NaN"], ["6135", "ORC union type is not supported and should throw", "Dain Sundstrom", "dain", "09/17/16, 11:43:46 PM", "NaN", "NaN"], ["6136", "Extend transactions to cover catalog name to connector instance", "Dain Sundstrom", "dain", "10/19/16, 10:12:51 PM", "NaN", "NaN"], ["6139", "Add flexibility to 3 byte code expressions", "Haozhun Jin", "haozhun", "09/20/16, 08:09:15 PM", "NaN", "NaN"], ["6142", "Skip logging errors for destroyed drivers", "David Phillips", "electrum", "09/20/16, 11:57:06 PM", "NaN", "NaN"], ["6147", "Hive security cleanup", "David Phillips", "electrum", "09/23/16, 01:14:51 AM", "NaN", "NaN"], ["6150", "Optimize DictionaryBlock.copyPositions()", "Christopher Berner", "cberner", "09/21/16, 05:08:57 PM", "This reduces the time complexity from O(n^2) to O(n)", "NaN"], ["6160", "Update release notes", "Christopher Berner", "cberner", "09/26/16, 04:27:42 PM", "NaN", "NaN"], ["6161", "Fix performance issue in dictionary blocks and add release notes", "Christopher Berner", "cberner", "09/22/16, 11:48:24 PM", "NaN", "NaN"], ["6167", "Apply nulls last order to array_sort", "Mark", "geraint0923", "09/26/16, 11:44:55 PM", "NaN", "NaN"], ["6168", "Fix shard recovery manager random interval", "Greg", "ggreg", "11/17/16, 09:09:14 PM", "NaN", "NaN"], ["6171", "Hotfix 0.152 for MySQL date functions", "David Phillips", "electrum", "09/23/16, 04:50:32 PM", "NaN", "NaN"], ["6181", "Add shuffle function for array", "Mark", "geraint0923", "09/26/16, 11:45:45 PM", "Fix #6176 ", "NaN"], ["6183", "Remove docker volumes once product-tests execution completes", "Sanjay Sharma", "sanjay990", "09/23/16, 11:33:34 PM", "NaN", "NaN"], ["6184", "Periodically cleanup old completed raptor transactions", "Aleksei Statkevich", "AlekseiS", "10/10/16, 11:26:59 PM", "Cleanup process will delete records of successful transactions older than a specified threshold\nand records of failed transaction older than a specified threshold and not referenced by\ncreated shards.\n\nNote, that an index on 'transactions' table, 'end_time' column is needed to\nimprove cleanup query performance.", "NaN"], ["6188", "Allow parsing -ea Java versions", "Martin Traverso", "martint", "09/26/16, 04:12:35 PM", "NaN", "NaN"], ["6189", "Fix typo in non-null checking message of 'queuedExecution'", "Kai Sasaki", "Lewuathe", "09/26/16, 07:16:17 PM", "Fix typo in exception message.", "NaN"], ["6192", "Fix typo in release note", "Anu Sudarsan", "anusudarsan", "09/26/16, 07:26:33 PM", "@electrum minor typo fix.", "NaN"], ["6195", "Fix incorrect stream property derivations from GroupIdNode", "Raghav Sethi", "raghavsethi", "09/27/16, 06:29:10 PM", "AddExchanges and AddLocalExchanges were inconsistent in the way that\nproperties were derived for the GroupIdNode. StreamPropertyDerivations\nincorrectly passed through all child properties, when in fact the\nGroupIdNode destroys partitioning properties on grouping columns that\nare not common to all grouping sets. In some cases this led to\nincorrect results. We now use the same logic in PropertyDerivations\nand StreamPropertyDerivations.\n\nFixes #6191.", "NaN"], ["6197", "Revert config option for treating Hive integrals as BIGINT", "Raghav Sethi", "raghavsethi", "09/30/16, 06:31:04 PM", "This reverts commit 05bba028b12fa2293c60440f15b4fc8499526633. This was\nnever turned on and intended to be reverted in the short term.", "NaN"], ["6198", "Lambda support", "Haozhun Jin", "haozhun", "11/22/16, 07:26:18 PM", "NaN", "NaN"], ["6203", "Fix the test for shuffle function", "Mark", "geraint0923", "09/29/16, 05:17:31 PM", "NaN", "NaN"], ["6204", "Make RaptorPageSink#finish() async", "Aleksei Statkevich", "AlekseiS", "11/01/16, 06:53:57 PM", "NaN", "NaN"], ["6205", "Include class name in exceptions for Block and BlockBuilder", "David Phillips", "electrum", "09/29/16, 02:05:09 AM", "When a default method throws UnsupportedOperationException, it is useful\nto know the type of the subclass that is missing the implementation.", "NaN"], ["6206", "Merge AddLocalExchanges fix into hotfix branch", "Raghav Sethi", "raghavsethi", "09/27/16, 08:21:40 PM", "AddExchanges and AddLocalExchanges were inconsistent in the way that\nproperties were derived for the GroupIdNode. StreamPropertyDerivations\nincorrectly passed through all child properties, when in fact the\nGroupIdNode destroys partitioning properties on grouping columns that\nare not common to all grouping sets. In some cases this led to\nincorrect results. We now use the same logic in PropertyDerivations\nand StreamPropertyDerivations.", "NaN"], ["6207", "Update release notes for 0.153 and 0.152.3", "Raghav Sethi", "raghavsethi", "09/29/16, 04:21:31 PM", "NaN", "NaN"], ["6208", "Fix deadlock in ContinuousTaskStatusFetcher#updateTaskStatus", "Dain Sundstrom", "dain", "10/03/16, 07:11:59 PM", "The updateTaskStatus method performs a callback while holding a lock.\nThe lock on this method is not needed at all.\n\nFixes #6196", "NaN"], ["6209", "Fix a leak of futures in DeleteOperator", "Aleksei Statkevich", "AlekseiS", "09/28/16, 06:57:18 PM", "Currently a new listenable future is created on every \"isBlocked()\" call\nin DeleteOperator which I expect to happen often. The issue with it is\nthat a new listenable future is referenced by completable future and as\na result cannot be cleaned up by GC. Furthermore, every time it happens\na new callback is added to a completable future which is not ideal.\nThe fix is to do conversion immediately when a future is created.", "NaN"], ["6210", "Fix potential leak in running queries JMX stat", "Christopher Berner", "cberner", "09/28/16, 01:13:15 AM", "NaN", "NaN"], ["6216", "Update to Airbase 58", "David Phillips", "electrum", "09/28/16, 08:45:37 PM", "NaN", "NaN"], ["6220", "Require MoreObjects to be used with static imports", "David Phillips", "electrum", "09/29/16, 06:50:53 AM", "NaN", "NaN"], ["6222", "Fix handling RIGHT JOIN over single stream properties", "Dain Sundstrom", "dain", "09/29/16, 04:31:42 AM", "When the probe of a RIGHT JOIN is partitioned on empty set the properties\ncan not be empty set because that effictively means single stream. Instead\nthe ouptu properties are the probe partitioning columns", "NaN"], ["6225", "Allow periods in resource group ids", "Christopher Berner", "cberner", "09/30/16, 05:53:59 PM", "This does not change the parsing of resource group config files, but\nallows constructs like user_group.${USER} to work with user names that\ncontain periods", "NaN"], ["6227", "Prevent declaration of multiple streams partitioned on empty set", "Dain Sundstrom", "dain", "09/29/16, 08:25:13 PM", "Prevent table scan from producing multiple streams partitioned on empty set\nChange RIGHT JOIN to always declare multiple streams with unknown partitioning\nVerify multiple is not partitioned on empty set", "NaN"], ["6229", "Fix semantic analysis of a 'join on' clause", "Aleksei Statkevich", "AlekseiS", "09/30/16, 01:12:33 AM", "'Join on' clause must only have a boolean expression as a parameter, so\nadd an explicit check in semantic analyzer for it.", "NaN"], ["6230", "Drop empty pages in Driver", "Mark", "geraint0923", "09/30/16, 01:31:47 AM", "Fix #6219 ", "NaN"], ["6231", "Fix naming in QueryAssertions.assertContains", "David Phillips", "electrum", "09/30/16, 01:32:42 AM", "NaN", "NaN"], ["6233", "Fix comparison for CHAR to take into account padding", "Maciej 'mac' Grzybek", "maciejgrzybek", "11/25/16, 06:25:52 PM", "Previous implementation of CHAR comparison didn't work properly with control chars (e.g. '\\0').\n\n---\n\nInternal TD review: https://github.com/Teradata/presto/pull/363\nCan be relevant: https://github.com/prestodb/presto/issues/2295", "NaN"], ["6234", "Optimize ArrayType.getObjectValue()", "Christopher Berner", "cberner", "09/30/16, 08:59:09 PM", "This avoids materializing a new Block for every call, and reduces the\ntime to run the full orc tests by 5-10%", "NaN"], ["6237", "Produce better hash for int-based and long-based types", "Martin Traverso", "martint", "10/02/16, 03:46:28 AM", "The previous implementation produces bad hashes, which can\ncause downstream skew when the hashes are not combined in a\n way that scrambles the bits properly (e.g., combine hash).", "NaN"], ["6238", "Several improvements to the web UI", "Raghav Sethi", "raghavsethi", "10/10/16, 10:56:17 PM", "Fixes #6226 \nFixes #5971 \nFixes #5599 \nFixes #5675", "NaN"], ["6242", "Fix NPE when casting a map with null elements in values", "Mark", "geraint0923", "10/03/16, 06:10:49 PM", "Fix #6239 ", "NaN"], ["6246", "Remove unused variables", "Aleksei Statkevich", "AlekseiS", "10/03/16, 09:00:26 PM", "After the following commit a few variables became unused, delete them:\n\"Use SubqueryPlanner in RelationPlanner\"", "NaN"], ["6251", "Allow setting parser options in TestingPrestoServer and DistributedQueryRunner", "Dain Sundstrom", "dain", "10/04/16, 02:52:55 AM", "NaN", "NaN"], ["6255", "Allow type mismatch between table and partition schema", "Mark", "geraint0923", "11/08/16, 07:18:20 PM", "1. The allowed coercions:\r\n   `tinyint` -> `smallint` -> `integer` -> `bigint`\r\n   `varchar` -> `tinyint` / `smallint` / `integer` / `bigint`\r\n   `tinyint` / `smallint` / `integer` / `bigint` -> `varchar`\r\n   `real` (`float`) -> `double`\r\n2. Insertion to an existing partition with mismatch partition schema and table schema is not allowed.\r", "NaN"], ["6256", "Refactor FunctionRegistry to improve function resolution performance", "Haozhun Jin", "haozhun", "10/05/16, 05:41:04 PM", "AbstractTestQueries.testLargeIn performance improves from 35s to 6s. (The test contains two queries with 5000 casts each, and two other queries not affected.)", "NaN"], ["6267", "Fix typo in getCommonSuperTypeForCovariantParametrizedType", "Raghav Sethi", "raghavsethi", "10/04/16, 07:47:30 PM", "NaN", "NaN"], ["6268", "Fix handling of null for try_cast", "David Phillips", "electrum", "10/05/16, 12:00:36 AM", "For cast source types such as integers or booleans that use a Java\nprimitive type, null input values were incorrectly converted to the\ndefault value for the type (zero or false) before casting.", "NaN"], ["6270", "Update Accumulo documentation", "David Phillips", "electrum", "10/04/16, 11:53:35 PM", "NaN", "NaN"], ["6277", "Add accumulo.instance property to example in docs", "Adam J. Shook", "adamjshook", "10/05/16, 09:30:44 PM", "Missing a required property in the example that could mislead users.", "NaN"], ["6279", "Fix ClassCastException when planning window node with coercions", "Martin Traverso", "martint", "10/06/16, 04:46:48 AM", "If the output of the window function has to be coerced, it's\nwrapped with a Cast node. The code wasn't accounting for that\nand was blindly pulling out the child of the Cast node, which\nmight be a SymbolReference if the function had already been\ntranslated.\n\nFixes https://github.com/prestodb/presto/issues/6278", "NaN"], ["6284", "Add validations and tests for @IsNull", "Mark", "geraint0923", "10/11/16, 02:04:30 AM", "NaN", "NaN"], ["6285", "Update 0.153 release notes", "Christopher Berner", "cberner", "10/10/16, 04:28:02 PM", "NaN", "NaN"], ["6286", "Make code cache collection trigger configurable", "Christopher Berner", "cberner", "10/13/16, 04:21:26 PM", "NaN", "NaN"], ["6294", "Produce better hash for double type", "Martin Traverso", "martint", "10/07/16, 09:30:30 PM", "The previous implementation produces bad hashes, which can\ncause downstream skew when the hashes are not combined in a\n way that scrambles the bits properly (e.g., combine hash).\n\nThis is a follow up to d79494747c7a81930971140326a661a782b28fca.", "NaN"], ["6298", "Create Raptor staging and storage directories on startup", "David Phillips", "electrum", "10/07/16, 10:37:15 PM", "This fixes the free space check for new installations.", "NaN"], ["6301", "Validate selectors when configuring the resource manager", "Nezih Yigitbasi", "nezihyigitbasi", "11/24/16, 09:47:41 PM", "Fixes #6022 ", "NaN"], ["6304", "Fix broken test in TestOrcStorageManager", "David Phillips", "electrum", "10/08/16, 08:03:14 PM", "This test broke due to FileStorageService creating the directory.", "NaN"], ["6306", "Update Hive connector examples", "David Phillips", "electrum", "10/10/16, 04:24:11 PM", "NaN", "NaN"], ["6310", "Add documentation for schema operations", "David Phillips", "electrum", "10/10/16, 04:23:25 PM", "NaN", "NaN"], ["6314", "Fix title of DROP SCHEMA documentation", "Haozhun Jin", "haozhun", "10/10/16, 08:27:25 PM", "NaN", "NaN"], ["6315", "Update 0.153 release notes", "David Phillips", "electrum", "10/11/16, 10:35:44 PM", "NaN", "NaN"], ["6316", "Fix naming of emptyApproxSet Java method\t", "David Phillips", "electrum", "10/12/16, 07:20:08 PM", "NaN", "NaN"], ["6322", "Fix search for human readable state on query details page", "Raghav Sethi", "raghavsethi", "10/11/16, 05:52:43 PM", "NaN", "NaN"], ["6326", "Make shard cleanup query more efficient", "Aleksei Statkevich", "AlekseiS", "10/11/16, 11:37:45 PM", "Note, that you need an index on (end_time, transaction_id) for\n\"transactions\" table.", "NaN"], ["6328", "Enable @IsNull for wrapper type Void.class", "Mark", "geraint0923", "10/14/16, 12:23:49 AM", "NaN", "NaN"], ["6331", "Fix broken pom.xml header in presto-jdbc", "Haozhun Jin", "haozhun", "10/12/16, 12:45:54 AM", "NaN", "NaN"], ["6335", "Use streaming hash instead of memory mapped file", "David Phillips", "electrum", "10/25/16, 09:36:37 PM", "NaN", "NaN"], ["6339", "Move access control tests out of smoke tests", "David Phillips", "electrum", "10/25/16, 09:20:39 PM", "These tests are for the engine and are not connector smoke tests.", "NaN"], ["6340", "Fix Raptor compaction", "David Phillips", "electrum", "10/13/16, 04:34:43 PM", "Compaction did not run at all due to a missing column in a SQL query.", "NaN"], ["6342", "Local File connector fixes", "David Phillips", "electrum", "10/13/16, 07:53:14 PM", "NaN", "NaN"], ["6347", "Remove legacy connector API", "David Phillips", "electrum", "10/24/16, 07:36:13 PM", "NaN", "NaN"], ["6348", "Add rule to evaluate a constant Apply expression", "Martin Traverso", "martint", "10/13/16, 09:59:57 PM", "Fixes https://github.com/prestodb/presto/issues/6346", "NaN"], ["6349", "Use expireAfterWrite instead in Hive metastore cache", "Mark", "geraint0923", "10/14/16, 01:54:03 AM", "NaN", "NaN"], ["6350", "Fix duplicate symbols in UnaliasSymbolReferences for table write", "Haozhun Jin", "haozhun", "10/14/16, 03:56:13 AM", "Fixes https://github.com/prestodb/presto/issues/6330", "NaN"], ["6352", "Fix import order in PlanOptimizers", "Mark", "geraint0923", "10/14/16, 01:05:07 AM", "NaN", "NaN"], ["6355", "Fix incorrect equality inference during predicate pushdown", "Martin Traverso", "martint", "10/16/16, 06:17:59 PM", "When one side of a join has an effective predicate expression in terms of the field\nuse in the join criteria (e.g., v = f(k1), with a join criteria of k1 = k2), and\nthat expression can produce null on non-null input (e.g., nullif, case, if, most of\nthe array/map functions, etc), queries can produce incorrect results.\n\nIn that scenario, predicate pushdown derives another join condition v = f(k2). Since\nf() can produce null on non-null input, it's possible for some value of k1 that's\nequal to k2, f(k2) is null or f(k1) is null. This will cause the join criteria to\nevaluate to null instead of true.\n\nA correct derivation, although less useful for predicate pushdown,  would be\n\n```\nk1 = k2 AND ((f(k1) IS NULL AND f(k2) IS NULL) OR f(k1) = f(k2)).\n```\n\nThis change prevents the equality inference logic from considering expressions\nthat may return null on non-null input.\n\nFixes https://github.com/prestodb/presto/issues/6332", "NaN"], ["6357", "Fix intermitent failures in resource group tests", "Christopher Berner", "cberner", "10/18/16, 05:49:59 PM", "Other queries could timeout because they were abandoned causing the test\nto fail", "NaN"], ["6362", "Remove unused TRY keyword from grammar", "David Phillips", "electrum", "10/18/16, 04:43:16 AM", "NaN", "NaN"], ["6364", "Fix memory usage stats for aggregration with no aggregators", "Grzegorz Kokosi\u0144ski", "kokosing", "10/24/16, 04:27:06 PM", "Fix memory usage stats for aggregration with no aggregators\n\nWhen InMemoryHashAggregationBuilder had no aggregators, it did not\nupdate memory usage (did not call updateMemory). This caused that memory\nused groupByHash was not included in stats. This commit fixes that.", "NaN"], ["6366", "Add missing dependency for Hadoop KMS", "David Phillips", "electrum", "10/18/16, 04:42:47 AM", "NaN", "NaN"], ["6367", "Fix incorrect equality inference during predicate pushdown", "Martin Traverso", "martint", "10/17/16, 05:14:46 PM", "CAST(JSON 'null' AS ...) will also return null", "NaN"], ["6370", "Support Avro in Hive connector", "David Phillips", "electrum", "10/19/16, 01:32:12 AM", "NaN", "NaN"], ["6375", "A few simple changes to Hive connector", "Haozhun Jin", "haozhun", "11/14/16, 09:36:49 PM", "NaN", "NaN"], ["6376", "Remove unused getNodeManager() from LocalQueryRunner", "David Phillips", "electrum", "11/01/16, 03:05:30 PM", "NaN", "NaN"], ["6377", "Remove sampled table support from Raptor", "David Phillips", "electrum", "10/18/16, 03:35:58 PM", "NaN", "NaN"], ["6378", "Update to Slice 0.27", "David Phillips", "electrum", "10/17/16, 11:02:19 PM", "This version avoids allocating arrays that are beyond the JVM limit.", "NaN"], ["6381", "Support quantified comparison predicates", "Aleksei Statkevich", "AlekseiS", "11/02/16, 08:26:36 PM", "NaN", "NaN"], ["6389", "Use dictionary blocks for DWRF row groups dictionary", "Dain Sundstrom", "dain", "10/19/16, 01:14:17 AM", "NaN", "NaN"], ["6393", "Fix function naming in ArrayIntersectFunction", "David Phillips", "electrum", "11/01/16, 06:25:15 PM", "NaN", "NaN"], ["6394", "Fix error messages for failures during commit", "David Phillips", "electrum", "10/20/16, 10:03:59 PM", "CompletableFuture wraps exceptions with CompletionException,\nwhich caused the original PrestoException to be hidden.", "NaN"], ["6399", "Make resource group test insensitive to startup time", "Christopher Berner", "cberner", "10/20/16, 04:01:08 PM", "This fixes an intermittent failure when the test server took too long to\nstart", "NaN"], ["6403", "Remove support for approximate queries", "Martin Traverso", "martint", "10/20/16, 06:57:47 PM", "This was an experimental feature that was never used in production\nand is not well supported. We don't want to keep carrying that code\naround, given how pervasive it is.", "NaN"], ["6406", "Remove incorrect @NotNull annotation from Metadata", "Dain Sundstrom", "dain", "10/23/16, 05:25:57 PM", "None of the methods on Metadata return null, and the code was\nusing the validation constrint @NotNull annotation.", "NaN"], ["6411", "Generate long-type hash code for ArrayType and MapType", "Mark", "geraint0923", "10/21/16, 04:36:03 AM", "This change also fixed the bug that hash operator and hash method\ngenerate different hash codes.\n\nShould fix #6407 ", "NaN"], ["6412", "Optimize NetworkLocation to reduce object allocation", "Christopher Berner", "cberner", "10/22/16, 12:56:05 AM", "This reduces GC frequency by ~15% on the coordinator, when using\ntopology aware scheduling", "NaN"], ["6418", "Expose Raptor cross shard organization as a table property", "Eric Hwang", "erichwang", "11/04/16, 11:46:41 PM", "This also adds the property to the system table.", "NaN"], ["6422", "Add support for a stateful join filter function", "Dain Sundstrom", "dain", "10/24/16, 06:01:30 PM", "This fixes a `NullPointerException` that occurs in generated `JoinFilterFunction` when the join function contains a stateful function like `array_intersect`.  These functions have temporary data structures and thus require that the filter instance is not shared across threads.  This PR reorganizes the join code so that each join probe operator has a private instance of the `LookupSource` containing a private filter function and shared `PagesHash`.", "NaN"], ["6423", "Desugar the expressions before evaluating the constant values", "Mark", "geraint0923", "11/02/16, 11:29:47 PM", "ValuesNode is trying to evaluate its expressions to be constant values\nwith ExpressionInterpreter before DesugaringOptimizer has been run,\nwhile ExpressionInterpreter is not aware of AT TIME ZONE. And this will\ncause query failure when using AT TIME ZONE in VALUES.\n\nThis change will ensure desugaring has been done when evaluating\nexpressions to be constant values.", "NaN"], ["6425", "Skip copying Constraint object in MetadataManager", "David Phillips", "electrum", "10/24/16, 06:35:53 PM", "NaN", "NaN"], ["6426", "Remove incorrect phrase from deployment instructions", "David Phillips", "electrum", "10/24/16, 07:18:25 PM", "The data directory does not contain \"local metadata\".", "NaN"], ["6427", "Change GroupIdNode to behave more like ProjectNode", "Raghav Sethi", "raghavsethi", "10/26/16, 11:56:46 PM", "This fixes a correctness issue in queries with multiple grouping sets\nthat resolve to the same set when unaliased. All inputs to\nAggregationNode are now written in terms of outputs from GroupIdNode\nor ProjectNode. This requires that GroupIdNode look a lot more like\nProjectNode, except that GroupIdNode just copies columns instead of\nprojecting them.\n\nFixes #6379", "NaN"], ["6429", "Support hash of array and row with nulls", "Dain Sundstrom", "dain", "11/02/16, 08:23:16 PM", "NaN", "NaN"], ["6430", "Performance optimizations for coordinator", "Christopher Berner", "cberner", "10/25/16, 09:36:22 PM", "NaN", "NaN"], ["6431", "Add queryIds and cpuUsage to ResourceGroupInfo", "Dongmin Yu", "miniway", "11/07/16, 07:56:37 PM", "I face situations that I need to figure out the following information for management and reporting. \n- What queries are running under this group\n- What resource group does this query is running under\n- What is current cpu-usage-millis to show and to find out how much it exceeds the soft/hard cpu limits", "NaN"], ["6433", "Move page dictionary compaction logic to Page", "Dain Sundstrom", "dain", "11/07/16, 08:10:11 PM", "The optimized dictionary compaction logic for pages can only be\nused to compact blocks from a single page, so to avoid errors this\nmethod should be prive to the Page code.\nThe compaction code in DictionaryBlock is reverted to the original\nsingle page version.", "NaN"], ["6441", "Make INCLUDING, EXCLUDING, and PROPERTIES unreserved keywords", "Mark", "geraint0923", "10/26/16, 08:37:41 PM", "fix #6440 ", "NaN"], ["6445", "Properly account for time spent creating page source", "David Phillips", "electrum", "10/27/16, 12:14:00 AM", "Fixes #6443", "NaN"], ["6447", "Remove racy assertion", "Christopher Berner", "cberner", "10/27/16, 12:38:42 AM", "Now that queries wait for their final info, they're not recorded as\ncompleted until they're finished and their final info has been received.\nThis wasn't an important part of the test, so just remove it.", "NaN"], ["6455", "Move operator preallocated memory and memory from partials to system memory pool", "Dain Sundstrom", "dain", "10/27/16, 10:35:43 PM", "NaN", "NaN"], ["6456", "Use IntArrayList in TypedSet to avoid wasting memory", "Mark", "geraint0923", "10/27/16, 11:29:18 PM", "Fixes #6313 \n\nBefore this change, TypedSet used IntBigArray which allocates at least\n8KB at initialization, while initialization of IntArrayList only\nconsumes no more than 256 bytes.\n\nThis change might fix the memory limit issues in map_agg and\nmultimap_agg.", "NaN"], ["6458", "Include hash symbols used by each node in plan output", "Martin Traverso", "martint", "10/28/16, 01:15:16 AM", "NaN", "NaN"], ["6459", "Upgrade to Airlift 0.139", "Dain Sundstrom", "dain", "10/27/16, 10:48:44 PM", "NaN", "NaN"], ["6462", "Fix thread safety for Avro", "David Phillips", "electrum", "10/28/16, 12:31:49 AM", "NaN", "NaN"], ["6463", "Enable TestHiveIntegrationSmokeTest to test ParquetPageSource", "Mark", "geraint0923", "11/01/16, 05:24:32 PM", "NaN", "NaN"], ["6467", "Add 0.155 release notes", "Dain Sundstrom", "dain", "10/28/16, 10:07:44 PM", "NaN", "NaN"], ["6469", "Improve TransactionMetadata thread safty", "Dain Sundstrom", "dain", "10/31/16, 10:14:27 PM", "NaN", "NaN"], ["6470", "Remove triggers from ResourceGroupsDao", "David Phillips", "electrum", "10/29/16, 08:46:43 PM", "NaN", "NaN"], ["6472", "Add a variant for from_unixtime function", "Mark", "geraint0923", "11/02/16, 08:35:57 PM", "The new variant will can turn unixtime into timestamp with the given\ntime zone.", "NaN"], ["6474", "Fix Kafka distributed tests", "David Phillips", "electrum", "10/29/16, 08:54:24 PM", "NaN", "NaN"], ["6476", "Add ability for event listeners to get connector-specific output metadata", "Raghav Sethi", "raghavsethi", "12/20/16, 06:05:42 PM", "NaN", "NaN"], ["6478", "Optimize scheduler", "Christopher Berner", "cberner", "11/04/16, 10:42:31 PM", "This reduces scheduler CPU usage by ~25x", "NaN"], ["6479", "Fix Redis distributed tests", "David Phillips", "electrum", "11/01/16, 03:02:51 PM", "NaN", "NaN"], ["6482", "Improve performance of default configs", "Dain Sundstrom", "dain", "10/31/16, 10:00:51 PM", "NaN", "NaN"], ["6488", "Sort system session properties in SHOW SESSION", "Dain Sundstrom", "dain", "10/31/16, 10:10:31 PM", "NaN", "NaN"], ["6489", "Optimistically assume raptor backup directories exist", "Eric Hwang", "erichwang", "10/31/16, 11:40:01 PM", "Only try to create them if the actual file create throws an java.io.FileNotFoundException", "NaN"], ["6490", "Make tests work with task_concurrency enabled by default", "Dain Sundstrom", "dain", "11/01/16, 04:14:51 AM", "NaN", "NaN"], ["6499", "Disallow filtered aggregates with DISTINCT", "Martin Traverso", "martint", "11/02/16, 09:31:20 PM", "NaN", "NaN"], ["6504", "Remove racy assertions from TestQueues", "Christopher Berner", "cberner", "11/02/16, 11:24:57 PM", "These are not an important part of the test, and are inherently racy", "NaN"], ["6505", "Fix bad version check for old RCFile header", "Dain Sundstrom", "dain", "11/02/16, 08:02:48 PM", "NaN", "NaN"], ["6510", "Update to Airbase 59", "David Phillips", "electrum", "11/06/16, 05:50:36 PM", "This change updates TestNG so that full stack traces are visible.", "NaN"], ["6512", "Add separate Travis matrix item for Maven checks", "David Phillips", "electrum", "11/03/16, 06:35:26 PM", "NaN", "NaN"], ["6513", "Skip compaction of Raptor shards with null min/max sort/temporal columns", "Raghav Sethi", "raghavsethi", "11/03/16, 04:05:36 PM", "NaN", "NaN"], ["6514", "Add documentation for quantified comparison", "Aleksei Statkevich", "AlekseiS", "11/03/16, 10:53:50 PM", "NaN", "NaN"], ["6518", "Use bash conditionals for Travis script", "David Phillips", "electrum", "11/03/16, 07:27:52 PM", "These are easier to read because the condition is not inverted.", "NaN"], ["6519", "Update aircompressor to version 0.4", "Martin Traverso", "martint", "11/08/16, 12:28:11 AM", "Improves compression performance for small inputs", "NaN"], ["6521", "Validate schema rename for Hive", "David Phillips", "electrum", "11/04/16, 05:12:57 AM", "Current versions of the Hive metastore silently ignore attempts to rename\na database. We now detect this and throw an appropriate error messsage.", "NaN"], ["6525", "Add FileHiveMetastore", "Dain Sundstrom", "dain", "01/07/17, 02:10:18 AM", "NaN", "NaN"], ["6526", "Support \"iso8601\" data format for Redis hash", "David Phillips", "electrum", "11/04/16, 05:17:09 PM", "NaN", "NaN"], ["6528", "Only run Travis install step when necessary", "David Phillips", "electrum", "11/05/16, 06:16:53 AM", "The matrix items that do Maven builds don't need an install step.\nIt is only needed for tests which don't run using Maven.", "NaN"], ["6533", "Add release notes for 0.156", "Christopher Berner", "cberner", "11/04/16, 08:06:37 PM", "NaN", "NaN"], ["6534", "Add raptor metadata DB optimizations", "Eric Hwang", "erichwang", "11/05/16, 12:11:10 AM", "Fixes #6497", "NaN"], ["6535", "Add Hive support for creating external tables", "David Phillips", "electrum", "11/05/16, 04:36:18 PM", "NaN", "NaN"], ["6536", "Remove outputBuffers.buffers from summarized TaskInfo", "Haozhun Jin", "haozhun", "11/17/16, 06:12:50 PM", "NaN", "NaN"], ["6537", "Only run MongoDB integration tests in CI profile", "David Phillips", "electrum", "11/05/16, 10:49:38 PM", "NaN", "NaN"], ["6538", "Fix Travis build for Maven tests", "David Phillips", "electrum", "11/05/16, 05:35:22 PM", "The matrix items that run Maven tests for specific modules\nneed the module dependencies installed.", "NaN"], ["6539", "Split out Raptor and move Hive in Travis build", "David Phillips", "electrum", "11/06/16, 08:51:20 PM", "NaN", "NaN"], ["6540", "Use included JDK for Travis build", "David Phillips", "electrum", "11/06/16, 05:01:17 AM", "NaN", "NaN"], ["6541", "Fix client transaction support", "Dain Sundstrom", "dain", "11/07/16, 08:55:29 PM", "NaN", "NaN"], ["6542", "Remove JDK selection from Travis build", "David Phillips", "electrum", "11/06/16, 08:51:02 PM", "The current Travis images seem to have both 8u101 and 8u65 installed.\r\nSpecifying the JDK invokes \"jdk_switcher use oraclejdk8\" which\r\nuses 8u65 instead of 8u101. Without this, version 8u101 is used.", "NaN"], ["6544", "Fix modernizer warnings and fail on violations", "David Phillips", "electrum", "11/09/16, 02:26:42 AM", "NaN", "NaN"], ["6557", "Revert \"Add queryIds and cpuUsage to ResourceGroupInfo\"", "Christopher Berner", "cberner", "11/08/16, 06:40:18 PM", "This reverts commit 7d98b8e649148f7f9eedd81c25a404867e98fa34.", "NaN"], ["6559", "Add access control for SHOW and listings", "Dain Sundstrom", "dain", "02/15/17, 01:38:17 AM", "This PR adds access control to fail `SHOW` commands and limit listing of unauthorized items.\r\n\r\nThe difficulty with implementing enumeration of objects is that there are two different systems in Presto:\r\n1) `SHOW` commands - in these commands we want to throw an access denied exception so the users knows they didn't mistype the name\r\n2) `INFORMATION_SCHEMA`, JDBC, and system tables - this commands should filter out unauthorized items, but should not throw exceptions\r\n\r\nAdditionally, `SHOW` commands may be implemented on top of `INFORMATION_SCHEMA`\r\n\r\nThis is done by adding new access control checks `checkCanShow*` and `Set<T> filter*(Set<T> items)` methods to `SystemAccessControl` and `ConnectorAccessControl`.  For a `SHOW` command both methods are called, and for a listing access only the filter method is called.  It is expected that the implementor assures that the check and filter methods agree.\r\n\r\nFurther, it is assumed that all users have access to `SHOW CATALOGS` so there is no access check, but there is a filter.\r\n\r\nAs with other new checks the default implementation is \"deny\", so all existing implementation will need to be updated.", "NaN"], ["6567", "Remove no-op apply nodes", "Martin Traverso", "martint", "11/08/16, 10:22:37 PM", "When the subquery of an apply node produces no columns\r\nand is guaranteed to produce one row, it can be removed.\r\n\r\nFixes https://github.com/prestodb/presto/issues/6551", "NaN"], ["6569", "Fix lock contention for Raptor deletes", "David Phillips", "electrum", "11/09/16, 02:01:02 AM", "NaN", "NaN"], ["6570", "Abort Travis build if Maven JVM OOMs", "David Phillips", "electrum", "11/09/16, 02:29:14 AM", "NaN", "NaN"], ["6571", "Simplify aggregation pushdown", "Martin Traverso", "martint", "11/09/16, 09:19:23 PM", "This is a rewrite of the partial aggregation pushdown\r\noptimizer to make the code easier to follow and reason\r\nabout.\r\n\r\nThe approach is as follows:\r\n1. Determine whether the optimization is applicable.\r\n   At a minimum, there must be an aggregation on top\r\n   of an exchange.\r\n2. If the aggregation is SINGLE, split it into a FINAL\r\n   on top of a PARTIAL and reprocess the resulting plan.\r\n3. If the aggregation is a PARTIAL, push it underneath\r\n   each branch of the exchange.\r\n\r\nWe use a couple of tricks to avoid having to juggle\r\nand rename field names as the nodes are rewired:\r\n\r\n1. When pushing the partial aggregation through the exchange,\r\n   the names of the outputs of the aggregation are preserved.\r\n2. If the input->output mappings in the exchange are not\r\n   simple identity projections without rename, we introduce\r\n   a projection under the partial aggregation. This helps\r\n   avoid having to rewrite all the aggregation functions\r\n   to refer to new names.\r\n\r\nIt also fixes a planning issue under certain scenarios\r\ninvolving aggregation subqueries and partitioned tables.\r\n\r\nE.g.,\r\n\r\n    SELECT *\r\n    FROM (\r\n        SELECT count(*)\r\n        FROM tpch.tiny.orders\r\n        HAVING count(DISTINCT custkey) > 1\r\n    )\r\n    CROSS JOIN t\r\n\r\nwhere \"t\" is a partitioned Hive table.\r\n\r\nFixes https://github.com/prestodb/presto/issues/6554", "NaN"], ["6572", "Add separate Travis install step for Maven", "David Phillips", "electrum", "11/09/16, 04:09:31 PM", "Maven wrapper downloads from Maven Central, so having a separate step\nallows recording the timing separately.", "NaN"], ["6582", "Hive tables bucketed on non-existent columns", "Haozhun Jin", "haozhun", "11/09/16, 10:33:01 PM", "Fixes #6552 and #6553.", "NaN"], ["6586", "Add documentation for implicit coercion in Hive connector", "Mark", "geraint0923", "11/10/16, 08:08:09 PM", "NaN", "NaN"], ["6587", "Fix scheduler CPU usage regression", "Christopher Berner", "cberner", "11/10/16, 02:41:49 AM", "NaN", "NaN"], ["6590", "Raptor deletion fixes", "David Phillips", "electrum", "11/10/16, 06:12:55 AM", "NaN", "NaN"], ["6591", "Disable describe output tests for mysql, cassandra and accumulo", "Martin Traverso", "martint", "11/10/16, 04:27:13 AM", "These connectors uses non-canonincal types for varchar columns\r\nin TPC-H, so the output doesn't match. Disable the tests for now.", "NaN"], ["6599", "Add 0.157 release notes", "Martin Traverso", "martint", "11/10/16, 09:33:32 PM", "NaN", "NaN"], ["6600", "Update Hive documentation for external tables", "David Phillips", "electrum", "11/10/16, 08:32:16 PM", "NaN", "NaN"], ["6607", "Add retries for adding a column in shard manager", "Aleksei Statkevich", "AlekseiS", "11/16/16, 07:22:57 PM", "NaN", "NaN"], ["6609", "Update to Airlift sphinx-maven-plugin", "David Phillips", "electrum", "11/15/16, 05:04:34 PM", "NaN", "NaN"], ["6620", "Remove DataDefinitionStatement base class from AST", "David Phillips", "electrum", "12/10/16, 12:05:47 AM", "NaN", "NaN"], ["6631", "Change ArrayBlock to use arrays internally", "Dain Sundstrom", "dain", "11/24/16, 12:18:49 AM", "Change ArrayBlock offsets to include an extra 0 entry in the first position.\nWith this change the length and offset functions do not need an if statement.", "NaN"], ["6632", "Fix explain analyze to not strip output columns", "Aleksei Statkevich", "AlekseiS", "11/17/16, 08:31:55 PM", "NaN", "NaN"], ["6634", "Fix warnings in Decimals and DecimalCasts", "David Phillips", "electrum", "12/09/16, 06:12:12 PM", "This replaces the old `BigDecimal.ROUND_xxx` constants with the new `RoundingMode` enum.", "NaN"], ["6645", "Fix NPE when scheduling non-remotely accessible splits", "Christopher Berner", "cberner", "11/17/16, 02:45:10 AM", "When scheduling a large initial batch of non-remotely accessible splits,\r\nthe split queue could fill up before the RemoteTask was created. This\r\ncauses a NPE when attempting to get the queue space future from the\r\ntask.\r\nInstead, we just skip these nodes as the RemoteTask will be created\r\nimmediately after split scheduling.", "NaN"], ["6646", "Fix unbounded lookahead when parsing parenthesized expression", "Martin Traverso", "martint", "11/16/16, 11:04:44 PM", "Fixes https://github.com/prestodb/presto/issues/6633", "NaN"], ["6648", "Validate that a plan has at most one OutputNode", "Aleksei Statkevich", "AlekseiS", "12/01/16, 09:31:28 PM", "NaN", "NaN"], ["6649", "Pass client-supplied payload field to EventListener", "Mark", "geraint0923", "12/21/16, 12:21:56 AM", "Currently we name this field `X-Presto-Client-Info`.", "NaN"], ["6650", "Rename #bucket_number to #bucket", "Igor Demura", "idemura", "11/17/16, 01:59:13 AM", "NaN", "NaN"], ["6656", "Respect summarize parameter when DELETE'ing task", "Christopher Berner", "cberner", "11/17/16, 06:47:27 PM", "A heap dump of the coordinator shows that in a busy cluster these\r\nobjects can take up more than half of the heap, and they scale both with\r\nthe complexity of the query and size of the cluster", "NaN"], ["6658", "Create failed query when session is invalid", "Dain Sundstrom", "dain", "01/13/17, 08:15:09 PM", "When the server recieves a properly formed Session but the contents are\ninvalid, create a failed query instead of returning an HTTP error response.\nThis allows clients to report errors using the normal failed query system.", "NaN"], ["6659", "Change session identity to be non-null", "Dain Sundstrom", "dain", "11/23/16, 11:20:34 PM", "NaN", "NaN"], ["6660", "Include class name in exceptions for AbstractType", "David Phillips", "electrum", "11/18/16, 08:02:27 PM", "When a default method throws UnsupportedOperationException, it is useful\r\nto know the type of the subclass that is missing the implementation.", "NaN"], ["6668", "Simplify Optional map call in RaptorPageSink", "David Phillips", "electrum", "11/18/16, 08:01:49 PM", "NaN", "NaN"], ["6671", "Fix excess memory & CPU usage in HttpRemoteTask", "Christopher Berner", "cberner", "11/18/16, 11:22:40 PM", "NaN", "NaN"], ["6673", "Make doTestMismatchSchemaTable protected", "Mark", "geraint0923", "11/18/16, 11:00:32 PM", "NaN", "NaN"], ["6674", "begin/cleanup query notifications", "Maciej 'mac' Grzybek", "maciejgrzybek", "02/10/17, 08:31:30 PM", "Per https://github.com/prestodb/presto/pull/6114#issuecomment-261607251, removed notifications for DDLs.\r\n\r\n---\r\n\r\nSupersedes https://github.com/prestodb/presto/pull/6114\r\n\r\nCC: @electrum, @dain ", "NaN"], ["6676", "Add release notes", "Christopher Berner", "cberner", "11/19/16, 12:47:06 AM", "NaN", "NaN"], ["6677", "Hotfix 0.157", "Christopher Berner", "cberner", "11/19/16, 06:09:04 PM", "NaN", "NaN"], ["6678", "Remove /v1/execute resource", "David Phillips", "electrum", "12/10/16, 02:21:53 AM", "NaN", "NaN"], ["6679", "Remove query creation from /v1/query", "David Phillips", "electrum", "12/10/16, 02:32:09 AM", "This allows direct creation of queries and bypasses the query purger.", "NaN"], ["6692", "Make DiscretePredicates support lazy generation", "Dain Sundstrom", "dain", "11/29/16, 03:42:28 PM", "Some Hive tables have a huge numbers of partitions, and generating\r\na tuple domain for each one can stress the GC.  Since DiscretePredicates\r\nis rarely used, it is better to delay creation of the tuple domains\r\nuntil necessary, and it is better to use an iterator so all tuple\r\ndomains do not have to be in memory at the same time.\r\n\r\nAdditionally, add an optimized `columnWiseUnion` which does not require all `TupleDomains` to be in memory at the same time.", "NaN"], ["6695", "Detect recursive view and throw SemanticException", "Rongrong Zhong", "rongrong", "01/06/17, 03:04:45 AM", "Resolves:  #6120", "NaN"], ["6696", "Fix missing Override annotations and method may be static warnings in presto-main", "Piotr Nowojski", "pnowojski", "11/24/16, 09:46:03 PM", "NaN", "NaN"], ["6709", "Method getAdditionalModules cannot be static as it is overridden in subclass", "Joy Yao", "joy-yao", "11/28/16, 10:36:47 PM", "NaN", "NaN"], ["6711", "Fix improper reuse of length vector in ORC map reader", "Dain Sundstrom", "dain", "11/29/16, 01:44:14 AM", "Allocate a new length vector for each ORC map reader bock, since the numeric\nreaders do skip null positions, and there is code that assumes the nulls have\nzero length.", "NaN"], ["6712", "Fix improper reuse of length vector in ORC list reader", "Dain Sundstrom", "dain", "11/29/16, 02:51:37 AM", "Allocate a new length vector for each ORC list reader bock, since the numeric\nreaders do skip null positions, and there is code that assumes the nulls have\nzero length.", "NaN"], ["6718", "Add jitter to hive retry driver", "Nezih Yigitbasi", "nezihyigitbasi", "01/07/17, 06:16:51 AM", "I have recently seen that querying tables with small splits increases the likelihood of getting throttled by S3. Since we don't have any jitter in the retry driver, after backoff threads can wake up at roughly same times and start hammering S3 again. This PR adds some jitter to that process. \r\n\r\nFor now I set the jitter to depend on the sleep time (`delayInMs`) as I guess when things go really bad and we have high sleep times it may help to spread the requests a little bit more.", "NaN"], ["6721", "Array transform, array filter, and array reduce function", "Haozhun Jin", "haozhun", "12/06/16, 05:45:19 PM", "NaN", "NaN"], ["6732", "Add release note for 0.158", "Haozhun Jin", "haozhun", "12/01/16, 01:56:10 AM", "NaN", "NaN"], ["6734", "Fix presto-tests", "David Phillips", "electrum", "12/01/16, 07:50:59 AM", "NaN", "NaN"], ["6736", "Change DictionaryBlock to use an array internally", "Dain Sundstrom", "dain", "12/15/16, 09:01:24 PM", "NaN", "NaN"], ["6740", "QueryTemplate related refactorings", "Grzegorz Kokosi\u0144ski", "kokosing", "04/03/17, 04:39:03 PM", "Use QueryTemplate.replaceAll over nested for loop", "NaN"], ["6742", "Remove exclamation marks from queued query errors", "David Phillips", "electrum", "12/09/16, 06:11:37 PM", "We don't need to shout in error messages.", "NaN"], ["6743", "Switch to airlift toJsonWithLengthLimit", "Raghav Sethi", "raghavsethi", "12/12/16, 08:45:38 PM", "NaN", "NaN"], ["6748", "Make fail function hidden", "David Phillips", "electrum", "12/09/16, 06:10:38 PM", "NaN", "NaN"], ["6751", "Optimize Hive partition filtering", "David Phillips", "electrum", "12/02/16, 04:04:37 PM", "Unfortunately, the partitions limit breaks `SHOW PARTITIONS` even with filters due to #6753.", "NaN"], ["6754", "Add 0.159 release notes", "David Phillips", "electrum", "12/02/16, 04:17:13 PM", "NaN", "NaN"], ["6762", "Provide empty map constructor map()", "Rongrong Zhong", "rongrong", "12/10/16, 01:18:06 AM", "NaN", "NaN"], ["6763", "Fix coordinator sending infinite task DELETEs to rebooted workers", "Haozhun Jin", "haozhun", "12/08/16, 10:58:15 PM", "If a worker finishes rebooting before the coordinator gives up on a query,\r\na REMOTE_TASK_MISMATCH failure will be triggered on the coordinator. In\r\nresponse, the coordinator will send a DELETE HTTP request to the worker to\r\nabort the task. A subtle bug in the code results in the coordinator sending\r\nsuch requests to the worker continuously without delay forever.", "NaN"], ["6782", "Add documentation for lambda functions", "Haozhun Jin", "haozhun", "12/08/16, 10:54:39 PM", "NaN", "NaN"], ["6784", "Improve communication error tolerance for long running queries", "Christopher Berner", "cberner", "01/09/17, 05:24:27 PM", "This dynamically adjusts the tolerance for communication errors based on the length of time that the query has run (without error)", "NaN"], ["6786", "Fix bugs in lambda and add map_filter function", "Haozhun Jin", "haozhun", "12/07/16, 05:32:47 AM", "NaN", "NaN"], ["6788", "Lower case Hive struct field names when converting to Presto type", "Haozhun Jin", "haozhun", "12/07/16, 09:03:19 PM", "NaN", "NaN"], ["6789", "Optimize map_concat to skip empty map", "Rongrong Zhong", "rongrong", "12/10/16, 12:59:06 AM", "```\r\nPerformance before:\r\nBenchmark                     (mapConfig)  Mode  Cnt     Score     Error  Units\r\nBenchmarkMapConcat.mapConcat   left_empty  avgt   20  1838.641 \u00b1 145.909  ns/op\r\nBenchmarkMapConcat.mapConcat  right_empty  avgt   20  1515.713 \u00b1  96.888  ns/op\r\nBenchmarkMapConcat.mapConcat   both_empty  avgt   20   615.368 \u00b1  57.051  ns/op\r\nBenchmarkMapConcat.mapConcat    non_empty  avgt   20  2753.727 \u00b1 503.407  ns/op\r\n\r\nPerformance After:\r\nBenchmark                     (mapConfig)  Mode  Cnt     Score     Error  Units\r\nBenchmarkMapConcat.mapConcat   left_empty  avgt   20   392.266 \u00b1  38.556  ns/op\r\nBenchmarkMapConcat.mapConcat  right_empty  avgt   20   376.552 \u00b1  32.928  ns/op\r\nBenchmarkMapConcat.mapConcat   both_empty  avgt   20    55.276 \u00b1   7.683  ns/op\r\nBenchmarkMapConcat.mapConcat    non_empty  avgt   20  3043.951 \u00b1 553.951  ns/op\r\n\r", "NaN"], ["6792", "Add comprehensive product tests for Cassandra", "Christina Wallin", "cawallin", "02/07/17, 07:08:48 PM", "Tests including\r\n* Dataypes\r\n* Selecting and filtering both partitioning keys and clustering keys\r\n* self-joins\r\n* joins with other data sources\r\n* negative tests", "NaN"], ["6796", "Fix potential deadlock in DB backed resource group manager", "Christopher Berner", "cberner", "12/09/16, 07:58:32 PM", "The memory pool listener acquires locks in the order\r\ngeneralPoolMemoryFraction -> root group\r\nHowever, configureGroup() acquires them in the order root group ->\r\ngeneralPoolMemoryFraction", "NaN"], ["6800", "Don't split non-straddling expressions across join", "Martin Traverso", "martint", "12/10/16, 12:44:40 AM", "The query planner currently splits comparison expressions in\r\njoin criteria and pushes each term to a different side of the join.\r\n\r\nThis causes expressions that can be evaluated entirely on one side\r\nof the tree (e.g., comparisons against constants) to be evaluated\r\nas join conditions instead of being pushed down entirely to either\r\nside of the join.\r\n\r\nThis fix treats any expression that is not a pure comparison that\r\nstraddles the join as a \"complex\" expression. Predicate pushdown\r\nand other optimizers then take care of moving it to the appropriate\r\nbranch of the join.\r\n\r\n```sql\r\nSELECT *\r\nFROM (VALUES 1) t(x)\r\n    JOIN (VALUES (1,2)) u(x,y) ON t.x = u.x AND cast(u.y AS varchar) = '2';\r\n```\r\n\r\nBefore:\r\n\r\n```\r\n     - Output[x, x, y] => [field:integer, field:integer, field_2:integer]\r\n             x := field\r\n             x := field\r\n             y := field_2\r\n         - Project => [field:integer, field_2:integer]\r\n             - InnerJoin[(\"field\" = \"field_1\") AND (\"expr_8\" = \"expr_10\")] => [field:integer, expr_8:varchar, expr_10:varchar, field_1:integer, field_2:intege\r\n                 - Project => [field:integer, expr_8:varchar]\r\n                         expr_8 := CAST('2' AS varchar)\r\n                     - Values => [field:integer]\r\n                             (1)\r\n                 - Project => [expr_10:varchar, field_1:integer, field_2:integer]\r\n                         expr_10 := CAST(\"field_2\" AS varchar)\r\n                     - Filter[(CAST(\"field_2\" AS varchar) = CAST('2' AS varchar))] => [field_1:integer, field_2:integer]\r\n                         - Values => [field_1:integer, field_2:integer]\r\n                                 (1, 2)\r\n```\r\n\r\nAfter:\r\n\r\n```\r\n     - Output[x, x, y] => [field:integer, field:integer, field_2:integer]\r\n             x := field\r\n             x := field\r\n             y := field_2\r\n         - Project => [field:integer, field_2:integer]\r\n             - InnerJoin[(\"field\" = \"field_1\")] => [field:integer, field_1:integer, field_2:integer]\r\n                 - Values => [field:integer]\r\n                         (1)\r\n                 - Filter[(CAST(\"field_2\" AS varchar) = CAST('2' AS varchar))] => [field_1:integer, field_2:integer]\r\n                     - Values => [field_1:integer, field_2:integer]\r\n                             (1, 2)\r\n```", "NaN"], ["6802", "Fix failure when complex join criteria contains parameter", "Martin Traverso", "martint", "12/09/16, 05:35:45 PM", "Fixes https://github.com/prestodb/presto/issues/6801", "NaN"], ["6804", "Include message in 403 response from coordinator", "Martin Traverso", "martint", "12/09/16, 07:53:02 PM", "Jax-rs' WebApplicationException produces a canned response\r\nthat doesn't include the provided message unless the Response\r\nobject is passed explicitly during construction.\r\n\r\nThis change provides a more user-friendly message in the HTTP\r\nresponse body in case of an \"access denied\" error.", "NaN"], ["6815", "Remove unused field from HiveMetadata", "David Phillips", "electrum", "12/09/16, 06:10:08 PM", "NaN", "NaN"], ["6817", "Reverse argument order of lambda in array reduce function", "Haozhun Jin", "haozhun", "12/08/16, 10:54:03 PM", "NaN", "NaN"], ["6819", "Add 0.160 release notes", "David Phillips", "electrum", "12/09/16, 01:12:24 AM", "NaN", "NaN"], ["6821", "Fix documentation build failure", "David Phillips", "electrum", "12/09/16, 01:51:26 AM", "NaN", "NaN"], ["6822", "Add -n option to Sphinx Makefile", "David Phillips", "electrum", "12/09/16, 06:09:39 PM", "This setting matches sphinx-maven-plugin.", "NaN"], ["6825", "Add a config parameter to enable writes to non-managed Hive tables", "Nezih Yigitbasi", "nezihyigitbasi", "03/28/17, 07:59:35 PM", "NaN", "NaN"], ["6830", "Add connector comment field for SHOW COLUMNS", "David Phillips", "electrum", "12/12/16, 08:17:11 PM", "This allows connectors to provide additional metadata such as\n\"partition key\" without needing to overload the comment field.", "NaN"], ["6831", "Make array_concat and map_concat variadic functions", "Mark", "geraint0923", "03/01/17, 12:05:59 AM", "NaN", "NaN"], ["6832", "Add lambda functions to array and map sections", "David Phillips", "electrum", "12/09/16, 11:06:16 PM", "NaN", "NaN"], ["6833", "Simplify function signatures in lambda documentation", "David Phillips", "electrum", "12/09/16, 11:13:37 PM", "NaN", "NaN"], ["6836", "Fix planning failure in lambda when subquery is present", "Haozhun Jin", "haozhun", "12/10/16, 12:44:44 AM", "NaN", "NaN"], ["6837", "Fix DESCRBE OUTPUT for CREATE VIEW", "David Phillips", "electrum", "12/12/16, 08:18:52 PM", "The output should be empty since the statement doesn't return anything.", "NaN"], ["6838", "Enable support date and string statistics for file and stripe", "Dain Sundstrom", "dain", "12/15/16, 09:10:55 PM", "Check ORC post script to determine if the file was written with a version\nthat fixes date and string statics at the file and stripe level", "NaN"], ["6847", "Require state to be non-null in ServerInfoResource::updateState", "Nezih Yigitbasi", "nezihyigitbasi", "12/15/16, 09:58:27 PM", "Fixes #6845. This makes the error message a little bit more descriptive.", "NaN"], ["6848", "Convert Teradata functions to AbstractTestFunctions", "David Phillips", "electrum", "12/15/16, 11:39:18 PM", "NaN", "NaN"], ["6849", "Guarantee queries 1MB of memory per node before blocking", "Christopher Berner", "cberner", "12/13/16, 05:37:05 AM", "This allows trivial queries to run even when the node is \"out of\r\nmemory\"", "NaN"], ["6850", "Validate S3 key length", "Nezih Yigitbasi", "nezihyigitbasi", "03/17/17, 09:46:01 PM", "With an `INSERT` query that has a huge value for a `varchar` partition key I have seen that the S3 backend responded with HTTP 400 as the key is too long. According to the S3 [docs](http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html) a valid key is a sequence of Unicode characters whose UTF-8 encoding is at most 1024 bytes long. This PR adds validation for that when creating keys from paths.", "NaN"], ["6851", "Revert \"Switch to airlift toJsonWithLengthLimit\"", "Raghav Sethi", "raghavsethi", "12/12/16, 10:42:00 PM", "This reverts commit af03259e84507db6c331763f9b27dafb752c97a5.", "NaN"], ["6852", "Categorize error for Hive ALTER TABLE failure", "David Phillips", "electrum", "12/12/16, 10:56:44 PM", "NaN", "NaN"], ["6853", "Use toJsonWithLength limit from airlift", "Raghav Sethi", "raghavsethi", "01/17/17, 04:39:53 PM", "NaN", "NaN"], ["6854", "Optimize MapFilterFunction by generating bytecode", "Mark", "geraint0923", "03/01/17, 12:06:30 AM", "NaN", "NaN"], ["6855", "Fix broken check for partitions in HiveTableLayoutHandle", "Raghav Sethi", "raghavsethi", "12/13/16, 01:46:52 AM", "NaN", "NaN"], ["6857", "Use new generated protobuf for presto-orc", "David Phillips", "electrum", "12/16/16, 04:12:35 PM", "NaN", "NaN"], ["6858", "Use correct output descriptor when creating view", "David Phillips", "electrum", "12/13/16, 04:16:34 AM", "NaN", "NaN"], ["6866", "Use airbase flags for checkstyle defaults", "David Phillips", "electrum", "12/15/16, 10:53:53 PM", "NaN", "NaN"], ["6868", "Add compression to exchanges", "Raghav Sethi", "raghavsethi", "01/25/17, 10:29:24 PM", "Some notes:\r\n- The way it turned out, it doesn't make a lot of sense to do serialization and compression in separate commits, as the compression-specific stuff is maybe 50 lines total.\r\n- This still needs benchmarking.\r\n- There is potential for reducing number of allocations by creating per-thread buffers we can re-use for serialization and compression (16MB per buffer per ExchangeOperator probably too much?). Maybe there is another way. We already have per-thread tables implicitly because of the way Lz4Compressor is designed.", "NaN"], ["6871", "Add config option and session property to restore legacy ORDER BY behavior", "Martin Traverso", "martint", "12/14/16, 12:38:59 AM", "We recently made a change to the column resolution rules for ORDER BY to\r\nmake them compliant with ANSI SQL. In order to ease the transition from\r\nthe old semantics, we now add a config option and session property that\r\ncontrols the behavior.\r\n\r\nThe session property is \"legacy_order_by\". The config option is\r\n\"deprecated.legacy-order-by\".\r\n\r\nThis will be removed in a not-too-distant version.\r\n\r\nFixes #6867", "NaN"], ["6872", "Remove unused variables", "Christopher Berner", "cberner", "12/15/16, 07:24:08 PM", "NaN", "NaN"], ["6877", "Improve error message for aggregations inside lambda", "Haozhun Jin", "haozhun", "12/21/16, 12:34:13 AM", "Fixes #6875", "NaN"], ["6878", "Add missing raptor schema index in metadata", "Eric Hwang", "erichwang", "12/14/16, 03:26:07 AM", "NaN", "NaN"], ["6880", "Ignore forced running splits when checking minimumNumberOfDrivers.", "Piotr Nowojski", "pnowojski", "01/07/17, 06:16:22 AM", "Otherwise with (for example) minimumNumberOfDrivers = 100, 200 \"forceStart\" splits\r\nand 100 \"normal\" splits, depending on order of appearing splits, number of simultaneously running splits may vary. If normal splits start first, there will be 300 running splits. If \"forceStart\" splits start first, there will be only 200 running splits.\r\n\r\nThis problem was observed on tpch q21 query:\r\n\r\n```\r\nset session join_distribution_type = 'automatic';\r\n\r\nSELECT s.name, count(*) as numwait FROM \"supplier\" s, \"lineitem\" l1, \"orders\" o, \"nation\" n WHERE s.suppkey = l1.suppkey AND o.orderkey = l1.orderkey AND o.orderstatus = 'F' AND l1.receiptdate> l1.commitdate AND EXISTS ( SELECT * FROM \"lineitem\" l2 WHERE l2.orderkey = l1.orderkey AND l2.suppkey <> l1.suppkey ) AND NOT EXISTS ( SELECT * FROM \"lineitem\" l3 WHERE l3.orderkey = l1.orderkey AND l3.suppkey <> l1.suppkey AND l3.receiptdate > l3.commitdate ) AND s.nationkey = n.nationkey AND n.name = 'SAUDI ARABIA' GROUP BY s.name ORDER BY numwait DESC, s.name LIMIT 1;\r\n```\r\n\r\nWith `set session join_distribution_type = 'repartitioned';` (old `distributed_joins=true`) this query completed in ~42 seconds against tpch 100gb on 8 node cluster. With `set session join_distribution_type = 'automatic';` it completed randomly between 30 seconds and 60 seconds WITHOUT this PR. With this PR it has stable execution times 27-29seconds.\r\n\r\ncode base: https://github.com/Teradata/presto/tree/sprint-44\r\n\r\nLink to explain for the above query: http://pastebin.com/DEiy65pb\r\n\r\nProblem was with race between splits from fragment 3 (probe side) and build splits from fragment 2.", "NaN"], ["6881", "Fix broken default for legacy_order_by session property", "Martin Traverso", "martint", "12/14/16, 06:06:02 PM", "The arguments were in the wrong order, so the value from\r\nFeaturesConfig was not being used to control the default value.", "NaN"], ["6884", "Simplify documentation for min_by and max_by", "David Phillips", "electrum", "12/14/16, 08:02:44 PM", "NaN", "NaN"], ["6885", "Fix planning issue when ORDER BY clause contains aggregations", "Martin Traverso", "martint", "12/14/16, 09:32:31 PM", "A recent commit (ec2e8976ba9cd59f3990b8e46288b8a728065b07) changed\r\nthe way ORDER BY expressions are handled in a way that causes certain\r\nexpression to not be \"analyzed\" and their types be recorded in the\r\nAnalysis object.\r\n\r\nextractAggregates() looks for aggregations in node.getOrderBy()\r\nand records them for later use by the planner. The new ORDER BY\r\nanalyzer process the rewritten expressions, which have a different\r\nobject identity. As a result, the aggregates don't have associated\r\ntype and implicit coercion information for the planner to use.\r\n\r\nThis change makes it so that the aggregates are extracted from\r\nthe rewritten expressions.\r\n\r\nFixes https://github.com/prestodb/presto/issues/6882", "NaN"], ["6886", "Optimize ArrayTransformFunction by generating bytecode", "Mark", "geraint0923", "01/25/17, 02:15:57 AM", "Fix #6780 \r\n\r\n```\r\nBenchmark                                 (name)  Mode  Cnt    Score   Error  Units\r\nBenchmarkArrayTransform.benchmark      transform  avgt   20   80.193 \u00b1 5.897  ns/op\r\nBenchmarkArrayTransform.benchmark  old_transform  avgt   20  133.509 \u00b1 4.141  ns/op\r\n```", "NaN"], ["6887", "Fix reporting for errors in ORDER BY expression", "Martin Traverso", "martint", "12/14/16, 09:33:25 PM", "Due to ec2e8976ba9cd59f3990b8e46288b8a728065b07, when analysis fails for certain\r\nexpressions, the error is misreported as happening in the SELECT clause\r\ninstead of in the ORDER BY clause. This is because the analyzer processes\r\nthe rewritten expressions, which contain inlined SELECT expressions and their\r\noriginal locations.\r\n\r\nThis change fixes the issue by analyzing the original unmodified expressions\r\nwith a synthetic scope built from the output of the SELECT clause\r\nthat can delegate resolution to the source scope for missing names (essentially,\r\nit implements the resolution rules per the SQL spec).\r\n\r\nOne side-effect of this change is that queries whose ORDER BY clause reference\r\ncolumns that appear multiple times in the SELECT clause are now considered\r\ninvalid due to ambiguous references -- this matches the expected behavior\r\naccording to the ANSI spec.\r\n\r\nFixes https://github.com/prestodb/presto/issues/6883\r\n\r\n(First commit is part of https://github.com/prestodb/presto/pull/6885)", "NaN"], ["6888", "Add release notes for 0.161", "Christopher Berner", "cberner", "12/15/16, 12:04:14 AM", "NaN", "NaN"], ["6890", "Fix broken test in TestAnalyzer", "Martin Traverso", "martint", "12/14/16, 11:45:33 PM", "This query shape is no longer valid due to ambiguous\r\ncolumn references.\r\n\r\nFixes https://github.com/prestodb/presto/issues/6889", "NaN"], ["6891", "Update 0.161 release notes", "Christopher Berner", "cberner", "12/15/16, 12:53:04 AM", "NaN", "NaN"], ["6892", "Support Parquet TupleDomain using ColumnDescriptor", "Zhenxiao Luo", "zhenxiao", "04/13/17, 04:22:50 PM", "Currently Parquet TupleDomain is constructed based on HiveColumnHandle. This would not work if Nested predicate are pushed down, e.g.\r\n```\r\nselect s.a\r\nfrom t\r\nwhere s.b > 10\r\n```\r\nThis patch construct Parquet TupleDomain with Parquet's ColumnDescriptor, so that it could work with nested predicate pushdown\r", "NaN"], ["6894", "Use toIntExact instead of Ints.checkedCast", "David Phillips", "electrum", "12/16/16, 02:19:19 AM", "NaN", "NaN"], ["6897", "Validate location to analysis error", "Martin Traverso", "martint", "12/15/16, 07:25:30 PM", "This is a followup to a6d036211eb33215537e3aa7cd83a76d06ca5b99 to\r\nensure the correct location is reported.", "NaN"], ["6902", "Fix new RCFile reader handling of non-selected columns", "Dain Sundstrom", "dain", "12/17/16, 11:00:31 PM", "NaN", "NaN"], ["6903", "Fix state check in JoinGraph", "Nezih Yigitbasi", "nezihyigitbasi", "12/16/16, 01:21:49 AM", "`edges` is of type `Multimap<PlanNodeId, Edge>` so a `PlanNodeId` should be passed to `containsKey` call.", "NaN"], ["6906", "Deduplicate ORDER BY keys after unaliasing", "Rongrong Zhong", "rongrong", "12/16/16, 06:37:59 PM", "Resolves #6905", "NaN"], ["6907", "Parallelize reading from remote sources", "Raghav Sethi", "raghavsethi", "01/08/17, 12:14:10 AM", "This changes planning such that multiple ExchangeOperators read from\r\nHttpPageBufferClient in parallel. This improves performance in many\r\ncases.", "NaN"], ["6908", "Add xxhash64 and to_big_endian_64 functions", "Eric Hwang", "erichwang", "12/17/16, 02:49:05 AM", "NaN", "NaN"], ["6914", "Checkstyle: toIntExact and requireNonNull", "Haozhun Jin", "haozhun", "12/16/16, 10:39:53 PM", "NaN", "NaN"], ["6918", "Block maintenance operations during deletes", "David Phillips", "electrum", "01/18/17, 02:47:43 AM", "NaN", "NaN"], ["6925", "Update CREATE TABLE documentation for column comments", "David Phillips", "electrum", "12/19/16, 08:52:23 PM", "NaN", "NaN"], ["6926", "New orc writer", "Dain Sundstrom", "dain", "07/27/17, 04:43:06 AM", "Some benchmark results:\r\n```\r\n  write       LINEITEM                        SNAPPY      PRESTO_ORC                 4.04    90.3MB/s \u00b1  1480.3kB/s ( 1.60%) (N = 150, \u03b1 = 99.9%)\r\n  write       LINEITEM                        SNAPPY      HIVE_ORC                   4.00    70.7MB/s \u00b1   509.5kB/s ( 0.70%) (N = 150, \u03b1 = 99.9%)\r\n  write       BIGINT_SEQUENTIAL               SNAPPY      PRESTO_ORC               158.48   351.2MB/s \u00b1  2167.1kB/s ( 0.60%) (N = 150, \u03b1 = 99.9%)\r\n  write       BIGINT_SEQUENTIAL               SNAPPY      HIVE_ORC                 155.38   151.3MB/s \u00b1   844.4kB/s ( 0.54%) (N = 150, \u03b1 = 99.9%)\r\n  write       BIGINT_RANDOM                   SNAPPY      PRESTO_ORC                 2.99   172.6MB/s \u00b1  1669.1kB/s ( 0.94%) (N = 150, \u03b1 = 99.9%)\r\n  write       BIGINT_RANDOM                   SNAPPY      HIVE_ORC                   2.99   108.4MB/s \u00b1   498.9kB/s ( 0.45%) (N = 150, \u03b1 = 99.9%)\r\n  write       VARCHAR_SMALL                   SNAPPY      PRESTO_ORC                 9.88    67.7MB/s \u00b1   256.4kB/s ( 0.37%) (N = 150, \u03b1 = 99.9%)\r\n  write       VARCHAR_SMALL                   SNAPPY      HIVE_ORC                   9.88    23.2MB/s \u00b1    51.6kB/s ( 0.22%) (N = 150, \u03b1 = 99.9%)\r\n  write       VARCHAR_LARGE                   SNAPPY      PRESTO_ORC                 9.88    67.6MB/s \u00b1   327.7kB/s ( 0.47%) (N = 150, \u03b1 = 99.9%)\r\n  write       VARCHAR_LARGE                   SNAPPY      HIVE_ORC                   9.88    23.5MB/s \u00b1    77.5kB/s ( 0.32%) (N = 150, \u03b1 = 99.9%)\r\n  write       VARCHAR_DICTIONARY              SNAPPY      PRESTO_ORC                23.77   115.6MB/s \u00b1   434.1kB/s ( 0.37%) (N = 150, \u03b1 = 99.9%)\r\n  write       VARCHAR_DICTIONARY              SNAPPY      HIVE_ORC                  23.67    69.0MB/s \u00b1   257.9kB/s ( 0.36%) (N = 150, \u03b1 = 99.9%)\r\n  write       MAP_VARCHAR_DOUBLE              SNAPPY      PRESTO_ORC                 2.13   107.6MB/s \u00b1   887.0kB/s ( 0.81%) (N = 150, \u03b1 = 99.9%)\r\n  write       MAP_VARCHAR_DOUBLE              SNAPPY      HIVE_ORC                   2.13    25.5MB/s \u00b1   424.6kB/s ( 1.63%) (N = 150, \u03b1 = 99.9%)\r\n  write       LARGE_MAP_VARCHAR_DOUBLE        SNAPPY      PRESTO_ORC                 1.59    34.8MB/s \u00b1   253.8kB/s ( 0.71%) (N = 150, \u03b1 = 99.9%)\r\n  write       LARGE_MAP_VARCHAR_DOUBLE        SNAPPY      HIVE_ORC                   1.70  9276.3kB/s \u00b1   120.0kB/s ( 1.29%) (N = 150, \u03b1 = 99.9%)\r\n  write       MAP_INT_DOUBLE                  SNAPPY      PRESTO_ORC                 1.44   200.4MB/s \u00b1  4242.5kB/s ( 2.07%) (N = 150, \u03b1 = 99.9%)\r\n  write       MAP_INT_DOUBLE                  SNAPPY      HIVE_ORC                   1.44    93.7MB/s \u00b1   404.6kB/s ( 0.42%) (N = 150, \u03b1 = 99.9%)\r\n  write       LARGE_MAP_INT_DOUBLE            SNAPPY      PRESTO_ORC                 1.17   196.2MB/s \u00b1  1404.0kB/s ( 0.70%) (N = 150, \u03b1 = 99.9%)\r\n  write       LARGE_MAP_INT_DOUBLE            SNAPPY      HIVE_ORC                   1.17    43.2MB/s \u00b1  1041.9kB/s ( 2.36%) (N = 150, \u03b1 = 99.9%)\r\n  write       LARGE_ARRAY_VARCHAR             SNAPPY      PRESTO_ORC                 2.12    32.8MB/s \u00b1   190.6kB/s ( 0.57%) (N = 150, \u03b1 = 99.9%)\r\n  write       LARGE_ARRAY_VARCHAR             SNAPPY      HIVE_ORC                   2.06  4676.1kB/s \u00b1    44.9kB/s ( 0.96%) (N = 150, \u03b1 = 99.9%)\r\n```\r\n\r\nThe new orc writer code starts at commit \"Add array, map, row blocks column decomposition\".  The previous commits are in the new-rcfile-writer pull request.", "NaN"], ["6927", "Implement zip with lambda for two arrays", "Rongrong Zhong", "rongrong", "02/10/17, 03:19:31 AM", "NaN", "NaN"], ["6931", "Add static import for toIntExact", "Raghav Sethi", "raghavsethi", "12/20/16, 07:08:49 PM", "NaN", "NaN"], ["6947", "Fix syntax for values in select documentation", "Sergey Goder", "cosinequanon", "12/22/16, 07:13:28 PM", "I tested the new queries in the CLI", "NaN"], ["6951", "Add javadoc for uncoercedSubquery method", "Karol Sobczak", "sopel39", "12/22/16, 04:59:24 PM", "added javadoc as requested", "NaN"], ["6955", "Fix new RCFile reader support for old files", "Dain Sundstrom", "dain", "12/23/16, 12:30:03 AM", "If the file has fewer columns than expected by the table, return a block of nulls", "NaN"], ["6956", "Iterative optimizer", "Martin Traverso", "martint", "01/07/17, 06:17:55 AM", "This optimizer decouples the traversal of the plan tree (IterativeOptimizer)\r\nfrom the transformation logic (Rule). The optimization loop applies rules\r\nrecursively until a fixpoint is reached.\r\n\r\nIt's implemented as PlanOptimizer so that it fits right into the existing\r\nframework.\r\n\r\nExtracted from https://github.com/prestodb/presto/pull/6700", "NaN"], ["6961", "Preserve relative order of conjuncts", "Martin Traverso", "martint", "12/23/16, 10:27:02 PM", "ExpressionUtils.and() and ExpressionUtils.combineConjuncts()\r\ntry to build a balanced tree of terms. Due to the way the code\r\nwas written, for odd-numbered expressions, the algorithm places\r\nthe last term as the first term in the tree. For example:\r\n\r\n    and(a, b, c, d, e) -> and(e, and(and(a, b), and(c, d)))\r\n\r\nThis can cause the predicate pushdown optimizer to reorder terms\r\nthat are sensitive to evaluation order.\r\n\r\nAdditionally, combineConjuncts attempts to remove duplicate terms,\r\nbut it does not preserve the relative ordering of the terms in the\r\ninput.", "NaN"], ["6962", "Update to antlr 4.6", "Martin Traverso", "martint", "01/07/17, 06:15:54 AM", "NaN", "NaN"], ["6974", "Avoid variable boxing in InMemoryHashAggregationBuilder (#6730)", "Andrzej Fiedukowicz", "fiedukow", "01/07/17, 06:19:12 AM", "NaN", "NaN"], ["6977", "Fix and refactor boolean expression optimization", "Eric Hwang", "erichwang", "01/04/17, 07:30:45 PM", "This fixes a correctness issue in the boolean expression optimization that\r\noccurs when a multiple nested boolean expression attempts to factor out some\r\ncommon sub-expressions.\r\n\r\nFor Example:\r\n((A AND B) OR (A AND C)) AND D\r\n\r\nFixes #6954 ", "NaN"], ["6978", "Support _HOST placeholder in kerberos principal in Hive connector", "Andrii Rosa", "arhimondr", "02/02/17, 05:43:08 AM", "Product test has been added.\r\n\r\n@supersedes https://github.com/prestodb/presto/pull/6861\r\n\r\nMust be merged after https://github.com/Teradata/docker-images/pull/9 is merged and released, and the version is changed from `latest` to `12` in `compose-commons.sh`", "NaN"], ["6979", "Minor JDBC driver improvements", "David Phillips", "electrum", "02/28/17, 11:42:01 PM", "Fixes #6929", "NaN"], ["6980", "Fix NPE when Like node has no escape", "Martin Traverso", "martint", "12/29/16, 08:08:36 PM", "This was a regression introduced by 24407ce9de51c8d567990648d30d57e86e8a4e68\r\nand a8d0940df0e777adf8b7bcd7c1e34546c7ffbeb2.\r\n\r\nLikePredicate.getNodes() was not handling the case where the escape is\r\nmissing (null), and it was trying to add a null to an ImmutableList.", "NaN"], ["6983", "Fix performance regression in getTableLayout", "Raghav Sethi", "raghavsethi", "12/30/16, 12:54:31 AM", "I'm not super happy with the test - it seems too specific to the current design, rather than a more generic performance sanity check.\r\n\r\nFixes #6970.", "NaN"], ["6992", "Add BucketBalancer to list of injected classes", "Raghav Sethi", "raghavsethi", "12/31/16, 11:02:21 PM", "Well, this is embarrassing.", "NaN"], ["6997", "Make QueryTemplate and Parameter constructors private", "Artur Gajowy", "ArturGajowy", "01/07/17, 06:15:23 AM", "Use factory methods for previous constructor usages.", "NaN"], ["6999", "Make HIVE_COLUMN_ORDER_MISMATCH into USER_ERROR", "Piotr Findeisen", "findepi", "01/07/17, 06:14:31 AM", "Fixes #6973", "NaN"], ["7000", "Fix Hive createPredicate for partitions with only null keys", "Raghav Sethi", "raghavsethi", "01/04/17, 01:34:28 AM", "NaN", "NaN"], ["7001", "Add limit on SQL query text length", "Wenlei Xie", "wenleix", "02/02/17, 01:33:28 AM", "As requested in https://github.com/prestodb/presto/issues/6664", "NaN"], ["7003", "Replace .stream().forEach() with forEach()", "Raghav Sethi", "raghavsethi", "01/04/17, 09:40:03 PM", "NaN", "NaN"], ["7004", "Remove unused ServerInfo class from SPI", "David Phillips", "electrum", "01/10/17, 07:44:42 PM", "NaN", "NaN"], ["7007", "Update for tempto 1.23", "Sanjay Sharma", "sanjay990", "01/07/17, 06:13:48 AM", "NaN", "NaN"], ["7013", "Remove thrift client from cassandra connector", "Andrii Rosa", "arhimondr", "02/10/17, 01:05:38 AM", "NaN", "NaN"], ["7015", "Support equality pushdown on clustering keys for Cassandra", "Amruta Gokhale", "amrutagokhale", "03/03/17, 02:34:42 PM", "This is still a work in progress. I opened a pull request so as to get some initial feedback from @arhimondr . There are some  `TODO`s in the code that I need to take care of and also test for corner cases. So this isn't a full-fledged implementation, and hence isn't ready for a detailed code review yet.", "NaN"], ["7016", "Add 0.162 release notes", "Dain Sundstrom", "dain", "01/06/17, 11:13:39 PM", "NaN", "NaN"], ["7018", "Update 0.162 release notes", "David Phillips", "electrum", "01/06/17, 11:24:36 PM", "NaN", "NaN"], ["7019", "Make ALL, SOME, and ANY unreserved keywords", "Mark", "geraint0923", "01/09/17, 06:19:25 PM", "NaN", "NaN"], ["7020", "Implement EXPLAIN (TYPE VALIDATE)", "Rongrong Zhong", "rongrong", "01/19/17, 03:06:45 AM", "Resolves #7017", "NaN"], ["7022", "Use WholeRowIterator in AccumuloRecordCursor", "Adam J. Shook", "adamjshook", "01/09/17, 06:41:04 PM", "This fixes an issue where some results returned by the AccumuloRecordCursor\r\nwere incomplete.  Due to the use of a BatchScanner, the entries returned\r\ncould span multiple rows, causing incorrect results.  By using a WholeRowIterator\r\nwe receive the entire row that is then decoded and deserialized.  It\r\nalso simplifies the code!", "NaN"], ["7024", "Separate docs and example in Queue docs", "Piotr Findeisen", "findepi", "01/09/17, 06:42:16 PM", "Make it clear where general documentation ends and an example begins.", "NaN"], ["7026", "Implement SUM and AVG on interval types #6966", "Andrzej Fiedukowicz", "fiedukow", "04/27/17, 03:07:53 AM", "Simple implementation based on already existing SUM and AVG variants, but provided for Interval types as requested in #6966 issue.\r\nI think there is no reason not to provide those variants and they seem very useful.", "NaN"], ["7028", "Update Modernizer plugin", "David Phillips", "electrum", "01/10/17, 07:54:57 PM", "NaN", "NaN"], ["7031", "Produce Hive splits for bucketed tables in round-robin fashion", "Haozhun Jin", "haozhun", "01/14/17, 02:13:48 AM", "This reduces the likelihood that the scheduler gets blocked when one worker has\r\nmore splits queued than limit while other workers have no splits. Without round\r\nrobin, for a bucketed partition, the split loader would produce a series of\r\nsplits that has node afinity like C, C, ..., C, A, A, ..., A, D, D, ..., D, B,\r\nB, ..., B. If there are more splits for node C than the number of queued splits\r\nallowed, node A, B, D would not have any split available to run because the\r\nscheduler is blocked.\r\n\r\nIn addition, this commit changes the policy to determine the target size for\r\ninitial splits. The original policy tries to produce a number of initial splits\r\nthat have similar size. For example, assuming maxInitialSplitSize = 9K,\r\nmaxSplitSize = 30K, and the file size is 48K.\r\n\r\n* When maxInitialSplits = 10, the file will be split into 8K, 8K, 8K, 8K, 8K, 8K.\r\n* When maxInitialSplits = 2, the file will be split into 8K, 8K, 16K, 16K.\r\n\r\nIn the new policy,\r\n\r\n* When maxInitialSplits = 10, the file will be split into 9K, 9K, 9K, 9K, 9K, 3K.\r\n* When maxInitialSplits = 2, the file will be split into 9K, 9K, 30K.\r\n\r\nYou can see that the old policy is better for case 1, while the new policy is\r\nbetter for case 2. A smart policy that are optimal in both cases are available.\r\nHowever, such policy has to know exactly how many initial splits are left.\r\nThis is not possible given the parallel nature of BackgroundHiveSplitLoader.", "NaN"], ["7034", "Remove unused getCommonSuperType() from TypeManager", "David Phillips", "electrum", "01/10/17, 07:54:26 PM", "NaN", "NaN"], ["7037", "Fix compiler failure when try/lambda is repeated in an expression", "Haozhun Jin", "haozhun", "01/13/17, 06:17:24 PM", "Fixes #7036 ", "NaN"], ["7039", "Fix duration logging in DistributedQueryRunner", "David Phillips", "electrum", "01/11/17, 01:06:45 AM", "NaN", "NaN"], ["7040", "Improve error message for missing keys used with map subscript", "Christopher Berner", "cberner", "01/13/17, 11:08:48 PM", "NaN", "NaN"], ["7041", "Fix formatting typo in 0.161 release notes", "David Phillips", "electrum", "01/11/17, 10:01:55 PM", "NaN", "NaN"], ["7042", "Update to Slice 0.28", "David Phillips", "electrum", "01/11/17, 01:51:14 AM", "NaN", "NaN"], ["7043", "Remove connector ID from JMX connector", "David Phillips", "electrum", "01/17/17, 11:48:44 PM", "NaN", "NaN"], ["7044", "Fix xxhash64 function to return varbinary instead of bigint", "Eric Hwang", "erichwang", "01/11/17, 07:34:46 PM", "NaN", "NaN"], ["7048", "Fix DSL matching for Aggregation and GroupId nodes", "Rebecca Schlussel", "rschlussel-zz", "01/13/17, 10:22:44 PM", "Use symbol aliases rather than symbols when looking for a match.", "NaN"], ["7050", "Add 0.163 release notes", "Martin Traverso", "martint", "01/12/17, 02:58:27 AM", "NaN", "NaN"], ["7052", "Change PageBuilder.declarePositions(int) to avoid confusion", "Haozhun Jin", "haozhun", "01/12/17, 06:36:16 PM", "See commit message for details", "NaN"], ["7054", "Add docs for from_big_endian_64", "Eric Hwang", "erichwang", "01/11/17, 10:30:30 PM", "NaN", "NaN"], ["7055", "Update presto-product-tests README for Cassandra", "Sanjay Sharma", "sanjay990", "01/12/17, 08:06:00 PM", "NaN", "NaN"], ["7059", "Fail non-aggregates with FILTER clause", "Martin Traverso", "martint", "01/13/17, 06:45:28 PM", "The analyzer was not ensuring that FILTER clauses appear only\r\nnext to aggregations functions, causing queries to succeed\r\nand produce incorrect results.\r\n\r\nFixes https://github.com/prestodb/presto/issues/7058", "NaN"], ["7060", "Push aggregations below outer joins", "Rebecca Schlussel", "rschlussel-zz", "05/16/17, 01:59:07 PM", "This rewrite pushes aggregations below outer joins when the grouping keys contain all columns in the outer table, and the outer table's rows are unique.  It is particularly useful for correlated scalar aggregations, which are rewritten to an aggregation above a left outer join.\r\n\r\nFixes https://github.com/prestodb/presto/issues/6988\r\nThe first commit is from https://github.com/prestodb/presto/pull/7048 so there can be planner tests.", "NaN"], ["7063", "Dump byte code when NegativeArraySizeException is thrown", "Rongrong Zhong", "rongrong", "01/18/17, 02:12:41 AM", "NaN", "NaN"], ["7067", "Search for subquery projection only above AggregationNode", "Karol Sobczak", "sopel39", "01/13/17, 05:52:21 PM", "Previously subquery ProjectNode was found below AggregationNode in\r\nTransformCorrelatedScalarAggregationToJoin, which caused\r\na new ProjectNode to be created with unsatisfied symbols.\r\n\r\nFixes: https://github.com/prestodb/presto/issues/7064", "NaN"], ["7071", "Add proper overflow handling when estimating boolean distribution expansion", "Eric Hwang", "erichwang", "01/13/17, 06:08:40 PM", "Fixes #7069 and Fixes #7070 ", "NaN"], ["7073", "Fix planning failure with nested selective aggregates", "Martin Traverso", "martint", "01/13/17, 09:08:06 PM", "The optimizer was stopping at the first aggregation node\r\nthat contained FILTER clauses.\r\n\r\nFixes https://github.com/prestodb/presto/issues/7072", "NaN"], ["7074", "Migrate ImplementFilteredAggregations to iterative optimizer", "Martin Traverso", "martint", "01/13/17, 11:00:25 PM", "NaN", "NaN"], ["7075", "Migrate ImplementSampleAsFilter to iterative optimizer", "Martin Traverso", "martint", "01/13/17, 10:43:37 PM", "NaN", "NaN"], ["7076", "Migrate CountConstantOptimizer to iterative optimizer", "Martin Traverso", "martint", "01/13/17, 09:43:36 PM", "NaN", "NaN"], ["7077", "Create symlink for configuration", "Eric Diven", "ebd2", "01/23/17, 06:57:01 PM", "Presto expects to find some of its configuration files in\r\n/usr/lib/presto/etc; create a symlink to /etc/presto in that location\r\nso we can configure the following plug-ins:\r\n- SystemAccessControl access-control.properties\r\n- EventListener event-listener.properties\r\n- ResourceGroups resource-groups.properties", "NaN"], ["7078", "Classify exceptions from RcFilePageSource", "Dain Sundstrom", "dain", "01/13/17, 10:22:23 PM", "NaN", "NaN"], ["7079", "Improve array_join performance", "Martin Traverso", "martint", "01/19/17, 05:51:25 PM", "The new implementation shows a ~50% improvement for arrays of 10 elements\r\n\r\n```\r\nbefore  avgt   60  366.130 \u00b1 5.974  ns/op\r\nafter   avgt   60  189.217 \u00b1 6.739  ns/op\r\n```", "NaN"], ["7080", "Add ArbitraryOutputBuffer", "Dain Sundstrom", "dain", "02/15/17, 10:26:49 PM", "Buffer implements a first come, first served, distribution which is useful for parallel client download and table writing.", "NaN"], ["7082", "Migrate SingleDistinctOptimizer to iterative optimizer", "Martin Traverso", "martint", "01/14/17, 02:42:04 AM", "NaN", "NaN"], ["7083", "Add PruneTableScanColumns rule", "Martin Traverso", "martint", "01/14/17, 04:24:55 AM", "NaN", "NaN"], ["7084", "Add PruneValuesColumns rule", "Martin Traverso", "martint", "01/14/17, 02:36:48 AM", "NaN", "NaN"], ["7085", "Add MergeLimits rule", "Martin Traverso", "martint", "01/14/17, 02:33:35 AM", "NaN", "NaN"], ["7086", "Add PushLimitThroughProject rule", "Martin Traverso", "martint", "01/14/17, 02:32:36 AM", "NaN", "NaN"], ["7087", "Add EvaluateZeroLimit rule", "Martin Traverso", "martint", "01/14/17, 02:30:59 AM", "NaN", "NaN"], ["7088", "Add RemoveRedundantProjections rule", "Martin Traverso", "martint", "01/22/17, 06:53:33 PM", "NaN", "NaN"], ["7090", "Add MergeLimitWithDistinct rule", "Martin Traverso", "martint", "01/17/17, 07:41:51 PM", "NaN", "NaN"], ["7091", "Add PushLimitThroughMarkDistinct rule", "Martin Traverso", "martint", "01/17/17, 07:34:41 PM", "NaN", "NaN"], ["7092", "Add MergeLimitWithTopN rule", "Martin Traverso", "martint", "01/17/17, 07:28:36 PM", "NaN", "NaN"], ["7093", "Add MergeLimitWithSort rule", "Martin Traverso", "martint", "01/17/17, 07:23:44 PM", "NaN", "NaN"], ["7094", "Add InlineProjections rule", "Martin Traverso", "martint", "04/14/17, 10:04:09 PM", "NaN", "NaN"], ["7095", "Fix partial aggregation push down through union", "Piotr Nowojski", "pnowojski", "03/22/17, 03:55:29 AM", "It seems like a regression caused by @martint, because couple months ago there was this pr https://github.com/prestodb/presto/pull/5981\r\n\r\nProblem was that `PartialAggregationPushDown` was not able to push aggregation through more then one `ExchangeNode`. When it found plan: `Aggregation -> Exchange -> Exchange`, it was transforming it to `Aggregation Final -> Exchange -> Aggregation Partial -> Project -> Exchange`. This projection was making it unable to push partial aggregation further down. \r\n\r\nAlternative fix to that, would be to run `PartialAggregationPushDown`, `UnaliasSymbolReferences` and `PruneIdentiyProjection` until some fix point would be reached (however currently `UnaliasSymbolReferences` can not be used after  `AddLocalExchanges`).\r\n\r\nQuery plan before fix:\r\n\r\n```\r\npresto:tiny> explain (type distributed) SELECT orderstatus, sum(orderkey) FROM (SELECT orderkey, orderstatus FROM orders UNION ALL SELECT orderkey, orderstatus FROM orders) x GROUP BY (orderstatus);\r\n                                                                      Query Plan                                                                      \r\n------------------------------------------------------------------------------------------------------------------------------------------------------\r\n Fragment 0 [SINGLE]                                                                                                                                  \r\n     Output layout: [orderstatus_19, sum]                                                                                                             \r\n     Output partitioning: SINGLE []                                                                                                                   \r\n     - Output[orderstatus, _col1] => [orderstatus_19:varchar(1), sum:bigint]                                                                          \r\n             orderstatus := orderstatus_19                                                                                                            \r\n             _col1 := sum                                                                                                                             \r\n         - RemoteSource[1] => [orderstatus_19:varchar(1), sum:bigint]                                                                                 \r\n                                                                                                                                                      \r\n Fragment 1 [HASH]                                                                                                                                    \r\n     Output layout: [orderstatus_19, sum]                                                                                                             \r\n     Output partitioning: SINGLE []                                                                                                                   \r\n     - Project[] => [orderstatus_19:varchar(1), sum:bigint]                                                                                           \r\n         - Aggregate(FINAL)[orderstatus_19][$hashvalue] => [orderstatus_19:varchar(1), $hashvalue:bigint, sum:bigint]                                 \r\n                 sum := \"sum\"(\"sum_37\")                                                                                                               \r\n             - LocalExchange[HASH][$hashvalue] (\"orderstatus_19\") => orderstatus_19:varchar(1), sum_37:bigint, $hashvalue:bigint                      \r\n                 - Aggregate(PARTIAL)[orderstatus_19][$hashvalue_40] => [orderstatus_19:varchar(1), $hashvalue_40:bigint, sum_37:bigint]              \r\n                         sum_37 := \"sum\"(\"orderkey_18\")                                                                                               \r\n                     - Project[] => [orderkey_18:bigint, orderstatus_19:varchar(1), $hashvalue_40:bigint]                                             \r\n                             $hashvalue_40 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orderstatus_19\"), 0))                        \r\n                         - Project[] => [orderkey_18:bigint, orderstatus_19:varchar(1), $hashvalue_38:bigint]                                         \r\n                                 orderkey_18 := \"orderkey\"                                                                                            \r\n                                 orderstatus_19 := \"orderstatus\"                                                                                      \r\n                             - RemoteSource[2] => [orderkey:bigint, orderstatus:varchar(1), $hashvalue_38:bigint]                                     \r\n                 - Aggregate(PARTIAL)[orderstatus_19][$hashvalue_43] => [orderstatus_19:varchar(1), $hashvalue_43:bigint, sum_37:bigint]              \r\n                         sum_37 := \"sum\"(\"orderkey_18\")                                                                                               \r\n                     - Project[] => [orderkey_18:bigint, orderstatus_19:varchar(1), $hashvalue_43:bigint]                                             \r\n                             $hashvalue_43 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orderstatus_19\"), 0))                        \r\n                         - Project[] => [orderkey_18:bigint, orderstatus_19:varchar(1), $hashvalue_41:bigint]                                         \r\n                                 orderkey_18 := \"orderkey_4\"                                                                                          \r\n                                 orderstatus_19 := \"orderstatus_6\"                                                                                    \r\n                             - RemoteSource[3] => [orderkey_4:bigint, orderstatus_6:varchar(1), $hashvalue_41:bigint]                                 \r\n                                                                                                                                                      \r\n Fragment 2 [tpch:orders:15000]                                                                                                                       \r\n     Output layout: [orderkey, orderstatus, $hashvalue_39]                                                                                            \r\n     Output partitioning: HASH [orderstatus][$hashvalue_39]                                                                                           \r\n     - ScanProject[table = tpch:tpch:orders:sf0.01, originalConstraint = true] => [orderkey:bigint, orderstatus:varchar(1), $hashvalue_39:bigint]     \r\n             $hashvalue_39 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orderstatus\"), 0))                                           \r\n             orderkey := tpch:orderkey                                                                                                                \r\n             orderstatus := tpch:orderstatus                                                                                                          \r\n                                                                                                                                                      \r\n Fragment 3 [tpch:orders:15000]                                                                                                                       \r\n     Output layout: [orderkey_4, orderstatus_6, $hashvalue_42]                                                                                        \r\n     Output partitioning: HASH [orderstatus_6][$hashvalue_42]                                                                                         \r\n     - ScanProject[table = tpch:tpch:orders:sf0.01, originalConstraint = true] => [orderkey_4:bigint, orderstatus_6:varchar(1), $hashvalue_42:bigint] \r\n             $hashvalue_42 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orderstatus_6\"), 0))                                         \r\n             orderkey_4 := tpch:orderkey                                                                                                              \r\n             orderstatus_6 := tpch:orderstatus                                                                                                        \r\n```\r\n\r\nand after fix:\r\n\r\n```\r\npresto:tiny> explain (type distributed) SELECT orderstatus, sum(orderkey) FROM (SELECT orderkey, orderstatus FROM orders UNION ALL SELECT orderkey, orderstatus FROM orders) x GROUP BY (orderstatus);\r\n                                                                        Query Plan                                                                        \r\n----------------------------------------------------------------------------------------------------------------------------------------------------------\r\n Fragment 0 [SINGLE]                                                                                                                                      \r\n     Output layout: [orderstatus_19, sum]                                                                                                                 \r\n     Output partitioning: SINGLE []                                                                                                                       \r\n     - Output[orderstatus, _col1] => [orderstatus_19:varchar(1), sum:bigint]                                                                              \r\n             orderstatus := orderstatus_19                                                                                                                \r\n             _col1 := sum                                                                                                                                 \r\n         - RemoteSource[1] => [orderstatus_19:varchar(1), sum:bigint]                                                                                     \r\n                                                                                                                                                          \r\n Fragment 1 [HASH]                                                                                                                                        \r\n     Output layout: [orderstatus_19, sum]                                                                                                                 \r\n     Output partitioning: SINGLE []                                                                                                                       \r\n     - Project[] => [orderstatus_19:varchar(1), sum:bigint]                                                                                               \r\n         - Aggregate(FINAL)[orderstatus_19][$hashvalue] => [orderstatus_19:varchar(1), $hashvalue:bigint, sum:bigint]                                     \r\n                 sum := \"sum\"(\"sum_37\")                                                                                                                   \r\n             - LocalExchange[HASH][$hashvalue] (\"orderstatus_19\") => orderstatus_19:varchar(1), sum_37:bigint, $hashvalue:bigint                          \r\n                 - Project[] => [orderstatus_19:varchar(1), sum_37:bigint, $hashvalue_40:bigint]                                                          \r\n                         $hashvalue_40 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orderstatus_19\"), 0))                                \r\n                     - Project[] => [orderstatus_19:varchar(1), sum_37:bigint, $hashvalue_38:bigint]                                                      \r\n                             orderstatus_19 := \"orderstatus\"                                                                                              \r\n                         - RemoteSource[2] => [orderstatus:varchar(1), sum_37:bigint, $hashvalue_38:bigint]                                               \r\n                 - Project[] => [orderstatus_19:varchar(1), sum_37:bigint, $hashvalue_43:bigint]                                                          \r\n                         $hashvalue_43 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orderstatus_19\"), 0))                                \r\n                     - Project[] => [orderstatus_19:varchar(1), sum_37:bigint, $hashvalue_41:bigint]                                                      \r\n                             orderstatus_19 := \"orderstatus_6\"                                                                                            \r\n                         - RemoteSource[3] => [orderstatus_6:varchar(1), sum_37:bigint, $hashvalue_41:bigint]                                             \r\n                                                                                                                                                          \r\n Fragment 2 [tpch:orders:15000]                                                                                                                           \r\n     Output layout: [orderstatus, sum_37, $hashvalue_39]                                                                                                  \r\n     Output partitioning: HASH [orderstatus][$hashvalue_39]                                                                                               \r\n     - Aggregate(PARTIAL)[orderstatus][$hashvalue_39] => [orderstatus:varchar(1), $hashvalue_39:bigint, sum_37:bigint]                                    \r\n             sum_37 := \"sum\"(\"orderkey\")                                                                                                                  \r\n         - ScanProject[table = tpch:tpch:orders:sf0.01, originalConstraint = true] => [orderkey:bigint, orderstatus:varchar(1), $hashvalue_39:bigint]     \r\n                 $hashvalue_39 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orderstatus\"), 0))                                           \r\n                 orderkey := tpch:orderkey                                                                                                                \r\n                 orderstatus := tpch:orderstatus                                                                                                          \r\n                                                                                                                                                          \r\n Fragment 3 [tpch:orders:15000]                                                                                                                           \r\n     Output layout: [orderstatus_6, sum_37, $hashvalue_42]                                                                                                \r\n     Output partitioning: HASH [orderstatus_6][$hashvalue_42]                                                                                             \r\n     - Aggregate(PARTIAL)[orderstatus_6][$hashvalue_42] => [orderstatus_6:varchar(1), $hashvalue_42:bigint, sum_37:bigint]                                \r\n             sum_37 := \"sum\"(\"orderkey_4\")                                                                                                                \r\n         - ScanProject[table = tpch:tpch:orders:sf0.01, originalConstraint = true] => [orderkey_4:bigint, orderstatus_6:varchar(1), $hashvalue_42:bigint] \r\n                 $hashvalue_42 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orderstatus_6\"), 0))                                         \r\n                 orderkey_4 := tpch:orderkey                                                                                                              \r\n                 orderstatus_6 := tpch:orderstatus                    \r\n```", "NaN"], ["7097", "Sort position links for faster non-equi joins", "Piotr Nowojski", "pnowojski", "04/14/17, 09:37:57 PM", "This is a implementation of:\r\n\r\nhttps://github.com/prestodb/presto/issues/6922\r\n\r\nFrom preliminary benchmarks this change didn't introduce regression in equality joins and for some in-equality joins this gives orders of magnitude speed ups.\r\n\r\n`BenchmarkInequalityJoin` speeds up from ~210ms down to ~50ms and existing `BenchmarkHashBuildAndJoinOperators` do not show a visible performance regression:\r\n\r\n```\r\nAFTER\r\n\r\nBenchmark                                              (hashColumns)  (matchRate)  Mode  Cnt    Score    Error  Units\r\nBenchmarkHashBuildAndJoinOperators.benchmarkBuildHash        varchar          N/A  avgt   10   56.792 \u00b1  7.107  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkBuildHash         bigint          N/A  avgt   10   35.530 \u00b1  2.304  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkBuildHash            all          N/A  avgt   10   69.249 \u00b1  7.824  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash         varchar          0.1  avgt   10  100.534 \u00b1  8.280  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash         varchar            1  avgt   10  127.782 \u00b1  5.132  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash         varchar            2  avgt   10  118.533 \u00b1 13.199  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash          bigint          0.1  avgt   10   79.058 \u00b1  4.773  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash          bigint            1  avgt   10  100.670 \u00b1 10.200  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash          bigint            2  avgt   10   74.568 \u00b1  2.282  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash             all          0.1  avgt   10  119.906 \u00b1  1.313  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash             all            1  avgt   10  156.546 \u00b1 12.703  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash             all            2  avgt   10  138.223 \u00b1  7.201  ms/op\r\n\r\nBEFORE:\r\n\r\nBenchmark                                              (hashColumns)  (matchRate)  Mode  Cnt    Score    Error  Units\r\nBenchmarkHashBuildAndJoinOperators.benchmarkBuildHash        varchar          N/A  avgt   10   54.626 \u00b1  5.888  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkBuildHash         bigint          N/A  avgt   10   35.253 \u00b1  1.619  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkBuildHash            all          N/A  avgt   10   73.002 \u00b1  8.299  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash         varchar          0.1  avgt   10   93.097 \u00b1  4.915  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash         varchar            1  avgt   10  130.529 \u00b1  7.421  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash         varchar            2  avgt   10  114.086 \u00b1 10.210  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash          bigint          0.1  avgt   10   71.442 \u00b1  4.633  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash          bigint            1  avgt   10   92.942 \u00b1 12.446  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash          bigint            2  avgt   10   77.394 \u00b1  2.632  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash             all          0.1  avgt   10  111.427 \u00b1  3.545  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash             all            1  avgt   10  150.452 \u00b1 15.728  ms/op\r\nBenchmarkHashBuildAndJoinOperators.benchmarkJoinHash             all            2  avgt   10  130.621 \u00b1  3.306  ms/op\r\n\r\n```\r\n\r\nThere are couple of TODOs (future work?):\r\n\r\n- `compare` method may not fit to `PagesHashStrategy`\r\n- improve decision between choosing `SortedPositionLinks` over `PositionLinksList`\r\n- maybe add `NoopPositionLinks` (that returns always `-1`) if all rows in build table have unique key (this would reduce memory usage and it might slightly improve performance)\r\n- add similar logic to cross join", "NaN"], ["7100", "CHAR(x) functions support in presto", "Andrzej Fiedukowicz", "fiedukow", "03/27/17, 06:02:45 PM", "Now when we integrated VARCHAR(x) function I can push CHAR(x) functions as well ;)\r\n@martint ", "NaN"], ["7105", "Improve strategy for replacing legacy optimizers with new rules", "Martin Traverso", "martint", "01/18/17, 09:07:40 PM", "IterativeOptimizer can now take a list of legacy rules and a set\r\nof new rules that are expected to replace them. When the new optimizer\r\nis enabled, the legacy rule is skipped and the new rules are run.", "NaN"], ["7108", "Fix map to map cast for real type", "Haozhun Jin", "haozhun", "01/17/17, 09:42:00 PM", "NaN", "NaN"], ["7110", "Improve documentation for bitwise functions", "David Phillips", "electrum", "01/21/17, 09:15:18 PM", "NaN", "NaN"], ["7112", "Update Modernizer plugin", "David Phillips", "electrum", "01/26/17, 05:43:20 PM", "(will wait for 1.5.0 to be released)", "NaN"], ["7114", "Fix incorrect pruning of join output columns", "Martin Traverso", "martint", "01/19/17, 07:29:46 PM", "Some nodes are sensitive to which columns are produced by\r\ntheir children (e.g., DistinctLimitNode). The change to\r\nadd support for pruning join columns did not correctly\r\ndeal with this, which results in queries like this one\r\nto produce incorrect results:\r\n\r\n    SELECT DISTINCT x\r\n    FROM (VALUES 1) t(x) JOIN (VALUES 10, 20) u(a) ON t.x < u.a\r\n    LIMIT 100\r\n\r\nAdditionally, the constructor in JoinNode that derives\r\nthe output columns from the children is error-prone, since\r\nit lends to misuse in places that expect the outputs not to\r\n change.\r\n\r\nThis change fixes the issue by making the specification of\r\njoin output columns explicit in every place that constructs\r\na Join node.\r\n\r\nThis is a candidate \"fix\" for https://github.com/prestodb/presto/issues/7111. ", "NaN"], ["7115", "Remove unnecessary variable", "Nezih Yigitbasi", "nezihyigitbasi", "01/18/17, 12:47:24 AM", "NaN", "NaN"], ["7117", "Implement MaxOrMinBy with bytecode generation", "Rongrong Zhong", "rongrong", "02/10/17, 03:17:33 AM", "Resolves #4126 ", "NaN"], ["7121", "Add distinctSymbols field to DistinctLimitNode", "Karol Sobczak", "sopel39", "05/22/17, 08:05:06 PM", "fixes: https://github.com/prestodb/presto/issues/7111", "NaN"], ["7124", "Use unique prefixes for TestHiveClient table names", "Dain Sundstrom", "dain", "01/18/17, 10:50:33 PM", "NaN", "NaN"], ["7125", "Fix leak of views in Hive tests", "David Phillips", "electrum", "01/19/17, 09:37:39 PM", "NaN", "NaN"], ["7128", "Expose rule execution stats via JMX", "Martin Traverso", "martint", "01/19/17, 07:31:56 PM", "NaN", "NaN"], ["7129", "Ensure split sources are always closed", "David Phillips", "electrum", "01/19/17, 02:24:18 AM", "NaN", "NaN"], ["7131", "Process expression instead of group reference", "Martin Traverso", "martint", "01/19/17, 04:34:20 PM", "When the result of applying a rule is a GroupReference\r\n(e.g., in the case where a node is being removed, such as\r\nfor X->Y->(1) ==> X->(1)), make sure the loop continues\r\nprocessing the expression contained in the target group.\r\n\r\nCurrently, the loop would try to continue processing\r\nthe GroupReference, which is non-sensical, since rules\r\ncan't match it.", "NaN"], ["7133", "Fix potentially invalid plans", "Martin Traverso", "martint", "01/19/17, 07:29:18 PM", "There are a couple of places in optimizers that make wrong assumptions about column ordering. This was uncovered by #7088, which is more aggressive at removing identity projections that may change the order of columns.", "NaN"], ["7135", "Clean up IterativeOptimizer constructor", "Martin Traverso", "martint", "01/22/17, 06:52:54 PM", "NaN", "NaN"], ["7137", "Close split sources in DistributedExecutionPlanner", "David Phillips", "electrum", "01/19/17, 09:36:35 PM", "This prevents leaking split sources if a call to getSplits() fails.", "NaN"], ["7144", "Fix Hive distributed tests", "David Phillips", "electrum", "01/20/17, 03:24:48 AM", "NaN", "NaN"], ["7145", "Fix null-handling bug in min_by/max_by", "Haozhun Jin", "haozhun", "01/20/17, 06:56:45 AM", "Null keys were previously incorrectly handled. They were effectively treated as default value of the type for comparison in the input function.", "NaN"], ["7191", "Update to TestNG 6.9.6", "David Phillips", "electrum", "01/23/17, 06:53:25 PM", "Check if this works in Travis.", "NaN"], ["7192", "Use in-memory H2 database for Raptor development", "David Phillips", "electrum", "01/23/17, 05:17:09 PM", "The server won't start after schema changes, and persisting data across\nrestarts is not that useful for development, so make it ephemeral.", "NaN"], ["7193", "Add 0.164 release notes", "David Phillips", "electrum", "01/21/17, 01:10:41 AM", "NaN", "NaN"], ["7194", "Replace QualifiedNameReference with Identifier", "Martin Traverso", "martint", "01/22/17, 06:52:18 PM", "This change introduces a new AST node, Identifier, designed to\r\nrepresent an identifier atom. Qualified name references are now\r\nencoded as DereferenceExpression across the board.\r\n\r\nIt does away with the special distinction between QualifiedNameReference\r\nand DereferenceExpression. This is the first step in being able\r\nto support quoted identifiers and other simplifications to the\r\nanalyzer.", "NaN"], ["7195", "Minor cleanup in TranslationMap/RelationPlan", "Martin Traverso", "martint", "01/24/17, 05:27:36 AM", "NaN", "NaN"], ["7196", "Fix invalid plan when converting to index join", "Martin Traverso", "martint", "01/21/17, 07:52:32 PM", "The projection needed to preserve the outputs of the original\r\njoin node needs to be added after the filter, which may need\r\nto reference the outputs of the join.", "NaN"], ["7200", "Rule assertion framework + test for evaluate zero limit", "Martin Traverso", "martint", "01/24/17, 04:26:07 AM", "NaN", "NaN"], ["7203", "Improve Node.getChildren() implementations", "Piotr Findeisen", "findepi", "01/23/17, 08:36:12 PM", "NaN", "NaN"], ["7207", "Update to TestNG 6.10", "David Phillips", "electrum", "01/26/17, 07:04:40 PM", "NaN", "NaN"], ["7209", "Include Hive partition name in error message", "David Phillips", "electrum", "01/26/17, 05:51:39 PM", "NaN", "NaN"], ["7210", "Capture support for lambda", "Haozhun Jin", "haozhun", "03/27/17, 11:39:58 PM", "NaN", "NaN"], ["7211", "Remove expression from typeOnlyCoercions when the type and the supertype is not TypeOnlyCoercion", "Yuya Ebihara", "ebyhr", "02/01/17, 05:04:29 PM", "#6984 \r\n\r\nI think that the expression whose type is not typeonly with the supertype should be removed from typeOnlyCoercions list.  \r\n\r\nIn this case, decimal(2,1)'s supertype become decimal(3,1) at first. It is OK because the difference is only the precision.\r\nNext, decimal(2,1)'s supertype become double. It is not OK because typeOnlyCoercions still contains the expression and the node {decimal(2,1) vaulue(2.0)} keeps typeonly true.\r\n\r", "NaN"], ["7213", "Extract Hive S3 config to separate class", "David Phillips", "electrum", "01/24/17, 07:42:04 PM", "NaN", "NaN"], ["7218", "Add optimizers for 0% and 100% sample nodes", "Dain Sundstrom", "dain", "02/14/17, 11:49:24 PM", "NaN", "NaN"], ["7220", "Make AT keyword non-reserved", "David Phillips", "electrum", "01/25/17, 02:50:10 AM", "NaN", "NaN"], ["7221", "Expose failure stats for optimizer rules", "Martin Traverso", "martint", "01/25/17, 05:58:42 AM", "This can help understand which rules are causing queries to fail due to exceptions.", "NaN"], ["7224", "Get QueryInfo only once when checking abandoned queries", "Piotr Findeisen", "findepi", "01/26/17, 01:41:38 AM", "QueryInfo is not free to get.", "NaN"], ["7233", "Add array_overlap function", "Igor Demura", "idemura", "03/08/17, 07:41:47 PM", "Checked and unchecked (for sorting) version is like 11 vs 19 seconds (walltime). I think compared to ~1.5-2 minutes it's still a win. They have very poor parallelization of this kind of a query (just 6 nodes for 100M rows). \r\n\r\nFixes #6823", "NaN"], ["7235", "Expose additional managed Raptor methods through JMX", "Raghav Sethi", "raghavsethi", "01/27/17, 11:15:20 PM", "NaN", "NaN"], ["7236", "Remove checkType utility methods", "David Phillips", "electrum", "02/10/17, 12:19:19 AM", "Due to improved generics inference in Java 8, the method is not\ntype safe at all, as it allows passing completely unrelated objects.\nA normal cast is safer and provides a reasonable error message.", "NaN"], ["7237", "Fix benchmarks", "Dain Sundstrom", "dain", "02/15/17, 10:27:59 PM", "NaN", "NaN"], ["7238", "Show hit/miss rate & request count in ExpressionCompiler via JMX", "Yuya Ebihara", "ebyhr", "02/02/17, 05:24:36 AM", "PR : #7118 \r\nIssue : #7081 \r\n\r\nI added the request count and modify&merge commits into one.", "NaN"], ["7239", "Distributed aggregation for ROLLUP and CUBE", "Piotr Nowojski", "pnowojski", "04/19/17, 05:40:09 AM", "**First commits are from #7223.**\r\n\r\nThis adds support for distributed aggregation of ROLLUP and CUBE (generally speaking, when there is some empty grouping set and also some non empty). This is accomplished by moving generating default output from Aggregation FINAL node to Aggregation PARTIAL node. \r\n\r\nBenchmark results (with broadcast join):\r\n\r\n| | tpcds/q22 | tpcds/q67 |\r\n| ------------- |:-------------:| -----:|\r\n| before | 171 | 91,98 |\r\n| after | 28,93 | 12,406 |\r\n\r\nOne tricky part is that PARTIAL Aggregation can sometimes be pushed down to SOURCE distributed plan fragment. When this happens, there is a possibility that for this fragment, some connector will return no splits. For that case, I have added `EmptySplit`, which represents no data and guarantees that at least one task will be created. If anyone has a better idea how to handle this situation, please speak freely :)\r\n\r\nExample plan after this change:\r\n\r\n```\r\npresto:tiny> explain (type distributed) SELECT regionkey, count(*) FROM nation GROUP BY ROLLUP (regionkey);\r\n                                                                                   Query Plan                                                                                    \r\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n Fragment 0 [SINGLE]                                                                                                                                                             \r\n     Output layout: [regionkey$gid, count]                                                                                                                                       \r\n     Output partitioning: SINGLE []                                                                                                                                              \r\n     - Output[regionkey, _col1] => [regionkey$gid:bigint, count:bigint]                                                                                                          \r\n             regionkey := regionkey$gid                                                                                                                                          \r\n             _col1 := count                                                                                                                                                      \r\n         - RemoteSource[1] => [regionkey$gid:bigint, count:bigint]                                                                                                               \r\n                                                                                                                                                                                 \r\n Fragment 1 [HASH]                                                                                                                                                               \r\n     Output layout: [regionkey$gid, count]                                                                                                                                       \r\n     Output partitioning: SINGLE []                                                                                                                                              \r\n     - Project[] => [regionkey$gid:bigint, count:bigint]                                                                                                                         \r\n         - LocalExchange[ROUND_ROBIN] () => regionkey$gid:bigint, groupid:bigint, count:bigint                                                                                   \r\n             - Project[] => [regionkey$gid:bigint, groupid:bigint, count:bigint]                                                                                                 \r\n                 - Aggregate(FINAL)[regionkey$gid, groupid][$hashvalue] => [regionkey$gid:bigint, groupid:bigint, $hashvalue:bigint, count:bigint]                               \r\n                         count := \"count\"(\"count_8\")                                                                                                                             \r\n                     - LocalExchange[SINGLE] () => regionkey$gid:bigint, groupid:bigint, count_8:bigint, $hashvalue:bigint                                                       \r\n                         - RemoteSource[2] => [regionkey$gid:bigint, groupid:bigint, count_8:bigint, $hashvalue_9:bigint]                                                        \r\n                                                                                                                                                                                 \r\n Fragment 2 [SOURCE]                                                                                                                                                             \r\n     Output layout: [regionkey$gid, groupid, count_8, $hashvalue_10]                                                                                                             \r\n     Output partitioning: HASH [regionkey$gid, groupid][$hashvalue_10]                                                                                                           \r\n     - Aggregate(PARTIAL)[regionkey$gid, groupid][$hashvalue_10] => [regionkey$gid:bigint, groupid:bigint, $hashvalue_10:bigint, count_8:bigint]                                 \r\n             count_8 := \"count\"(*)                                                                                                                                               \r\n         - Project[] => [regionkey$gid:bigint, groupid:bigint, $hashvalue_10:bigint]                                                                                             \r\n                 $hashvalue_10 := \"combine_hash\"(\"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"regionkey$gid\"), 0)), COALESCE(\"$operator$hash_code\"(\"groupid\"), 0)) \r\n             - GroupId[[regionkey], []] => [regionkey$gid:bigint, groupid:bigint]                                                                                                \r\n                     regionkey$gid := regionkey                                                                                                                                  \r\n                 - TableScan[tpch:tpch:nation:sf0.01, originalConstraint = true] => [regionkey:bigint]                                                                           \r\n                         regionkey := tpch:regionkey\r\n```\r\n\r\nBefore this change:\r\n```\r\nexplain (type distributed) SELECT regionkey, count(*) FROM nation GROUP BY ROLLUP (regionkey);\r\n                                                                                   Query Plan                                                                                    \r\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n Fragment 0 [SINGLE]                                                                                                                                                             \r\n     Output layout: [regionkey$gid, count]                                                                                                                                       \r\n     Output partitioning: SINGLE []                                                                                                                                              \r\n     - Output[regionkey, _col1] => [regionkey$gid:bigint, count:bigint]                                                                                                          \r\n             regionkey := regionkey$gid                                                                                                                                          \r\n             _col1 := count                                                                                                                                                      \r\n         - Project[] => [regionkey$gid:bigint, count:bigint]                                                                                                                     \r\n             - LocalExchange[ROUND_ROBIN] () => regionkey$gid:bigint, groupid:bigint, count:bigint                                                                               \r\n                 - Project[] => [regionkey$gid:bigint, groupid:bigint, count:bigint]                                                                                             \r\n                     - Aggregate(FINAL)[regionkey$gid, groupid][$hashvalue] => [regionkey$gid:bigint, groupid:bigint, $hashvalue:bigint, count:bigint]                           \r\n                             count := \"count\"(\"count_8\")                                                                                                                         \r\n                         - LocalExchange[SINGLE] () => regionkey$gid:bigint, groupid:bigint, count_8:bigint, $hashvalue:bigint                                                   \r\n                             - RemoteSource[1] => [regionkey$gid:bigint, groupid:bigint, count_8:bigint, $hashvalue_9:bigint]                                                    \r\n                                                                                                                                                                                 \r\n Fragment 1 [SOURCE]                                                                                                                                                             \r\n     Output layout: [regionkey$gid, groupid, count_8, $hashvalue_10]                                                                                                             \r\n     Output partitioning: SINGLE []                                                                                                                                              \r\n     - Aggregate(PARTIAL)[regionkey$gid, groupid][$hashvalue_10] => [regionkey$gid:bigint, groupid:bigint, $hashvalue_10:bigint, count_8:bigint]                                 \r\n             count_8 := \"count\"(*)                                                                                                                                               \r\n         - Project[] => [regionkey$gid:bigint, groupid:bigint, $hashvalue_10:bigint]                                                                                             \r\n                 $hashvalue_10 := \"combine_hash\"(\"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"regionkey$gid\"), 0)), COALESCE(\"$operator$hash_code\"(\"groupid\"), 0)) \r\n             - GroupId[[regionkey], []] => [regionkey$gid:bigint, groupid:bigint]                                                                                                \r\n                     regionkey$gid := regionkey                                                                                                                                  \r\n                 - TableScan[tpch:tpch:nation:sf0.01, originalConstraint = true] => [regionkey:bigint]                                                                           \r\n                         regionkey := tpch:regionkey\r\n```", "NaN"], ["7241", "Fix cumulative memory counter to track query memory", "Raghav Sethi", "raghavsethi", "02/13/17, 05:10:59 PM", "Previously, this counter tracked system memory, which is incorrect.\r\n\r\nFixes #6442.", "NaN"], ["7242", "Remove unused methods from FunctionListBuilder", "David Phillips", "electrum", "02/28/17, 11:42:55 PM", "NaN", "NaN"], ["7243", "Add peak memory counters to driver and split stats", "Raghav Sethi", "raghavsethi", "02/01/17, 08:45:42 PM", "NaN", "NaN"], ["7244", "Categorize RejectedExecutionException in StateMachine", "David Phillips", "electrum", "01/30/17, 09:04:36 PM", "NaN", "NaN"], ["7251", "Optimize sort node whose input is already sorted.", "Wenlei Xie", "wenleix", "03/22/17, 04:30:40 AM", "Work in progress, some thoughts:\r\n\r\n1. Refactor/merge with some code that checks the sort ordering? (e.g. refactor into a method `meetsSortRequirements`?\r\n\r\n2. Adding test cases.\r\n\r\nSome initial tests:\r\n- Sort is eliminated:\r\n```\r\npresto:tiny> explain SELECT linenumber, quantity, row_number() OVER (ORDER BY quantity) FROM lineitem ORDER BY quantity;\r\n                                                         Query Plan\r\n-------------------------------------------------------------------------------------------------\r\n - Output[linenumber, quantity, _col2] => [linenumber:integer, quantity:double, row_number_1:bigi\r\n         _col2 := row_number_1\r\n     - Window[order by (quantity ASC_NULLS_LAST)] => [linenumber:integer, quantity:double, row_nu\r\n             row_number_1 := row_number()\r\n         - LocalExchange[SINGLE] () => linenumber:integer, quantity:double\r\n             - RemoteExchange[GATHER] => linenumber:integer, quantity:double\r\n                 - TableScan[tpch:tpch:lineitem:sf0.01, originalConstraint = true] => [linenumber\r\n                         linenumber := tpch:linenumber\r\n                         quantity := tpch:quantity\r\n```\r\n\r\n- Sort is reserved: (sort by different column)\r\n```\r\npresto:tiny> explain SELECT linenumber, quantity, row_number() OVER (ORDER BY quantity) FROM lineitem order by tax;\r\n                                                                   Query Plan\r\n-------------------------------------------------------------------------------------------------\r\n - Output[linenumber, quantity, _col2] => [linenumber:integer, quantity:double, row_number_1:bigi\r\n         _col2 := row_number_1\r\n     - Project[] => [linenumber:integer, quantity:double, row_number_1:bigint]\r\n         - Sort[tax ASC_NULLS_LAST] => [linenumber:integer, quantity:double, tax:double, row_numb\r\n             - Window[order by (quantity ASC_NULLS_LAST)] => [linenumber:integer, quantity:double\r\n                     row_number_1 := row_number()\r\n                 - LocalExchange[SINGLE] () => linenumber:integer, quantity:double, tax:double\r\n                     - RemoteExchange[GATHER] => linenumber:integer, quantity:double, tax:double\r\n                         - TableScan[tpch:tpch:lineitem:sf0.01, originalConstraint = true] => [li\r\n                                 linenumber := tpch:linenumber\r\n                                 quantity := tpch:quantity\r\n                                 tax := tpch:tax\r\n```\r", "NaN"], ["7252", "Fix atop connector to work with dedicated coordinator", "Christopher Berner", "cberner", "01/31/17, 06:16:47 PM", "Currently, if node-scheduler.include-coordinator=false, a split is still\r\ngenerated for the coordinator, which will then not be able to process\r\nit, failing the query.", "NaN"], ["7253", "Add 0.165 release notes", "Eric Hwang", "erichwang", "02/01/17, 01:05:07 AM", "NaN", "NaN"], ["7254", "Use type names for procedure argument definitions", "David Phillips", "electrum", "02/14/17, 01:36:04 AM", "NaN", "NaN"], ["7256", "Migrate ProjectionPushDown to iterative optimizer", "\u0141ukasz Osipiuk", "losipiuk", "05/17/17, 02:56:54 PM", "NaN", "NaN"], ["7259", "Add Hive file format test for TextFile", "David Phillips", "electrum", "02/01/17, 12:40:03 AM", "NaN", "NaN"], ["7266", "Enable ALTER TABLE tests for Hive connector", "David Phillips", "electrum", "02/07/17, 08:01:31 PM", "NaN", "NaN"], ["7267", "Enable CI tests for presto-tests module on Travis CI", "David Phillips", "electrum", "03/23/17, 11:53:46 PM", "NaN", "NaN"], ["7268", "Remove Travis CI workaround for Codehaus repository", "David Phillips", "electrum", "02/02/17, 12:34:24 AM", "This has been fixed on the Travis side.", "NaN"], ["7269", "Update to airlift 0.140", "Martin Traverso", "martint", "02/02/17, 03:20:01 PM", "Improves performance and memory usage patterns for QuantileDigest. It requires\r\nadjustments due to changes to QuantileDigest serialization API.", "NaN"], ["7270", "Updates to the web UI", "Raghav Sethi", "raghavsethi", "02/18/17, 06:40:15 AM", "Fixes #7217 \r\nFixes #7240 ", "NaN"], ["7273", "Verify insert", "Rongrong Zhong", "rongrong", "03/21/17, 06:47:06 PM", "NaN", "NaN"], ["7274", "Update Maven RPM plugin to remove debug messages", "David Phillips", "electrum", "02/02/17, 12:23:52 AM", "NaN", "NaN"], ["7275", "Short-circuit inner and right join when right side is empty", "Rongrong Zhong", "rongrong", "02/10/17, 03:18:25 AM", "NaN", "NaN"], ["7277", "Encapsulate function/call/mask in aggregation assignments", "Martin Traverso", "martint", "02/15/17, 11:55:06 PM", "NaN", "NaN"], ["7282", "Upgrade docker-images version to 14", "Anu Sudarsan", "anusudarsan", "02/02/17, 07:38:57 PM", "Updated the kerberos docker image to generate principal with the fqdn.\r\nHence update the existing kerberos profile.", "NaN"], ["7284", "Add more documentation to InternalResourceGroup", "Christopher Berner", "cberner", "02/10/17, 01:44:51 AM", "NaN", "NaN"], ["7286", "Fix checkstyle violation", "Andrii Rosa", "arhimondr", "02/02/17, 09:12:13 PM", "NaN", "NaN"], ["7287", "Fix product tests debug environment", "Andrii Rosa", "arhimondr", "02/02/17, 09:31:53 PM", "Local environment for product tests debugging doesn't start, because cassandra connector\r\nis not registered, when it is used by cassandra catalog.", "NaN"], ["7289", "Handle escape in LIKE predicate optimization", "Rongrong Zhong", "rongrong", "02/14/17, 06:57:11 PM", "NaN", "NaN"], ["7295", "Remove redundant strings from PrestoS3SignerType", "David Phillips", "electrum", "02/10/17, 12:17:00 AM", "NaN", "NaN"], ["7300", "Move reorder windows optimizer to the new iterative optimizer", "Maciej 'mac' Grzybek", "maciejgrzybek", "04/10/17, 04:33:49 PM", "First pass was here: https://github.com/Teradata/presto/pull/493", "NaN"], ["7302", "Update to airlift 0.141", "Martin Traverso", "martint", "02/04/17, 01:07:46 AM", "Fixes a thread-safety issue in the copy constructor of Distribution", "NaN"], ["7303", "Fix bad type signature generated by optimizer", "Wenlei Xie", "wenleix", "02/10/17, 09:23:36 PM", "Fix the bug reported in https://github.com/prestodb/presto/issues/7229", "NaN"], ["7310", "Update to Airbase 61", "David Phillips", "electrum", "02/17/17, 12:03:07 AM", "This includes the commits from #6995", "NaN"], ["7313", "Fix getSizeInBytes in ArrayBlock", "Haozhun Jin", "haozhun", "02/07/17, 02:20:26 AM", "NaN", "NaN"], ["7314", "Add debug logging for class definition", "David Phillips", "electrum", "02/07/17, 05:00:20 AM", "NaN", "NaN"], ["7321", "Add transform_key/transform_value lambda functions", "Wenlei Xie", "wenleix", "02/16/17, 11:39:47 PM", "Related issue: https://github.com/prestodb/presto/issues/7285\r\n\r\nMore unit test cases in `TestMapTransformValueFunction` are coming...", "NaN"], ["7324", "Update Hadoop and ZooKeeper for Accumulo", "David Phillips", "electrum", "02/14/17, 07:30:41 PM", "NaN", "NaN"], ["7329", "Remove unused unamgleNames from SqlFormatter", "Grzegorz Kokosi\u0144ski", "kokosing", "02/15/17, 03:22:42 PM", "Remove unused unamgleNames from SqlFormatter", "NaN"], ["7332", "Create the list of type-specific unnesters in UnnestOperator only once", "Erli Ding", "delding", "03/22/17, 03:46:12 AM", "Fixes #7323 \r\n\r\nAdd setBlock method to interface `Unnester` so that unnesters can be reused across different pages by calling `fillUnnesters` method which sets new blocks, unnesters are only initialized once for each `UnnestOperator`\r", "NaN"], ["7335", "Improve error handling when Hive partitions already exist", "David Phillips", "electrum", "02/09/17, 05:27:44 PM", "If we get an AlreadyExists error from the metastore, we don't know which\npartitions already exists, so indicate that in the error message. Also\ninclude the original metastore error in case it has helpful information.", "NaN"], ["7336", "Update to airlift 0.142", "Martin Traverso", "martint", "02/09/17, 05:10:09 AM", "Fixes a contention issue in Distribution's copy constructor\r\nthat can cause coordinator->worker requests to get \"stuck\"\r\ncomputing various stats.", "NaN"], ["7339", "Add 0.166 release notes", "Haozhun Jin", "haozhun", "02/09/17, 06:43:48 PM", "NaN", "NaN"], ["7341", "Delete staging files in the background on startup", "Nezih Yigitbasi", "nezihyigitbasi", "03/09/17, 02:01:51 AM", "Fixes #5828", "NaN"], ["7343", "Improve handling of missing Fields in RelationType.indexOf", "Piotr Findeisen", "findepi", "02/10/17, 03:52:28 PM", "Previously, the method would throw NullPointerException without a\r\nmessage when passed field was null or not found, although documentation\r\nstated differently.\r\n\r\nNow exception is always with a message, and documentation is aligned\r\nwith the code.", "NaN"], ["7345", "Document zip_with", "Rongrong Zhong", "rongrong", "02/10/17, 10:52:10 PM", "NaN", "NaN"], ["7347", "Remove unnecessary locking in assertQueryFails method", "David Phillips", "electrum", "02/16/17, 11:50:56 PM", "Locking is not needed since no QueryRunner state is changed.", "NaN"], ["7351", "Document cast from/to JSON", "Haozhun Jin", "haozhun", "03/04/17, 01:02:17 AM", "NaN", "NaN"], ["7352", "Remove unnecessary conversions from IntStream to LongStream", "Piotr Findeisen", "findepi", "02/12/17, 11:12:27 PM", "NaN", "NaN"], ["7354", "Support Subqueries in aggregation analyzer", "Piotr Findeisen", "findepi", "04/27/17, 02:48:46 AM", "This PR takes `Scope`-based field resolution to the next level. Scopes are hard to create correctly, as they need to follow SQL language rules. `StatementAnalyzer` creates them and `ExpressionAnalyzer` resolves column references. The PR introduces `FieldId` concept which allows `ExpressionAnalyzer` to _save_ resolved column references in `Analysis` and let `AggregationAnalyzer` use that. This unlocks ability to analyze subqueries in `AggregationAnalyzer`.\r\n\r\nFixes #7030. Based on @sopel39's #7315 ", "NaN"], ["7355", "Simple PlanNode matching in IterativeOptimizer", "Grzegorz Kokosi\u0144ski", "kokosing", "06/09/17, 10:20:28 AM", "Simple PlanNode matching in IterativeOptimizer\n\nThis simple PlanNode class based pattern matching.\nThis might be a trow-away code once the more sophisticated pattern\nmatching get in.\n\nEven though it is very simple it has a value as it filter outs many\nrules which have no chance to to match given plan node.", "NaN"], ["7357", "Support Unicode escaped strings", "James Sun", "highker", "04/12/17, 09:32:04 PM", "Incorporate strings with escaped unicode characters. The string can be a\r\nmix of escaped characters and normal unicode characters. An escaped\r\ncharacter can be either of length 4 or 6 in hex. No surrogate pairs are\r\nsupported at this point.", "NaN"], ["7359", "Add read-only system access controller", "Eric Hwang", "erichwang", "02/15/17, 07:34:59 PM", "NaN", "NaN"], ["7360", "Fix merge conflict in TestDecimalCasts", "David Phillips", "electrum", "02/13/17, 11:18:41 PM", "NaN", "NaN"], ["7362", "Fix final query info not set for data definition queries", "Haozhun Jin", "haozhun", "03/24/17, 10:34:01 PM", "This is built on top of @highker's work in #7337.", "NaN"], ["7366", "Remove unused test dependency cassandra-driver-mapping", "David Phillips", "electrum", "02/14/17, 08:01:29 PM", "NaN", "NaN"], ["7369", "Fix potential overflow when calculating size", "Nezih Yigitbasi", "nezihyigitbasi", "03/27/17, 06:55:10 AM", "NaN", "NaN"], ["7370", "Add test for Block.getSizeInBytes", "Haozhun Jin", "haozhun", "02/15/17, 09:32:31 PM", "I verified that the tests fail if I rollback ff00fa072e054d4718bc18d8ebbb63acf3157c94", "NaN"], ["7380", "Fix overflowed CLI progress bar for long running queries", "Wenlei Xie", "wenleix", "05/20/19, 10:08:12 PM", "Address https://github.com/prestodb/presto/issues/6293\r\n\r\nNot adding test case for this PR since it would requires to refactoring the `StatuePrinter` (which should probably be done separately).  ", "NaN"], ["7383", "Add JMX stats for longest running split elapsed time", "James Sun", "highker", "02/17/17, 11:14:18 PM", "All ongoing splits' starting time are recorded in a set in the task executor. This provides the\r\ninsight of how long these splits have been running. A new interface is added to return the longest\r\nconsecutive split execution time.", "NaN"], ["7384", "Cleanup ArbitraryOutputBuffer and TestArbitraryOutputBuffer", "Dain Sundstrom", "dain", "02/16/17, 06:48:37 PM", "NaN", "NaN"], ["7385", "Block.getSliceLength and Block.getRegionSizeInBytes", "Haozhun Jin", "haozhun", "02/17/17, 11:28:53 PM", "NaN", "NaN"], ["7387", "Expose resource group information for queued queries", "Rongrong Zhong", "rongrong", "03/13/17, 07:40:33 PM", "Work in progress. Please provide early feedback about structures, etc. Thanks!", "NaN"], ["7389", "Rename rewriteQualifiedNamesToSymbolReferences", "Grzegorz Kokosi\u0144ski", "kokosing", "02/17/17, 06:06:17 AM", "Rename rewriteQualifiedNamesToSymbolReferences\r\n\r\nQualifiedNameReference no longer exists so the name of method seem to be\r\nobsolete. This commit updates this name of this method to refer that it\r\nrewrites Identifiers to SymbolReferences.\r", "NaN"], ["7392", "Remove legacy Hive connectors from server tarball", "David Phillips", "electrum", "02/16/17, 09:41:13 PM", "NaN", "NaN"], ["7393", "Implement LazyOutputBuffer.getUtilization", "Dain Sundstrom", "dain", "02/17/17, 08:45:11 PM", "NaN", "NaN"], ["7395", "Remove checkNotSameThreadExecutor", "David Phillips", "electrum", "02/17/17, 04:48:00 PM", "NaN", "NaN"], ["7396", "Add synchronized PagesSerde for testing", "David Phillips", "electrum", "02/17/17, 02:38:14 AM", "This fixes race conditions in TestArbitraryOutputBuffer.", "NaN"], ["7397", "Add evaluation limit to LookupJoinOperator to yield driver thread", "Haozhun Jin", "haozhun", "02/18/17, 01:55:00 AM", "NaN", "NaN"], ["7403", "Remove running split info before a split is set to finished", "James Sun", "highker", "02/18/17, 01:58:57 AM", "ListenableFuture::get is listening on the finish of split. If a running\r\nsplit info is removed after it has been set to finished, There may still\r\nhave remaining running split info alive. This can fail the unit test due\r\nto race.", "NaN"], ["7405", "Remove isFinished check for updating quanta stats", "Raghav Sethi", "raghavsethi", "02/20/17, 03:57:35 AM", "The check does not appear to have any material effect on the reported\r\nresults, and has unintended side effects (including test failures).", "NaN"], ["7406", "Remove unused BlockEncodingSerde from ExchangeClientFactory", "David Phillips", "electrum", "03/01/17, 05:18:50 AM", "NaN", "NaN"], ["7417", "Fix query failure with mixed distinct aggregation", "Wenlei Xie", "wenleix", "03/25/17, 03:23:40 AM", "Fix the issue that MixedDistinctAggregations optimizer fails when\r\nGroupBy columns and aggregation columns have overlap.", "NaN"], ["7418", "Migrate PushTableWriteThroughUnion to the Iterative Optimizer", "Rebecca Schlussel", "rschlussel-zz", "07/12/17, 04:09:13 AM", "NaN", "NaN"], ["7419", "Optimize window frame computation for empty frames", "David Phillips", "electrum", "02/28/17, 11:40:53 PM", "We can check for an empty first and short-circuit the computation.", "NaN"], ["7420", "Remove redundant check for new locations in ExchangeClient", "Dain Sundstrom", "dain", "02/22/17, 06:17:25 PM", "ExchangeClient checks for new locations on every completed request, which\nis very expensive.  Instead of this add new buffer locations immediately\nin addLocation.", "NaN"], ["7424", "Fix bugs in DictionaryBlock and Block.getRegionSizeInBytes", "Haozhun Jin", "haozhun", "02/22/17, 03:56:39 AM", "Fixes #7414", "NaN"], ["7425", "UI niggles", "Raghav Sethi", "raghavsethi", "02/22/17, 06:28:18 AM", "NaN", "NaN"], ["7427", "Push partial aggregation through inner join", "Piotr Nowojski", "pnowojski", "05/18/17, 11:30:22 PM", "Adding flag for pushing partial aggregation through join.\r\n\r\nThis is yet another PR that depends on jmh benchmarks defined in https://github.com/prestodb/presto/pull/5809. Results for simple query:\r\n```\r\nBenchmark                                                       (enabled)  Mode  Cnt    Score    Error  Units\r\nBenchmarkPartialAggregationPushdown.pushdownPartialThroughJoin       true  avgt   10  210.992 \u00b1  7.088  ms/op\r\nBenchmarkPartialAggregationPushdown.pushdownPartialThroughJoin      false  avgt   10  494.724 \u00b1 58.796  ms/op\r\n```\r\n\r\nWith support for `filter function` (there is now `TODO` in the code for this) this change would significantly outperform SPARK 2.0 in [CERN use case](https://databricks.com/blog/2016/10/03/voice-from-cern-apache-spark-2-0-performance-improvements-investigated-with-flame-graphs.html)", "NaN"], ["7429", "Move TransformCorrelatedScalarAggregationToJoin to the iterative optimizer", "Maciej 'mac' Grzybek", "maciejgrzybek", "05/17/17, 08:23:32 PM", "Fix from https://github.com/maciejgrzybek/presto/commit/77deae9ff0384b9c0c7f9fb76a7e5b73e8df5f84 is nothing specific to the new optimizer. Previous optimizer (in the old framework) was also not preserving the invariant of output symbols but we didn't have a check for that so we never noticed.", "NaN"], ["7431", "Implement array_except function", "Rongrong Zhong", "rongrong", "03/08/17, 05:23:53 AM", "NaN", "NaN"], ["7434", "Update OrcMetadataReader to use new orc-protobuf", "David Phillips", "electrum", "03/01/17, 05:19:41 AM", "NaN", "NaN"], ["7437", "Optimize ArrayFilter by type-specialized method", "Wenlei Xie", "wenleix", "03/09/17, 10:29:40 PM", "This addresses https://github.com/prestodb/presto/issues/6779 and is based on annotation`TypeParameterSpecialization`\r\n\r\nBenchmark before optimization:\r\n```\r\nBenchmark                             (name)  Mode  Cnt    Score   Error  Units\r\nBenchmarkArrayFilter.benchmark        filter  avgt   20  134.575 \u00b1 6.136  ns/op\r\nBenchmarkArrayFilter.benchmark  exact_filter  avgt   20   83.188 \u00b1 5.809  ns/op\r\n```\r\n\r\nAfter optimization\r\n```\r\nBenchmark                             (name)  Mode  Cnt   Score   Error  Units\r\nBenchmarkArrayFilter.benchmark        filter  avgt   20  85.582 \u00b1 4.233  ns/op\r\nBenchmarkArrayFilter.benchmark  exact_filter  avgt   20  83.075 \u00b1 3.952  ns/op\r\n```", "NaN"], ["7444", "Rename map transform functions", "David Phillips", "electrum", "02/24/17, 10:43:24 PM", "NaN", "NaN"], ["7445", "Fix invalid \"No more buffers already set\" in Broadcast buffer", "Dain Sundstrom", "dain", "03/08/17, 09:08:27 PM", "When the broadcast buffer is closed early the buffer may not have received\nthe final buffer declaration, so buffer creation must be allowed in this case.\n\nThis causes queries to fail with the following stack:\n```\njava.lang.IllegalStateException: No more buffers already set\n    at com.google.common.base.Preconditions.checkState(Preconditions.java:173)\n    at com.facebook.presto.execution.buffer.BroadcastOutputBuffer.getBuffer(BroadcastOutputBuffer.java:301)\n    at com.facebook.presto.execution.buffer.BroadcastOutputBuffer.get(BroadcastOutputBuffer.java:246)\n    at com.facebook.presto.execution.buffer.LazyOutputBuffer.get(LazyOutputBuffer.java:177)\n    at com.facebook.presto.execution.SqlTask.getTaskResults(SqlTask.java:341)\n    at com.facebook.presto.execution.SqlTaskManager.getTaskResults(SqlTaskManager.java:335)\n    at com.facebook.presto.server.TaskResource.getResults(TaskResource.java:242)\n```", "NaN"], ["7446", "Add 0.167 release notes", "David Phillips", "electrum", "02/24/17, 11:31:22 PM", "NaN", "NaN"], ["7450", "Convert StatementResource to use async HTTP responses", "Dain Sundstrom", "dain", "10/07/17, 01:02:13 AM", "This PR also includes commits to fix a few bugs in ExchangeClient and\nis includes the pre-work of replaceing CompletableFuture with ListenableFuture", "NaN"], ["7451", "Support INSERT for Cassandra connector", "Yuya Ebihara", "ebyhr", "06/15/17, 07:50:00 PM", "#6502 \r\n\r\nSince there is no update from zz22394 for 4-months, I tried developing this enhancement. ", "NaN"], ["7452", "Update to JDBI 2.78", "David Phillips", "electrum", "02/27/17, 07:10:17 PM", "NaN", "NaN"], ["7455", "Remove duplicate dependency from POM", "David Phillips", "electrum", "02/27/17, 07:05:25 PM", "NaN", "NaN"], ["7456", "Improve no more splits check in TaskSource", "Dain Sundstrom", "dain", "02/27/17, 10:48:57 PM", "NaN", "NaN"], ["7459", "Convert common Hive external errors to user errors", "David Phillips", "electrum", "03/01/17, 06:27:49 AM", "NaN", "NaN"], ["7461", "Add benchmark for TransformKey and TransformValue", "James Sun", "highker", "04/05/17, 07:04:43 AM", "Two new benchmarks added for evaluating the possible outcome for\r\nbytecode generation for map transform key/value. Both results show a\r\nslightly better outcome.\r\n\r\nBenchmarkTransformKey is to transform a long type key `x` to `x + 1`.\r\nBenchmarkTransformValue is to transform a long type value `y` to a\r\nboolean type if `y > 0`.\r\n\r\nBenchmarkTransformKey\r\n```\r\nBenchmark     (name)  (type)  Mode  Cnt    Score   Error  Units\r\n      transform_keys  DOUBLE  avgt   20  143.077 \u00b1 2.378  ns/op\r\nexact_transform_keys  DOUBLE  avgt   20  136.607 \u00b1 7.131  ns/op\r\n      transform_keys  BIGINT  avgt   20  107.877 \u00b1 5.898  ns/op\r\nexact_transform_keys  BIGINT  avgt   20   92.876 \u00b1 4.231  ns/op\r\n```\r\n\r\n```\r\nBenchmark       (name)   (type)  Mode  Cnt   Score   Error  Units\r\n      transform_values   BIGINT  avgt   20  67.534 \u00b1 4.069  ns/op\r\nexact_transform_values   BIGINT  avgt   20  50.477 \u00b1 2.556  ns/op\r\n      transform_values   DOUBLE  avgt   20  62.217 \u00b1 1.273  ns/op\r\nexact_transform_values   DOUBLE  avgt   20  50.615 \u00b1 3.348  ns/op\r\n      transform_values  VARCHAR  avgt   20  89.682 \u00b1 3.663  ns/op\r\nexact_transform_values  VARCHAR  avgt   20  76.351 \u00b1 3.868  ns/op\r\n```", "NaN"], ["7466", "Update docker configs for Hive integration tests", "Anton", "petroav", "02/28/17, 05:22:14 PM", "- Update the DNS port number to something in the IANA dynamic port range.\r\n  The current 5353 conflicts with a system service on Ubuntu.\r\n- Update the version of the CDH docker image used along with the\r\n  Metastore Thrift proxy port.", "NaN"], ["7471", "Add support for getting type info with jdbc driver", "Nezih Yigitbasi", "nezihyigitbasi", "03/18/17, 12:21:23 AM", "Fixes #7350.", "NaN"], ["7472", "Allow inserts into tables above partition limit", "Wenlei Xie", "wenleix", "05/16/17, 03:11:44 AM", "Introduced in [#5396](https://github.com/prestodb/presto/pull/5396/files#diff-13aa0bdc2b255255b85a1b0763c2cb6dL1332), `HiveMetadata` needs `BucketHandler` bucket writing and execution in Hive. Current implementation get the `BucketHandler` by first obtaining\r\n`HivePartitionResult`. This is unnecessary and cause issues when the output table has over 100,000 partitions.", "NaN"], ["7474", "Hive RCFile updates", "David Phillips", "electrum", "03/01/17, 06:11:58 AM", "NaN", "NaN"], ["7475", "Update documentation for array concat and map concat functions", "Mark", "geraint0923", "03/08/17, 06:28:26 PM", "Update documentation due to the function changes introduced in #6831 ", "NaN"], ["7482", "Add stats and change progress heuristic", "Raghav Sethi", "raghavsethi", "03/06/17, 11:48:46 PM", "NaN", "NaN"], ["7485", "Implement toString for CachingOrcDataSource", "David Phillips", "electrum", "03/10/17, 08:26:05 PM", "This method is used for user error messages.", "NaN"], ["7495", "Enable iterative optimizer by default", "Martin Traverso", "martint", "03/26/17, 12:21:50 AM", "NaN", "NaN"], ["7503", "Remove unused approximate query assertions", "David Phillips", "electrum", "03/07/17, 06:58:02 PM", "NaN", "NaN"], ["7505", "Improve resource group sanity check", "Rongrong Zhong", "rongrong", "03/28/17, 02:33:30 AM", "resolves #7494", "NaN"], ["7508", "Bytecode generation for MapTransformKeyFunction", "James Sun", "highker", "04/07/17, 08:32:23 PM", "Rewrite MapTransformKeyFunction in forms of bytecode generation.", "NaN"], ["7515", "Change page projection to always process columnar", "Dain Sundstrom", "dain", "04/15/17, 01:16:12 AM", "- Dynamically size projection batch size based on previous output\r\n- Dynamically enable/disable dictionary processing based on previous effectiveness\r\n- Add fast path for constant projection\r\n- Add fast path for identity projection\r\n- Skip filtering for TRUE filter\r\n- Change block builders to lazily allocate initial buffers to improve memory efficiency\r\n- Remove processing_optimization session property and optimizer.processing-optimization configuration property\r\n\r\n# Performance\r\nThis change should not improve or hurt performance of columnar or dictionary, but simply makes it possible to enable them at all times (no configuration).  I ran the following benchmarks to verify this:\r\n\r\n## Before\r\n```\r\nBenchmark                          Mode  Cnt     Score    Error  Units\r\nBenchmarkPageProcessor.compiled   thrpt   50  4399.932 \u00b1 66.100  ops/s\r\nBenchmarkPageProcessor.handCoded  thrpt   50  4686.470 \u00b1 76.373  ops/s\r\n\r\nBenchmark                                         (name)  Mode  Cnt    Score   Error  Units\r\nBenchmarkArrayDistinct.arrayDistinct      array_distinct  avgt   20   44.867 \u00b1 0.957  ns/op\r\nBenchmarkArrayDistinct.arrayDistinct  old_array_distinct  avgt   20  163.298 \u00b1 4.158  ns/op\r\n\r\nBenchmark                             (name)  Mode  Cnt    Score   Error  Units\r\nBenchmarkArrayFilter.benchmark        filter  avgt   20  120.572 \u00b1 5.021  ns/op\r\nBenchmarkArrayFilter.benchmark  exact_filter  avgt   20   76.784 \u00b1 3.005  ns/op\r\n\r\nBenchmark                     Mode  Cnt    Score   Error  Units\r\nBenchmarkArrayJoin.benchmark  avgt   60  186.730 \u00b1 3.446  ns/op\r\n\r\nBenchmark                             (name)  Mode  Cnt    Score   Error  Units\r\nBenchmarkArraySort.arraySort      array_sort  avgt   20   99.980 \u00b1 8.269  ns/op\r\n\r\nBenchmark                     (mapConfig)  Mode  Cnt     Score    Error  Units\r\nBenchmarkMapConcat.mapConcat   left_empty  avgt   20   265.374 \u00b1  4.648  ns/op\r\nBenchmarkMapConcat.mapConcat  right_empty  avgt   20   257.638 \u00b1  3.667  ns/op\r\nBenchmarkMapConcat.mapConcat   both_empty  avgt   20    38.838 \u00b1  1.135  ns/op\r\nBenchmarkMapConcat.mapConcat    non_empty  avgt   20  1817.320 \u00b1 70.188  ns/op\r\n\r\nBenchmark                           (mapSize)      (name)  Mode  Cnt     Score    Error  Units\r\nBenchmarkMapSubscript.mapSubscript          1   fix-width  avgt   20    45.505 \u00b1  7.904  ns/op\r\nBenchmarkMapSubscript.mapSubscript          1   var-width  avgt   20    84.766 \u00b1  1.569  ns/op\r\nBenchmarkMapSubscript.mapSubscript          1  dictionary  avgt   20    67.824 \u00b1  5.565  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13   fix-width  avgt   20  1481.749 \u00b1 18.282  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13   var-width  avgt   20  2538.951 \u00b1 42.323  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13  dictionary  avgt   20  2167.449 \u00b1 42.817  ns/op\r\n\r\n```\r\n\r\n## After\r\n```\r\nBenchmark                          Mode  Cnt     Score     Error  Units\r\nBenchmarkPageProcessor.compiled   thrpt   50  4367.725 \u00b1 171.266  ops/s\r\nBenchmarkPageProcessor.handCoded  thrpt   50  4522.360 \u00b1  94.679  ops/s\r\n\r\nBenchmark                                         (name)  Mode  Cnt    Score   Error  Units\r\nBenchmarkArrayDistinct.arrayDistinct      array_distinct  avgt   20   45.514 \u00b1 2.885  ns/op\r\nBenchmarkArrayDistinct.arrayDistinct  old_array_distinct  avgt   20  160.116 \u00b1 3.091  ns/op\r\n\r\nBenchmark                             (name)  Mode  Cnt    Score   Error  Units\r\nBenchmarkArrayFilter.benchmark        filter  avgt   20  123.771 \u00b1 4.506  ns/op\r\nBenchmarkArrayFilter.benchmark  exact_filter  avgt   20   77.407 \u00b1 1.768  ns/op\r\n\r\nBenchmark                     Mode  Cnt    Score   Error  Units\r\nBenchmarkArrayJoin.benchmark  avgt   60  183.565 \u00b1 3.695  ns/op\r\n\r\nBenchmark                             (name)  Mode  Cnt    Score   Error  Units\r\nBenchmarkArraySort.arraySort      array_sort  avgt   20   87.615 \u00b1 4.817  ns/op\r\n\r\nBenchmark                     (mapConfig)  Mode  Cnt     Score    Error  Units\r\nBenchmarkMapConcat.mapConcat   left_empty  avgt   20   259.042 \u00b1  6.192  ns/op\r\nBenchmarkMapConcat.mapConcat  right_empty  avgt   20   261.633 \u00b1  7.658  ns/op\r\nBenchmarkMapConcat.mapConcat   both_empty  avgt   20    38.872 \u00b1  0.308  ns/op\r\nBenchmarkMapConcat.mapConcat    non_empty  avgt   20  1690.355 \u00b1 34.162  ns/op\r\n\r\nBenchmark                           (mapSize)      (name)  Mode  Cnt     Score    Error  Units\r\nBenchmarkMapSubscript.mapSubscript          1   fix-width  avgt   20    37.986 \u00b1  0.705  ns/op\r\nBenchmarkMapSubscript.mapSubscript          1   var-width  avgt   20    82.147 \u00b1  2.596  ns/op\r\nBenchmarkMapSubscript.mapSubscript          1  dictionary  avgt   20    60.646 \u00b1  1.504  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13   fix-width  avgt   20  1517.320 \u00b1 37.219  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13   var-width  avgt   20  2365.492 \u00b1 88.507  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13  dictionary  avgt   20  2087.881 \u00b1 19.953  ns/op\r\n```", "NaN"], ["7516", "Allow configuration of environment in TestingNodeManager", "Dain Sundstrom", "dain", "03/06/17, 10:39:55 PM", "NaN", "NaN"], ["7517", "Bytecode generation for MapTransformValueFunction", "Wenlei Xie", "wenleix", "05/09/17, 12:18:35 AM", "This is related to https://github.com/prestodb/presto/issues/7436\r\n\r\nBenchmark results:\r\n\r\nBefore:\r\n```\r\nBenchmark                                          (name)   (type)  Mode  Cnt   Score   Error  Units\r\nBenchmarkTransformValue.benchmark        transform_values   BIGINT  avgt   20  64.830 \u00b1 2.420  ns/op\r\nBenchmarkTransformValue.benchmark        transform_values   DOUBLE  avgt   20  62.140 \u00b1 2.225  ns/op\r\nBenchmarkTransformValue.benchmark        transform_values  VARCHAR  avgt   20  89.144 \u00b1 3.883  ns/op\r\nBenchmarkTransformValue.benchmark  exact_transform_values   BIGINT  avgt   20  50.635 \u00b1 2.510  ns/op\r\nBenchmarkTransformValue.benchmark  exact_transform_values   DOUBLE  avgt   20  46.683 \u00b1 0.580  ns/op\r\nBenchmarkTransformValue.benchmark  exact_transform_values  VARCHAR  avgt   20  74.325 \u00b1 1.145  ns/op\r\n```\r\n\r\n\r\n\r\nAfter:\r\n```\r\nBenchmark                                          (name)   (type)  Mode  Cnt   Score   Error  Units\r\nBenchmarkTransformValue.benchmark        transform_values   BIGINT  avgt   20  55.218 \u00b1 1.116  ns/op\r\nBenchmarkTransformValue.benchmark        transform_values   DOUBLE  avgt   20  53.021 \u00b1 2.259  ns/op\r\nBenchmarkTransformValue.benchmark        transform_values  VARCHAR  avgt   20  81.336 \u00b1 2.749  ns/op\r\nBenchmarkTransformValue.benchmark  exact_transform_values   BIGINT  avgt   20  50.420 \u00b1 1.304  ns/op\r\nBenchmarkTransformValue.benchmark  exact_transform_values   DOUBLE  avgt   20  47.965 \u00b1 1.345  ns/op\r\nBenchmarkTransformValue.benchmark  exact_transform_values  VARCHAR  avgt   20  77.922 \u00b1 1.746  ns/op\r\n```", "NaN"], ["7518", "Refactor AbstractTestQueryFramework", "David Phillips", "electrum", "03/07/17, 12:38:31 AM", "NaN", "NaN"], ["7519", "Temporarily remove type check in DomainTranslator", "Eric Hwang", "erichwang", "03/09/17, 11:02:25 PM", "Since implicit coercions are switched to explicit coercions in the planner,\r\nwe expect all optimizers to maintain that invariant. The introduction of this check\r\ndemonstrates that we have optimizers that break this invriant. To revert to the existing\r\nbehavior, we should just remove this check until we can fix the optimizers.", "NaN"], ["7522", "Return pages instead of records from ConnectorIndex", "Aleksei Statkevich", "AlekseiS", "03/08/17, 08:18:16 PM", "NaN", "NaN"], ["7530", "Add RemoveEmptyDelete rule v2", "Piotr Findeisen", "findepi", "03/08/17, 02:53:02 AM", "NaN", "NaN"], ["7532", "Add optimization to include intermediate aggregations", "Eric Hwang", "erichwang", "04/27/17, 03:39:55 AM", "Adds an intermediate aggregation stage with a round robin exchange to non-grouping aggregations.", "NaN"], ["7537", "Improve Parquet reader decimal type support", "Nezih Yigitbasi", "nezihyigitbasi", "03/16/17, 04:19:01 PM", "Fixes #7533.  @dain @losipiuk can you please take a look at this one?\r\nI tested this patch with the test Parquet files attached to #7533 only, because it's difficult to write a non-hacky unit test to reproduce it.", "NaN"], ["7538", "Fix \"No more locations already set\"", "Dain Sundstrom", "dain", "03/09/17, 01:21:37 AM", "When adding a new location, check if the exchange is closed, before\nchecking if no-more-locations is set.  This is because no-more-locations\ncan be set after the exchange is closed, but locations can not be added\nwhen closed.\n\nThis also includes several improvements to Driver which were made\nwhile searching for the root casue.", "NaN"], ["7539", "Run AbstractTestHiveClient using local metastore", "David Phillips", "electrum", "03/08/17, 10:36:21 PM", "NaN", "NaN"], ["7540", "Replace CompletableFuture with ListenableFuture", "Dain Sundstrom", "dain", "03/11/17, 10:08:25 PM", "The CompletableFuture interface is difficult to use correctly, and it is\neasy to introduce subtle bugs that result in lost notifications. For\nexample CompletableFuture does not propagate cancel from wrapped futures to\nthe core future. Additionally, it is easy to accidentally schedule tasks on\na system wide fork join pool.\n\nListenableFuture on the other hand is easy to understand and reasonable,\nand the transformations in Guava handle cancelation properly.", "NaN"], ["7541", "Improvements for exchange client", "Dain Sundstrom", "dain", "03/08/17, 10:56:56 PM", "- Fix memory reservation leak in ExchangeClient\n- Remove extra scheduling calls in ExchangeClient\n- Do not call ExchangeClient addPage with empty list", "NaN"], ["7542", "Migrate PickLayout to the iterative optimizer", "Christina Wallin", "cawallin", "07/27/17, 07:56:25 PM", "* Various test framework changes required to write TableLayout tests\r\n* Migrating the rule itself", "NaN"], ["7543", "Move expression optimization from Analyzer to PlanOptimizers part2", "Maciej 'mac' Grzybek", "maciejgrzybek", "03/09/17, 05:25:36 PM", "Supersedes https://github.com/prestodb/presto/pull/7529.", "NaN"], ["7544", "Separate wall time from scheduled time split stats", "Raghav Sethi", "raghavsethi", "03/09/17, 05:45:16 AM", "NaN", "NaN"], ["7545", "Fix size estimates of various typed data structures", "Nezih Yigitbasi", "nezihyigitbasi", "03/27/17, 06:41:46 PM", "NaN", "NaN"], ["7548", "Add overall CPU and scheduled time counters", "Raghav Sethi", "raghavsethi", "03/09/17, 11:21:30 PM", "NaN", "NaN"], ["7549", "Fix setting update type in TestingPrestoClient", "David Phillips", "electrum", "03/09/17, 07:22:15 PM", "NaN", "NaN"], ["7552", "Test presto-hive by itself in Travis", "David Phillips", "electrum", "03/10/17, 01:57:16 AM", "Current build times:\r\n```\r\n[INFO] presto-kafka ....................................... SUCCESS [01:21 min]\r\n[INFO] presto-redis ....................................... SUCCESS [01:03 min]\r\n[INFO] presto-hive ........................................ SUCCESS [22:26 min]\r\n[INFO] presto-mysql ....................................... SUCCESS [ 59.127 s]\r\n[INFO] presto-postgresql .................................. SUCCESS [ 45.863 s]\r\n```", "NaN"], ["7553", "Increase default exchange HTTP max content length to 32MB", "Mark", "geraint0923", "03/10/17, 02:30:39 AM", "NaN", "NaN"], ["7556", "Remove postJoinConjucts code from StatementAnalyzer#visitJoin", "Karol Sobczak", "sopel39", "03/14/17, 05:01:57 PM", "The code operates on rewritten (normalized) expression in Analysis and doesn't\r\nperform any new analysis. Therefore it's conclusions can be ignored.\r\nThe whole section can be replaced with simple recordSubqueries.", "NaN"], ["7558", "Cleanup external error codes", "David Phillips", "electrum", "03/13/17, 06:59:58 PM", "NaN", "NaN"], ["7559", "Add 0.168 release notes", "Dain Sundstrom", "dain", "03/10/17, 11:40:24 PM", "NaN", "NaN"], ["7565", "Fix and Rewrite complex type casting to JSON", "Wenlei Xie", "wenleix", "03/30/17, 12:06:27 AM", "Work in progress. Will have a separate commit for CAST ROW to JSON.\r\n\r\nThe current implementation of casting complex type to JSON is based on `Type.getObjectValue()`, which should not be used for CAST but only for REST service.\r\n\r\nThis has various issues:\r\n1. `Type.getObjectValue()` should only be used for REST service, not for CAST purpose.\r\n2. Various issues for nested data type (https://github.com/prestodb/presto/issues/7226, https://github.com/prestodb/presto/issues/7328)\r\n3. Inconsistent behavior of CAST `ARRAY<X>/MAP<X>` to JSON vs. CAST `X` to JSON. For example, currently  `DATE`/`TIME`/`TIMESTAMP`/`INTERVAL` are not supported for casting to JSON, but `ARRAY<DATE>/MAP<DATE>`... are supported. ", "NaN"], ["7569", "Update to Guava 21", "David Phillips", "electrum", "04/05/17, 07:51:59 PM", "NaN", "NaN"], ["7570", "Add ORD() function", "Jiexi Lin", "jessesleeping", "03/23/17, 09:59:34 PM", "Returns the Unicode code point of a single character string.\r\n\r\nCloses #7560 ", "NaN"], ["7573", "Print stacktrace for long running split", "James Sun", "highker", "03/29/17, 07:23:33 PM", "A split can run for a long time in the worst case. It is worth\r\ninvestigating for such exceptional cases. Print out stacktrace for\r\nsplits who have been running for more than 1000 seconds.", "NaN"], ["7575", "Ensure FileSystem is created with PrestoHadoopConfiguration", "David Phillips", "electrum", "03/14/17, 07:10:18 PM", "We use PrestoHadoopConfiguration to pass already created objects to\nFileSystem instances. This custom Configuration class is created by\nHdfsConfiguration.getConfiguration(), but not all code paths that can\ncreate a FileSystem instance use this. We can hack around this by\ncreating the FileSystem eagerly and relying on the Hadoop FileSystem\ncache that ensures all FileSystems instances are singletons.", "NaN"], ["7576", "Reduce the memory footprint of map_agg and map_union", "Nezih Yigitbasi", "nezihyigitbasi", "04/06/17, 07:49:53 PM", "I moved the key deduping logic from `TypedSet` to `KeyValuePairs` to prevent holding onto the blocks in #7421. Instead of keeping references to individual block elements I now hold indices into the `keyBlockBuilder` to maintain a single copy of the keys. With this change I see ~30-40% less memory being used with the test query in #7342.\r\n\r\nFixes #7342.\r\nSupersedes #7421.", "NaN"], ["7579", "Apply canonicalization recursively for join nodes", "Martin Traverso", "martint", "03/14/17, 11:50:01 PM", "When canonicalization of expression for joins was moved to the optimizer\r\n(commit 4c175ed28dc61cd73b8e82ad89a5135d95b820a1), the code had a bug\r\nwhere it wouldn't recurse and apply the transformations to the children\r\nof the join node.\r\n\r\nFixes https://github.com/prestodb/presto/issues/7577", "NaN"], ["7580", "Consider instance size when calculating the estimated size of BigintGroupByHash", "Nezih Yigitbasi", "nezihyigitbasi", "03/14/17, 08:48:28 PM", "NaN", "NaN"], ["7587", "Add limit on input size for Levenshtein distance", "David Phillips", "electrum", "03/15/17, 07:01:23 PM", "NaN", "NaN"], ["7588", "Add Aircompressor LZO and LZOP codecs for text", "Dain Sundstrom", "dain", "04/27/17, 06:27:41 PM", "Fixes #7348", "NaN"], ["7589", "Cleanup ORC reader and tests", "Dain Sundstrom", "dain", "03/20/17, 11:18:43 PM", "NaN", "NaN"], ["7590", "Add MapBlock with built-in hash table to enable O(1) map access", "Haozhun Jin", "haozhun", "04/27/17, 11:09:08 PM", "NaN", "NaN"], ["7591", "Use OrderBy AST node in Over clause", "Maciej 'mac' Grzybek", "maciejgrzybek", "03/22/17, 03:35:03 AM", "Analogous to c782454b2a2c5b970f573a9404587869bd3577a6.", "NaN"], ["7593", "Add 0.169 release notes", "David Phillips", "electrum", "03/16/17, 01:15:53 AM", "NaN", "NaN"], ["7594", "Update 0.169 release notes", "David Phillips", "electrum", "03/16/17, 03:29:08 AM", "NaN", "NaN"], ["7598", "Consider block builder status size when calculating retained size", "Nezih Yigitbasi", "nezihyigitbasi", "03/16/17, 10:00:18 PM", "There is some inconsistency in the way we calculate the retained sizes in block builders. For example, in `FixedWidthBlockBuilder` and `ArrayBlockBuilder` we take `BlockBuilderStatus.INSTANCE_SIZE` into account while in some other block builders we don't. This PR fixes that.", "NaN"], ["7599", "Remove the outer join differences from Hive migration doc", "Nezih Yigitbasi", "nezihyigitbasi", "03/22/17, 10:10:00 PM", "This info became obsolete for recent Hive versions.\r\n\r\nFixes #7547.", "NaN"], ["7602", "Re-enable Modernizer plugin", "David Phillips", "electrum", "03/20/17, 06:32:27 PM", "This was accidentally disabled when it was moved into Airbase.", "NaN"], ["7603", "Disallow concurrent queries within a transaction", "Jiexi Lin", "jessesleeping", "04/03/17, 08:02:25 PM", "- Detect concurrent queries in SqlQueryManager and throw an exception.\r\n- Refactor the test cases and add a test case for concurrent queries.\r\n- When generate a new transaction for test usage in TransactionBuilder, we mark it as inactive before executing the next query.\r\n\r\n#7564 ", "NaN"], ["7604", "Add zstd decompressor", "Martin Traverso", "martint", "03/22/17, 03:56:53 AM", "Still pending:\r\n- [x] Add comments\r\n- [x] Bounds checks\r\n\r\nBenchmarks (vs JNI version):\r\n\r\n```\r\nairlift_zstd      silesia/dickens       354.3MB/s \u00b1    10.9MB/s ( 3.07%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/dickens       362.5MB/s \u00b1    12.8MB/s ( 3.54%) (N = 30, \u03b1 = 99.9%)\r\n\r\nairlift_zstd      silesia/mozilla       471.7MB/s \u00b1  9302.1kB/s ( 1.93%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/mozilla       413.4MB/s \u00b1    14.7MB/s ( 3.55%) (N = 30, \u03b1 = 99.9%)\r\n\r\nairlift_zstd      silesia/mr            426.8MB/s \u00b1    10.8MB/s ( 2.52%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/mr            374.4MB/s \u00b1    13.4MB/s ( 3.57%) (N = 30, \u03b1 = 99.9%)\r\n\r\nairlift_zstd      silesia/nci           994.1MB/s \u00b1    34.9MB/s ( 3.51%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/nci           828.7MB/s \u00b1    25.4MB/s ( 3.06%) (N = 30, \u03b1 = 99.9%)\r\n\r\nairlift_zstd      silesia/ooffice       326.9MB/s \u00b1    11.2MB/s ( 3.42%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/ooffice       312.0MB/s \u00b1  7922.3kB/s ( 2.48%) (N = 30, \u03b1 = 99.9%)\r\n\r\nairlift_zstd      silesia/osdb          551.3MB/s \u00b1    19.7MB/s ( 3.57%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/osdb          431.8MB/s \u00b1    10.5MB/s ( 2.43%) (N = 30, \u03b1 = 99.9%)\r\n\r\nairlift_zstd      silesia/reymont       478.3MB/s \u00b1    18.0MB/s ( 3.77%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/reymont       438.0MB/s \u00b1    12.8MB/s ( 2.92%) (N = 30, \u03b1 = 99.9%)\r\n\r\nairlift_zstd      silesia/samba         658.9MB/s \u00b1    21.1MB/s ( 3.19%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/samba         609.9MB/s \u00b1    18.6MB/s ( 3.05%) (N = 30, \u03b1 = 99.9%)\r\n\r\nairlift_zstd      silesia/sao           367.1MB/s \u00b1 10198.2kB/s ( 2.71%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/sao           263.4MB/s \u00b1  2803.9kB/s ( 1.04%) (N = 30, \u03b1 = 99.9%)\r\n\r\nairlift_zstd      silesia/webster       465.2MB/s \u00b1    15.8MB/s ( 3.40%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/webster       413.8MB/s \u00b1    11.0MB/s ( 2.67%) (N = 30, \u03b1 = 99.9%)\r\n\r\nairlift_zstd      silesia/x-ray         258.5MB/s \u00b1  9845.5kB/s ( 3.72%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/x-ray         231.8MB/s \u00b1  7933.1kB/s ( 3.34%) (N = 30, \u03b1 = 99.9%)\r\n\r\nairlift_zstd      silesia/xml          1004.2MB/s \u00b1    29.1MB/s ( 2.89%) (N = 30, \u03b1 = 99.9%)\r\nzstd_jni          silesia/xml           828.1MB/s \u00b1    17.1MB/s ( 2.07%) (N = 30, \u03b1 = 99.9%)\r\n```\r\n\r", "NaN"], ["7609", "Remove misleading \"HDFS\" from Hive error messages", "David Phillips", "electrum", "03/17/17, 08:19:47 PM", "We could be reading from a different file system such as S3.", "NaN"], ["7617", "Handle NoSuchElementException in QueryStateInfoResource", "Rongrong Zhong", "rongrong", "03/21/17, 05:19:22 PM", "NaN", "NaN"], ["7621", "Update block size estimates when creating new page builders in group by hash", "Nezih Yigitbasi", "nezihyigitbasi", "04/05/17, 06:22:42 PM", "In `MultiChannelGroupByHash` for every new page started a brand new `PageBuilder` gets instantiated and each new page builder starts with a rough estimate for the number of expected entries. Typically `PageBuilder::reset()` is used to update these estimates, but due to the way `MultiChannelGroupByHash` works right now it cannot call `reset()` and this may result in a waste of memory due to the inaccuracy of these estimates.  This PR fixes that by updating these estimates through a new method added to the `PageBuilder`. I have seen up to ~20% memory savings with synthetic benchmarks. \r\n\r", "NaN"], ["7622", "Use standard charsets in cassandra connector", "Aleksei Statkevich", "AlekseiS", "03/20/17, 08:42:30 PM", "Fixes checkstyle error during the build.", "NaN"], ["7624", "Fix CREATE TABLE IF NOT EXISTS to return 0 when table exists", "David Phillips", "electrum", "03/20/17, 11:40:19 PM", "NaN", "NaN"], ["7625", "Introduce decompressor interface for ORC reader", "Martin Traverso", "martint", "03/21/17, 02:49:23 PM", "NaN", "NaN"], ["7629", "Fix performance regression in ExchangeClient", "Haozhun Jin", "haozhun", "03/21/17, 05:13:29 PM", "ExchangeClient.addPages was not called for empty responses. This skews\r\naverageBytesPerRequest significantly, causing degraded performance for\r\ncertain queries.\r\n\r\nWhen clientCount reduces to a small number like 9, and when almost all\r\nbuffers are empty, fetching a single page can take more than a minute\r\nwhen there are 300 remote sources because the client has to long poll\r\neach one.\r\n\r\nThis regression was introduced in 0.168 by 5cd644f", "NaN"], ["7630", "Use the additional wait time constant in TaskResource instead of hardcoding", "Nezih Yigitbasi", "nezihyigitbasi", "03/21/17, 07:44:28 AM", "NaN", "NaN"], ["7631", "Create a user-friendly message for Raptor error", "Jiexi Lin", "jessesleeping", "06/27/17, 12:22:26 AM", "Log down the number of columns of the table and the number of shards inserted. In the error message, suggest that the number of columns may be the cause of a metadata error thrown during an insertion.\r\n\r\n#7612 \r", "NaN"], ["7637", "Add missing type only coercions during planning UNION", "\u0141ukasz Osipiuk", "losipiuk", "03/23/17, 09:58:02 PM", "Type only coercions were not materialized in logical plan\r\nfor outputs of UNION sources.\r\n\r\nThis is not correct behaviour. Type-only coercions should be present in\r\nlogical plan and only removed during execution.\r\n\r\nThis fixes first query from https://github.com/prestodb/presto/issues/7496", "NaN"], ["7638", "Add query start time to QueryStateInfo", "Rongrong Zhong", "rongrong", "03/24/17, 06:09:44 PM", "NaN", "NaN"], ["7639", "Fix race condition in Driver interruption", "Dain Sundstrom", "dain", "03/22/17, 03:32:21 AM", "When the Diver is closed there is a race between interrupting the current\ndriver thread and the driver thread releasing the lock. In very rare cases,\nThis can result in a thread being interrupted while procing another driver.\n\nWhen a task finishes all Drivers are closed and any pending work interrupted.\nThis interruption should never result in a query failure because the driver\nis only closed when the task is already complete. This change detects this\nsituation and fails with a new error containing the stack of the interrupting\nthread.", "NaN"], ["7644", "Unqarantine TestAllDatatypes tests", "Grzegorz Kokosi\u0144ski", "kokosing", "03/24/17, 06:26:47 AM", "Unqarantine TestAllDatatypes tests", "NaN"], ["7647", "Fix name of query.max-age property in product tests configuration", "Maciej 'mac' Grzybek", "maciejgrzybek", "03/22/17, 09:28:55 PM", "This fixes\r\n```WARN\tmain\tBootstrap\tWarning: Configuration property 'query.max-age' has been replaced. Use 'query.min-expire-age' instead```.\r\nThe config property was renamed in d49f6c615c70bf7338b0b054ad87c76c86865f51.\r\n\r\nCC: @fiedukow", "NaN"], ["7648", "Remove finished queries in QueryStateInfoResource", "Rongrong Zhong", "rongrong", "03/24/17, 01:28:16 AM", "The API is for monitor running queries. We probably won't display finished queries. Remove it to reduce response size. Alternatively we can add a separate parameter to filter query state, which might be a better approach in general. Let me know.", "NaN"], ["7649", "Only unblock exchange readers when pages are added", "Dain Sundstrom", "dain", "03/22/17, 07:49:32 PM", "NaN", "NaN"], ["7650", "Fix duplicate node id when pushing aggregation through union", "Martin Traverso", "martint", "03/23/17, 12:29:46 AM", "PartialAggregationPushdown was creating a copy of the partial\r\naggregation node under each branch of the union, but it wasn't\r\nallocating a new plan node id.\r\n\r\nFixes https://github.com/prestodb/presto/issues/7642", "NaN"], ["7651", "Add missing offset to getDecompressedSize call", "Martin Traverso", "martint", "03/23/17, 12:40:04 AM", "NaN", "NaN"], ["7652", "Add 0.170 release notes", "Martin Traverso", "martint", "03/23/17, 05:57:25 PM", "It's still missing a few items.", "NaN"], ["7653", "Remove obsolete PerfTest class", "David Phillips", "electrum", "03/23/17, 10:50:41 PM", "NaN", "NaN"], ["7655", "Running status", "Rongrong Zhong", "rongrong", "04/03/17, 07:28:10 PM", "NaN", "NaN"], ["7659", "Fix nondeterministic WF test", "Maciej 'mac' Grzybek", "maciejgrzybek", "03/23/17, 03:37:00 PM", "Lag function is not deterministic when there are duplicates in ORDER BY column.\r\nTest wouldn't fail intermittently because there's a fixed order in processing WF functions in Presto currently.\r\nHowever, there is more than one correct result for a Lag function when there are duplicates in ORDER BY column.\r\n\r\nE.g.:\r\nfor `select lag(x) over(order by y) from (values (1,2),(2,2),(3,3)) t(x,y)` both results are correct:\r\nNULL, 1, 2\r\nand\r\nNULL, 2, 1\r\n\r\nbecause (1,2) could be ordered either before or after (2,2).\r\n\r\nThat's why ORDER BY column for lag function in the test was changed to part.name which does not have duplicates.\r\nAlso, removed floating point numbers, to avoid inaccuracy problems.\r\n\r\n---\r\n\r\nTest was introduced here: https://github.com/prestodb/presto/pull/6814/commits/6e88b0670ff3e95f390214a374f7bdba93d3173e to cover `ReorderWindows` correctness cases. The test after rewrite preserves the same features: both WFs have the same `partition by` but different `order by`. Also, `lag` function has a constant parameter.\r\n\r\n---\r\n\r\nSQLServer even marks the `lag` function as nondeterministic in their documentation (see https://msdn.microsoft.com/en-us/library/hh231256.aspx#Anchor_3).", "NaN"], ["7661", "Add additional statistics for WindowOperator in Explain Analyze continued", "Maciej 'mac' Grzybek", "maciejgrzybek", "07/03/17, 09:42:26 AM", "Statistics are low level, therefore `explain analyze verbose` mode was added (syntax as in PostgreSQL; https://www.postgresql.org/docs/9.6/static/sql-explain.html).\r\n\r\nExample output:\r\n\r\nNON-VERBOSE\r\n\r\n```sql\r\n         - Window[partition by (shipmode)][$hashvalue] => [quantity:double, shipmode:varchar(10), $hashvalue:bigint, sum:double]\r\n                 Cost: 92.60%, Output: 60175 rows (2.08MB)\r\n                 Input avg.: 3760.94 lines, Input std.dev.: 139.24%\r\n                 sum := sum(\"quantity\")\r\n```\r\n\r\n---\r\n\r\nVERBOSE:\r\n```sql\r\n         - Window[partition by (shipmode)][$hashvalue] => [quantity:double, shipmode:varchar(10), $hashvalue:bigint, sum:double]\r\n                 Cost: 94.33%, Output: 60175 rows (2.08MB)\r\n                 Input avg.: 3760.94 lines, Input std.dev.: 139.24%\r\n                 Active Drivers: [ 6 / 16 ]\r\n                 Index size in bytes: { count: 23, total: 5238332, min: 0, p05: 0, p25: 0, p50: 316238, p75: 329535, p95: 648366, max: 648366 }\r\n                 Rows in partitions per index: { count: 23, total: 60175, min: 0, p05: 0, p25: 0, p50: 0, p75: 8491, p95: 8669, max: 8710 }\r\n                 Rows in partitions per driver: { count: 16, total: 60175, min: 0, p05: 0, p25: 0, p50: 0, p75: 8616, p95: 17192, max: 17192 }\r\n                 sum := sum(\"quantity\")\r\n```\r\n\r\n---\r\n\r\n- `Active drivers` is a number of operators which had any non-empty data set to process to number of all available drivers.\r\n- `Index size in bytes` is a distribution of the size of index among all the drivers. This may help identify problems when some indexes were bigger than others (data skew / unnecessarily built indexes [possibility to reuse maybe?]).\r\n- `Rows in partitions per index` - how big were partitions per index (index may be built for more than one partition).\r\n- `Rows in partitions per driver` tells us how many rows per partition each driver had to handle (this may help to identify data skew but differently than `index size in bytes` because it's possible to have evenly distributed size of indexes while rows per partition are not, e.g. one big index with single partition and another index with multiple small partitions).\r\n\r\nThey are very low level statistics. Even advanced Presto user may not understand them but they would be helpful to debug the implementation of WF operator (by us, developers) and identify problems visible at customer's site.\r\n`Explain analyze verbose` simply provides a shortcut to extra information exposed in JSON.\r\n\r\n---\r\n\r\nSupersedes https://github.com/prestodb/presto/pull/7382.", "NaN"], ["7663", "Increase S3 client default max retry count", "Nezih Yigitbasi", "nezihyigitbasi", "03/23/17, 10:57:25 PM", "The default max S3 backoff time is `10m` so we have some headroom for retrying more when reading from S3.", "NaN"], ["7664", "Support Block as first State in TwoNullableValueState", "Rongrong Zhong", "rongrong", "03/24/17, 06:52:41 PM", "resolves #7646", "NaN"], ["7666", "Run LDAP tests on Travis", "David Phillips", "electrum", "03/24/17, 11:53:19 PM", "I temporarily removed the other test matrix items and added an additional one to just run the LDAP tests (hopefully they are fast enough to run as part of the existing product tests profile).", "NaN"], ["7677", "Make db/TestQueues.testBasic deterministic", "Jiexi Lin", "jessesleeping", "04/03/17, 04:37:19 PM", "Instead of asserting the running/completed query count, we wait for the count to reach the expected value.\r\n\r\n#7667 ", "NaN"], ["7681", "Fix thread ui", "Nezih Yigitbasi", "nezihyigitbasi", "03/28/17, 05:11:36 AM", "Add back the js library that the /ui/thread depends on.\r\n\r\nFixes #7680", "NaN"], ["7682", "Support ORDER BY clause for aggregations", "Rongrong Zhong", "rongrong", "11/16/17, 06:12:07 AM", "Resolves #3890", "NaN"], ["7684", "Bump tempto version from 1.27 to 1.30", "Grzegorz Kokosi\u0144ski", "kokosing", "04/03/17, 04:38:04 PM", "Bump tempto version from 1.27 to 1.30\r", "NaN"], ["7687", "Remove legacy optimizers", "Martin Traverso", "martint", "03/29/17, 11:10:39 PM", "We've been running the new rules with the\r\niterative optimizer successfully for some time.", "NaN"], ["7691", "Identify REMOTE_HOST_GONE for transport failure", "Wenlei Xie", "wenleix", "04/27/17, 06:53:21 AM", "Related Issue: https://github.com/prestodb/presto/issues/7618\r\n\r\nWork in progress. Early comments on structure are welcome.\r\n\r\nTODO: adding tests.\r\n\r", "NaN"], ["7694", "Fix read max-data-per-node correctly in memory connector config", "Yuya Ebihara", "ebyhr", "04/03/17, 04:35:51 PM", "#7678 \r\nFixed it.\r\nI tested with 200MB and [here](https://gist.github.com/ebyhr/0d5628252e936e4107b176bbb03a04d2#file-memory-connector-log-L725) is the confirmed result.\r", "NaN"], ["7699", "Allow TypeParameters with concrete types in ScalarImplementation", "James Sun", "highker", "05/05/17, 06:24:35 AM", "When a TypeParameter is used in a scalar function, it has to be\r\nannotated. This disallow some concrete types to be passed in to a\r\nfunction, e.g.: array(map(varchar, integer)). This patch relaxes the\r\nrestriction by allows StandardTypes to be used in a signature without\r\nannotation.", "NaN"], ["7702", "Refactor benchto", "Karol Sobczak", "sopel39", "03/30/17, 07:04:26 PM", "NaN", "NaN"], ["7706", "Fix typo: Use denyShowTables in checkCanShowTables", "Yuya Ebihara", "ebyhr", "04/03/17, 04:34:23 PM", "I checked other access control methods and they are LGTM. \r\n\r\n#7703 ", "NaN"], ["7707", "Fix TPCDS data generation", "Piotr Findeisen", "findepi", "04/03/17, 04:37:40 PM", "NaN", "NaN"], ["7714", "Plan tests small improvements", "Piotr Findeisen", "findepi", "04/27/17, 03:06:26 AM", "Stuff that I found useful when working on #7715, but would be non-essential there.\r\n\r\n### `PlanMatchPattern printing` change\r\n\r\nBefore:\r\n```\r\njava.lang.AssertionError: Plan does not match, expected [\r\n\r\nanyTree\r\n    node(UnionNode)\r\n        node(TableScanNode)\r\n            TableScanMatcher{expectedTableName=bang}\r\n        node(TableScanNode)\r\n            TableScanMatcher{expectedTableName=nation}\r\n        node(TableScanNode)\r\n            TableScanMatcher{expectedTableName=nation}\r\n        node(TableScanNode)\r\n            TableScanMatcher{expectedTableName=nation}\r\n\r\n] but found [\r\n\r\n- Output[nationkey, name, regionkey, comment] => [expr_70:bigint, expr_71:varchar(25), expr_72:bigint, expr_73:varchar(152)]\r\n        nationkey := expr_70\r\n        name := expr_71\r\n        regionkey := expr_72\r\n        comment := expr_73\r\n    - Aggregate[expr_70, expr_71, expr_72, expr_73] => [expr_70:bigint, expr_71:varchar(25), expr_72:bigint, expr_73:varchar(152)]\r\n        - Union => [expr_70:bigint, expr_71:varchar(25), expr_72:bigint, expr_73:varchar(152)]\r\n            - TableScan[local:tpch:nation:sf0.01, originalConstraint = null] => [nationkey:bigint, name:varchar(25), regionkey:bigint, comment:varchar(152)]\r\n                    nationkey := tpch:nationkey\r\n                    name := tpch:name\r\n                    regionkey := tpch:regionkey\r\n                    comment := tpch:comment\r\n            - TableScan[local:tpch:nation:sf0.01, originalConstraint = null] => [nationkey_7:bigint, name_8:varchar(25), regionkey_9:bigint, comment_10:varchar(152)]\r\n                    nationkey_7 := tpch:nationkey\r\n                    name_8 := tpch:name\r\n                    regionkey_9 := tpch:regionkey\r\n                    comment_10 := tpch:comment\r\n            - TableScan[local:tpch:nation:sf0.01, originalConstraint = null] => [nationkey_32:bigint, name_33:varchar(25), regionkey_34:bigint, comment_35:varchar(152)]\r\n                    nationkey_32 := tpch:nationkey\r\n                    name_33 := tpch:name\r\n                    regionkey_34 := tpch:regionkey\r\n                    comment_35 := tpch:comment\r\n            - TableScan[local:tpch:nation:sf0.01, originalConstraint = null] => [nationkey_45:bigint, name_46:varchar(25), regionkey_47:bigint, comment_48:varchar(152)]\r\n                    nationkey_45 := tpch:nationkey\r\n                    name_46 := tpch:name\r\n                    regionkey_47 := tpch:regionkey\r\n                    comment_48 := tpch:comment\r\n\r\n]\r\n```\r\n\r\nAfter (same indentation length is used):\r\n```\r\n- anyTree\r\n    - node(UnionNode)\r\n        - node(TableScanNode)\r\n            TableScanMatcher{expectedTableName=bang}\r\n        - node(TableScanNode)\r\n            TableScanMatcher{expectedTableName=nation}\r\n        - node(TableScanNode)\r\n            TableScanMatcher{expectedTableName=nation}\r\n        - node(TableScanNode)\r\n            TableScanMatcher{expectedTableName=nation}\r\n\r\n] but found [\r\n\r\n- Output[nationkey, name, regionkey, comment] => [expr_70:bigint, expr_71:varchar(25), expr_72:bigint, expr_73:varchar(152)]\r\n        nationkey := expr_70\r\n        name := expr_71\r\n        regionkey := expr_72\r\n        comment := expr_73\r\n    - Aggregate[expr_70, expr_71, expr_72, expr_73] => [expr_70:bigint, expr_71:varchar(25), expr_72:bigint, expr_73:varchar(152)]\r\n        - Union => [expr_70:bigint, expr_71:varchar(25), expr_72:bigint, expr_73:varchar(152)]\r\n            - TableScan[local:tpch:nation:sf0.01, originalConstraint = null] => [nationkey:bigint, name:varchar(25), regionkey:bigint, comment:varchar(152)]\r\n                    nationkey := tpch:nationkey\r\n                    name := tpch:name\r\n                    regionkey := tpch:regionkey\r\n                    comment := tpch:comment\r\n            - TableScan[local:tpch:nation:sf0.01, originalConstraint = null] => [nationkey_7:bigint, name_8:varchar(25), regionkey_9:bigint, comment_10:varchar(152)]\r\n                    nationkey_7 := tpch:nationkey\r\n                    name_8 := tpch:name\r\n                    regionkey_9 := tpch:regionkey\r\n                    comment_10 := tpch:comment\r\n            - TableScan[local:tpch:nation:sf0.01, originalConstraint = null] => [nationkey_32:bigint, name_33:varchar(25), regionkey_34:bigint, comment_35:varchar(152)]\r\n                    nationkey_32 := tpch:nationkey\r\n                    name_33 := tpch:name\r\n                    regionkey_34 := tpch:regionkey\r\n                    comment_35 := tpch:comment\r\n            - TableScan[local:tpch:nation:sf0.01, originalConstraint = null] => [nationkey_45:bigint, name_46:varchar(25), regionkey_47:bigint, comment_48:varchar(152)]\r\n                    nationkey_45 := tpch:nationkey\r\n                    name_46 := tpch:name\r\n                    regionkey_47 := tpch:regionkey\r\n                    comment_48 := tpch:comment\r\n\r\n]\r\n```", "NaN"], ["7719", "Add 0.171 release notes", "Haozhun Jin", "haozhun", "03/31/17, 11:56:48 PM", "NaN", "NaN"], ["7722", "Support table comment in CREATE TABLE", "Rongrong Zhong", "rongrong", "04/10/17, 06:20:22 PM", "Resolves #7717\r\nOnly implemented for hive connector.\r\n\r\nI'm not sure whether it needs to be in two commits. But it's probably easier to review it this way. I tested it against production manually. Where should I add unit test for it?", "NaN"], ["7725", "Add SSL support to JDBC driver", "David Phillips", "electrum", "06/23/17, 11:00:04 PM", "This is a replacement for #6410.", "NaN"], ["7731", "Support MaterializedView in Cassandra connector", "Yuya Ebihara", "ebyhr", "06/01/18, 10:37:34 AM", "CachingCassandraSchemaProvider's `loadTable` method may not be efficient since it gets all tables in the schema.\r\n\r\n#7363", "NaN"], ["7736", "Update Curator for Accumulo to 2.12.0", "David Phillips", "electrum", "04/04/17, 06:22:57 PM", "This version uses a shaded version of Guava.", "NaN"], ["7737", "Fix unsafe access in zstd decompressor", "Martin Traverso", "martint", "04/04/17, 12:45:41 AM", "When matchOutputLength differs from fastOutputLength by MINMATCH,\r\nthe fast path can end up writing beyond the limit.", "NaN"], ["7738", "Improve verifier error message", "Rongrong Zhong", "rongrong", "04/06/17, 07:40:53 PM", "* If test query timed out during set up, mark as TIMEOUT rather than FAILED_TO_SETUP.\r\n* Only compare results if both control and test queries are SUCCESS.", "NaN"], ["7743", "Fix counters for overall CPU and scheduled time", "Raghav Sethi", "raghavsethi", "04/05/17, 12:49:37 AM", "NaN", "NaN"], ["7748", "Fail when encrypted objects don't have unencrypted length header", "Nezih Yigitbasi", "nezihyigitbasi", "06/08/17, 09:43:38 PM", "We now fail fast when an encrypted object doesn't have the\r\nx-amz-unencrypted-content-length header as that will cause\r\nproblems when file formats try decoding them.\r\n\r\nThe reason that this header is not set is probably due to a bug on the client side and when queries fail because of this problem those objects have to be fixed.", "NaN"], ["7750", "Fix formatting for EXPLAIN fragment descriptions", "David Phillips", "electrum", "04/05/17, 07:39:58 PM", "Wrapped text needs to be aligned with the list bullet.\r\nBut using a definition list is better for this.\r\n\r\n<img width=\"1056\" alt=\"screen shot 2017-04-05 at 12 25 43 pm\" src=\"https://cloud.githubusercontent.com/assets/9230/24723114/0ca8ed78-19fb-11e7-90b1-73cbbb6e92cb.png\">\r", "NaN"], ["7751", "Update note about running Sphinx manually", "David Phillips", "electrum", "04/05/17, 07:50:42 PM", "NaN", "NaN"], ["7756", "Replace ImmutableCollectors with Guava version", "David Phillips", "electrum", "04/06/17, 05:15:40 AM", "NaN", "NaN"], ["7759", "Fix join property derivations when not all columns are output", "Martin Traverso", "martint", "04/07/17, 06:02:16 AM", "Property derivation logic was producing properties in terms of\r\ncolumns that might not be output by the join. This is an invalid\r\nderivation.\r\n\r\nAs a result, AddExchanges could make suboptimal decisions when\r\ndeciding whether an operation could leverage the properties from\r\nits inputs.\r\n\r\nThis change fixes it the logic to either remove or translate the\r\ncolumns (where appropriate, based on equality conditions) so that\r\nproperties only reference output columns from the join.\r\n\r\nFixes #7753 ", "NaN"], ["7763", "Build archives of presto-benchto-benchmarks", "Karol Sobczak", "sopel39", "04/06/17, 05:54:13 PM", "Builds ZIP, TAR.GZ, TAR.BZ2 archives of benchmarks.\r\n\r\nThe archive doesn't contain base directory inside.", "NaN"], ["7764", "Fix typo in spill error message", "David Phillips", "electrum", "04/17/17, 08:58:24 PM", "NaN", "NaN"], ["7769", "Fix copying of identity-based collections", "Piotr Findeisen", "findepi", "04/06/17, 08:29:05 PM", "Identity-based set should not be `ImmutableSet.copyOf`-ed, ever.", "NaN"], ["7770", "Miscellaneous UI fixes", "Raghav Sethi", "raghavsethi", "04/06/17, 10:50:29 PM", "NaN", "NaN"], ["7772", "Revert \"Disallow concurrent queries within a transaction\"", "Nezih Yigitbasi", "nezihyigitbasi", "04/06/17, 09:57:45 PM", "This reverts commit b7e8ed7d2dd34789c6e84db905cc6478696b30f3.\r\n\r\nFixes #7771", "NaN"], ["7774", "Remove DesguaringOptimizer.visitApply", "Haozhun Jin", "haozhun", "04/07/17, 01:19:32 AM", "This partially backs out 7846e86. A few queries in AbstractTestQueries\r\notherwise fails planning.\r\n\r\ncc @sopel39 @kokosing #7527 #7674 \r\n\r\nSee https://travis-ci.org/prestodb/presto/builds/219481565", "NaN"], ["7777", "Add intermediate plan validatation", "Grzegorz Kokosi\u0144ski", "kokosing", "06/21/17, 11:04:32 AM", "Add intermediate plan validatation\r\n\r\nCheck if plan is valid before applying any optimizers.\r\nIf plan is invalid then some optimizer may throw very misleading\r\nexception. For example please see the #7773 issue, where access control\r\nerror was thrown, but it was caused by unexpected subquery expression\r\nwithin an plan node tree.\r\n\r\nNow the query from #7773 would throw something like below:\r\n```\r\njava.lang.IllegalStateException: Unexpected subquery expression in\r\nlogical plan: (SELECT 1\r\n\r\n    )\r\n        at\r\n        com.facebook.presto.sql.planner.sanity.NoSubqueryExpressionLeftChecker$1.visitSubqueryExpression(NoSubqueryExpressionLeftChecker.java:43)\r\n        ...\r\n        com.facebook.presto.sql.planner.sanity.PlanSanityChecker.validateIntermediatePlan(PlanSanityChecker.java:54)\r\n        ...\r\n```", "NaN"], ["7783", "Fix Maven warning for deprecated variable", "David Phillips", "electrum", "04/07/17, 08:19:38 PM", "NaN", "NaN"], ["7785", "Throw rather than lie in IdentityLinkedHashMap", "Piotr Findeisen", "findepi", "04/10/17, 04:37:39 PM", "NaN", "NaN"], ["7792", "Support NULL on left side of semi join", "Piotr Findeisen", "findepi", "05/17/17, 05:30:14 PM", "- predicate push down for semi join's build side got disabled\r\n- new replication mode \"replicate nulls and any row\"\r\n- nulls on left side of `IN` are correctly supported now\r\n- there is session flag unlocking old behavior \r\n\r\nFixes #6991. Related to #7208, #6622", "NaN"], ["7794", "Parse parenthesized query in CREATE TABLE AS", "Yuya Ebihara", "ebyhr", "04/27/17, 02:47:57 AM", "Fixed #7745 \r", "NaN"], ["7795", "Remove deprecated ReorderWindows optimizer", "Maciej 'mac' Grzybek", "maciejgrzybek", "04/12/17, 05:33:08 PM", "Iterative optimizer is turned on by default so the new SwapAdjacentWindowsByPartitionsOrder rule is used instead.\r\n\r\nPruneIdentityProjections does not exist anymore, so we cannot fall back to the ReorderWindows optimizer which required that rewrite as a prerequisite.\r\nTherefore ReorderWindows rule is not needed any more.\r\n\r\nNew rule does not provide a feature flag which therefore becomes obsolete as well.", "NaN"], ["7797", "Include aggregation mask when rewriting count(1)", "Martin Traverso", "martint", "04/11/17, 02:37:50 AM", "SimplifyCountOverConstant was ignoring the mask when rewriting\r\nthe aggregation, which resulted in queries producing incorrect\r\nresults due to the FILTER (WHERE ...) clause being lost.\r\n\r\nFixes https://github.com/prestodb/presto/issues/7796", "NaN"], ["7799", "Support table comment in ShowQueriesRewrite", "Rongrong Zhong", "rongrong", "04/10/17, 11:16:12 PM", "NaN", "NaN"], ["7800", "Fix typo in resource group sanity check message", "Nezih Yigitbasi", "nezihyigitbasi", "04/12/17, 04:21:26 PM", "NaN", "NaN"], ["7803", "Add 0.173 release notes", "Dain Sundstrom", "dain", "04/11/17, 03:47:28 PM", "NaN", "NaN"], ["7811", "Support coercion of non-function positions in lambda", "Haozhun Jin", "haozhun", "05/15/17, 08:35:30 PM", "`f(array(int), int, (*, int)->bigint)` fails during analysis for\r\n`f(array(T), S, (S, T)->S)` because SignatureBinder fails when trying\r\nto match bigint (in function return type) with S (which is presumed to\r\nbe int based on parameters seen so far).\r\n\r\nAn example of such an `f` is reduce function.", "NaN"], ["7812", "Fix comparing varchar for window function tests", "David Phillips", "electrum", "04/12/17, 12:47:20 AM", "Not all connectors properly support bounded varchar, so only compare\nthe rows and not the expected types, similar to other tests. Also,\nthese tests are actually for the engine, so we really should not be\nrunning them for each connector, but that's a bigger issue.", "NaN"], ["7813", "Add greatest/least function documentations", null, "janewangfb", "05/22/17, 03:08:20 PM", "NaN", "NaN"], ["7817", "Migrate merge windows rule to iterative optimizer", "Amruta Gokhale", "amrutagokhale", "05/16/17, 12:58:49 AM", "The last commit removes the MergeWindows optimizer, in favor of the new MergeAdjacentWindows optimizer rule. This commit can be merged a bit later, after the first three commits go in.\r\n\r\nTD review here: https://github.com/Teradata/presto/pull/495", "NaN"], ["7819", "Change default metastore cache timeout to 0 in Hive connector", "Haozhun Jin", "haozhun", "04/19/17, 08:25:55 PM", "Per-transaction metastore cache addresses the issue of repeated access\r\nof same data in the metastore. As a result, having a connector-wide\r\nmetastore cache is no longer necessary.", "NaN"], ["7820", "Fix test failure in master", "Nezih Yigitbasi", "nezihyigitbasi", "04/12/17, 10:53:43 PM", "NaN", "NaN"], ["7833", "Support clustering key predicate pushdown(Equal, IN and Range) in Cassandra", "Sumit Jain", "sumitkgec", "06/19/17, 06:51:48 PM", "**Problem statement:** \r\nDue to predicate push down for clustering keys was missing, Presto performance was getting hit. It taking entire partition data out of Cassandra and doing the calculation, this can be faster if Presto will pass the partition and clustering keys to Cassandra and do the calculation on the small portion of the data.\r\n\r\nI opened this pull request to support Cassandra predicate pushdown on clustering columns(partition key pushdown already available in Presto).\r\n\r\nJust for reference - Cassandra allow below restriction in WHERE clause for SELECT statements(except Secondary Index)\r\n**Partition keys restrictions**\r\nThe partition key columns support only two operators(except token function): = and IN.\r\n**Clustering column restrictions**\r\nClustering columns support the =, IN, >, >=, <=, < restrictions. Range restrictions only allowed in the last clustering column being restricted. IN is only supported in the last clustering column in Cassandra version < 2.2.X. ", "NaN"], ["7836", "Remove use of 'let' in utils.js", "Adam J. Shook", "adamjshook", "04/18/17, 11:46:55 PM", "This fixes an issue where the UI would not load in older versions of\r\nFirefox due to lack of support of the 'let' JavaScript keyword.", "NaN"], ["7839", "Clean up TaskExecutor and add stats", "Raghav Sethi", "raghavsethi", "04/15/17, 03:39:10 AM", "NaN", "NaN"], ["7841", "Fix No factory for connector sqlserver error in product tests", "Yuya Ebihara", "ebyhr", "04/21/17, 05:50:38 PM", "I think that `config.properties` in product tests is missing presto-sqlserver line.\r\nRunning without the line fails in intelliJ.\r", "NaN"], ["7842", "Remove unused feature flag", "Maciej 'mac' Grzybek", "maciejgrzybek", "04/16/17, 11:32:39 PM", "I missed that bit in https://github.com/prestodb/presto/pull/7795.", "NaN"], ["7848", "Display buffered bytes by stage on live plan", "Piotr Nowojski", "pnowojski", "04/19/17, 06:10:27 PM", "![buffered_bytes](https://cloud.githubusercontent.com/assets/8957547/25121539/53849fcc-2422-11e7-93dd-95a5c0b0ca68.png)\r", "NaN"], ["7849", "Add support for BETWEEN to expression matcher", null, "fmeiser", "04/20/17, 04:02:28 AM", "This PR adds support for BETWEEN to expression matcher.\r\nAnd establishes the possibility for rules for optimizing some date functions in a later PR.", "NaN"], ["7850", "Updating to latest testing-mysql-server", "David Phillips", "electrum", "04/19/17, 04:03:48 PM", "NaN", "NaN"], ["7851", "Throw PrestoExceptions in date time operators", "Dain Sundstrom", "dain", "04/18/17, 11:00:43 PM", "NaN", "NaN"], ["7853", "Merge state change listeners in QueryStateMachine::beginWithTicker", "Nezih Yigitbasi", "nezihyigitbasi", "04/19/17, 12:12:29 AM", "NaN", "NaN"], ["7854", "Remove `Block.reset()`", "Haozhun Jin", "haozhun", "04/19/17, 08:26:23 PM", "It is hard to reason about reset in BlockBuilder because it performs\r\nnon-appending operation.\r\n\r\nFor example, with reduce function, the result from previous invocation of\r\narray_concat may be feed back into array_concat as input. If reset is called\r\nwhile producing the next result, the input (the previous result) may no\r\nlonger be valid.", "NaN"], ["7858", "Reduce aggregation to complex type memory usage", "James Sun", "highker", "06/06/17, 07:23:25 PM", "Make BlockBuilderStatus nullable in various block builders to reduce\r\nthe memory usage for functions that aggregate columns into complex\r\ntypes; e.g., array_agg, map_agg, histogram, min_by, etc. In theory,\r\nthe change can reduce memory utilization up to 43%, which is the ratio\r\nof the size of BlockBuilderStatus over the size of BlockBuilder. In\r\npractice, we may observe a memory safe around 25 - 30% for large scale\r\nof data.\r\n\r\nSmall-scale benchmark for array_agg on double type:\r\n```\r\nbefore: sql_double_array_agg ::   33.493 cpu ms :: 3.13MB peak memory\r\nafter:  sql_double_array_agg ::   33.414 cpu ms :: 2.33MB peak memory\r\n```", "NaN"], ["7859", "Update mysql-connector to 5.1.41", "Yi He", "hellium01", "05/01/17, 10:42:09 PM", "The previous version is from 2015. A few bug fixes are included in newer\r\nversions of mysql driver.", "NaN"], ["7860", "Feature fix 7824 skip limit subquery", "Karol Sobczak", "sopel39", "04/19/17, 05:18:28 PM", "Fix for: #7824\r\n\r\nFYI: @kokosing ", "NaN"], ["7861", "Make LambdaReferenceExtractor search for unbounded lambda references", "Karol Sobczak", "sopel39", "05/18/17, 10:52:02 PM", "AggregationAnalyzer checks if expression is equal to one of group\r\nexpressions. If the expression contains lambda then it needs to verify\r\nthat all lambda argument references are bound before comparing\r\nwith group expressions.\r\n\r\nFixes: https://github.com/prestodb/presto/issues/7780", "NaN"], ["7864", "Fix selected level counters", "Raghav Sethi", "raghavsethi", "04/20/17, 10:47:03 PM", "NaN", "NaN"], ["7865", "Handle failures in dictionary aware filter and projection", "Dain Sundstrom", "dain", "04/19/17, 08:29:58 PM", "When processing a dictionary failures can occure for unused entries which causes\nthe query to fail.  As a work around, when the failure occurs, the dictionary\nprocessing is disabled for that dictionary.", "NaN"], ["7866", "Avoid the binding operation when generating lambda byte-code", "Wenlei Xie", "wenleix", "05/15/17, 12:45:40 AM", "Work in progress. Binding lambda method to `this` is not fixed yet. ", "NaN"], ["7869", "CREATE TABLE check for duplicate columns", "Yuya Ebihara", "ebyhr", "05/14/17, 06:11:45 PM", "I wrote the test in `TestAnalyzer.java` firstly, but the analyze does't cause exception.\r\nTherefore, I wrote it in `AbstractTestIntegrationSmokeTest.java`.", "NaN"], ["7870", "Small fixes for product test infrastructure", "Andrii Rosa", "arhimondr", "04/27/17, 04:01:48 PM", "NaN", "NaN"], ["7874", "Fix slice big array memory estimate", "Nezih Yigitbasi", "nezihyigitbasi", "04/20/17, 10:15:33 PM", "We now include the slice object overhead in our estimates.", "NaN"], ["7875", "Fix memory size estimates in various places", "Nezih Yigitbasi", "nezihyigitbasi", "04/21/17, 06:09:09 PM", "NaN", "NaN"], ["7878", "Add entrypoint to get QueryStateInfo by QueryId", "Rongrong Zhong", "rongrong", "04/21/17, 01:02:56 AM", "NaN", "NaN"], ["7881", "Source union", "Piotr Nowojski", "pnowojski", "04/21/17, 05:20:49 PM", "Addresses https://github.com/prestodb/presto/issues/7877\r\n\r\nThis is more of a hot fix I think, than a real solution. Why Presto can not handle stages that have inputs that one is source distributed and other is hash distributed? Like this fragment:\r\n\r\n```\r\n Fragment 1 [HASH]\r\n     Output layout: [suppkey_18, expr_19]\r\n     Output partitioning: SINGLE []\r\n     - LocalExchange[ROUND_ROBIN] () => suppkey_18:bigint, expr_19:bigint\r\n         - ScanProject[table = tpch:tpch:supplier:sf0.01, originalConstraint = true] => [expr_4:bigint, suppkey:bigint]\r\n                 expr_4 := BIGINT '1'\r\n                 suppkey := tpch:suppkey\r\n         - Aggregate(FINAL)[suppkey_5] => [suppkey_5:bigint, count:bigint]\r\n                 count := \"count\"(\"count_24\")\r\n             - LocalExchange[HASH][$hashvalue] (\"suppkey_5\") => suppkey_5:bigint, count_24:bigint, $hashvalue:bigint\r\n                 - RemoteSource[2] => [suppkey_5:bigint, count_24:bigint, $hashvalue_25:bigint]\r\n```\r\n\r\nIs there any fundamental problem with that? Or is it difficult because requires changes in lots of places? I tried to add support for that, but after hitting 3rd `checkState` I gave up.", "NaN"], ["7882", "Prevent nulls by requiring override of PlanVisitor.visitPlan", "Piotr Findeisen", "findepi", "04/27/17, 02:47:20 AM", "`PlanVisitor` is a generic class and does not know the type of context `C`, so should not assume `null` is a good value to return by default.\r\n\r", "NaN"], ["7883", "Include slice object overhead in size calculation", "Nezih Yigitbasi", "nezihyigitbasi", "04/21/17, 10:02:30 PM", "Seems like we have yet another place where we don't consider slice shallow size.", "NaN"], ["7889", "Prefer undistributed plans for the output", "Piotr Nowojski", "pnowojski", "04/27/17, 03:02:41 AM", "Addresses: https://github.com/prestodb/presto/issues/7886\r\n\r\nThis allows UnionNodes which are just under OutputNode to\r\ncollapse into single RemoteExchange[GATHER] instead of\r\nRemoteExchange[ROUND_ROBIN] followed by RemoteExchange[GATHER]\r\n\r\nFor example before:\r\n```\r\n> explain (type distributed) SELECT suppkey FROM supplier UNION ALL SELECT nationkey FROM nation\r\n\r\n Fragment 0 [SINGLE]\r\n     Output layout: [suppkey_8]\r\n     Output partitioning: SINGLE []\r\n     - Output[suppkey] => [suppkey_8:bigint]\r\n             suppkey := suppkey_8\r\n         - RemoteSource[1] => [suppkey_8:bigint]\r\n\r\n Fragment 1 [ROUND_ROBIN]\r\n     Output layout: [suppkey_8]\r\n     Output partitioning: SINGLE []\r\n     - RemoteSource[2,3] => [suppkey_8:bigint]\r\n\r\n Fragment 2 [SOURCE]\r\n     Output layout: [suppkey]\r\n     Output partitioning: ROUND_ROBIN []\r\n     - TableScan[tpch:tpch:supplier:sf0.01, originalConstraint = true] => [suppkey:bigint]\r\n             suppkey := tpch:suppkey\r\n\r\n Fragment 3 [SOURCE]\r\n     Output layout: [nationkey_2]\r\n     Output partitioning: ROUND_ROBIN []\r\n     - TableScan[tpch:tpch:nation:sf0.01, originalConstraint = true] => [nationkey_2:bigint]\r\n             nationkey_2 := tpch:nationkey\r\n```\r\nand after:\r\n```\r\n Fragment 0 [SINGLE]\r\n     Output layout: [suppkey_8]\r\n     Output partitioning: SINGLE []\r\n     - Output[suppkey] => [suppkey_8:bigint]\r\n             suppkey := suppkey_8\r\n         - RemoteSource[1,2] => [suppkey_8:bigint]\r\n\r\n Fragment 1 [SOURCE]\r\n     Output layout: [suppkey]\r\n     Output partitioning: SINGLE []\r\n     - TableScan[tpch:tpch:supplier:sf0.01, originalConstraint = true] => [suppkey:bigint]\r\n             suppkey := tpch:suppkey\r\n\r\n Fragment 2 [SOURCE]\r\n     Output layout: [nationkey_2]\r\n     Output partitioning: SINGLE []\r\n     - TableScan[tpch:tpch:nation:sf0.01, originalConstraint = true] => [nationkey_2:bigint]\r\n             nationkey_2 := tpch:nationkey\r\n```", "NaN"], ["7890", "Reduce memory over limitation exception for OrderByOperator", "Yi He", "hellium01", "04/28/17, 10:45:34 PM", "Before OrderByOperator is compacted, we will try to compact the memory first.", "NaN"], ["7891", "Temporary disable sqlserver tests", "Andrii Rosa", "arhimondr", "04/26/17, 03:21:10 AM", "SQL server image sporadically hangs during the startup\r\n\r\nhttps://github.com/Microsoft/mssql-docker/issues/76", "NaN"], ["7893", "Remove unused method", "Nezih Yigitbasi", "nezihyigitbasi", "04/24/17, 11:37:04 PM", "NaN", "NaN"], ["7894", "Fix memory leak in task queue space listeners", "Dain Sundstrom", "dain", "04/24/17, 11:30:30 PM", "When taks queue space listener is no longer needed, cancel future\nto allow task to release reference to future.", "NaN"], ["7895", "Introduce revocable user memory concept", "Piotr Nowojski", "pnowojski", "08/10/17, 08:19:25 AM", "This is another iteration of https://github.com/prestodb/presto/pull/6280\r\n\r\nThis PR introduces concecpt of revocable memory. If operator reserves revocable memory it should be able to revoke this memory upon a request from presto.\r\n\r\nAs an example this PR includes usage of this concept in spill to disk in `HashAggregationOperator`.\r\n\r\nCC @losipiuk ", "NaN"], ["7897", "Cassandra intermittent failures", "Andrii Rosa", "arhimondr", "05/16/17, 05:13:23 PM", "Fixes the failure when table statistics are not yet refreshed when generating splits.\r\n\r\nFixes the failure when NoHostAvailableException is thrown during the session creation. In such case cluster become closed, and must be reopened.", "NaN"], ["7903", "Add 0.174 release notes", "Martin Traverso", "martint", "04/26/17, 05:47:57 PM", "NaN", "NaN"], ["7904", "Add resource group info to query events", "Nezih Yigitbasi", "nezihyigitbasi", "04/27/17, 10:45:40 PM", "NaN", "NaN"], ["7905", "Refactor CassandraRecordSink", "Andrii Rosa", "arhimondr", "05/17/17, 01:58:25 PM", "Build query with query builder provided by the DataStax driver.\r\nUse PreparedStatement to perform inserts.\r\nThis will ensure proper column and table names escape, and improve\r\nperformance.\r\n\r\nThis is a pre-requisite for https://github.com/prestodb/presto/pull/7451", "NaN"], ["7906", "Update release notes", "Nezih Yigitbasi", "nezihyigitbasi", "04/26/17, 07:11:45 PM", "NaN", "NaN"], ["7908", "Add repeat function", "James Sun", "highker", "06/06/17, 05:51:28 PM", "Allow repeating an element for a given number of times. The return\r\nresult is an array.", "NaN"], ["7912", "Enable cross joins elimination by default", "Piotr Nowojski", "pnowojski", "04/27/17, 04:07:50 PM", "As far as I know every database eliminates cross joins and in most (all?) third party benchmarks presto is run on almost default config options. Besides, so far I haven't seen either a benchmark or production query for which enabling this option would worsen performance in such case, why don't we enable it by default?\r\n\r\nCC @KBP-TDC ", "NaN"], ["7913", "Add information about rows distribution per task to exchange analyze", "Maciej 'mac' Grzybek", "maciejgrzybek", "07/03/17, 07:42:48 AM", "This will help identify data skew on repartitioning (remote exchanges).\r\n\r\nExample output:\r\n```\r\npresto:default> explain analyze select sum(nationkey) over(partition by part) from foo;\r\n                                                                        Query Plan\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------\r\n Fragment 1 [HASH]\r\n     Cost: CPU 105.81ms, Input: 26 rows (598B). Per task: avg.: 13.0 std.dev.: 12.0, Output: 26 rows (234B)\r\n     Output layout: [sum]\r\n     Output partitioning: SINGLE []\r\n     - Project[] => [sum:bigint]\r\n             Cost: 4.52%, Input: 26 rows (832B), Output: 26 rows (234B), Filtered: 0.00%\r\n             Input avg.: 0.81 lines, Input std.dev.: 535.10%\r\n         - Window[partition by (part)][$hashvalue] => [nationkey:bigint, part:integer, $hashvalue:bigint, sum:bigint]\r\n                 Cost: 44.72%, Output: 26 rows (832B)\r\n                 Input avg.: 0.81 lines, Input std.dev.: 535.10%\r\n                 sum := sum(\"nationkey\")\r\n             - LocalExchange[HASH][$hashvalue] (\"part\") => nationkey:bigint, part:integer, $hashvalue:bigint\r\n                     Cost: 46.73%, Output: 26 rows (598B)\r\n                     Input avg.: 0.81 lines, Input std.dev.: 535.10%\r\n                 - RemoteSource[2] => [nationkey:bigint, part:integer, $hashvalue_5:bigint]\r\n                         Cost: 4.02%, Output: 26 rows (598B)\r\n                         Input avg.: 0.81 lines, Input std.dev.: 535.10%\r\n\r\n Fragment 2 [SOURCE]\r\n     Cost: CPU 2.41s, Input: 26 rows (598B). Per task: avg.: 13.0 std.dev.: 12.0, Output: 26 rows (598B)\r\n     Output layout: [nationkey, part, $hashvalue_6]\r\n     Output partitioning: HASH [part][$hashvalue_6]\r\n     - ScanProject[table = hive:hive:default:foo, originalConstraint = true] => [nationkey:bigint, part:integer, $hashvalue_6:bigint]\r\n             Cost: 100.00%, Input: 26 rows (1.33kB), Output: 26 rows (598B), Filtered: 0.00%\r\n             Input avg.: 13.00 lines, Input std.dev.: 92.31%\r\n             $hashvalue_6 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"part\"), 0))\r\n             LAYOUT: hive\r\n             nationkey := HiveColumnHandle{clientId=hive, name=nationkey, hiveType=bigint, hiveColumnIndex=0, columnType=REGULAR, comment=Optional.empty}\r\n             part := HiveColumnHandle{clientId=hive, name=part, hiveType=int, hiveColumnIndex=4, columnType=REGULAR, comment=Optional.empty}\r\n\r\n\r\n(1 row)\r\n```\r\n\r\nPlease mind *Input: 26 rows (598B). Per task: avg.: 13.0 std.dev.: 12.0* in the beginning of `Fragment 1` description.\r\n\r\nThis is an output for a table with 26 rows where 25 had the same value of `part` column and only 1 had different value. There were 2 workers + 1 coordinator used to execute that query, so we ended up with 2 task collecting input from Fragment 2 (`hash-partition-count` was set to `100`).\r\n\r\nCC: @KBP-TDC ", "NaN"], ["7914", "Get rid of CachingCassandraSchemaProvider", "Andrii Rosa", "arhimondr", "05/16/17, 03:20:20 PM", "Cassandra DataStax driver caches all the meta information internally,\r\nand reloads it asynchronously. There is no need for yet another layer\r\nof caching.", "NaN"], ["7918", "Add toString to EquiJoinClauseProvider", "Artur Gajowy", "ArturGajowy", "04/28/17, 04:34:47 PM", "NaN", "NaN"], ["7919", "Make PlanVisitor type param order same as AstVisitor's (<R, C>)", "Artur Gajowy", "ArturGajowy", "05/24/17, 08:17:18 PM", "This is to align the type parameter order between Plan and Ast visitors. So far they differ (`PlanVisitor<C, R>` vs `AstVisitor<R, C>`), which is confusing. I think we should unify it one way or another. Personally I lean towards the `<R, C>` order, but either will be good as long as they're the same. The other parameter order PR is here: #7920.", "NaN"], ["7923", "Remove deprecated methods in AggregationNode", "Rongrong Zhong", "rongrong", "06/06/17, 08:45:10 PM", "Resolves #7922 ", "NaN"], ["7924", "Fix memory accounting in array block builder", "Nezih Yigitbasi", "nezihyigitbasi", "04/29/17, 12:43:03 AM", "NaN", "NaN"], ["7925", "Fix writePositionTo in RunLengthEncodedBlock", "Haozhun Jin", "haozhun", "04/29/17, 12:26:07 AM", "NaN", "NaN"], ["7927", "Reduce size of byte code generated for projections and filters", "Dain Sundstrom", "dain", "04/29/17, 05:41:32 PM", "NaN", "NaN"], ["7929", "Fix TestingConnectorContext.getTypeManager", "Haozhun Jin", "haozhun", "04/29/17, 10:06:00 PM", "NaN", "NaN"], ["7930", "Support column aliases in CREATE TABLE AS", "Yuya Ebihara", "ebyhr", "07/21/17, 07:17:23 PM", "Resolves #7746 ", "NaN"], ["7933", "Add unit tests for block retained size", "Nezih Yigitbasi", "nezihyigitbasi", "05/25/17, 11:42:32 PM", "This PR adds unit tests for block retained sizes which have been biting us for some time.", "NaN"], ["7934", "Disable usage of DWRF file level stats", "Dain Sundstrom", "dain", "05/01/17, 10:17:55 PM", "Also add stats to custom file format readers", "NaN"], ["7936", "Handle create_table failures in Hive metastore more robustly", "Haozhun Jin", "haozhun", "05/02/17, 03:39:26 AM", "NaN", "NaN"], ["7939", "Fix DELETE queries that do not produce splits", "David Phillips", "electrum", "05/02/17, 01:41:33 AM", "NaN", "NaN"], ["7942", "Add event time to write complete event", "James Sun", "highker", "05/24/17, 10:53:04 PM", "Record the event time when a write completes", "NaN"], ["7949", "Fix SIZE_OF_SEGMENT constant in IntBigArray", "Nezih Yigitbasi", "nezihyigitbasi", "05/05/17, 05:41:14 PM", "NaN", "NaN"], ["7951", "Include object overhead in big array sizeOf calculations", "Nezih Yigitbasi", "nezihyigitbasi", "06/06/17, 05:55:35 PM", "These `sizeOf()` methods are called from various parts of the code to calculate retained sizes, so we should include the shallow sizes of the big array instances.", "NaN"], ["7958", "Fix node state returned by FailureDetector", "Wenlei Xie", "wenleix", "05/03/17, 11:21:41 PM", "- \"GONE\" means either the host or the Presto process is gone. So\r\nConnectException will be considered as \"GONE\".\r\n- SocketTimeoutException can happen when host is gone, or host is up but\r\nthe process is not listening. Consider it as \"UNRESPONSIVE\" for now.", "NaN"], ["7959", "Add 0.175 release notes", "David Phillips", "electrum", "05/03/17, 11:22:33 PM", "NaN", "NaN"], ["7961", "Improve local scheduler fairness", "Raghav Sethi", "raghavsethi", "06/30/17, 06:00:48 PM", "@dain You've reviewed most of the first commit (I added longer explanations to the experiments). Also, GitHub screwed up the ordering.", "NaN"], ["7962", "Prefer undistributed plans for TopN and move some rules to iterative optimizer", "Piotr Nowojski", "pnowojski", "05/15/17, 07:00:32 PM", "Prefer undistributed plans for TopN and to avoid unnecessary additional exchanges, add new PushTopNThroughUnion optimizer.\r\n\r\nThis is a follow up worker of previous changes with distributed union.\r\n\r\n```    \r\n> explain SELECT * FROM (SELECT regionkey FROM nation UNION ALL SELECT nationkey FROM nation) t(a) ORDER BY a LIMIT 1;\r\n```    \r\nbefore:\r\n```\r\n     - Output[a] => [regionkey_9:bigint]\r\n             a := regionkey_9\r\n         - TopN[1 by (regionkey_9 ASC_NULLS_LAST)] => [regionkey_9:bigint]\r\n             - LocalExchange[SINGLE] () => regionkey_9:bigint\r\n                 - RemoteExchange[GATHER] => regionkey_9:bigint\r\n                     - TopN[1 by (regionkey_9 ASC_NULLS_LAST)] => [regionkey_9:bigint]\r\n                         - RemoteExchange[REPARTITION] => regionkey_9:bigint\r\n                             - TableScan[tpch:tpch:nation:sf0.01, originalConstraint = true] => [regionkey:bigint]\r\n                                     regionkey := tpch:regionkey\r\n                             - TableScan[tpch:tpch:nation:sf0.01, originalConstraint = true] => [nationkey_2:bigint]\r\n                                     nationkey_2 := tpch:nationkey\r\n```    \r\nafter:\r\n```\r\n     - Output[a] => [regionkey_9:bigint]\r\n             a := regionkey_9\r\n         - TopN[1 by (regionkey_9 ASC_NULLS_LAST)] => [regionkey_9:bigint]\r\n             - LocalExchange[SINGLE] () => regionkey_9:bigint\r\n                 - RemoteExchange[GATHER] => regionkey_9:bigint\r\n                     - TopN[1 by (regionkey ASC_NULLS_LAST)] => [regionkey:bigint]\r\n                         - TableScan[tpch:tpch:nation:sf0.01, originalConstraint = true] => [regionkey:bigint]\r\n                                 regionkey := tpch:regionkey\r\n                     - TopN[1 by (nationkey_2 ASC_NULLS_LAST)] => [nationkey_2:bigint]\r\n                         - TableScan[tpch:tpch:nation:sf0.01, originalConstraint = true] => [nationkey_2:bigint]\r\n                                 nationkey_2 := tpch:nationkey\r\n```\r\n\r\nMaybe we could also leave additional partial `TopN` between `LocalExchange[SINGLE]` and `RemoteExchange[GATHER]` for  higher concurrency of the final `TopN`?", "NaN"], ["7963", "Fix creating empty tables in presto-memory", "Piotr Nowojski", "pnowojski", "05/23/17, 05:26:10 PM", "https://github.com/prestodb/presto/issues/7733\r\n\r\nPreviously following scenario would fail:\r\n```\r\nCREATE TABLE memory.default.test (a BIGINT);\r\nINSERT INTO memory.default.test SELECT nationkey FROM tpch.tiny.nation;\r\n```\r\nwith an error message, that table test was not found on a worker.\r\n\r\nIt is fixed by allowing to \"initialize\" table on writes (not as it was before only on creates) and by moving sanity checks for detecting worker crashes to asserting expected number of rows per worker.\r\n\r\nThere are two additional small commits, that add support for non default schemas.", "NaN"], ["7965", "Remove static import of functions named \"of\"", "Haozhun Jin", "haozhun", "05/05/17, 05:28:25 AM", "NaN", "NaN"], ["7966", "Add lookup join operator statistics", "Igor Demura", "idemura", "05/23/17, 11:35:20 PM", "NaN", "NaN"], ["7967", "Decrease thread usage for presto-tests", "David Phillips", "electrum", "05/04/17, 11:33:28 PM", "NaN", "NaN"], ["7969", "Add preprocessor support to CLI", "Dain Sundstrom", "dain", "05/19/17, 04:02:42 PM", "When PRESTO_PREPROCESSOR environment variable is set, all commands\nare piped through the specified program before being sent to the\nPresto server.  The default exection timeout is 10 seconds and can\nbe overridden with PRESTO_PREPROCESSOR_TIMEOUT environment variable.\nAdditionally, the PRESTO_CATLOG and PRESTO_SCHEMA environment variables\nare available to the preprocessor command.  If the query is not\nexecuted in a context of a catalog or schems the variable will be\nset to an empty value.", "NaN"], ["7973", "Update Hadoop artifact versions", "David Phillips", "electrum", "05/07/17, 05:01:42 PM", "The artifacts are now versioned based on the upstream versions.", "NaN"], ["7974", "Add plan checker ensuring no duplicate plan node ids", "Piotr Findeisen", "findepi", "05/14/17, 06:11:04 PM", "Add plan checker ensuring plan node ids are not duplicated.\r\n\r\nThis change has sense as it is, but best if combined with @sopel39's suggestion to @kokosing's PR: https://github.com/prestodb/presto/pull/7777/files#r110341855 .  This is because even if a rule/optimizer produces duplicate ids, subsequent optimizer may fix that by rewriting and re-ID-ing whole plan tree.", "NaN"], ["7975", "Remove identity-based collections from analysis", "Piotr Findeisen", "findepi", "05/30/17, 09:25:15 PM", "NaN", "NaN"], ["7976", "Require OrcDataSourceId in OrcCorruptionException", "Dain Sundstrom", "dain", "05/13/17, 07:18:30 PM", "NaN", "NaN"], ["7977", "Improve documentation for parse_duration", "David Phillips", "electrum", "05/19/17, 06:19:47 PM", "<img width=\"858\" alt=\"screen shot 2017-05-07 at 9 56 04 am\" src=\"https://cloud.githubusercontent.com/assets/9230/25783124/7c5f472e-330b-11e7-8106-d57f968a0a09.png\">\r", "NaN"], ["7979", "Execute rule unit tests in transactions", "Rebecca Schlussel", "rschlussel-zz", "05/15/17, 08:18:05 PM", "This is necessary in order to be able to get the connector info\r\nfor tests involving TableScanNodes.\r\n\r\n@martint I originally added this for the rule to push constraints into table scan nodes, but @alandpost needs it for adding tests for PruneUnreferencedOutputs, so I'm extracting it to its own PR. \r\n\r\nSee https://github.com/Teradata/presto/commit/5783e812c921b09b43afeebcbac8f70837121f05 for an example of tests that need this change.", "NaN"], ["7983", "Add missing REAL/TINYINT/SMALLINT to JSON casts", "Igor Demura", "idemura", "05/23/17, 11:37:16 PM", "Issue #7981\r", "NaN"], ["7986", "Remove Varchar to Int coercion test", "Sanjay Sharma", "sanjay990", "05/09/17, 11:25:07 PM", "Newer version of Hive (Hive 2.1.1) doesn\u2019t allow Varchar to Int conversion", "NaN"], ["7988", "Relax TypeParameters restriction for ScalarImpl constructors", "James Sun", "highker", "05/10/17, 12:04:13 AM", "When a TypeParameter is used in a constructor, it has to be\r\nannotated. This disallow some concrete types to be passed in to a\r\nconstructor, e.g.: array(map(varchar, integer)). This patch relaxes\r\nthe restriction by allows StandardTypes to be used in a signature\r\nwithout annotation.", "NaN"], ["7991", "Add getBlockedQueuedQueries to InternalResourceGroup", "Rongrong Zhong", "rongrong", "05/11/17, 07:31:45 PM", "resolves #7931\r\nblockedQueuedQueries of a resource group is the number of queries that can run on its subgroups\r\nbut blocked on this resource group.", "NaN"], ["7992", "Fix leak of query and task when cleanup request is rejected", "Haozhun Jin", "haozhun", "05/10/17, 06:05:05 PM", "When HTTP request sent by the coordinator to the worker to fail a task is\r\nrejected by HttpClient with RejectedExecutionException, proper cleanup is not\r\ncarried out. The task is not marked as failed on the coordinator. Now that the\r\ntask is still marked as running on the coordinator, the coordinator will\r\ncontinue to fetch task info from worker. Since HTTP request to fail the task\r\nwas rejected, the worker also considers the task still running because it never\r\ngot the message to fail the task.\r\n\r\nAs a result, the coordinator will continue to try to fetch task info from\r\nworkers because it's not in a done state. And the worker will report the tasks\r\nas running when task info is requested. As a result, given both the coordinator\r\nand the worker think the task is still running, the coordinator will continue\r\nto continuously fetch the TaskInfo from the workers, keeping them from\r\nexpiration.\r\n\r\nNotably, there are two places on the coordinator that keeps track of task\r\nstate. One in ContinuousTaskStatusFetcher. The other in TaskInfoFetcher. In\r\nthis particular case, the former correctly marked the task as failed, but the\r\nlatter didn't.", "NaN"], ["7993", "Add stats to block size read from Orc file", "James Sun", "highker", "05/22/17, 08:33:11 PM", "A cell in a table can be huge. A worker can OOM when reading too many\r\nlarge cells. Export block size to monitor the distribution of loaded\r\nblocks.", "NaN"], ["7995", "Bump tempto version to 1.31", "\u0141ukasz Osipiuk", "losipiuk", "05/10/17, 11:14:51 PM", "The release fixes a bug in rowsEqual assertion to check \r\nif expected and actual columns count match.\r", "NaN"], ["7997", "Migrate existing reader/function to produce new map block", "Haozhun Jin", "haozhun", "06/22/17, 01:15:47 AM", "This pull request makes sure that maps always use `MapBlock` / `SingleMapBlock`.\r\n\r\nThe next step will be to make code that consumes map depends on this change wherever it helps (which will be a future pull request).", "NaN"], ["7998", "Wrap remote exception in SimpleHttpResponseHandler", "Wenlei Xie", "wenleix", "05/24/17, 07:03:34 PM", "Wrap the remote exception in SimpleHttpResponseHandler in a\r\nproper PrestoException.\r\n\r\nIssue: https://github.com/prestodb/presto/issues/7994", "NaN"], ["7999", "Avoid calling unreflect repeatedly in MethodHandleUtil", "Haozhun Jin", "haozhun", "05/10/17, 11:43:48 PM", "Calling unreflect leads to creation of JNI global weak references, which\r\nG1 cannot efficiently collect.", "NaN"], ["8000", "Add javadoc to methodHandle helper methods to warn about cost", "Haozhun Jin", "haozhun", "05/15/17, 08:12:54 PM", "NaN", "NaN"], ["8002", "Improve message for invalid lambda parameter count", "Yuya Ebihara", "ebyhr", "05/18/17, 10:52:27 PM", "Resolves #7899 ", "NaN"], ["8004", "Fix validation of floating point values in verifier", "Nezih Yigitbasi", "nezihyigitbasi", "05/22/17, 05:28:42 PM", "Truncation is better than rounding up the floating point values before comparison.\r\n\r\nFor example, 0.9045 vs. 0.9045000000000001. With rounding mode half_up (default) we end up with 0.904 vs. 0.905 verification to fail. I think what we really want is truncating to a given precision.", "NaN"], ["8008", "Fix off-by-one error when reading FSE table", "Martin Traverso", "martint", "05/12/17, 07:14:54 AM", "The code was incorrectly comparing with < instead of <=", "NaN"], ["8009", "Print position links' sorting expression in explain", "Piotr Nowojski", "pnowojski", "05/14/17, 06:10:45 PM", "For query:\r\n```\r\npresto:tiny> explain SELECT o.orderkey, o.orderdate, l.shipdate FROM orders o JOIN lineitem l ON l.orderkey = o.orderkey AND l.shipdate < o.orderdate + INTERVAL '10' DAY;\r\n```\r\nexplain before:\r\n```\r\n - Output[orderkey, orderdate, shipdate] => [orderkey:bigint, orderdate:date, shipdate:date]                                             \r\n     - RemoteExchange[GATHER] => orderkey:bigint, orderdate:date, shipdate:date                                                          \r\n         - InnerJoin[(\"orderkey\" = \"orderkey_0\") AND (\"expr_3\" > \"shipdate\")] => [orderkey:bigint, orderdate:date, shipdate:date]        \r\n             - ScanProject[table = tpch:tpch:orders:sf0.01, originalConstraint = true] => [expr_3:date, orderkey:bigint, orderdate:date] \r\n                     expr_3 := (\"orderdate\" + \"$literal$interval day to second\"(BIGINT '864000000'))                                     \r\n                     orderkey := tpch:orderkey                                                                                           \r\n                     orderdate := tpch:orderdate                                                                                         \r\n             - LocalExchange[HASH] (\"orderkey_0\") => orderkey_0:bigint, shipdate:date                                                    \r\n                 - RemoteExchange[REPARTITION] => orderkey_0:bigint, shipdate:date                                                       \r\n                     - TableScan[tpch:tpch:lineitem:sf0.01, originalConstraint = true] => [orderkey_0:bigint, shipdate:date]             \r\n                             orderkey_0 := tpch:orderkey                                                                                 \r\n                             shipdate := tpch:shipdate\r\n```\r\nand after:\r\n```\r\n - Output[orderkey, orderdate, shipdate] => [orderkey:bigint, orderdate:date, shipdate:date]                                             \r\n     - RemoteExchange[GATHER] => orderkey:bigint, orderdate:date, shipdate:date                                                          \r\n         - InnerJoin[(\"orderkey\" = \"orderkey_0\") AND (\"expr_3\" > \"shipdate\")] => [orderkey:bigint, orderdate:date, shipdate:date]        \r\n                 SortExpression[\"shipdate\"]                                                                                              \r\n             - ScanProject[table = tpch:tpch:orders:sf0.01, originalConstraint = true] => [expr_3:date, orderkey:bigint, orderdate:date] \r\n                     expr_3 := (\"orderdate\" + \"$literal$interval day to second\"(BIGINT '864000000'))                                     \r\n                     orderkey := tpch:orderkey                                                                                           \r\n                     orderdate := tpch:orderdate                                                                                         \r\n             - LocalExchange[HASH] (\"orderkey_0\") => orderkey_0:bigint, shipdate:date                                                    \r\n                 - RemoteExchange[REPARTITION] => orderkey_0:bigint, shipdate:date                                                       \r\n                     - TableScan[tpch:tpch:lineitem:sf0.01, originalConstraint = true] => [orderkey_0:bigint, shipdate:date]             \r\n                             orderkey_0 := tpch:orderkey                                                                                 \r\n                             shipdate := tpch:shipdate\r\n```", "NaN"], ["8013", "Fix NoSuchElementException when scheduling empty split", "David Phillips", "electrum", "05/12/17, 04:30:55 PM", "NaN", "NaN"], ["8018", "Add 0.176 release notes", "Nezih Yigitbasi", "nezihyigitbasi", "05/12/17, 11:03:57 PM", "#8003 ", "NaN"], ["8019", "Add object overhead to estimated memory size in various state classes", "Nezih Yigitbasi", "nezihyigitbasi", "06/09/17, 09:52:41 PM", "Many state classes don't include object overhead in their estimated memory size calculations, this PR fixes that.", "NaN"], ["8020", "Add missing backticks for proper formatting", "Sanjay Sharma", "sanjay990", "05/16/17, 11:04:17 PM", "NaN", "NaN"], ["8022", "Litany of UI fixes and improvements", "Raghav Sethi", "raghavsethi", "05/18/17, 09:25:24 PM", "Closes #7984, #7953, #7945, #7941, #7940, #7660\r", "NaN"], ["8031", "Rewrite the lambda execution ", "Wenlei Xie", "wenleix", "06/12/17, 11:01:00 PM", "Solves #7935\r\n\r\nPlan:\r\n- [x] Allow internal binding to take arbitrary number of captured arguments\r\n- [x] Add lambdaJavaClass into ScalarFunctionImplementation\r\n- [x] Refactor to generate captured lambda in one step\r\n- [x] Generate captured lambda via `invokedynamic` through `MetaFactory` \r\n- [x] Migrate all the existing lambda functions into the new framework\r\n\r\nFurther clean-ups and refactors as future work: \r\n- Collapse nullFlags, nullable and lambdaInterface into a class\r\n- Clean up non-used fields in `CompiledLambdaMetadata`\r\n- Maybe refactor some code in lambda generation? ", "NaN"], ["8032", "Add VARBINARY concatenation support", "Wenlei Xie", "wenleix", "06/28/17, 01:41:37 AM", "Resolve #8006\r\n\r", "NaN"], ["8034", "Close InputStream in IsolatedClass.getBytecode", "Piotr Findeisen", "findepi", "05/16/17, 06:46:19 PM", "NaN", "NaN"], ["8037", "Do not support non-deterministic SortExpressions", "Piotr Nowojski", "pnowojski", "05/16/17, 07:08:39 PM", "For the following query:\r\n\r\n```\r\nSELECT count(*)\r\nFROM\r\n    customer c1 JOIN customer c2 ON c1.nationkey=c2.nationkey\r\nWHERE\r\n    c1.custkey - RANDOM(c1.custkey) < c2.custkey\r\n```\r\n\r\npresto returned ~16000 rows instead of ~69000.\r\n\r\nWhen using SortExpression, SortedPositionLinks assumes that once a filtering\r\nfunction returns false for one row, it must be false for all the the following rows.\r\nThis is not true for non-deterministic functions.", "NaN"], ["8040", "Upgrade to airlift 0.7", "Dain Sundstrom", "dain", "05/17/17, 02:27:05 AM", "NaN", "NaN"], ["8041", "Improve exception handling in ORC reader", "Dain Sundstrom", "dain", "05/17/17, 06:44:54 PM", "NaN", "NaN"], ["8043", "Feature grouping", "Anton", "petroav", "05/21/17, 10:33:00 PM", "Supersedes https://github.com/prestodb/presto/pull/7712.\r\n\r\n@martint it's ready for review now.", "NaN"], ["8045", "Rename Java method name for JSON to REAL cast", "Haozhun Jin", "haozhun", "05/17/17, 05:38:18 PM", "NaN", "NaN"], ["8046", "Fix compilation failure", "Martin Traverso", "martint", "05/17/17, 05:55:15 PM", "This was due to a merge conflict.", "NaN"], ["8047", "Remove deprecated MergeProjections optimizer", "Martin Traverso", "martint", "05/17/17, 08:24:05 PM", "NaN", "NaN"], ["8048", "Tighten syntax for SHOW STATS", "David Phillips", "electrum", "05/18/17, 10:02:04 PM", "NaN", "NaN"], ["8051", "Fix imports", "Martin Traverso", "martint", "05/17/17, 11:57:10 PM", "NaN", "NaN"], ["8052", "Fix checkstyle", "Haozhun Jin", "haozhun", "05/18/17, 01:12:10 AM", "NaN", "NaN"], ["8054", "Remove Cassandra cache configs", "Christina Wallin", "cawallin", "05/18/17, 05:50:30 PM", "Since Cassandra no longer has a cache in Presto, remove the configs. We can\r\nalso remove the ExecutorService which was used only by the caching\r\nschema provider.", "NaN"], ["8056", "Drop unused probeHashEnabled parameter", "Piotr Findeisen", "findepi", "05/18/17, 10:52:43 PM", "It's just a trivial cleanup", "NaN"], ["8062", "Rename test table name in testRcTextCharDecoding", "Haozhun Jin", "haozhun", "05/18/17, 05:41:36 PM", "Test methods are run in parallel. As a result, different methods must not\r\nuse the same table name.\r\n\r\ncc @findepi ", "NaN"], ["8064", "Upgrade to airbase 63", "Haozhun Jin", "haozhun", "05/18/17, 09:49:47 PM", "NaN", "NaN"], ["8065", "Move iterative rule tests from rule/test/ to rule/", "Alan Post", "alandpost", "05/18/17, 10:51:38 PM", "The rule.test package is for the testing framework, not for the tests\r\nthemselves.", "NaN"], ["8066", "Call MethodHandles::unreflect in Reflection utils", "Nezih Yigitbasi", "nezihyigitbasi", "05/22/17, 05:32:22 PM", "Since unreflect calls caused GC issues recently unifying their callsites\r\nwill help us better track their usage.", "NaN"], ["8069", "Support generating interface", "Rongrong Zhong", "rongrong", "06/06/17, 10:59:16 PM", "NaN", "NaN"], ["8070", "Log a message when a task is scheduled to run or removed", "Raghav Sethi", "raghavsethi", "05/18/17, 10:18:48 PM", "NaN", "NaN"], ["8071", "Fix checkstyle", "Raghav Sethi", "raghavsethi", "05/18/17, 10:27:54 PM", "NaN", "NaN"], ["8072", "Make Raptor resistant to corruption", "David Phillips", "electrum", "05/19/17, 05:02:20 AM", "NaN", "NaN"], ["8073", "Fix incorrect import", "Raghav Sethi", "raghavsethi", "05/18/17, 11:10:18 PM", "NaN", "NaN"], ["8075", "Change log level for TaskExecutor messages to DEBUG", "Raghav Sethi", "raghavsethi", "05/19/17, 12:08:08 AM", "NaN", "NaN"], ["8078", "Remove legacy RCFile reader", "Dain Sundstrom", "dain", "05/22/17, 08:06:12 PM", "NaN", "NaN"], ["8079", "Fast ineqality join flag cleanup", "Piotr Findeisen", "findepi", "05/22/17, 03:07:15 PM", "NaN", "NaN"], ["8082", "Add unit test for PruneValuesColumns", "Alan Post", "alandpost", "05/22/17, 03:05:07 PM", "Add a new unit test for PruneValuesColumns.\r\nExtend ValuesMatcher to optionally validate rows, and adjust the\r\nPlanMatchPattern.values() functions accordingly.  The previous approach\r\nwas odd, in that there was a ValuesMatcher for each alias to be created;\r\nit's more direct to implement Matcher and create the aliases in\r\ndetailMatches().  The new ValuesMatcher should be read as new code,\r\nnot as a diff.\r\n\r\nhttps://github.com/prestodb/presto/issues/7154", "NaN"], ["8083", "Raptor shards system table improvements", "David Phillips", "electrum", "05/19/17, 06:05:22 PM", "NaN", "NaN"], ["8085", "Add support for resource group selection based on \"query type\"", "Nezih Yigitbasi", "nezihyigitbasi", "06/30/17, 04:53:49 PM", "Previously both data/metadata queries were submitted to the same queue\r\nand it was possible that metadata queries were stuck behind data queries\r\nfor a long time. With this change it's possible to configure metadata\r\nqueries to have their own dedicated queues.\r\n\r\nOnce this is good to go I will push a commit to update the resource groups docs.", "NaN"], ["8087", "Add release notes for 0.177", "Haozhun Jin", "haozhun", "05/19/17, 10:21:54 PM", "NaN", "NaN"], ["8088", "Verify local Raptor files after writing to disk", "David Phillips", "electrum", "05/19/17, 07:57:07 PM", "NaN", "NaN"], ["8089", "Process pending reads upon noMorePages event", "Andrii Rosa", "arhimondr", "08/29/17, 05:34:36 PM", "When http request is waiting for a result on a client buffer future\r\nit is being unblocked only when new pages are added, but not when\r\nnoMorePages event is received.\r\n\r\nIt causes 1-2s delays on sub-second queries.", "NaN"], ["8090", "Fix stack trace formatter on query details page", "Raghav Sethi", "raghavsethi", "05/19/17, 10:14:13 PM", "NaN", "NaN"], ["8095", "Rename getCpuQuotaPeriodMillis to getCpuQuotaPeriod", "Rongrong Zhong", "rongrong", "05/22/17, 06:38:15 PM", "NaN", "NaN"], ["8096", "Reorder methods in Operator to match lifecycle", "Piotr Findeisen", "findepi", "05/22/17, 07:08:46 PM", "NaN", "NaN"], ["8097", "Remove recursive computeIfAbsent method call", "Karol Sobczak", "sopel39", "05/22/17, 03:22:54 PM", "Recursive computeIfAbsent (that modifies the same\r\ncollection) may corrupt HashMap,\r\nsee: http://bugs.java.com/bugdatabase/view_bug.do?bug_id=8172951\r\n\r\nalso see: https://github.com/Teradata/presto/pull/555#discussion_r117630317", "NaN"], ["8100", "Add test for PruneTableScanColumns", "Alan Post", "alandpost", "05/22/17, 07:08:18 PM", "Includes a PlanBuilder.tableScan() overload that provides the\r\nTableHandle.  That overload is lifted from not-yet-merged 9cbe62cb,\r\nwhich implements a PushDownTableConstraints rule.  The change is\r\nidentical, so later merging or rebasing should go smoothly.\r\n\r\nUses TPCH, because that's what RuleTester sets up.\r\n\r\nhttps://github.com/prestodb/presto/issues/7154", "NaN"], ["8101", "Create ResourceGroupId from full segmented name", "Rongrong Zhong", "rongrong", "05/24/17, 12:05:56 AM", "NaN", "NaN"], ["8102", "Replace query queues with resource groups", "David Phillips", "electrum", "03/26/18, 07:39:34 PM", "NaN", "NaN"], ["8103", "Fix concurrency of Avro schema cache", "David Phillips", "electrum", "05/31/17, 10:34:36 PM", "NaN", "NaN"], ["8104", "Limit the size of ORC reader block to 1 GB", "Dain Sundstrom", "dain", "06/22/17, 10:10:11 PM", "NaN", "NaN"], ["8111", "Remove unused type argument", "Nezih Yigitbasi", "nezihyigitbasi", "05/23/17, 11:01:09 PM", "NaN", "NaN"], ["8112", "Include object overhead in PagesHash/PagesHashStrategy size estimates", "Nezih Yigitbasi", "nezihyigitbasi", "05/25/17, 06:21:08 PM", "NaN", "NaN"], ["8113", "Preserve decompression error causes when decoding ORC files", "David Phillips", "electrum", "05/24/17, 05:48:35 PM", "NaN", "NaN"], ["8116", "Add reference counting for BlockBigArray", "James Sun", "highker", "06/21/17, 10:36:07 PM", "Add a map in BlockBigArray to track the underlying data of the blocks it sets. The map aims to avoid overcounting the memory usage.", "NaN"], ["8121", "Prune mark distinct columns", "Alan Post", "alandpost", "06/22/17, 07:29:38 AM", "Add PruneMarkDistinctColumns and its associated tooling:\r\n* a builder function for MarkDistinctNode\r\n* a matcher for MarkDistinctNode\r\n* a builder function for the matcher\r\n* the rule itself\r\n* a unit test for the rule\r\n* enabling the rule in presto's rule list\r\n\r\nThe rule elides the MarkDistinctNode entirely if the mark output is not\r\nused.\r\n\r\nhttps://github.com/prestodb/presto/issues/7154\r\n\r\nAfter rebasing after the commit of PruneSemiJoinColumns, the first commit in this PR is now more stand-alone (it used to also contain the first introduction of `Util.restrictOutputs()`)  so I adjusted the commit message.", "NaN"], ["8122", "Make STATS non-reserved", "Martin Traverso", "martint", "05/24/17, 08:18:15 PM", "This fixes a regression introduced when support for SHOW STATS was added.\r\n\r\nFixes https://github.com/prestodb/presto/issues/8117", "NaN"], ["8123", "Print resolved plan when rule test fails", "Piotr Findeisen", "findepi", "05/30/17, 06:58:03 PM", "Since #7715 rule tests use `Lookup` and test expectations can match plan with `GroupReferences` or without. When a rule test fails, both options should be printed.\r\n\r\n### Before\r\n```\r\njava.lang.AssertionError: Plan does not match, expected [\r\n\r\n- node(FilterNode)\r\n    FilterMatcher{predicate=((\"a\" < 42) AND (\"b\" > 46))}\r\n    - node(ValuesNode)\r\n        ValuesMatcher{outputSymbolAliases={a=0, b=1}, expectedOutputSymbolCount=Optional.empty, expectedRows=Optional.empty}\r\n\r\n] but found [\r\n\r\n- Filter[filterPredicate = ((\"a\" < 42) AND (\"b\" > 44))] => [a:bigint, b:bigint]\r\n        Cost: {rows: ?, bytes: ?}\r\n    - GroupReference[2] => [a:bigint, b:bigint]\r\n\r\n]\r\n```\r\n\r\n### After\r\n```\r\njava.lang.AssertionError: Plan does not match, expected [\r\n\r\n- node(FilterNode)\r\n    FilterMatcher{predicate=((\"a\" < 42) AND (\"b\" > 46))}\r\n    - node(ValuesNode)\r\n        ValuesMatcher{outputSymbolAliases={a=0, b=1}, expectedOutputSymbolCount=Optional.empty, expectedRows=Optional.empty}\r\n\r\n] but found [\r\n\r\n- Filter[filterPredicate = ((\"a\" < 42) AND (\"b\" > 44))] => [a:bigint, b:bigint]\r\n        Cost: {rows: ?, bytes: ?}\r\n    - GroupReference[2] => [a:bigint, b:bigint]\r\n\r\n] which resolves to [\r\n\r\n- Filter[filterPredicate = ((\"a\" < 42) AND (\"b\" > 44))] => [a:bigint, b:bigint]\r\n        Cost: {rows: 0, bytes: ?}\r\n    - Values => [a:bigint, b:bigint]\r\n            Cost: {rows: 0, bytes: ?}\r\n\r\n] \r\n```", "NaN"], ["8124", "Change the return type of Block's size-related methods to long", "Nezih Yigitbasi", "nezihyigitbasi", "06/20/17, 06:34:53 PM", "Currently the getSizeInBytes/getRegionSizeInBytes/getRetainedSizeInBytes\r\nmethods all return integers. To implement these methods Block implementations\r\nusually sum up the size of their internal states and cast that sum to integer\r\nusing different cast functions, which are different in different Block implementations,\r\nand occasionally incorrect (using Ints.saturatedCast()).", "NaN"], ["8125", "Upgrade to airlift 0.147", "Haozhun Jin", "haozhun", "05/25/17, 04:49:07 PM", "NaN", "NaN"], ["8126", "Fix query/task leak when task instance id mismatches", "Haozhun Jin", "haozhun", "06/01/17, 04:38:18 AM", "NaN", "NaN"], ["8131", "Various minor memory accounting fixes", "Nezih Yigitbasi", "nezihyigitbasi", "05/25/17, 10:40:23 PM", "NaN", "NaN"], ["8138", "Add environment to ResourceGroupConfigurationManagerContext", "David Phillips", "electrum", "05/30/17, 06:45:14 PM", "NaN", "NaN"], ["8140", "Fix Microsoft ACCEPT_EULA url in product-test doc", "Yuya Ebihara", "ebyhr", "05/30/17, 06:43:04 PM", "Current url (go.microsoft.com/fwlink/?LinkId=746388) become https://github.com/prestodb/presto/blob/master/presto-product-tests/go.microsoft.com/fwlink/?LinkId=746388 and it returns 404 not found.", "NaN"], ["8146", "Rename DependencyExtractor to SymbolsExtractor", "Grzegorz Kokosi\u0144ski", "kokosing", "06/21/17, 08:02:28 AM", "Rename DependencyExtractor to SymbolsExtractor\n\nDependencyExtractor suggests that this class is able to extract all the\nsymbols which are dependant to given node, while it does something\ndifferent. This class by default extracts all the symbols of given node\nand its children so SymbolsExtractor is a better name.", "NaN"], ["8148", "PlanBuilder related cleanups", "\u0141ukasz Osipiuk", "losipiuk", "06/12/17, 02:35:09 PM", "NaN", "NaN"], ["8156", "Update joda-time to 2.9.9", "Yi He", "hellium01", "06/09/17, 01:25:30 AM", "In new version, printTo does not throw IOException for stringbuilder\r\nanymore, so removed the try/catch.\r\n\r\nLooking at release notes: \r\nhttp://www.joda.org/joda-time/changes-report.html\r\n\r\nlooks like only enhancement and bug fixes are added. Will see if anything breaks in tests. ", "NaN"], ["8158", "Prune index source columns", "Alan Post", "alandpost", "07/06/17, 10:17:44 AM", "Add PruneIndexSourceColumns and its associated tooling:\r\n* a builder function for IndexSourceNode\r\n* a matcher for IndexSourceNode\r\n* a bulider function for the matcher\r\n* the rule itself\r\n* a unit test for the rule\r\n* enabling the rule in presto's rule list\r\n\r\nAlso, refactor some TableScanNode matching code so it can be shared\r\nbetween TableScanNode and IndexSourceNode, because they are similar.\r\n\r\nhttps://github.com/prestodb/presto/issues/7154", "NaN"], ["8159", "Add more perf stats to QueryCompletedEvent", "Raghav Sethi", "raghavsethi", "06/01/17, 02:49:31 PM", "NaN", "NaN"], ["8160", "Fix code locations of to_ieee754_32/64 functions", "Wenlei Xie", "wenleix", "05/31/17, 11:01:49 PM", "The documents and implementations of the to_ieee754_32/64\r\nfunctions are incorrectly put between the to_big_endian_64\r\nand from_big_endian_64 functions.", "NaN"], ["8161", "Add checks for Kerberos properties in Hive connector", "David Phillips", "electrum", "06/07/17, 10:35:13 PM", "Ensure that we have all the properties set for Hive connector\nwhen using Kerberos to access the Hive metastore or HDFS.\nThis splits out the configs and adds validations.", "NaN"], ["8163", "Change computation of resource group waitingQueuedQueries", "Raghav Sethi", "raghavsethi", "06/02/17, 07:04:24 PM", "waitingQueuedQueries should not be 0 even if the group is able to run\r\nmore queries.", "NaN"], ["8168", "Prune unreferenced outputs before correlated IN", "Piotr Findeisen", "findepi", "06/02/17, 03:15:37 PM", "Fixes #8165 ", "NaN"], ["8170", "Add yield signals to operators", "James Sun", "highker", "08/20/17, 06:54:26 AM", "Splits can run for hours even after they have been killed. This is due\r\nto expensive functions (e.g., regexp) or cross join. Generally, an\r\noperator is not going to stop until it hits the page limit. The stop\r\ncondition is not subjective to time constraints given time checking is\r\nat driver level. This patch adds an abort signal that will be set when a\r\ndriver is closed. An operator is actively checking the abort signal and\r\ncan stop even when the page is not full.", "NaN"], ["8174", "Add release notes for 0.178", "Raghav Sethi", "raghavsethi", "06/05/17, 06:03:27 PM", "NaN", "NaN"], ["8176", "Make Java Keystores Doc easier to understand / Fix Typo in command", "Shin So", "zz22394", "02/22/19, 06:58:30 PM", "1. make doc clear that CN should be a hostname, not \"presto-coordinator\" text.\r\n2. keytool command have -keystore option, not -k ", "NaN"], ["8177", "Fix exception handling in SpillableHashAggregationBuilder", "Piotr Findeisen", "findepi", "06/12/17, 06:29:09 AM", "`SpillableHashAggregationBuilder` should not interrupt current thread if no interruption occurred.", "NaN"], ["8184", "Fix handling of normal form keywords", "Martin Traverso", "martint", "06/07/17, 05:37:59 PM", "NaN", "NaN"], ["8187", "Document and validate reserved keywords", "David Phillips", "electrum", "06/16/17, 01:58:27 PM", "NaN", "NaN"], ["8192", "Remove unused field", "Nezih Yigitbasi", "nezihyigitbasi", "06/06/17, 05:55:57 PM", "NaN", "NaN"], ["8196", "Fix ORDER BY expressions which evaluate to the same canonical", "Grzegorz Kokosi\u0144ski", "kokosing", "06/07/17, 06:29:44 AM", "Fix ORDER BY expressions which evaluate to the same canonical", "NaN"], ["8197", "Optimize count queries for fixed cardinality ", null, "navinvishy", "07/04/17, 12:20:34 PM", "Fixes #7628", "NaN"], ["8202", "Prune join columns", "Alan Post", "alandpost", "06/22/17, 06:11:31 AM", "This patch adds three rules to the iterative optimizer for pruning the  outputs\r\nand inputs of JoinNode.\r\n\r\nCross joins are handled separately, as they always pass all their child columns\r\nthrough.  Thus, any project-off can be pushed down to below the cross join.\r\n\r\nNon-cross joins pass through a subset of their child columns, so we need to have\r\na rule (PruneJoinChildrenColumns) to turn any projecting-off of child columns\r\ninto actual project nodes for the other rules to match against.  There's also a\r\nrule to incorporate any projecting-off from a parent project node into the join\r\nnode itself (PruneJoinColumns).\r\n\r\nhttps://github.com/prestodb/presto/issues/7154", "NaN"], ["8206", "Use Hive Text instead of Java String in GenericHiveRecordCursor", "Dain Sundstrom", "dain", "06/07/17, 05:29:21 PM", "NaN", "NaN"], ["8208", "Bugfix/parquet decimal predicate", "Smith Mathieu", "smith-m", "06/15/17, 12:02:32 AM", "Resolves issue where certain Parquet decimal types cannot be read by the ParquetReader when a predicate is applied. \r\n\r\nhttps://github.com/prestodb/presto/issues/8183", "NaN"], ["8209", "Extract conversion from EquiJoinClause to ComparisonExpression", "Piotr Findeisen", "findepi", "06/08/17, 04:29:34 PM", "NaN", "NaN"], ["8210", "Support GROUPING() in legacy ORDER BY", "Anton", "petroav", "06/07/17, 04:26:39 PM", "Fixes https://github.com/prestodb/presto/issues/8207.\r\n\r\nIs the single test enough? I couldn't think of a cleaner way. I could run all `grouping` tests through legacy order by as well.", "NaN"], ["8212", "Add pushdown for Hive $path column", "German Gil", "ggilfb", "08/04/17, 12:04:32 AM", "If there there is a predicate that matches column: $path, only process files that match such predicate.\r\nResolves #8098\r", "NaN"], ["8213", "Remove legacy Hive connectors", "David Phillips", "electrum", "06/07/17, 08:12:01 PM", "NaN", "NaN"], ["8214", "Use array as ResourceGroupId's JSON representation", "Rongrong Zhong", "rongrong", "06/07/17, 09:36:52 PM", "NaN", "NaN"], ["8216", "Remove unnecessary exception handlers", "Nezih Yigitbasi", "nezihyigitbasi", "06/08/17, 02:52:23 AM", "`ExceededMemoryLimitException` is no longer thrown from `KeyValuePairs`/`MultiKeyValuePairs`.", "NaN"], ["8217", "Optimize JSON to ARRAY/MAP Cast", "Wenlei Xie", "wenleix", "07/25/17, 09:52:37 PM", "Ready for initial review.\r\n\r\nSolves https://github.com/prestodb/presto/issues/8175\r\n\r\nA couple of things to discuss:\r\n- What should be the exception thrown by token-level cast? I currently throw an PrestoException and it will be wrapped in the out-most cast function. The rational behind this is I want to make the final error message contains the full JSON being casted to help user understand which row encountered cast error (instead of just providing a single token). However, as @losipiuk  pointed out, wrapping a PrestoException with another PrestoException is strange. So the alternatives are:\r\n      1.  Throw exceptions like `NumberFormatException` in token-level cast, and wrap it with `PrestoException` at JSON level cast. However, it's unclear what is the right exception for VARCHAR/BOOLEAN, etc.\r\n      2. Still throw `PrestoException` but rewrite it at the JSON level cast.\r\n\r\n- `JsonUtil.HashTable` is something I concerned the most. At least we need more tests...\r\n\r\n------------------------\r\n\r\nFuture work:\r\n\r\n- Split JsonUtil\r\n- Throw consistent error message when there is trailing token (through `JsonCastException`? )\r\n- HashTable can be used elsewhere :)", "NaN"], ["8218", "Support LATERAL join (v4)", "Grzegorz Kokosi\u0144ski", "kokosing", "06/15/17, 11:48:12 AM", "Support LATERAL join", "NaN"], ["8220", "Prune semijoin columns", "Alan Post", "alandpost", "06/15/17, 11:42:05 AM", "This patch adds two rules to the iterative optimizer for pruning the outputs and inputs of SemiJoinNode.\r\n\r\nBecause SemiJoinNode passes through all columns from the left child (aka \"source\"), we can do a project-off push-down from above the SemiJoinNode to above the left child.  That rule can also elide the SemiJoinNode entirely if the join-match flag (aka \"semiJoinOutput\") is not needed.\r\n\r\nBecause SemiJoinNode does not pass through the right child (aka \"filtering source\") columns, we need a separate rule to insert a project-off above the right child.\r\n\r\nhttps://github.com/prestodb/presto/issues/7154", "NaN"], ["8224", "Recover the optimization in SliceArrayBlock", "Nezih Yigitbasi", "nezihyigitbasi", "06/13/17, 05:46:36 PM", "This optimization was removed as part of a previous change as it was complicating\r\nthe retained size calculations, but it seems like we should bring it back as it\r\ncomes up towards top in perf output.", "NaN"], ["8225", "Add missing timezones to zone-index", "Yi He", "hellium01", "06/09/17, 04:11:00 PM", "Added additional time zones that are included until IANA time zone data version 2017a.", "NaN"], ["8226", "Extract method to copy list of lists", "Piotr Findeisen", "findepi", "06/12/17, 04:48:23 AM", "NaN", "NaN"], ["8227", "Remove accidentally committed file", "Piotr Findeisen", "findepi", "06/09/17, 07:32:56 PM", "NaN", "NaN"], ["8231", "Fix typo in resource groups documentation", "Nezih Yigitbasi", "nezihyigitbasi", "06/12/17, 05:27:33 PM", "Also increased the `maxRunning` of the `admin` queue. Because, in the doc it says \"No more than 100 total queries may run at once, unless they're from the admin\", but the admin's `maxRunning` was also `100`.", "NaN"], ["8234", "Fix cli preprocessor stderr handling", "Dain Sundstrom", "dain", "06/14/17, 12:19:33 AM", "Capture and save stderr from preprocessor independently from stdout. If preprocessor exits with non-zero code, add stderr to error message.\r\n\r\n### Test: stdout, stderr, fail\r\n```bash\r\n(>&2 echo \"message here\")\r\necho select 88\r\nexit 42\r\n```\r\n```\r\n# normal\r\npresto> select 1;\r\nQuery preprocessor failed: message here\r\n\r\n# debug\r\nQuery preprocessor failed: message here\r\ncom.facebook.presto.cli.QueryPreprocessorException: Query preprocessor failed: message here\r\n\tat com.facebook.presto.cli.QueryPreprocessor.lambda$preprocessQueryInternal$5(QueryPreprocessor.java:172)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\n### Test: stdout, no-stderr, fail\r\n```bash\r\necho select 88\r\nexit 42\r\n```\r\n```\r\n# normal\r\npresto> select 1;\r\nQuery preprocessor failed: Exit code 42\r\n\r\n# debug\r\npresto> select 1;\r\nQuery preprocessor failed: Exit code 42\r\ncom.facebook.presto.cli.QueryPreprocessorException: Query preprocessor failed: Exit code 42\r\n\tat com.facebook.presto.cli.QueryPreprocessor.lambda$preprocessQueryInternal$5(QueryPreprocessor.java:172)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\n\r", "NaN"], ["8236", "Move providing OuterPositionIterator to LookupSourceFactory [spilljoin]", "Piotr Findeisen", "findepi", "07/04/17, 05:54:05 PM", "OuterPositionIterator is effect of using all LookupSource from a\r\nfactory, thus it should not be provided by LookupSource. This commit\r\nmoves the responsibility to LookupSourceFactory.\r\n\r\nThis is extracted from #8166.", "NaN"], ["8238", "Add OperatorContext.toString", "Piotr Findeisen", "findepi", "06/13/17, 04:50:26 PM", "NaN", "NaN"], ["8241", "Fix typo in reserved-system-memory description", "Sanjay Sharma", "sanjay990", "06/12/17, 03:35:12 PM", "NaN", "NaN"], ["8242", "Close unused RuleTester instances in tests", "Sanjay Sharma", "sanjay990", "06/13/17, 07:58:56 PM", "NaN", "NaN"], ["8246", "Add ResourceGroupStateInfo entry point", "Rongrong Zhong", "rongrong", "06/20/17, 11:00:43 PM", "NaN", "NaN"], ["8247", "Fix correctness bug in PageFunctionCompiler", "Dain Sundstrom", "dain", "06/13/17, 04:04:13 AM", "Block builder was only being reset at the end of the projection, so if\r\nan exception is thrown in processing, block builder contains data from\r\nprevious failed run. The DictionaryAwarePageFilter attempts to apply\r\nprojection to entire dictionary, and if this fails, the page is processed\r\nas normal using the same projection function, which triggers this bug.\r\n\r\nFixes #8262", "NaN"], ["8248", "Bound sizes of blocks read from ORC files", "James Sun", "highker", "06/21/17, 06:13:58 AM", "The max number of rows to read from an Orc file is 1024. That can cause\r\na problem when each row is with a large size. The patch keeps recording\r\nthe largest size for each row and limit the number of rows to read to be\r\n16MB / (observed largest size per row).", "NaN"], ["8249", "Refactor PositionLinks - extract Factory", "Grzegorz Kokosi\u0144ski", "kokosing", "06/13/17, 02:01:30 PM", "NaN", "NaN"], ["8251", "Remove unused method", "Piotr Findeisen", "findepi", "06/13/17, 02:27:02 PM", "NaN", "NaN"], ["8252", "Expose aggregation keys that are also source fields to ORDER BY", "Piotr Findeisen", "findepi", "07/03/17, 08:19:54 AM", "Fixes #8250. However, more proper fix would be to remove need for scopes in translation map and use `FieldId`-s exclusively (#7398)", "NaN"], ["8253", "Remove extra blank line", "Sanjay Sharma", "sanjay990", "06/13/17, 02:24:33 PM", "NaN", "NaN"], ["8259", "Call cleanupQuery for queries that are canceled", "Nezih Yigitbasi", "nezihyigitbasi", "06/14/17, 01:23:53 AM", "SqlQueryExecution::start() registers the metadata in MetadataManager and\r\nadds to the catalogsByQueryId map. However, the metadata entry is not removed\r\nfor canceled queries (only called for finished/failed queries), which results\r\nin a leak of metadata objects.", "NaN"], ["8263", "Fix thread-safety of SpillContexts [spilljoin]", "Piotr Findeisen", "findepi", "07/17/17, 09:06:00 AM", "* OperatorSpillContext declared thread-safety and this commit makes the\r\n  class thread-safe\r\n* LocalSpillContext is used in asynchronous spilling, so it is also made\r\n  thread-safe", "NaN"], ["8266", "Make PagesIndex.clear() free memory [spilljoin]", "Piotr Findeisen", "findepi", "07/10/17, 11:56:23 AM", "Currently the method is used only in `WindowOperator` and that place doesn't require pages index truncation as it is later reused. In spill for join, however, HBO needs to clear its `PagesIndex` once it gets revoke memory request. Clearing with trimming makes more sense, as sole arrays (filled with nulls) may occupy megabytes of memory even for `tpch.sf10.orders` build side.\r\n\r\n(If it is not obvious that trimming doesn't harm `WindowOperator`, I'd propose `clear(boolean compact)`.)", "NaN"], ["8267", "Add 0.179 release notes", "Dain Sundstrom", "dain", "06/14/17, 07:00:30 PM", "NaN", "NaN"], ["8270", "Save the JVM error log from product tests", "Christina Wallin", "cawallin", "11/07/17, 10:43:56 AM", "NaN", "NaN"], ["8271", "Update Travis Java version", "Christina Wallin", "cawallin", "06/14/17, 11:42:23 PM", "We are running into periodic JVM crashes during the dwrf run of\r\nTestHiveStorageFormats#testInsertIntoPartitionedTable. Update the Java\r\nversion to avoid that.", "NaN"], ["8273", "Ignore partition bucketing if table is not bucketed", "David Phillips", "electrum", "06/15/17, 03:38:12 AM", "Previously, queries would fail if the partition was bucketed but the\ntable was not. We can ignore the partition bucketing in such cases.", "NaN"], ["8275", "Make TestFileBasedSystemAccessControl support concurrent test", "Dain Sundstrom", "dain", "06/15/17, 03:16:15 AM", "NaN", "NaN"], ["8276", "Fix incorrect error on type mismatch for set operation", "Martin Traverso", "martint", "06/15/17, 05:00:34 AM", "The arguments to the error message were swapped.", "NaN"], ["8277", "Uppercase set operation name in error message", "David Phillips", "electrum", "06/15/17, 08:29:54 PM", "NaN", "NaN"], ["8280", "Remove obsolete comment", "Piotr Findeisen", "findepi", "06/16/17, 07:34:59 PM", "The comment become obsolete in commit 6155b1da694ce4a69f72dda258426b0085527ad9.", "NaN"], ["8281", "Fix error message with invalid catalog and schema", "Sanjay Sharma", "sanjay990", "11/07/17, 10:43:09 AM", "Currently, with invalid catalog and schema,\r\npresto gives\r\n`Schema xxx does not exist`, instead of\r\n`Catalog xxx does not exist`", "NaN"], ["8284", "Simplify AstUtils", "Piotr Findeisen", "findepi", "06/17/17, 06:40:32 PM", "NaN", "NaN"], ["8285", "Fix formatting", "Sanjay Sharma", "sanjay990", "06/17/17, 06:34:32 PM", "NaN", "NaN"], ["8288", "Fix calls to HostAndPort.getHostText", "Piotr Findeisen", "findepi", "06/17/17, 09:00:01 PM", "The method is removed in Guava 22.0", "NaN"], ["8292", "Print catalog session properties on a new line", "Vamshi Pasunuru", "pvam", "07/20/17, 07:32:07 PM", "I think there's a formatting typo here in printing catalog session properties. Like system session properties each of these need to be on a new line.", "NaN"], ["8296", "Move exec-maven-plugin configuration to execution", "David Phillips", "electrum", "06/19/17, 07:44:10 PM", "This allows having multiple executions.", "NaN"], ["8297", "Preserve javadoc and ordering for Thrift IDL", "Aleksei Statkevich", "AlekseiS", "06/21/17, 06:03:01 PM", "NaN", "NaN"], ["8298", "Remove Maven development profile for CLI", "David Phillips", "electrum", "06/19/17, 08:04:22 PM", "This is rarely used and never worked very well.", "NaN"], ["8302", "Improve error categorization in RCFileReader/OrcPageSourceFactory", "Nezih Yigitbasi", "nezihyigitbasi", "06/21/17, 06:40:58 PM", "Also in `RCFileReader` constructor we were throwing `IllegalArgumentException` for corrupt files, we now throw a proper `RcFileCorruptionException`.", "NaN"], ["8303", "Organize imports", "Piotr Findeisen", "findepi", "06/20/17, 09:16:50 AM", "Fixes build on Travis.", "NaN"], ["8309", "Remove unused PrestoHadoopConfiguration", "David Phillips", "electrum", "06/21/17, 06:17:18 AM", "NaN", "NaN"], ["8310", "Reject query if active worker nodes is insufficient", "Leiqing Cai", "caithagoras", "07/26/17, 11:37:15 PM", "Introduce config query.min-worker-nodes and reject queries if active\r\nworker count is less than this value. However, once it is reached,\r\ndropping below the threshold does not prevent query execution.", "NaN"], ["8312", "Do not prune non scalar LateralNode", "Grzegorz Kokosi\u0144ski", "kokosing", "06/22/17, 07:22:28 AM", "Do not prune non scalar LateralNode\n\nPreviously, when LateralNode outputs were not used, whole LateralNode\nwas pruned. It was incorrect as it might affect cardinality.\nSuch pruning is only valid if subquery is scalar.", "NaN"], ["8319", "Add connectorId to NamenodeStats JMX name", "Dain Sundstrom", "dain", "06/22/17, 12:44:36 AM", "NaN", "NaN"], ["8322", "Feature/rule migrations v2", "Artur Gajowy", "ArturGajowy", "06/23/17, 05:38:30 AM", "Supersedes #7536 and #7294.", "NaN"], ["8325", "Remove Predicates.always{True,False} obsoleted by Guava", "Piotr Findeisen", "findepi", "06/22/17, 09:59:54 AM", "NaN", "NaN"], ["8330", "Fix formatting in RuleAssert", "Rebecca Schlussel", "rschlussel-zz", "06/22/17, 05:37:12 PM", "NaN", "NaN"], ["8332", "Make Util.pruneInputs return Optional<Set<Symbol>>", "Alan Post", "alandpost", "06/23/17, 05:50:04 AM", "Make Util.pruneInputs return Optional<Set<Symbol>>, rather than\r\nOptional<List<Symbol>>, because for most nodes we're thinking about sets\r\nof used symbols, rather than the ordering of the output symbol list.\r\n\r\nIn two specific cases, ValueNode and TableScanNode, retain the previous\r\nordering explicitly, rather than losing it in the set, to avoid making\r\nsuperfluous changes to the node during optimization.  The\r\nPruneJoinColumns rule was already retaining the order of its outputs,\r\nand this patch does not affect that behavior.", "NaN"], ["8333", "Revert \"Do not pass IsScalar throught FilterNode\"", "Grzegorz Kokosi\u0144ski", "kokosing", "06/22/17, 07:06:20 PM", "Revert \"Do not pass IsScalar throught FilterNode\"\n\nThis reverts commit 9bb48d09e3b1a53f2c52be941d2205abac740b56.", "NaN"], ["8334", "Update docker images version to 19", "Christina Wallin", "cawallin", "06/22/17, 08:25:17 PM", "docker-images release 19 updates the Java version from 102 to 131. Hopefully this will eliminate\r\nsome intermittent failures.", "NaN"], ["8335", "Fix error handling of RLE filter and projection", "Dain Sundstrom", "dain", "06/23/17, 04:36:56 AM", "When an error occurs in the filter or project expression, we would throw\na VerifyException instead of the actual exception", "NaN"], ["8336", "Factor out ProjectOffPushDownRule", "Alan Post", "alandpost", "07/15/17, 05:36:40 AM", "This PR is the first of two alternatives that I have submitted to factor out this boilerplate.  As there will be roughly one such rule per node type, I'd like to do one of these to reduce copy/pasting.  The second PR (https://github.com/prestodb/presto/pull/8346) does not use a base class, and instead retains the Pattern boilerplate and delegates to a util function in apply().\r\n\r\nAdd a generic ProjectOffPushDownRule to factor out the boilerplate of\r\nrules migrated from PruneUnreferencedOutputs.\r\n\r\nProjectOffPushDownRule looks for a Project parent over a child of some\r\ntype N, such that the parent doesn't use all the output columns of the\r\nchild.  Given that situation, it invokes the abstract protected\r\npushDownProjectOff to possibly rewrite the child to produce fewer\r\noutputs.", "NaN"], ["8339", "Fix TestBlockRetainedSizeBreakdown", "James Sun", "highker", "06/22/17, 11:23:08 PM", "Make TestBlockRetainedSizeBreakdown thread-safe to run all tests in\r\nparallel.", "NaN"], ["8340", "Enable new RCFile without validation by default", "Dain Sundstrom", "dain", "06/23/17, 07:56:05 PM", "NaN", "NaN"], ["8344", "Feature/migrate canonicalize expressions v2", "Artur Gajowy", "ArturGajowy", "07/03/17, 07:19:18 PM", "Supersedes #7607.", "NaN"], ["8347", "Code polishing of Prune(TableScanColumns, ValuesColumns)", "Grzegorz Kokosi\u0144ski", "kokosing", "07/06/17, 08:51:28 PM", "NaN", "NaN"], ["8350", "Replace ScalarQueryUtil with QueryCardinalityUtil", "Grzegorz Kokosi\u0144ski", "kokosing", "06/27/17, 08:34:28 PM", "Replace ScalarQueryUtil with QueryCardinalityUtil\n\nScalarQueryUtil was wrong. Its goal semantic was to tell if given query\nis scalar (always return a row), but it was returning true for queries\nwrapped with FilterNode for which it is not known how many rows will be\nreturned.\n\nQueryCardinalityUtil solves that as it returns a cardinality range,\nwhere for scalar queries range is <1,1>.", "NaN"], ["8353", "Use older Travis image", "David Phillips", "electrum", "06/23/17, 07:32:18 PM", "The Travis builds hang on TestHiveTableStatistics with the new images.\n\nhttps://blog.travis-ci.com/2017-06-21-trusty-updates-2017-Q2-launch", "NaN"], ["8354", "Check position count in Block and Page assertions", "Christina Wallin", "cawallin", "06/23/17, 07:36:52 PM", "NaN", "NaN"], ["8355", "Fix intermittent failures in Cassandra tests", "Amruta Gokhale", "amrutagokhale", "06/26/17, 05:51:34 PM", "The failing tests execute `CREATE TABLE` or `CREATE KEYSPACE` statements consecutively, followed by sql statements that retrieve the metadata. I suspect that the metadata doesn't get updated on some of the nodes after the executing of DDL statements, hence the tests fail.\r\n\r\nBackground: After executing a schema-altering query such as `CREATE TABLE`, datastax driver waits for a certain time period for \"schema agreement\", before refreshing the schema. If the schema agreement doesn't happen within the given time period, the driver will give up waiting, which will lead to stale metadata being seen on some of the nodes. To prevent this from happening, we can do a number of things such as:\r\n1) Increase the default timeout to a larger value\r\n2) Add a retry mechanism to queries that retrieve schema metadata\r\n3) Execute the queries that retrieve metadata only if there is a schema agreement between the nodes. \r\n\r\nI have added code to do 1) and 2) above, but not 3). Those first two things should allow sufficient time for schema refreshment across all the nodes. Not sure if strictly executing the tests under the check is necessary. (We may need to incorporate that if this doesn't fix it).\r\n\r\nRan the failing tests locally 1000 times, and didn't see failures. Should fix failures such as those reported in https://github.com/prestodb/presto/issues/8180.", "NaN"], ["8359", "Fix statement resource handling of zero size pages", "Dain Sundstrom", "dain", "06/24/17, 02:31:10 AM", "If page.getSizeInBytes() is zero, the page may not be returned to the client.\nThe only known case when this happens is if the page contains only SliceArrayBlocks\nand all values are null.", "NaN"], ["8362", "Add documentation for Thrift Connector", "Aleksei Statkevich", "AlekseiS", "06/28/17, 10:40:47 PM", "Automatically generate Thrift IDL file from Swift definitions as part of\r\nthe documentation build.", "NaN"], ["8366", "Fix formatting in QueryRewriter", "Sanjay Sharma", "sanjay990", "06/26/17, 04:41:14 PM", "NaN", "NaN"], ["8369", "Fix message for Hive partitions dropped during execution", "David Phillips", "electrum", "06/26/17, 09:19:15 PM", "The Hive connector lists partition names during planning, then fetches\nmetadata separately while generating splits. The latter can fail if\nthe partition is dropped in between the two metastore calls.", "NaN"], ["8372", "Fix blocked state checks and running drivers count", "Raghav Sethi", "raghavsethi", "06/27/17, 05:28:39 PM", "Fixes https://github.com/prestodb/presto/issues/8316", "NaN"], ["8375", "Disable invalid saturated_floor_casts", "\u0141ukasz Osipiuk", "losipiuk", "06/27/17, 09:59:52 PM", "\r", "NaN"], ["8376", "Move pattern matching to separate package", "Grzegorz Kokosi\u0144ski", "kokosing", "07/07/17, 08:11:40 AM", "Move pattern matching to separate package\n\nMove pattern matching to separate package and make it independent from\noptimizer.\nThanks to that it will be possible to use pattern matching not only for\noptimizer, but other components as well.", "NaN"], ["8377", "Build instruction with maven wrapper", "Kai Sasaki", "Lewuathe", "06/27/17, 01:52:26 PM", "Build with maven wrapper is recommended because it transparently allows users to take advantage of new maven features without any burden on their part. So maven wrapper should be used in build instruction too.", "NaN"], ["8379", "Fix failing test for SQL Server", "Sanjay Sharma", "sanjay990", "07/11/17, 05:02:20 AM", "NaN", "NaN"], ["8382", "Add outputRows/Bytes and writtenRows/Bytes to query statistics", "German Gil", "ggilfb", "08/04/17, 12:11:34 AM", "Solves: #8282 ", "NaN"], ["8383", "Fix flaky test (TestQueues::testTooManyQueries)", "Nezih Yigitbasi", "nezihyigitbasi", "06/30/17, 10:13:52 PM", "This should hopefully fix #8268.", "NaN"], ["8385", "Remove extra blank line in JDBC metadata query", "David Phillips", "electrum", "06/28/17, 03:13:39 PM", "NaN", "NaN"], ["8388", "Fix NoClassDefFound for annotation based non-static scalar functions", "Haozhun Jin", "haozhun", "06/28/17, 07:39:21 PM", "During query execution, projection/filter do not have access to classes\r\ndefined in plugins. As a result, Object type (instead of the actual\r\nclasses) must be used.", "NaN"], ["8391", "Remove partial flush for TopN", "Jiexi Lin", "jessesleeping", "09/11/17, 07:42:41 PM", "Disable partial flush for TopN operator.\r\n\r\n#8384 ", "NaN"], ["8392", "Ignore ALREADY_EXISTS in CreateTableTask if notExists is set", "Nezih Yigitbasi", "nezihyigitbasi", "07/14/17, 04:37:59 PM", "This is for finalizing the work that @sfilipco started in #8107.", "NaN"], ["8393", "Remove unused provider file for TPCH plugin", "David Phillips", "electrum", "08/23/17, 06:06:35 PM", "NaN", "NaN"], ["8395", "Add new error code HIVE_PARTITION_DROPPED_DURING_QUERY", "David Phillips", "electrum", "06/29/17, 05:23:36 AM", "This occurs when a partition was dropped after we listed the\npartition names but before we could fetch the metadata.", "NaN"], ["8397", "Assign type to symbol in TestRemoveUnreferencedScalarLateralNodes", "\u0141ukasz Osipiuk", "losipiuk", "07/03/17, 05:57:48 AM", "NaN", "NaN"], ["8399", "Add 0.180 release notes", "David Phillips", "electrum", "06/30/17, 12:40:42 AM", "NaN", "NaN"], ["8403", "Use throwIfInstanceOf instead of the deprecated propagateIfInstanceOf", "Nezih Yigitbasi", "nezihyigitbasi", "06/30/17, 06:25:52 PM", "`Throwables::propagateIfInstanceOf()` is deprecated. The only gotcha is we shouldn't be passing a `null` to `throwIfInstanceOf()`.", "NaN"], ["8405", "Add more flexible and complete WindowNode matching", "Alan Post", "alandpost", "07/10/17, 12:07:06 PM", "The previous window matching pattern didn't validate all fields, and\r\ndidn't support a WindowFrame which contained Symbols (non-empty start or\r\nend).  This commit borrows the builder idiom from PlanBuilder to create\r\nflexible matching for WindowNode -- fields not mentioned to the builder\r\nwill not be validated.  Test code can simply supply the constraints they\r\nwish to check.\r\n\r\nI was motivated to extend WindowNode matching because output pruning\r\nneeds to look at the WindowFrame symbols, and rather than extending it\r\nin an incomplete way, I thought I'd try and do it thoroughly.  If people\r\nlike this approach, we can do it for other node types as well.", "NaN"], ["8406", "Fix NPE during connector registration cleanup", "David Phillips", "electrum", "10/06/17, 07:16:25 PM", "NaN", "NaN"], ["8408", "Add missing JsonCreator annotation", "Dongmin Yu", "miniway", "06/30/17, 03:10:05 PM", "NaN", "NaN"], ["8410", "Add Maven dependency to JDBC driver documentation", "David Phillips", "electrum", "07/05/17, 11:49:56 PM", "NaN", "NaN"], ["8412", "Update docker images version to 20", "Sanjay Sharma", "sanjay990", "07/04/17, 05:50:20 PM", "NaN", "NaN"], ["8414", "Fix flaky level multipliers test by increasing tolerances", "Raghav Sethi", "raghavsethi", "06/30/17, 10:37:58 PM", "Fixes #8413", "NaN"], ["8417", "Optimize json_parse with cast", "James Sun", "highker", "08/04/17, 01:28:05 AM", "Json objects are stored as strings in database in many use cases. It is inefficient to convert a string into json and then convert the json into an array or map. The patch optimize the two steps into way.\r\n\r\nTPCH benchmark shows a CPU save around **30% - 40%**\r\n\r\n100 entries in an array\r\n```\r\nbefore sql_cast_json_parse :: 1012.584 cpu ms ::    0B peak memory :: in   15K,      0B,   14.8K/s,      0B/s :: out   15K,  7.22MB,   14.8K/s,  7.13MB/s\r\nafter  sql_cast_json_parse ::  593.713 cpu ms ::    0B peak memory :: in   15K,      0B,   25.3K/s,      0B/s :: out   15K,  7.22MB,   25.3K/s,  12.2MB/s\r\n```\r\n\r\n\r\n10 enties in an array\r\n```\r\nbefore sql_cast_json_parse ::  162.843 cpu ms ::    0B peak memory :: in   15K,      0B,   92.1K/s,      0B/s :: out   15K,   806KB,   92.1K/s,  4.83MB/s\r\nafter  sql_cast_json_parse ::  113.290 cpu ms ::    0B peak memory :: in   15K,      0B,    132K/s,      0B/s :: out   15K,   806KB,    132K/s,  6.94MB/s\r\n```\r\n\r\nThe PR resolves https://github.com/prestodb/presto/issues/8286\r", "NaN"], ["8418", "Fix broken links in product-tests README", "Sanjay Sharma", "sanjay990", "07/01/17, 03:44:54 AM", "NaN", "NaN"], ["8422", "Glue IterativeOptimizers which are next to each other", "Grzegorz Kokosi\u0144ski", "kokosing", "07/04/17, 06:37:31 AM", "Glue IterativeOptimizers which are next to each other", "NaN"], ["8423", "Simplify Assignments usage", "Grzegorz Kokosi\u0144ski", "kokosing", "07/04/17, 04:44:37 AM", "Simply Assignments usage", "NaN"], ["8424", "Bound block sizes read from ORC files with metadata", "James Sun", "highker", "08/04/17, 01:29:29 AM", "Previously a block size is bounded by historical data collected when\r\nloading a block. This approach can effectively bound 95% block sizes\r\naccording to the exported stats; however, It misses the first load that\r\ncould be as large as hundreds of MBs. This patch collects data in stripe\r\nreader to estimiate the average row size in the next row group to read\r\nand bound the block size with the estimation.", "NaN"], ["8429", "Fix bug where PageBuilder is not reset when full in functions", "Haozhun Jin", "haozhun", "07/05/17, 09:27:56 PM", "A few functions cache a PageBuilder internally to avoid allocation\r\nand size estimation on every invocation. However, it need to be\r\nperiodically cleared to make sure PageBuilder doesn't grow too\r\nbig, which would cause query failures (array exceeding 2G positions)\r\nand GC pressure (unaccounted memory of huge arrays per function\r\ncallsite in the SQL query).\r\n\r\nI filed #8428 to add test coverage.\r\n\r\n@highker  is working on fixing the benchmarks, which could be used to expose this issue. The benchmarks have been broken after @dain's columnar change.", "NaN"], ["8430", "Fix SingleMapBlock instance size field", "Nezih Yigitbasi", "nezihyigitbasi", "07/05/17, 07:54:49 PM", "NaN", "NaN"], ["8431", "Fix potential overflow in region size calculations", "Nezih Yigitbasi", "nezihyigitbasi", "07/05/17, 11:46:07 PM", "NaN", "NaN"], ["8433", "Fix benchmarks using page processors", "James Sun", "highker", "07/20/17, 06:00:37 AM", "The benchmark needs to iterate through the pages.", "NaN"], ["8434", "Improve documentation for EXPLAIN ANALYZE VERBOSE", "David Phillips", "electrum", "07/06/17, 12:17:40 AM", "NaN", "NaN"], ["8436", "Fix potential overflow in getSizeInBytes", "Nezih Yigitbasi", "nezihyigitbasi", "07/06/17, 10:36:59 PM", "NaN", "NaN"], ["8440", "Reset block builder before throwing in various map functions", "Nezih Yigitbasi", "nezihyigitbasi", "07/11/17, 02:14:23 AM", "Otherwise, when these functions are used within TRY() the BlockBuilders\r\nused for intermediate state in these functions may be left in an incorrect\r\nstate causing failures on subsequent calls.\r\n\r\nThis is a temporary fix for #8323, the ultimate fix is more complicated as described in that issue.", "NaN"], ["8442", "Minor improvements for Thrift connector", "Aleksei Statkevich", "AlekseiS", "07/07/17, 01:45:29 AM", "1. Don't return the whole split information in \"getSplitInfo()\". The info\r\ncontains split id which is an arbitrary byte sequence and can be\r\nrelatively large. As a result, this can potentially slow down split\r\nstats collection mechanism.\r\n\r\n2. Don't check if a future is done in \"ThriftPageSource.isBlocked()\"\r\nthis check is unnecessary, because all current callers of this method\r\nare doing it.", "NaN"], ["8443", "Introduce Rule.Context", "Grzegorz Kokosi\u0144ski", "kokosing", "07/12/17, 05:20:49 AM", "Introduce Rule.Context\n\nMotivation:\n - when implementing a rule it is easier to pass rule context parameters\n to other methods.\n - less boilerplate in rule implementations\n - easier to modify Rule#apply method parameters", "NaN"], ["8444", "Make travis upload PR artifacts to different directory", "Andrzej Fiedukowicz", "fiedukow", "07/10/17, 09:37:37 AM", "NaN", "NaN"], ["8446", "Update testing PostgreSQL server", "David Phillips", "electrum", "08/08/17, 11:20:55 PM", "NaN", "NaN"], ["8448", "Remove all usages of Airlift FileUtils", "David Phillips", "electrum", "08/09/17, 04:22:34 PM", "NaN", "NaN"], ["8449", "Add RemoveUnreferencedScalarApplyNodes rule", "Grzegorz Kokosi\u0144ski", "kokosing", "07/11/17, 07:39:58 PM", "Add RemoveUnreferencedScalarApplyNodes rule", "NaN"], ["8450", "Fix unit test flakiness", "Grzegorz Kokosi\u0144ski", "kokosing", "07/10/17, 12:56:49 PM", "Fix unit test flakiness\r\n\r\nPatterns in MathchingEngine are stored within a map and types\r\nare returned by TypeToken are stored in a set. These two facts\r\ncause that order of returned Patterns from\r\ngetCandidates method is non deterministic.\r", "NaN"], ["8452", "Fix InlineProjections on subqueries", "Alan Post", "alandpost", "07/08/17, 06:07:51 PM", "InlineProjections assumed that all input symbols to the parent\r\nProjectNode came from the child, but that's not always the case in\r\nsubqueries, where symbols are available even when not defined by the\r\nchild.  This resulted in a loop where the rule fired, but emitted an\r\nunchanged plan fragment.\r\n\r\nThis patch simply corrects the logic for identifying inline-able\r\ndefinitions to ignore symbols that don't come from the child..\r\n\r\nhttps://github.com/prestodb/presto/issues/8451", "NaN"], ["8453", "Loosen option constraint for Java version", "Kurt Ruppel", "kruppel", "02/28/18, 06:51:32 PM", "Later versions of 1.8.0 include options beyond 'ea'. Example:\r\n\r\n```\r\n\u2af8  docker run -t java:8-jdk-alpine java -version\r\nopenjdk version \"1.8.0_111-internal\"\r\nOpenJDK Runtime Environment (build 1.8.0_111-internal-alpine-r0-b14)\r\nOpenJDK 64-Bit Server VM (build 25.111-b14, mixed mode)\r\n```", "NaN"], ["8455", "Fix native memory leak in RCFile writer", "James Sun", "highker", "07/10/17, 02:18:10 AM", "When closing RcFileWriters, exception may appear. We need to make sure\r\nall compressors are returned even exception occurs.", "NaN"], ["8458", "Avoid memory allocation when copy a compact block", "Wenlei Xie", "wenleix", "11/01/17, 10:31:25 PM", "When building the hash table for join or aggregation and it cannot reserve more memory (either because the local host is running out of memory, or the query is running out of global/local memory), it will tries to compact the index: https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/operator/HashBuilderOperator.java#L235\r\n\r\nHowever, compacting the blocks requires to copy the block: https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/operator/PagesIndex.java#L201. This will put a pressure on heap memory, especially when the old block data is already in OldGen. It sometimes causes full GC .\r\n\r\nWhen the block is already in compact representation, it's unnecessary to copy into a new block. Previously, the following code tries to avoid such unnecessary compact:\r\n\r\n\r\n    if (block.getSizeInBytes() < block.getRetainedSizeInBytes()) {\r\n        ......\r\n    }\r\n\r\nHowever, retained size is always larger than the logical size, as retained size includes the block instance size. This heuristics don't work as expected.\r\n\r\n\r\nIn this Pull Request, we avoid memory allocation in `Block.copyRegion` if it is copying a whole compact block.\r\n\r\nThis pull request depends on slice 0.30 with this pull request patched: https://github.com/airlift/slice/pull/85\r\n\r\n\r", "NaN"], ["8460", "Extract simple refactorings from #8435", "Grzegorz Kokosi\u0144ski", "kokosing", "07/15/17, 07:59:34 AM", "NaN", "NaN"], ["8462", "Implement cancellation of non-query statements", "David Phillips", "electrum", "10/28/17, 06:01:36 AM", "NaN", "NaN"], ["8464", "Add async retry to RetryDriver", "Jiexi Lin", "jessesleeping", "09/15/17, 10:57:58 PM", "Wrap a returned future with a fallback that will retry upon async failure. The retry will be scheduled in a separate thread pool and will not block either the IO thread or the engine thread.\r\n\r\n#8463 ", "NaN"], ["8467", "Add 2 date columns to shards system table", "Jiexi Lin", "jessesleeping", "09/18/17, 08:08:02 PM", "Add 2 DATE columns to 'shards' system table to display the shard ranges for tables with DATE type temporal columns.\r\n\r\n#7884", "NaN"], ["8468", "Test that scalar function with instanceFactory has bounded retained size", "Haozhun Jin", "haozhun", "07/11/17, 11:49:34 PM", "Fixes #8428 ", "NaN"], ["8478", "Handle UncheckedIOException in RcFileFileWriter", "Dain Sundstrom", "dain", "07/12/17, 06:00:29 PM", "NaN", "NaN"], ["8479", "Add basic example to JDBC documentation", "David Phillips", "electrum", "07/11/17, 08:12:35 PM", "The existing examples were not runnable without modification.\r\n\r\nFixes #8472", "NaN"], ["8481", "Use zlib from JDK instead of Hadoop for RCFile", "James Sun", "highker", "07/17/17, 08:07:45 PM", "Gzip compressors created by apache hadoop use system native memory and\r\nhold them forever in a codec pool. This can easily lead to memory leak\r\nand make memory tracking hard. Replace the compression library with the\r\njava one.", "NaN"], ["8483", "Rename method to recurseOnlyWhen", "Grzegorz Kokosi\u0144ski", "kokosing", "07/15/17, 07:55:50 AM", "Rename method to recurseOnlyWhen\n\nskipOnlyWhen name was very confusing, not it should be easier to\nunderstand what it means.", "NaN"], ["8489", "Raise proper error when subquery is not supported", "Grzegorz Kokosi\u0144ski", "kokosing", "08/04/17, 10:05:14 AM", "Raise proper error when subquery is not supported", "NaN"], ["8490", "Add sub group information to ResourceGroupStateInfo", "Rongrong Zhong", "rongrong", "07/15/17, 04:18:03 AM", "NaN", "NaN"], ["8503", "Shutdown executor used in FileSingleStreamSpillerFactory", "Piotr Findeisen", "findepi", "07/26/17, 12:16:00 PM", "NaN", "NaN"], ["8505", "Export split cpu time counters", "Dongmin Yu", "miniway", "07/20/17, 07:06:21 PM", "NaN", "NaN"], ["8506", "Fixup merge conflict for CREATE TABLE IF NOT EXISTS", "James Sun", "highker", "07/14/17, 05:52:48 PM", "Override dropColumn\r", "NaN"], ["8507", "Reorder and merge WindowNodes across ProjectNodes", "Alan Post", "alandpost", "09/13/17, 06:09:58 AM", "Enhance the two optimization rules that first reorder, and then merge,\r\nadjacent WindowNodes that share the same specification to also apply\r\nwhen the WindowNodes are separated by one or more ProjectNodes, when\r\npossible.  Move the two optimizations into a single RuleSet so they can\r\neasily share common code.  Match against a fixed number of projects in\r\neach rule, because the pattern system doesn't yet support Kleene star.\r\n\r\nThe window function that typically produces this is LAG(X, 1), where the\r\nlag offset constant becomes a projection below the WindowNode.\r\n\r\nFixes https://github.com/prestodb/presto/issues/6681\r\n\r\nThis work is further motivated by the PruneUnreferencedOutputs migration,\r\nwhich introduces a new ProjectNode between nearly every pair of other\r\nnodes.", "NaN"], ["8508", "Delete unused RetryDriver class in Cassandra connector", "David Phillips", "electrum", "08/04/17, 07:44:17 PM", "NaN", "NaN"], ["8512", "Add PruneProjectColumns rule", "Alan Post", "alandpost", "07/20/17, 08:19:23 PM", "Migrate PruneUnreferencedOutputs handling of ProjectNode to\r\nthe iterative optimizer.  This rule finds a project that's under another\r\nproject, such that the parent doesn't use all the outputs of the child,\r\nand drops the unused projections from the child.  Because the child is\r\na ProjectNode, it can trigger a subsequent rule to prune the grandchild.", "NaN"], ["8513", "Add PruneFilterColumns rule", "Alan Post", "alandpost", "07/26/17, 06:13:05 AM", "Migrate PruneUnreferencedOutputs handling of FilterNode to\r\nthe iterative optimizer.  This rule finds a filter that's under\r\na project, such that the grandchild produces columns not needed by\r\neither the parent or the child, and creates a new project grandchild to\r\ndiscard the unused columns.  Subsequent rules may match the pattern of\r\nthe new grandchild (project) over the old grandchild, and do further\r\npruning.", "NaN"], ["8514", "Add PruneOutputColumns rule", "Alan Post", "alandpost", "07/16/17, 07:53:34 PM", "Migrate PruneUnreferencedOutputs handling of OutputNode to the iterative\r\noptimizer.  This rule adds an explicit project-off underneath an\r\nOutputNode which doesn't use all of it's child's outputs, so that other\r\npruning rules will match against the pattern of a project over the\r\noriginal child.", "NaN"], ["8515", "Add PruneTopNColumns Rule", "Alan Post", "alandpost", "07/20/17, 05:14:14 AM", "Migrate PruneUnreferencedOutputs handling of TopNNode to the iterative\r\noptimizer.  This rule finds a TopNNode that's under a project, such that\r\nthe grandchild produces columns not needed by either the parent or the\r\nchild, and creates a new project grandchild to discard the unused\r\ncolumns.  Subsequent rules may match the pattern of the new grandchild\r\n(project) over the old grandchild, and do further pruning.", "NaN"], ["8516", "Prune aggregation columns", "Alan Post", "alandpost", "07/26/17, 06:20:13 AM", "Migrate PruneUnreferencedOutputs handling of AggregationNode to the iterative optimizer.\r\nThis patch adds two rules, one which looks for a project-off of AggregationNode outputs, and one which inserts a project-off below an AggregationNode, discarding unused inputs.  There are two rules because it may be possible to discard unused inputs even if there is no project-off above the AggregationNode.", "NaN"], ["8519", "Reverse deprecation in PlanNodeSearcher", "Grzegorz Kokosi\u0144ski", "kokosing", "07/17/17, 07:58:09 PM", "Reverse deprecation in PlanNodeSearcher\n\nUsing a plan node searcher without a lookup is completely fine, as it\nrequires to have accessible fully fledge plan node tree.\n\nOn the other side, using a plan node searcher with lookup enables\noptimizer rules to overuse Lookup. Rule by design should not traverse\nthe tree. Hence, such plan node searcher usage should be replaced with\nusage of traits and better pattern matching.", "NaN"], ["8524", "Statistics SPI changes", "\u0141ukasz Osipiuk", "losipiuk", "08/04/17, 09:53:36 AM", "Change SPI for further statistics/cost work:\r\n* change nulls count to nulls fraction\r\n* introduce support for single value range to model low and high value in table.\r\n\r\nThis is one of many small PRs originating from original \"statistics followup PR\"", "NaN"], ["8525", "Use latest Travis image", "David Phillips", "electrum", "08/04/17, 07:46:19 PM", "NaN", "NaN"], ["8528", "Replace ExpressionInterpreter.invoke with FunctionInvoker", "Maciej 'mac' Grzybek", "maciejgrzybek", "07/21/17, 10:57:26 PM", "Supersedes https://github.com/prestodb/presto/pull/8502\r\n\r\n---\r\n\r\nFixes #8329 ", "NaN"], ["8529", "Fix potential native memory leak", "Nezih Yigitbasi", "nezihyigitbasi", "07/17/17, 08:26:10 PM", "Make sure we `close()` the output stream during writes.", "NaN"], ["8532", "Make sure to call destroy() on close", "Nezih Yigitbasi", "nezihyigitbasi", "07/20/17, 08:11:42 PM", "`AircompressorDecompressor::destroy()` is a nop so for now this is not a problem, but\r\n`destroy()` returns codecs to the pool for `HadoopDecompressor`s so it has to be called\r\nto prevent leaks.", "NaN"], ["8535", "Fix flaky test TestPrestoDriver::testQueryTimeout()", "Nezih Yigitbasi", "nezihyigitbasi", "07/20/17, 04:46:32 PM", "Hit this one again ([travis job](https://travis-ci.org/prestodb/presto/jobs/254646458), previously reported in #8272). \r\n\r\nI guess the reason for flakiness is that the `SqlQueryManager` thread that enforces the runtime limits run every second so it's easy to hit the timeout of 2 seconds.\r\n\r\nWithout the change I can easily repro this locally, and with the change I ran the test ~30 times and all passed. Note that the method has a timeout of 4 seconds.", "NaN"], ["8537", "Allow checking if transaction exists in TransactionManager", "David Phillips", "electrum", "08/08/17, 11:19:41 PM", "This is useful in tests that utilize transactions.", "NaN"], ["8538", "Update to presto-maven-plugin 0.2", "David Phillips", "electrum", "08/04/17, 07:46:05 PM", "This version preserves the exception when scanning for plugin classes fails.\r", "NaN"], ["8539", "Add generic expression rewrite rule and migrate canonicalize expression", "Szymon Matejczyk", "szymonm", "07/25/17, 12:48:24 PM", "NaN", "NaN"], ["8541", "Add PruneWindowColumns Rule", "Alan Post", "alandpost", "09/14/17, 05:09:03 AM", "Migrate PruneUnreferencedOutputs handling of WindowNode to the iterative\r\noptimizer.  This rule finds a WindowNode that's under a project, such\r\nthat the WindowNode defines a new column not needed by the parent, or\r\nthe grandchild produces columns not needed by either the parent or the\r\nWindowNode, and remove any unneeded window functions, and creates a new\r\nproject grandchild to discard any unused columns.  Subsequent rules may\r\nmatch the pattern of the new grandchild (project) over the old\r\ngrandchild, and do further pruning.\r\n\r\nThis PR contains a commit cherry-picked from https://github.com/prestodb/presto/pull/8507\r\nwhich reviewers should ignore.  That commit enhances the window reorder / merge\r\noptimizations so they aren't blocked by the project nodes introduced by\r\nPruneWindowColumns.  PR https://github.com/prestodb/presto/pull/8507 needs to get merged before this one.", "NaN"], ["8543", "Add 0.181 release notes", "Martin Traverso", "martint", "07/19/17, 04:52:49 AM", "NaN", "NaN"], ["8544", "Implement IPADDRESS type", "Rongrong Zhong", "rongrong", "07/28/17, 04:28:58 AM", "Fixes #8509 ", "NaN"], ["8546", "Fix humongous allocation when computing retained size for SliceArrayBlock", "Wenlei Xie", "wenleix", "07/19/17, 06:36:14 AM", "\r\nWhen computing the retained size of a SliceArrayBlock,\r\nan IdentityHashMap will be created  with expected size set to\r\nthe number of slices. For a SliceArrayBlock contains 1 million slices,\r\nthe IdentityHashMap will contain an Object array with 4,194,304 entires\r\nwhich will cause humongous allocation.\r\n\r\nMost of the slices will share the same base byte array, and setting\r\nsuch huge expected size is a waste. This commit changes to use the\r\ndefault expected size. This aligns with other usage of IdentityHashMap\r\nincluding SliceArrayBlock.deepCopyAndCompact.\r\n\r\n![hugeobjectarray2](https://user-images.githubusercontent.com/799346/28351199-31f11664-6c03-11e7-8510-b503917c38a0.png)\r", "NaN"], ["8549", "Remove redundant Lookup#resolve call", "Piotr Findeisen", "findepi", "07/20/17, 04:48:18 AM", "cc @ArturGajowy ", "NaN"], ["8551", "Preserve symbols required by join during partial aggregation pushdown", "Karol Sobczak", "sopel39", "07/25/17, 11:54:49 AM", "Partial aggregation needs to produce symbols requried by\r\njoin equi-conditions and filter expression\r\n\r\nFixes: https://github.com/prestodb/presto/issues/8550", "NaN"], ["8554", "Add ORC bloom filter table properties", "Dain Sundstrom", "dain", "07/24/17, 08:52:18 PM", "NaN", "NaN"], ["8558", "Remove redundant path mapping", "Sanjay Sharma", "sanjay990", "07/20/17, 08:42:34 PM", "NaN", "NaN"], ["8563", "Tpcds connector no docs", "Anton", "petroav", "07/21/17, 04:39:09 AM", "Supersedes: https://github.com/prestodb/presto/pull/6374\r\nDocs for the connector are in a separate PR: https://github.com/prestodb/presto/pull/8562", "NaN"], ["8564", "Update MinMaxHelper to use an existing utility method", "Nezih Yigitbasi", "nezihyigitbasi", "08/07/17, 06:14:33 PM", "NaN", "NaN"], ["8567", "Allow empty password in JDBC driver", "David Phillips", "electrum", "10/06/17, 07:00:22 PM", "Fixes #8566", "NaN"], ["8570", "Implement subtraction of two timestamps", "Yuya Ebihara", "ebyhr", "07/23/17, 05:16:26 PM", "Relates #8135 \r\n\r\nImplemented only subtraction. I looked for information about addition timestamps on sql spec but couldn't find it. Followings are other database's results.\r\n\r\nmysql\r\n```\r\nmysql> select current_timestamp + current_timestamp;\r\n+---------------------------------------+\r\n| current_timestamp + current_timestamp |\r\n+---------------------------------------+\r\n|                        40341446245414 |\r\n+---------------------------------------+\r\n```\r\n\r\nsql server\r\n```\r\nselect CURRENT_TIMESTAMP + CURRENT_TIMESTAMP;\r\n\r\n2135-02-13 19:14:49.067\r\n```\r\n\r\npostgresql and teradata throw error.", "NaN"], ["8571", "Order entries in AggregationNode", "Grzegorz Kokosi\u0144ski", "kokosing", "07/24/17, 07:50:02 PM", "Order entries in AggregationNode\n\n * do not mix inner classes with class methods\n * put methods after constructor\n * do not mix getters (@JsonProperty methods) with domain methods\n * group inner classes together", "NaN"], ["8572", "Fix BindExpression doc", "Szymon Matejczyk", "szymonm", "07/26/17, 05:46:03 PM", "@haozhun Could you please have a look at this?", "NaN"], ["8574", "Test Join using stateful filter function", "Piotr Findeisen", "findepi", "07/26/17, 09:35:45 AM", "Filter function is encapsulated in `LookupSource`. It's imperative that\r\n`LookupJoinOperator`-s do not share `LookupSource` instances.", "NaN"], ["8575", "Clear interrupt flag after exiting from pager", "Andrii Rosa", "arhimondr", "07/25/17, 01:18:10 AM", "Http client is responsive for interrupts, therefore sometimes it happens\r\nthat the final \"abort\" request is not being executed", "NaN"], ["8576", "Add Checkstyle rule for indentation and spaces", "David Phillips", "electrum", "07/24/17, 06:32:32 PM", "NaN", "NaN"], ["8579", "Preserve identifier quoting in AST", "Martin Traverso", "martint", "08/04/17, 05:17:53 PM", "Track names that originate from identifiers in the grammar as\r\nproper Identifier AST nodes and record whether they are to\r\nbe treated verbatim (i.e., they were originally quoted).\r", "NaN"], ["8581", "Fix a missing negation in MergeLimitWithDistinct#isDistinct", "Artur Gajowy", "ArturGajowy", "07/25/17, 11:32:53 AM", "NaN", "NaN"], ["8583", "New matcher: equals and filter patterns", "Artur Gajowy", "ArturGajowy", "08/04/17, 05:48:32 AM", "Bases on #8568. The actual PR / review material is just the 8 last commits, starting from `Add filtering Pattern-s`.", "NaN"], ["8584", "Fix date constant in test", "Piotr Findeisen", "findepi", "07/25/17, 01:24:18 PM", "The date constant needs to fit into an int, as `SqlDate` takes an int.\r\nThe constant value used in the test was clearly mistyped.", "NaN"], ["8585", "Make artifacts upload less chatty", "Artur Gajowy", "ArturGajowy", "07/25/17, 09:35:18 PM", "This should prevent the occasional '4 MB log file limit exceeded'\r\non Travis", "NaN"], ["8586", "Cleanup CanonicalizeExpression", "Szymon Matejczyk", "szymonm", "07/26/17, 12:36:10 PM", "NaN", "NaN"], ["8587", "Factor out Rule#isEnabled(Session)", "Artur Gajowy", "ArturGajowy", "07/26/17, 04:36:56 AM", "This will help unclutter `Rule#apply` and allow for non-negated conditional logic.\r\n\r\nIn theory, we could fit this into the new pattern matcher - but that would feel like an overkill. Also, evaluating the Rule's pattern (or the rule itself, as it is now with the check in `Rule#apply`) is a bit late for checking if it's enabled.", "NaN"], ["8590", "Update use of deprecated JsonParseException constructor", "Nezih Yigitbasi", "nezihyigitbasi", "07/25/17, 10:34:42 PM", "NaN", "NaN"], ["8591", "Throw PrestoException with HIVE_BAD_DATA code for corrupt RC files", "Nezih Yigitbasi", "nezihyigitbasi", "07/25/17, 11:23:04 PM", "`RcFileReader` constructor can throw `RcFileCorruptionException`, which should be propagated as `HIVE_BAD_DATA` by the page source factory.", "NaN"], ["8592", "Change expected type to be Object instead of Block in Accumulator", "Maciej 'mac' Grzybek", "maciejgrzybek", "07/25/17, 11:01:03 PM", "Fixes #8559.", "NaN"], ["8593", "Rewrite join filters in SimplifyExpressions", "Rebecca Schlussel", "rschlussel-zz", "07/26/17, 05:28:24 AM", "SimplifyExpressions wasn't simplifying join filters to CNF. This\r\nprevented predicate push down for conditions such as the one below\r\nwhen it appeared in a join filter rather than a filter node:\r\n\r\n((n1.name = 'FRANCE' AND n2.name = 'GERMANY') OR (n1.name = 'GERMANY'\r\nAND n2.name = 'FRANCE') )'.\r\n\r\nNow (n1.name='FRANCE OR n1.name = 'GERMANY') will get pushed to one side\r\nof the join and (n2.name = 'FRANCE' OR n2.name = 'GERMANY') will get\r\npushed to the other.\r\n\r\nThis improves performance for tpc-h q7 and should help other queries too.", "NaN"], ["8596", "Avoid eager loading all pages in PageProcessor", "Dain Sundstrom", "dain", "08/07/17, 07:16:48 PM", "LazyBlock size calls force the block to load, so add a special size\ncalculation for page projections.\n\nAdd tests to ensure lazy blocks are not eagerly loaded.", "NaN"], ["8597", "Add map_from_entries function", "Leiqing Cai", "caithagoras", "08/08/17, 07:21:03 PM", "map_from_entries function takes an array of two-element rows of the\r\nsame type, and builds a map, treating each first element as the key\r\nand each second element as the value. Duplicate keys or null key are\r\ndisallowed.", "NaN"], ["8598", "Add cache to PageFunctionCompiler", "Dain Sundstrom", "dain", "08/08/17, 10:31:49 PM", "NaN", "NaN"], ["8599", "Fix dependency for presto-resource-group-managers", "Leiqing Cai", "caithagoras", "07/26/17, 02:53:31 AM", "NaN", "NaN"], ["8605", "Do not print configuration in tests to avoid Travis log limit", "Piotr Findeisen", "findepi", "07/26/17, 07:48:37 PM", "We're close to Travis log length limit of 4 MB in the build\r\nconfiguration testing `presto-tests`.\r\n\r\nThis commits disables configuration printing during test setup.\r\nConfiguration printing is what takes a lot of the log space in\r\n`presto-tests` tests.", "NaN"], ["8608", "Rewrite grouping queries", "Anton", "petroav", "07/26/17, 09:26:49 PM", "NaN", "NaN"], ["8610", "Populate sub resource group stats correctly", "Rongrong Zhong", "rongrong", "07/27/17, 05:57:20 PM", "NaN", "NaN"], ["8614", "Extend fast inequality join", "Anu Sudarsan", "anusudarsan", "09/11/17, 09:39:25 AM", "This extends functionality added in https://github.com/prestodb/presto/pull/7097, for #6922. \r\n\r\nInternal review - https://github.com/Teradata/presto/pull/630\r\n\r\nThe PR extends the functionality to speed up query with range predicates eg: benchmarkRangePredicateJoin . But I added benchmark tests for other queries which were already addressed by the optimization. So you can see the comparison below with and without this optimization.\r\n```\r\n\t\t\t\t\t\t\t\t(buckets)  (fastInequalityJoins)     (master)\t            (PR branch)\r\nBenchmarkInequalityJoin.benchmarkJoin \t\t\t\t100\t\ttrue\t\t222.267 \u00b1  36.490  ms/op   234.191 \u00b1  33.999  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoin \t\t\t\t100\t\tfalse\t       2409.789 \u00b1 193.371  ms/op  2360.016 \u00b1 189.837  ms/op\r\nBenchmarkInequalityJoin. benchmarkJoinWithArithmeticInPredicate\t100\t\ttrue\t\t279.125 \u00b1  23.798  ms/op   280.396 \u00b1  17.991  ms/op\r\nBenchmarkInequalityJoin. benchmarkJoinWithArithmeticInPredicate\t100\t\tfalse\t       2375.963 \u00b1  78.662  ms/op  2376.180 \u00b1 125.109  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithFunctionPredicate\t100\t\ttrue\t\t193.858 \u00b1  12.845  ms/op   216.786 \u00b1  12.600  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithFunctionPredicate\t100\t\tfalse\t       2288.445 \u00b1  55.931  ms/op  2408.483 \u00b1  97.140  ms/op\r\nBenchmarkInequalityJoin.benchmarkRangePredicateJoin\t\t100\t\ttrue\t      2435.688 \u00b1 143.372  ms/op\t   247.428 \u00b1  11.549  ms/op\r\nBenchmarkInequalityJoin.benchmarkRangePredicateJoin\t\t100\t\tfalse\t      2433.708 \u00b1  64.086  ms/op   2487.442 \u00b1  60.085  ms/op\r\n```\r\n\r\nComplete Benchmarking results \r\n```\r\nBenchmark                                                     (buckets)  (fastInequalityJoins)  (filterOutCoefficient)  Mode  Cnt     Score     Error  Units\r\nBenchmarkInequalityJoin.benchmarkJoin                               100                   true                      10  avgt   30   234.191 \u00b1  33.999  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoin                               100                  false                      10  avgt   30  2360.016 \u00b1 189.837  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoin                              1000                   true                      10  avgt   30   187.426 \u00b1  24.792  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoin                              1000                  false                      10  avgt   30   414.487 \u00b1  27.297  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoin                             10000                   true                      10  avgt   30   198.977 \u00b1  35.756  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoin                             10000                  false                      10  avgt   30   239.980 \u00b1  18.026  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoin                             60000                   true                      10  avgt   30   173.009 \u00b1   6.956  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoin                             60000                  false                      10  avgt   30   181.165 \u00b1   8.649  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithArithmeticInPredicate        100                   true                      10  avgt   30   280.396 \u00b1  17.991  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithArithmeticInPredicate        100                  false                      10  avgt   30  2376.180 \u00b1 125.109  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithArithmeticInPredicate       1000                   true                      10  avgt   30   210.539 \u00b1  11.744  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithArithmeticInPredicate       1000                  false                      10  avgt   30   471.721 \u00b1  45.251  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithArithmeticInPredicate      10000                   true                      10  avgt   30   203.669 \u00b1   9.373  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithArithmeticInPredicate      10000                  false                      10  avgt   30   259.281 \u00b1  13.036  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithArithmeticInPredicate      60000                   true                      10  avgt   30   203.048 \u00b1  10.953  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithArithmeticInPredicate      60000                  false                      10  avgt   30   199.349 \u00b1   9.362  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithFunctionPredicate          100                   true                      10  avgt   30   216.786 \u00b1  12.600  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithFunctionPredicate          100                  false                      10  avgt   30  2408.483 \u00b1  97.140  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithFunctionPredicate         1000                   true                      10  avgt   30   195.763 \u00b1  13.215  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithFunctionPredicate         1000                  false                      10  avgt   30   580.025 \u00b1 120.265  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithFunctionPredicate        10000                   true                      10  avgt   30   226.885 \u00b1  24.685  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithFunctionPredicate        10000                  false                      10  avgt   30   274.404 \u00b1  18.313  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithFunctionPredicate        60000                   true                      10  avgt   30   197.643 \u00b1  11.469  ms/op\r\nBenchmarkInequalityJoin.benchmarkJoinWithFunctionPredicate        60000                  false                      10  avgt   30   210.390 \u00b1  15.268  ms/op\r\nBenchmarkInequalityJoin.benchmarkRangePredicateJoin                 100                   true                      10  avgt   30   247.428 \u00b1  11.549  ms/op\r\nBenchmarkInequalityJoin.benchmarkRangePredicateJoin                 100                  false                      10  avgt   30  2487.442 \u00b1  60.085  ms/op\r\nBenchmarkInequalityJoin.benchmarkRangePredicateJoin                1000                   true                      10  avgt   30   240.810 \u00b1  14.584  ms/op\r\nBenchmarkInequalityJoin.benchmarkRangePredicateJoin                1000                  false                      10  avgt   30   527.124 \u00b1  47.483  ms/op\r\nBenchmarkInequalityJoin.benchmarkRangePredicateJoin               10000                   true                      10  avgt   30   226.683 \u00b1  11.559  ms/op\r\nBenchmarkInequalityJoin.benchmarkRangePredicateJoin               10000                  false                      10  avgt   30   270.130 \u00b1  13.167  ms/op\r\nBenchmarkInequalityJoin.benchmarkRangePredicateJoin               60000                   true                      10  avgt   30   226.237 \u00b1   8.305  ms/op\r\nBenchmarkInequalityJoin.benchmarkRangePredicateJoin               60000                  false                      10  avgt   30   218.149 \u00b1   9.184  ms/op\r\n```\r\n@losipiuk ", "NaN"], ["8617", "Improve Page splitting for Pages that hold certain types of Blocks", "Nezih Yigitbasi", "nezihyigitbasi", "08/07/17, 09:35:57 PM", "For Pages that hold certain types of Blocks, such as RLE blocks, the size in\r\nbytes will remain constant through the recursion. For such cases the\r\nrecursion will only terminate when the positions in the Page is one, resulting\r\nin potentially a large number of Pages of size one.", "NaN"], ["8618", "Upgrade to ASM 5.2", "Martin Traverso", "martint", "07/27/17, 10:06:49 PM", "Fixes an issue due to invalid bytecode when invoking static interface\r\nmethods in Java 9, which has stricter verification.\r\n\r\n    java.lang.IncompatibleClassChangeError: Method com.facebook.presto.operator.project.PageFilter.positionsArrayToSelectedPositions([ZI)Lcom/facebook/presto/operator/project/SelectedPositions; must be InterfaceMethodref constant\r\n        at com.facebook.presto.$gen.PageFilter_35.filter(Unknown Source)\r\n        at com.facebook.presto.operator.project.DictionaryAwarePageFilter.filter(DictionaryAwarePageFilter.java:87)\r\n        at com.facebook.presto.operator.project.PageProcessor.process(PageProcessor.java:81)\r\n        ...\r\n\r\nRelates to https://bugs.openjdk.java.net/browse/JDK-8147755", "NaN"], ["8619", "Use Set to check if output symbols are correct", "Grzegorz Kokosi\u0144ski", "kokosing", "07/28/17, 07:16:45 AM", "Use Set to check if output symbols are correct\n\nUsing List causes n*n complexity to check if given collection contains\nall elements from other collection. It could lead to performance\ndegradation in case of join where relations have plenty of columns.", "NaN"], ["8620", "Prefer PlanNodeSearcher over custom plan node visitors", "Grzegorz Kokosi\u0144ski", "kokosing", "08/09/17, 12:16:59 PM", "Prefer PlanNodeSearcher over custom plan node visitors", "NaN"], ["8621", "Properly release native buffers when RcFileWriter is closed", "Nezih Yigitbasi", "nezihyigitbasi", "07/28/17, 02:07:16 AM", "When the RcFileWriter is closed it destroys the column encoders, which in turn\r\ndestroys the output stream. However, for the output streams created with `AircompressorCompressor` the on destroy action is nop resulting in a leak.", "NaN"], ["8623", "Remove legacy block representation for map", "Haozhun Jin", "haozhun", "08/07/17, 05:50:19 PM", "NaN", "NaN"], ["8624", "Make Table Stats Configurable", "Zhenxiao Luo", "zhenxiao", "08/21/17, 05:42:51 PM", "Hit performance issue when large number of partitions:\r\nhttps://groups.google.com/forum/#!topic/presto-users/WPevZUAGo4E\r\nSo users could disable it", "NaN"], ["8629", "Use dictionary mask to avoid block copy in join", "James Sun", "highker", "11/29/17, 02:25:59 AM", "A chain of joins in a single driver can lead to unnecessary copies of\r\nblocks. The patch introduces masks to dictionary blocks so that during\r\nthe join phase, we generate dictionary ids and mask them on top of the\r\noriginal block to be a dictionary view. For now, only the probe side is\r\nwith masks.\r\n\r\nbenchmark result:\r\n```\r\nbefore: sql_consecutive_join :: 5450.407 cpu ms :: 17.2MB peak memory \r\nafter:  sql_consecutive_join :: 2478.944 cpu ms :: 17.2MB peak memory\r\n```\r", "NaN"], ["8631", "Fix TestFullOrcReader", "Dain Sundstrom", "dain", "07/29/17, 11:16:32 PM", "NaN", "NaN"], ["8632", "Fix CORR for negative correlations", "Martin Traverso", "martint", "08/01/17, 02:02:48 AM", "The function is incorrectly implemented as:\r\n\r\n    SQRT( COVAR(x,y)^2 / (VAR(x) * VAR(y)) )\r\n\r\ndue to a bug in the SQL spec (section 10.9.8.c.x.3). The correct formula is:\r\n\r\n    COVAR(x, y) / SQRT(VAR(x) * VAR(y))\r\n\r\nAs a result, negative correlations are incorrectly indicated\r\nas being positive.", "NaN"], ["8634", "Checkstyle for whitespaces before closing brackets", "Grzegorz Kokosi\u0144ski", "kokosing", "08/04/17, 08:35:37 AM", "Checkstyle for whitespaces before closing brackets", "NaN"], ["8641", "Remove obsolete test files for old Hive versions", "David Phillips", "electrum", "08/23/17, 06:16:04 PM", "NaN", "NaN"], ["8642", "Remove unused Hive tests for DWRF", "David Phillips", "electrum", "08/08/17, 11:21:26 PM", "NaN", "NaN"], ["8644", "Make line wrapping consistent in HiveStorageFormat", "David Phillips", "electrum", "08/04/17, 05:39:09 PM", "NaN", "NaN"], ["8645", "Add rmiregistry port to deployment documentation", "Alex Land", "landalex", "09/25/17, 06:23:49 PM", "The JMX `rmiregistry` port is a required property to be able to connect to the JMX server. The documentation currently only mentions the `rmiserver` port, which the registry uses to redirect client requests, but clients actually connect to the `rmiregistry` port.\r\n\r\nI have completed the Facebook CLA for this GitHub account, but this is my first contribution.", "NaN"], ["8647", "Add function to convert interval day to second to millis", "Raghav Sethi", "raghavsethi", "08/01/17, 07:34:28 PM", "NaN", "NaN"], ["8648", "Add documentation for to_milliseconds function", "Raghav Sethi", "raghavsethi", "08/02/17, 06:55:33 PM", "NaN", "NaN"], ["8649", "Bump tempto", "Anton", "petroav", "08/04/17, 10:08:54 AM", "NaN", "NaN"], ["8653", "Remove trailing spaces from CHAR function results", "Christina Wallin", "cawallin", "08/04/17, 10:08:12 AM", "CHAR values are stored in Slices without the trailing spaces (and there is a checkState\r\nfor that). Some CHAR functions can produce values with a space at the end (for example,\r\nsubstr if there are spaces in the CHAR that you are taking a substring of), so those\r\nfunctions need to have the trailing spaces trimmed too.", "NaN"], ["8655", "Add dedicated RowBlock ", "Wenlei Xie", "wenleix", "08/26/17, 05:14:59 AM", "Ready for review.\r\nNote the final 3 migration commits are somewhat separated arbitrarily. We can probably merge them into one given they are not too large.\r\n\r\n    The current internal representation of a row type in Presto is an\r\n    interleaved block wrapped by an array block. A dedicated RowBlock is\r\n    easier to understand.\r\n\r\n\r", "NaN"], ["8662", "Add 0.182 release notes", "Nezih Yigitbasi", "nezihyigitbasi", "08/03/17, 08:17:48 PM", "NaN", "NaN"], ["8664", "Implement compareTo in AbstractSingleMapBlock", "Wenlei Xie", "wenleix", "08/07/17, 08:13:12 PM", "Block.compareTo should be implemented if Block.getSlice is implemented.", "NaN"], ["8665", "Fix version in presto-matching module", "Grzegorz Kokosi\u0144ski", "kokosing", "08/04/17, 07:40:09 AM", "NaN", "NaN"], ["8667", "Improve detection of completely corrupt ORC files", "David Phillips", "electrum", "08/04/17, 07:18:16 PM", "Validate that post script size is less than file size. This handles\nthe case of small files that are not ORC files.", "NaN"], ["8668", "Require coalesce to have at least two arguments", "David Phillips", "electrum", "08/23/17, 05:52:46 PM", "The SQL specification syntactically allows a single argument, but does\nnot contemplate it in the description of the semantics. Most commercial\ndatabases require at least two arguments, which indicates that this is\na correct interpretation of the specification.", "NaN"], ["8669", "Support statistics in TPCH connector", "Grzegorz Kokosi\u0144ski", "kokosing", "08/08/17, 07:34:48 PM", "Support statistics in TPCH connector", "NaN"], ["8670", "Simple test for HiveMetadata.getTableStatistics()", "Alan Post", "alandpost", "08/09/17, 04:45:43 AM", "Make sure that requesting stats for partitioned and unpartitioned Hive\r\ntables returns stats for the correct set of columns.  Do not actually\r\nvalidate the returned stats, which is complicated as they are floating\r\npoint numbers.", "NaN"], ["8681", "Update use of several deprecated methods/fields", "Nezih Yigitbasi", "nezihyigitbasi", "11/22/17, 06:21:03 PM", "This PR updates the use of four deprecated methods/fields:\r\n- `Throwables.propagate`\r\n    - I updated the use for **only** checked exceptions (as unchecked exceptions probably need a more case by case analysis). Currently for checked exceptions the behavior of `Throwables.propagate` is to simply throw a `RuntimeException` wrapping that. I did the same for all checked exceptions except `IOException`, where I used `UncheckedIOException`.\r\n- `propagateIfInstanceOf`\r\n- `JAVA_ISO_CONTROL`\r\n- `SimpleType`", "NaN"], ["8684", "Inline Rule#withMatch method which was used only in tests", "Grzegorz Kokosi\u0144ski", "kokosing", "08/07/17, 10:19:27 AM", "NaN", "NaN"], ["8686", "Do not use raw generic in RuleAssert", "Grzegorz Kokosi\u0144ski", "kokosing", "08/09/17, 04:40:49 AM", "Do not use raw generic in RuleAssert", "NaN"], ["8689", "Add overhead to slice array block memory counting", "James Sun", "highker", "08/08/17, 01:21:34 AM", "Reference counting in block memory counting misses overheads of Slice.\r\nThis has led to OOM and fast full GC issues in production.", "NaN"], ["8690", "Fix create table if not exists", "Nezih Yigitbasi", "nezihyigitbasi", "09/28/17, 07:56:02 PM", "Fixes #8607.\r\n\r\nThis PR adds a new method to the connector interface that takes a `failIfExists` flag when creating a table, and propagates that all the way down to the connectors. \r\n\r\nAlso, for the case where table already exists we added checks to see whether the existing table has the same schema with the new table to minimize the risk of problems later in a transaction (e.g., a tx with a create table and then an insert).", "NaN"], ["8691", "Merge tiny pages returned from PageProcessor", "Andrii Rosa", "arhimondr", "11/08/17, 01:22:14 PM", "FilterAndProject and ScanFilterAndProject operators may produce\r\ntiny pages (few rows) in case of highly selective filter.\r\n\r\nSuch tiny pages introduce considerable synchronization overhead\r\nin subsequent operators. In particular HashBuilderOperator\r\ndoes memory reservations for every single page. Considering that\r\nmemory manager is all the way synchronized - most of the time spent\r\nby worker thread is just waiting for a lock.", "NaN"], ["8692", "Migrate map_filter function to use MapBlock", "Haozhun Jin", "haozhun", "08/07/17, 10:42:04 PM", "NaN", "NaN"], ["8693", "Simplify CacheLoader creation using lambdas", "David Phillips", "electrum", "09/28/17, 02:49:28 PM", "NaN", "NaN"], ["8696", "Fix memory accounting in HashAggregationOperator", "Rongrong Zhong", "rongrong", "08/11/17, 08:24:39 PM", "Resolves #8694 ", "NaN"], ["8699", "Fix handling of invalid UTF-8 in ORC/DWRF stats", "Dain Sundstrom", "dain", "08/16/17, 03:15:26 AM", "NaN", "NaN"], ["8700", "Expose TPCDS statistics", "Grzegorz Kokosi\u0144ski", "kokosing", "08/10/17, 04:33:06 AM", "Expose TPCDS statistics", "NaN"], ["8702", "Dedup retained size for PageProcessor", "James Sun", "highker", "08/09/17, 01:44:21 AM", "PageProcessor can save previously computed results, which contribute to\r\nits retained size. There are cases where the previous computed results\r\nare the same as the blocks in the input page. We need to avoid double\r\ncounting the size for such cases.", "NaN"], ["8704", "Implement map_entries function", "Leiqing Cai", "caithagoras", "08/15/17, 02:26:34 AM", "NaN", "NaN"], ["8705", "Fix formatting in EXPLAIN ANALYZE output", "Nezih Yigitbasi", "nezihyigitbasi", "08/09/17, 05:24:36 PM", "NaN", "NaN"], ["8706", "Add JSON to ROW cast", "Wenlei Xie", "wenleix", "09/14/17, 06:12:25 AM", "Ready for review.\r\n\r\nBased on https://github.com/prestodb/presto/pull/8655. The first 6 commits are irrelevant  .\r\n\r\nThis is part of https://github.com/prestodb/presto/issues/8175", "NaN"], ["8707", "Prune QueryStats when pruning QueryInfo", "Nezih Yigitbasi", "nezihyigitbasi", "08/20/17, 07:16:49 PM", "When expired queries are pruned by the SqlQueryManager QueryStats were not pruned\r\nat all, which may hold onto a good amount of memory through the OperatorStats.\r\nEspecially the ExchangeClientStatus (implements OperatorInfo) can get huge in a large\r\ncluster.\r\n\r\nWe have seen cases where these exchange statuses were filling the heap causing full GCs on the coordinator.", "NaN"], ["8708", "Fix flaky TestQueues test", "Nezih Yigitbasi", "nezihyigitbasi", "08/09/17, 08:40:53 PM", "Closing DistributedQueryRunner can take a variable amount of time\r\ncausing test timeouts. With this change I move the management of the runner\r\nout of the unit tests to separate setup/tearDown methods to reduce the flakiness.\r\n\r\nShould fix #8602, #8589.\r\n\r\nAlso includes #8650.", "NaN"], ["8710", "Use automatic width for tables in documentation", "David Phillips", "electrum", "08/23/17, 03:35:13 PM", "NaN", "NaN"], ["8712", "Return VariableWidthBlock in slice direct reader", "James Sun", "highker", "08/26/17, 01:00:43 AM", "SliceArrayBlock creates huge overhead for each Slice created. Slice\r\narray sometimes can contain 1M entries which leads to about 56M\r\noverhead. This causes full GC issues in production. Switch to\r\nVariableWidthBlock to save memory.", "NaN"], ["8715", "Use timezone different Asia/Kathmandu timezone in product tests", "Andrzej Fiedukowicz", "fiedukow", "08/10/17, 04:18:28 PM", "Product tests are using UTC for simplicity but it doesn't cover all scenarios that we are interested in which can (and did) cause bug to remain undetected.\r\nThis is especially important in the context of #7480 which will fail those tests at this point. Some adaptation and fixes are meant to be done in #7480 after rebasing it into those patches.\r\n\r\nCC: @haozhun ", "NaN"], ["8718", "Restore SchedullingOrderVisitor", "Grzegorz Kokosi\u0144ski", "kokosing", "08/11/17, 07:56:56 AM", "Restore SchedullingOrderVisitor", "NaN"], ["8719", "Revert the code cache collection threshold to 70%", "Nezih Yigitbasi", "nezihyigitbasi", "08/16/17, 06:10:52 PM", "Initially the threshold for triggering a full GC to workaround the code cache bug (in some JVM versions) was 70%. At some point the threshold was made configurable (in #6286) and the threshold was set to 40%, which is quite low and has caused issues for multiple users (and myself previously).\r\n\r\nWe don't know whether the latest JVM releases have this bug, but until we remove this workaround we can increase this threshold.", "NaN"], ["8720", "Update product-tests timezone setup doc for manual run", "Andrzej Fiedukowicz", "fiedukow", "08/11/17, 11:37:53 AM", "NaN", "NaN"], ["8722", "Add queued query statistics", "German Gil", "ggilfb", "08/21/17, 04:13:50 PM", "Adding the number of queued queries at any given point, this is similar to the number of running queries. Also adding the queued time.\r", "NaN"], ["8725", "Add ArgumentProperty to describe scalar function argument", "Wenlei Xie", "wenleix", "10/13/17, 05:03:14 AM", "\r\n\r\nScalar function already has the following three properties:\r\n- nullable\r\n- hasNullFlag\r\n- lambdaInterface\r\n\r\nThis makes code difficult to understand, and difficult to maintain\r\nin the feature if more properties need to be added.\r\n\r\nThis commit refactors existing three properties into ArgumentOption.", "NaN"], ["8726", "Add yield signals to page projection", "James Sun", "highker", "09/25/17, 10:25:03 PM", "Trunk (with `PageProjection` generated and without checking yield signal):\r\n```\r\nBenchmark                          Mode  Cnt     Score     Error  Units\r\nBenchmarkPageProcessor.compiled   thrpt   50  5266.166 \u00b1 170.086  ops/s\r\nBenchmarkPageProcessor.handCoded  thrpt   50  6046.107 \u00b1  25.990  ops/s\r\n```\r\n\r\nWith both `PageProjectionOutput` and `PageProjection` generated:\r\n```\r\nBenchmark                          Mode  Cnt     Score    Error  Units\r\nBenchmarkPageProcessor.compiled   thrpt   50  5399.072 \u00b1 71.479  ops/s\r\nBenchmarkPageProcessor.handCoded  thrpt   50  6041.955 \u00b1 32.595  ops/s\r\n```\r\n\r\nWith `PageProjectionOutput` generated and `PageProjection` hand-coded:\r\n```\r\nBenchmark                          Mode  Cnt     Score    Error  Units\r\nBenchmarkPageProcessor.compiled   thrpt   50  5362.882 \u00b1 45.355  ops/s\r\nBenchmarkPageProcessor.handCoded  thrpt   50  5967.048 \u00b1 81.459  ops/s\r\n```", "NaN"], ["8728", "Make sure everything gets closed in SpillableHashAggregationBuilder", "Piotr Findeisen", "findepi", "08/28/17, 01:12:44 PM", "NaN", "NaN"], ["8736", "Provide final stats in JDBC progress callback", "David Phillips", "electrum", "08/23/17, 03:02:13 PM", "Always invoke the callback with the final stats at query completion.", "NaN"], ["8739", "Handle GROUPING when aggregation expressions require implicit coercions", "Martin Traverso", "martint", "08/15/17, 04:49:04 AM", "During planning of aggregation operations, the code was returning early\r\nif the aggregation expressions required implicit coercions.\r\n\r\nAs a result, GROUPING was not being rewritten, causing downstream\r\nfailures during planning.\r\n\r\nFixes #8738", "NaN"], ["8740", "Synchronize access to reservedBytes and reservedRevocableBytes", "Nezih Yigitbasi", "nezihyigitbasi", "08/15/17, 07:13:26 AM", "NaN", "NaN"], ["8741", "Remove explicit joda-time version in presto-tpcds", "\u0141ukasz Osipiuk", "losipiuk", "08/15/17, 07:58:03 AM", "NaN", "NaN"], ["8744", "Set digest type and functions", "Martin Traverso", "martint", "01/18/18, 06:56:41 PM", "Function names and type TBD.\r\n\r\nAddresses #8401", "NaN"], ["8745", "Fix timestamp escaping in FileHiveMetastore", "Rebecca Schlussel", "rschlussel-zz", "11/24/17, 12:36:12 AM", "Filtering on char(x) or long decimal partition columns would return zero\r\nrows. This was because when we checked if we could prune any partitions,\r\nwe didn't convert the filter values correctly from slices to strings, so\r\nthe metastore would think there were no matching partitions.\r\n\r\nFixes #8743 ", "NaN"], ["8746", "Fix AllColumn formatting for SqlFormatter", "Leiqing Cai", "caithagoras", "08/18/17, 12:05:17 AM", "When formatting all columns, t.* will be formatted as t.* but should be \"t\".*, in case the prefix is a reserved word.", "NaN"], ["8748", "Fix error categorization in local file connector", "David Phillips", "electrum", "08/23/17, 06:03:53 PM", "NaN", "NaN"], ["8749", "Fix test concurrency in TestPageProcessorCompiler", "Dain Sundstrom", "dain", "08/17/17, 09:09:27 PM", "NaN", "NaN"], ["8751", "Set alwaysRun = true on all @AfterClass", "Dain Sundstrom", "dain", "08/23/17, 02:33:12 PM", "NaN", "NaN"], ["8752", "Return TPCDS statistics values in Presto internal representation", "Grzegorz Kokosi\u0144ski", "kokosing", "08/22/17, 08:42:07 AM", "Return TPCDS statistics values in Presto internal representation", "NaN"], ["8754", "Add release notes for 0.183", "Haozhun Jin", "haozhun", "08/17/17, 05:29:27 PM", "NaN", "NaN"], ["8755", "Remove redundant field from AbstractSingleMapBlock", "Haozhun Jin", "haozhun", "08/22/17, 06:10:19 PM", "NaN", "NaN"], ["8756", "Track output buffer memory usage in the task context", "Nezih Yigitbasi", "nezihyigitbasi", "08/20/17, 07:17:33 PM", "We were tracking the system memory usage of the output buffers in the query context instead of task context, this PR fixes that.", "NaN"], ["8764", "Fixes in product tests execution script c'ed", "Maciej 'mac' Grzybek", "maciejgrzybek", "08/25/17, 07:33:34 AM", "Supersedes https://github.com/prestodb/presto/pull/8685", "NaN"], ["8766", "Reduce scheduler contention", "Raghav Sethi", "raghavsethi", "08/30/17, 02:36:38 PM", "NaN", "NaN"], ["8767", "Fix warnings and cleanup code in SpnegoFilter", "David Phillips", "electrum", "10/06/17, 07:15:44 PM", "NaN", "NaN"], ["8768", "Remove JSON serialization from metastore Action", "David Phillips", "electrum", "08/21/17, 10:13:33 PM", "This is vestigial from the development process and is not used.", "NaN"], ["8770", "Fix 'Cannot cast' wording", "Piotr Findeisen", "findepi", "08/21/17, 08:48:32 PM", "NaN", "NaN"], ["8772", "Fix theoretical race condition in getSizeInBytes()", "Piotr Findeisen", "findepi", "08/21/17, 05:42:00 PM", "found by @sopel39 ", "NaN"], ["8774", "Fix bug in benchmark binary file spiller where pagebuilder is not reset between accumulating pages", "Marc Beitchman", "mbeitchman", "08/21/17, 07:02:20 AM", "The pagebuilder should be reset so it\r\ndoesn't accumulate pages during each loop.", "NaN"], ["8775", "Fix TestFullOrcReader", "Dain Sundstrom", "dain", "08/17/17, 10:53:36 PM", "NaN", "NaN"], ["8777", "Make TestMinWorkerRequirement single threaded", "David Phillips", "electrum", "08/18/17, 09:27:32 PM", "This avoids creating multiple query runners at once.", "NaN"], ["8778", "Reduce potential memory usage in TestMemoryManager", "David Phillips", "electrum", "08/18/17, 10:09:13 PM", "NaN", "NaN"], ["8779", "Fix flaky test due to double arithmetic in AbstractTestQueries", "Haozhun Jin", "haozhun", "08/20/17, 05:06:49 AM", "NaN", "NaN"], ["8781", "Add UI to show worker status, memory pools and thread stacks", "Raghav Sethi", "raghavsethi", "08/31/17, 01:49:07 AM", "![UI Screenshot](https://user-images.githubusercontent.com/715788/29480867-8ff1b9f4-8430-11e7-9621-778d170d792c.png)\r", "NaN"], ["8782", "Fix Unnest.equals() by comparing field withOrdinality", "Leiqing Cai", "caithagoras", "08/21/17, 07:20:50 PM", "NaN", "NaN"], ["8783", "Move formatting logic in Unnest.toString() into SqlFormatter", "Leiqing Cai", "caithagoras", "08/20/17, 10:04:40 PM", "NaN", "NaN"], ["8784", "Remove AssertJ dependency which is now in Airbase", "David Phillips", "electrum", "08/23/17, 06:05:55 PM", "NaN", "NaN"], ["8785", "Add yield signals to page processors", "James Sun", "highker", "08/24/17, 08:17:41 PM", "Wire yield signals into the iterator created by page processors. During\r\nthe computation of the next page, it may pause and return null pages.\r\nThe computation will continue when the iterator is called again. The\r\nyield is at column processing granularity.\r\n\r\n\r\n**Further splitting the PR into two (together with https://github.com/prestodb/presto/pull/8726) given that one would be rather complicated; let's move one step a time.**", "NaN"], ["8791", "Add Property node to treat property keys as identifiers ", "Leiqing Cai", "caithagoras", "10/04/17, 07:18:05 PM", "Table/Schema properties were represented as String-to-Expression map,\r\nwhich cannot handle delimited identifiers correctly as quotes are\r\ntreated as part of the property name.\r\nCreate SQL tree node Property to replace the maps used in CreateSchema,\r\n CreateTable, and CreateTableAsSelect.", "NaN"], ["8792", "Block memory accounting fix and refactor", "Wenlei Xie", "wenleix", "08/22/17, 01:21:04 AM", "NaN", "NaN"], ["8799", "Clean up TestQuerySpillLimits", "Piotr Findeisen", "findepi", "08/24/17, 01:50:31 PM", "Fixes #8798", "NaN"], ["8808", "Refactor expression rewrite rules", "Szymon Matejczyk", "szymonm", "08/23/17, 05:11:05 AM", "Make ExpressionRewriteRules static and public for use outside of ExpressionReweriteRuleSet.", "NaN"], ["8809", "Migrate desugaring expression to iterative optimizer", "Szymon Matejczyk", "szymonm", "09/07/17, 04:59:24 AM", "NaN", "NaN"], ["8810", "Fix split_to_map function to produce map block", "Haozhun Jin", "haozhun", "08/22/17, 06:27:53 PM", "This fixes query involving `split_to_map(...)[...]` where f can be any function\r\nor operator.\r\n\r\nPreviously, split_to_map produces a VariableWidthBlock where keys and values\r\ntake even and odd positions in the same Block. This clever trick no longer\r\nworks now that maps are required to use map block.", "NaN"], ["8811", "Fix tests that produce legacy block for maps", "Haozhun Jin", "haozhun", "08/22/17, 11:36:58 PM", "NaN", "NaN"], ["8813", "Improve map element_at performance by utilizing MapBlock O(1) access", "Haozhun Jin", "haozhun", "08/24/17, 10:41:03 PM", "NaN", "NaN"], ["8814", "Create build directory for presto-docs", "David Phillips", "electrum", "08/23/17, 02:36:33 PM", "The Thrift IDL generator requires it to exist. Previously, the build\nonly worked because various checkers would create the directory, and\nit would fail if they were disabled.", "NaN"], ["8815", "Add lookupSourcePositions to JoinOperatorInfo", "Raghav Sethi", "raghavsethi", "08/24/17, 07:35:31 PM", "NaN", "NaN"], ["8819", "Refactor join yield tests", "Piotr Findeisen", "findepi", "08/24/17, 03:07:10 PM", "NaN", "NaN"], ["8820", "Cassandra fix flaky test", "Andrii Rosa", "arhimondr", "08/23/17, 05:36:58 PM", "Fixes #8788", "NaN"], ["8821", "Add additional context information to HdfsEnvironment", "David Phillips", "electrum", "08/23/17, 10:56:16 PM", "Expose user, queryId, schema and table to HdfsEnvironment so file system\nimplementations can attribute usage to a query.", "NaN"], ["8822", "Don't override sort order for duplicated symbols", "Maciej 'mac' Grzybek", "maciejgrzybek", "09/14/17, 05:37:00 AM", "In order to handle cases like \"ORDER BY a ASC, a DESC\" correctly we must not override the orderings.\r\nCorrect semantics according to ISO/IEC 9075-2:2011(E) 10.10 (especially relevant here is 10.10.h) is to consider sort key in ordering only if previously encountered sort keys didn't establish the order.\r\nIt is not possible to have a tie which could have been broken by having the same symbol further in the <sorting specification list>, unless we extend <sort specification> to support more than just <ordering specifcation> and <null ordering>, e.g. by adding collations.\r\n\r\n---\r\n\r\nFixes https://github.com/prestodb/presto/issues/8816", "NaN"], ["8824", "Add source to ConnectorSession", "David Phillips", "electrum", "08/23/17, 08:41:10 PM", "NaN", "NaN"], ["8829", "Run PruneCountAggregationOverScalar only for single output default aggregation", "Grzegorz Kokosi\u0144ski", "kokosing", "08/25/17, 07:19:29 AM", "Run PruneCountAggregationOverScalar only for single output default aggregation", "NaN"], ["8830", "Allow overriding verifier database connection acquisition", "David Phillips", "electrum", "08/25/17, 04:08:40 PM", "NaN", "NaN"], ["8831", "Rename grammar nodes tableProperty and tableProperties", "Leiqing Cai", "caithagoras", "08/28/17, 10:17:22 PM", "CreateSchema uses the tableProperty nodes to represent schema property,\r\nand thus they should be changed to use a more general name.", "NaN"], ["8832", "Preserve AST order when formatting properties", "Leiqing Cai", "caithagoras", "09/08/17, 09:42:50 PM", "NaN", "NaN"], ["8833", "Add support for zero-argument lambda expression", "Haozhun Jin", "haozhun", "08/24/17, 10:36:58 PM", "This is pre-requisite for re-implementing try as a syntactic sugar.", "NaN"], ["8835", "Update tempto to 1.36", "Grzegorz Kokosi\u0144ski", "kokosing", "09/13/17, 06:18:07 AM", "    Update tempto to 1.36\r\n    \r\n    Changes:\r\n     - using sf1 scale for TPCH (instead of tiny). Result files were\r\n     generated by reverting this commit (417622c).\r\n     - inject statistics into hive for TPCH and TPCDS tables.\r\n     Stats are not injected when kerberos is used for Hive.\r\n    \r\n    Motivation behind using sf1:\r\n     - expected better behaviour of cost based optimizer rules, since\r\n     differences in sizes of tables are bigger\r\n     - do not hide memory related issues. General expectancy (rule) is that sf1\r\n     TPCH queries should pass on Presto with Xmx=2G. There might be valid\r\n     exception to this, but IMO it has to be understood why given such\r\n     exceptional query consumes so much memory. Anyway, today all queries\r\n     passes, so when something change then we will get regression.\r\n    \r\n    Cons of using sf1:\r\n     - longer product-test travis job, now it 10 minutes more (40 min in total).\r\n\r", "NaN"], ["8841", "Fix invalid array_min/max result of containing NaN", "Yuya Ebihara", "ebyhr", "11/15/17, 07:34:13 PM", "Fix #8836 ", "NaN"], ["8847", "Small refactorings around IterativeOptimizer", "Grzegorz Kokosi\u0144ski", "kokosing", "09/07/17, 07:15:51 AM", "NaN", "NaN"], ["8850", "Make resolveGroup to accept only GroupReference", "Grzegorz Kokosi\u0144ski", "kokosing", "08/29/17, 04:38:38 AM", "Make resolveGroup to accept only GroupReference\n\nresolveGroup suggests that it resolves groups (GroupReference`s).\nAccepting regular plan nodes is error-prone, because:\n - it returns proper node even if the node is not stored within a Memo\n - it is not defensive as it does not matter what caller pass there,\n method is going to work anyway\n - resolving regular plan nodes does not make much sense", "NaN"], ["8851", "Use Rule.Result instead of Optional for result of Rule#apply", "Grzegorz Kokosi\u0144ski", "kokosing", "09/13/17, 07:16:50 AM", "Use Rule.Result instead of Optional for result of Rule#apply", "NaN"], ["8855", "Allow max-memory-per-node to be exactly (xmx - reservedMemory)", "Andrii Rosa", "arhimondr", "08/29/17, 07:36:33 AM", "Now it is possible to get a very weird error message:\r\n\r\n```\r\nquery.max-memory-per-node set to 9GB, but only 9663676416B of useable heap available\r\n```", "NaN"], ["8856", "Use TupleDomain.all() in the blackhole TableLayout", "Rebecca Schlussel", "rschlussel-zz", "08/28/17, 09:25:40 PM", "The TupleDomain for the table layouts in the blackhole connector should\r\nbe TupleDomain.all() instead of none() since the blackhole connector\r\nproduces zeroed data for the tables it creates.", "NaN"], ["8861", "Additional ORC writer fixes", "Dain Sundstrom", "dain", "12/16/17, 02:55:46 AM", "NaN", "NaN"], ["8864", "Migrate simplify expressions", "Szymon Matejczyk", "szymonm", "09/07/17, 05:02:35 AM", "NaN", "NaN"], ["8868", "Add Checkstyle rule to avoid statically importing copyOf and valueOf", "Wenlei Xie", "wenleix", "09/06/17, 08:39:52 PM", "NaN", "NaN"], ["8870", "Add stat for number of queries queued on non-leaf groups", "Raghav Sethi", "raghavsethi", "08/31/17, 02:40:52 AM", "This stat is more accurate than simply counting all queries queued on\r\nthe root resource group, because it caps the number at each leaf to\r\nthe maximum runnable at that node. However, this is still imprecise,\r\nbecause it assumes that internal (non-leaf) resource groups are\r\nconfigured perfectly.", "NaN"], ["8872", "Fix warnings in StatementClient", "David Phillips", "electrum", "10/06/17, 07:01:12 PM", "NaN", "NaN"], ["8873", "Fix semantic check of CreateTable and CreateSchema on properties", "Leiqing Cai", "caithagoras", "09/09/17, 12:03:25 AM", "Each table or schema property value in CreateTable and CreateSchema\r\nshould constant, like what is done in CreateTableAsSelect.", "NaN"], ["8875", "Add retry interval when adding column in raptor", "Jiexi Lin", "jessesleeping", "09/12/17, 05:13:15 PM", "The wait interval is hardcoded to be 3 seconds and will retry for up to 100 times which add up to 5 min. ALTER TABLE failure should be rare and we would like to ensure that we make enough retries.\r\n\r\n#8874 ", "NaN"], ["8877", "Remove legacy InterleavedBlock ", "Wenlei Xie", "wenleix", "08/30/17, 02:00:47 AM", "NaN", "NaN"], ["8879", "Make sure RcFileWriter closer registers all streams", "James Sun", "highker", "08/30/17, 09:35:29 PM", "We have seen in production where writeRowGroup() throws an exception\r\nthat causes output streams fail to close.\r\n\r\ne.g.:\r\n```\r\nCaused by: java.io.UncheckedIOException: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: \r\n        ...\r\n        at io.airlift.slice.OutputStreamSliceOutput.writeToOutputStream(OutputStreamSliceOutput.java:355)\r\n        at io.airlift.slice.OutputStreamSliceOutput.writeBytes(OutputStreamSliceOutput.java:178)\r\n        at io.airlift.slice.OutputStreamSliceOutput.writeBytes(OutputStreamSliceOutput.java:169)\r\n        at com.facebook.presto.rcfile.RcFileWriter.writeRowGroup(RcFileWriter.java:319)\r\n        at com.facebook.presto.rcfile.RcFileWriter.close(RcFileWriter.java:195)\r\n        at com.facebook.presto.hive.RcFileFileWriter.rollback(RcFileFileWriter.java:150)\r\n        ... 23 more\r\n```", "NaN"], ["8882", "Use more precise pattern in PickTableLayout", "Grzegorz Kokosi\u0144ski", "kokosing", "09/07/17, 05:04:11 AM", "Use more precise pattern in PickTableLayout", "NaN"], ["8884", "Sort connector list in documentation", "David Phillips", "electrum", "08/30/17, 09:41:07 PM", "NaN", "NaN"], ["8886", "Update concepts documentation", "David Phillips", "electrum", "08/30/17, 07:03:30 PM", "NaN", "NaN"], ["8889", "Convert TRY to a function (desugar rewrite)", "Shixuan Fan", "shixuan-fan", "09/15/17, 07:07:18 AM", "Settled #8221 \r\n\r\n-------------\r\n\r\nFuture work: refactor LambdaAndTryExpressionExtractor and PageFunctionCompiler.", "NaN"], ["8893", "Remove loadedBlockBytes stats", "James Sun", "highker", "08/31/17, 01:04:23 AM", "Adding stats for every block loaded is burning CPU. Given the problem of\r\nbounding the size of blocks has been resolved, the stats is no longer\r\nneeded.\r\n\r\nResolves #8885 ", "NaN"], ["8896", "Add substr function for varbinary", "Liang Xia", "liangxia2006", "09/07/17, 10:51:35 PM", "support substr for varbinary types. It differs from varchar in that varchar works on code points, while varbinary works on bytes.", "NaN"], ["8905", "Add release notes for 0.184", "Raghav Sethi", "raghavsethi", "09/05/17, 11:28:55 PM", "@electrum your notes are also missing.", "NaN"], ["8907", "Fix query stated accounting with failed queries", "Dain Sundstrom", "dain", "09/06/17, 07:48:44 PM", "NaN", "NaN"], ["8916", "Remove duplicate license header", "David Phillips", "electrum", "09/07/17, 03:03:47 PM", "NaN", "NaN"], ["8920", "Use airlift zstd decompressor", "Martin Traverso", "martint", "09/07/17, 04:09:33 PM", "NaN", "NaN"], ["8924", "Update to aircompressor 0.9", "Martin Traverso", "martint", "09/14/17, 12:53:54 AM", "- Fixes a possible buffer overrun on corrupt input\r\n- Improves error message when encountering legacy (v0.7) format", "NaN"], ["8925", "S3 cleanup", "David Phillips", "electrum", "09/19/17, 10:24:50 PM", "NaN", "NaN"], ["8929", "Remove unused Response from task delete buffer", "Dain Sundstrom", "dain", "09/15/17, 07:45:41 PM", "NaN", "NaN"], ["8931", "Remove 0 from the documented index range of element_at", "Anton", "petroav", "09/08/17, 05:42:53 AM", "The element_at function was incorrectly documented as accepting\r\nindex values in the range >= 0. However, SQL array indices start\r\nfrom 1.", "NaN"], ["8932", "Add session's client info to QueryStateInfo", "Rongrong Zhong", "rongrong", "09/12/17, 02:03:28 AM", "NaN", "NaN"], ["8933", "Product test for ignoring bucketing if table is not bucketed", "Anton", "petroav", "11/07/17, 08:15:25 AM", "This test covers the functionality introduced by this commit:\r\n2f12402577207fdce0197ee8af966b01ff23fa0e.", "NaN"], ["8934", "Add sections and table of contents for data types", "David Phillips", "electrum", "09/20/17, 07:59:09 PM", "<img width=\"1157\" alt=\"screen shot 2017-09-07 at 6 17 44 pm\" src=\"https://user-images.githubusercontent.com/9230/30191931-02651e18-93f9-11e7-9b9a-2e6c8df63032.png\">\r", "NaN"], ["8937", "Use older Travis image", "Piotr Findeisen", "findepi", "09/08/17, 08:21:05 PM", "The Travis builds hang on first test of basic product tests.", "NaN"], ["8938", "Add support for using EMRFS with Hive connector", "David Phillips", "electrum", "10/26/17, 09:22:18 PM", "EMRFS provides [consistency](http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-consistent-view.html) for file listings by storing metadata separately in DynamoDB. Ideally, we could integrate this into `PrestoS3FileSystem`, but Amazon doesn't provide an API for this (only a CLI that runs on EMR hosts).\r\n\r\nWhen Presto is setup on EMR, the EMRFS JAR will need to be symlinked into the plugin directory for the Hive connector plugin.\r\n\r\nThis PR is untested -- someone who uses EMR will need to try it: `hive.use-emr-file-system=true`", "NaN"], ["8942", "Add option to enable path style access for hive s3", "darklore", "darklore", "09/27/17, 05:07:55 PM", "Dear team,\r\n\r\nThe current hive connector have a property to set customized S3 endpoint for S3-compatible storage.\r\nBut, the  storage must have virtual host style access. (Example: http://bucketname.s3.amazonaws.com/path/to/object )\r\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html\r\n\r\nFor these storage, this PR adds a property that enables path style access of s3 client in hive connector.\r\n(Example: http://s3.amazonaws.com/bucketname/path/to/object )", "NaN"], ["8943", "Fix DROP COLUMN handling for tables with hidden columns", "David Phillips", "electrum", "09/12/17, 01:34:45 AM", "Disallow dropping hidden columns and only include visible columns when\ndeterming if a table only has one column.", "NaN"], ["8944", "Disable TestDistributedSpilledQueries.testAssignUniqueId", "\u0141ukasz Osipiuk", "losipiuk", "09/19/17, 09:37:12 PM", "The `TestDistributedSpilledQueries.testAssignUniqueId` produced many\r\nspill files on disk. Current merge algorithm in\r\n`SpillableHashAggregationBuilder` opened all the files simultaneously which\r\nresulted in exceedeen system limits of simultaneously opened files.\r\n\r\nThis is workaround fix until https://github.com/prestodb/presto/issues/8926 is resolved\r\n\r\nfixes: #8787", "NaN"], ["8945", "Fix OrcCorruptionException to always include data source", "David Phillips", "electrum", "09/14/17, 12:08:22 AM", "NaN", "NaN"], ["8946", "Spill for join (v2)", "Piotr Findeisen", "findepi", "09/28/17, 10:36:30 AM", "Supersedes #8166.\r\n\r\nSpill for join using revocable memory. Fixes #5897.", "NaN"], ["8948", "Remove unused StreamProperties#exactColumnOrder field", "Piotr Findeisen", "findepi", "09/23/17, 03:31:34 PM", "NaN", "NaN"], ["8951", "Grouped execution support for JOINs with Hive connector", "Haozhun Jin", "haozhun", "12/09/17, 01:49:13 AM", "With the right table organization (e.g. bucketing in Hive), it is possible to process a subset of data at  a time for JOINs. This reduces the amount of memory needed to hold the hash table.\r", "NaN"], ["8952", "Do not allow renaming hidden columns", "David Phillips", "electrum", "09/12/17, 02:09:08 AM", "NaN", "NaN"], ["8956", "Yield HashAggregationOperator if rehash will exceed memory limit", "James Sun", "highker", "10/06/17, 05:25:10 AM", "Rehash can cause 2X memory increase in the worst case. It is dangerous to allocate a hash tabe then report memory usage. We could OOM before hitting the memory limit. In production, we found frequent full GCs due to large hash table allocation. This patch caches unfinished page in the operator if the operator needs to rehash the hash table when there is not enough memory. The operator will yield for memory allocation and retry rehashing once unblocked.\r\n\r\n\r\nSome points worth mentioning:\r\n- I didn't add a 10K limit before `addPage` or `getGroupIds` given the loop now is completed controlled by `Work`. `Work` can decide when to yield.\r\n- I only added yield for `HashAggregationOperator` for now; for all other operators that use `GroupByHash`, we can add them step by step.\r\n\r\nbenchmark:\r\n\r\nbefore\r\n```\r\nBenchmark                               (channelCount)  (groupCount)  (hashEnabled)  Mode  Cnt    Score   Error  Units\r\nBenchmarkGroupByHash.addPagePreCompute               1       3000000           true  avgt  160  268.448 \u00b1 7.902  ns/op\r\nBenchmarkGroupByHash.bigintGroupByHash               1       3000000           true  avgt  160  139.813 \u00b1 4.279  ns/op\r\n```\r\n\r\n\r\n\r\nafter\r\n```\r\nBenchmark                               (channelCount)  (groupCount)  (hashEnabled)  Mode  Cnt    Score   Error  Units\r\nBenchmarkGroupByHash.addPagePreCompute               1       3000000           true  avgt  160  262.753 \u00b1 6.924  ns/op\r\nBenchmarkGroupByHash.bigintGroupByHash               1       3000000           true  avgt  160  140.275 \u00b1 3.867  ns/op\r\n```", "NaN"], ["8961", "Do not construct precondition check message eagerly", "Martin Traverso", "martint", "09/13/17, 05:26:55 AM", "It was being created for every character the code looped over,\r\nwhich causes significant performance problems.", "NaN"], ["8966", "Do not construct check message eagerly", "Piotr Findeisen", "findepi", "09/13/17, 10:20:38 PM", "NaN", "NaN"], ["8972", "Fix incorrect column names in QueryCompletedEvent", "Martin Traverso", "martint", "09/14/17, 07:22:02 PM", "The names were being derived from the toString representation\r\nof Column, which causes them to be incorrectly formatted as\r\n\"Column{xyz, varchar}\".", "NaN"], ["8976", "Update mysql connector version to 5.1.44", "Yi He", "hellium01", "09/15/17, 08:56:41 PM", "There is no change between 5.1.41 and 5.1.44 will break current\r\nbehavior:\r\nhttps://dev.mysql.com/doc/relnotes/connector-j/5.1/en/news-5-1-44.html\r\n\r\nWe need the newer version for new security features.", "NaN"], ["8983", "Fix compilation error", "Nezih Yigitbasi", "nezihyigitbasi", "09/15/17, 08:11:35 PM", "Broken with commit 2bbdb43542044edcbb6cde8dd90af4a784087a2f\r\n\r", "NaN"], ["8985", "Sunset ruleset", "Piotr Findeisen", "findepi", "10/14/17, 07:06:01 PM", "While set or related rules is a useful concept allowing concise definition of multiple related rules, it does not require named interface. Also, there is no befit from having one, since the optimizer cannot leverage group of rules in any way.\r\n\r\nThis PR removes `RuleSet` (and `RuleSetAdapter`), while maintaining existing functionality. This also means that unit tests become more unit.", "NaN"], ["8986", "Read small ORC ranges lazily", "Dain Sundstrom", "dain", "09/19/17, 09:50:30 PM", "Instead of reading small ORC ranges eagerly, lazily read the segments (and\nneighboring segments) only when accessed.  This can reduce the IO load on\nthe storage system for highly filtered queries.", "NaN"], ["8987", "Increase RCFile reader buffer size from 1 to 8 megabytes", "Dain Sundstrom", "dain", "09/19/17, 10:10:47 PM", "NaN", "NaN"], ["8989", "Add detailed documentation for json_format and json_parse", "Wenlei Xie", "wenleix", "09/28/17, 12:01:33 AM", "Explain the difference between:\r\n- json_format vs. CAST(json as VARCHAR)\r\n- json_parse vs. CAST(string as JSON)", "NaN"], ["8992", "Add tags to session", "Rongrong Zhong", "rongrong", "09/27/17, 10:05:36 PM", "NaN", "NaN"], ["8994", "End deflater properly in orc writer", "James Sun", "highker", "09/18/17, 06:50:28 PM", "Make sure end() is called to prevent native memory leak.", "NaN"], ["8996", "Update to slice 0.30", "Nezih Yigitbasi", "nezihyigitbasi", "09/18/17, 06:35:50 PM", "This release updates retained size related methods to return `long` instead of `int`.", "NaN"], ["8998", "Update doc about TRY function in lambda expression", "Shixuan Fan", "shixuan-fan", "09/19/17, 06:23:26 PM", "Follow up for #8889.", "NaN"], ["8999", "Update to H2 1.4.196", "David Phillips", "electrum", "10/06/17, 07:16:49 PM", "NaN", "NaN"], ["9000", "Refactor verifier into verify command", "Wenlei Xie", "wenleix", "12/15/17, 10:18:23 PM", "Part of https://github.com/prestodb/presto/issues/8981\r\n\r\nThis PR just tries to refactor the verify work into `verifier verify`. A separate PR will be sent out for adding `replay` command.\r\n\r\nReady for Review", "NaN"], ["9001", "Align output to buffer size in rcfile writer", "James Sun", "highker", "09/19/17, 10:01:38 PM", "Flushing data in arbitrary sizes can be a problem for memory\r\nfragmentation. This is generally fine unless the output stream can\r\nallocate native memory (e.g., gzip streams). Native memory allocator may\r\nnot be able to compact fragmentation, which can cause native memory OOM.\r\n\r\nResolves #8993", "NaN"], ["9005", "Increase the code cache collection threshold to 70%", "Nezih Yigitbasi", "nezihyigitbasi", "09/22/17, 10:00:13 PM", "[This PR has already been approved before in #8719, at that time we had to revert it as we were doing a release. Since it has been some time I wanted to open a new PR instead of merging it directly].\r\n\r\nInitially the threshold for triggering a full GC to workaround the code cache bug (in some JVM versions) was 70%. At some point the threshold was made configurable (in #6286) and the threshold was set to 40%, which is quite low and has caused issues for multiple users (and myself previously).\r\n\r\nWe don't know whether the latest JVM releases have this bug, but until we remove this workaround we can increase this threshold.", "NaN"], ["9007", "Improve ORC and RCFile writer exception handling", "Dain Sundstrom", "dain", "09/20/17, 01:59:00 AM", "NaN", "NaN"], ["9017", "Prevent zero byte DiskRanges in ORC", "Dain Sundstrom", "dain", "09/21/17, 02:30:59 AM", "NaN", "NaN"], ["9018", "Refactor S3 integration to be optional", "David Phillips", "electrum", "09/23/17, 05:29:10 AM", "This is the refactoring part of #8938", "NaN"], ["9020", "Use BatchScanner when loading non-exact ranges", "Adam J. Shook", "adamjshook", "11/15/17, 07:21:54 PM", "This enables more parallelism by splitting the Range on tablet\r\nboundaries", "NaN"], ["9021", "Avoid binding per each row in MapToMapCast", "Wenlei Xie", "wenleix", "09/28/17, 09:25:48 PM", "When the dependent key or value cast function takes ConnectorSession\r\nas input, MapToMapCast will do the binding per row. This will cause\r\ngenerating excessive BoundMethodHandle classes and result in full GC due\r\nto Metaspace full when the map contains more than 128 entires.\r\n\r\nThis issue has the same root cause with https://github.com/prestodb/presto/issues/7935. To reproduce the issue, first prepare the dataset:\r\n\r\n```SQL\r\nCREATE TABLE  test_10k_rows AS \r\nSELECT id as rid\r\nFROM\r\n(\r\n  select sequence(1, 10000) AS seq\r\n) tmp\r\nCROSS JOIN UNNEST(seq) AS t(id)\r\n\r\n\r\nCREATE TABLE test_map_array AS\r\nSELECT map_from_entries(transform(sequence(1, 128), x->(x,ARRAY[x]))) as mm\r\nFROM test_10k_rows\r\n```\r\n\r\nAnd run the query \r\n\r\n```SQL\r\nSELECT checksum(cast(mm as MAP<integer, ARRAY<integer>>))\r\nFROM test_map_array\r\n```\r\n\r\n\r\nWith the following VM argument enables:\r\n\r\n```\r\n-verbose:class\r\n-Djava.lang.invoke.MethodHandle.DUMP_CLASS_FILES=true\r\n```\r\n\r\nWe will see excessive  `LambdaForm$BMH*.class` files generated without the patch.\r", "NaN"], ["9022", "Fix query failure race in BroadcastOutputBuffer", "Dain Sundstrom", "dain", "09/21/17, 04:11:03 AM", "Client buffer added to a broadcast output buffer were being allowed to be completed\ncleanly. This means, if a broadcast buffer is failed before any client buffers are\nadded, the client stage will see a finished buffer and will initiate stage shutdown.\nThis results in a race between the propigation of the failure, and the client stage\nshutting down.  In rare cases this can cause a query to incorrectly report a susccess\nstate.", "NaN"], ["9027", "Revert Hadoop Kerberos refresh changes", "David Phillips", "electrum", "09/21/17, 06:51:42 PM", "The file system wrapper is not referenced by input or output streams\nand thus gets collected and closed too early, which fails queries.", "NaN"], ["9029", "Add 0.185 release notes", "Dain Sundstrom", "dain", "09/22/17, 01:40:46 AM", "NaN", "NaN"], ["9032", "Remove total bytes from page source and record cursor", "David Phillips", "electrum", "09/23/17, 12:28:14 AM", "The method is not used by anything in the engine.", "NaN"], ["9034", "Add missing entries for 0.185 release notes", "Martin Traverso", "martint", "09/23/17, 04:50:42 PM", "NaN", "NaN"], ["9035", "Implement implicit coersion for ROW type", "Shixuan Fan", "shixuan-fan", "09/25/17, 10:35:14 PM", "Addresses #9014.", "NaN"], ["9036", "Improve hash computation for MapType", "Wenlei Xie", "wenleix", "09/25/17, 09:26:03 PM", "The hash value for MapType is computed via adding the hash values of\r\nall the keys and values. This will produce same hash value for the map\r\nwith same key set and value set, even the mapping is different.", "NaN"], ["9043", "Support SessionPropertyConfigurationManager plugin", "Rongrong Zhong", "rongrong", "11/08/17, 04:48:33 AM", "NaN", "NaN"], ["9044", "Update to Airbase 69 and Airlift 0.150", "David Phillips", "electrum", "09/28/17, 01:34:06 AM", "NaN", "NaN"], ["9048", "Remove redundant cast", "David Phillips", "electrum", "09/28/17, 02:53:54 PM", "NaN", "NaN"], ["9049", "Refactor memory tracking", "Nezih Yigitbasi", "nezihyigitbasi", "12/22/17, 04:37:38 AM", "Currently memory tracking is fragile, it's all over the place, and it uses various ad-hoc classes/interfaces to do the tracking. After the initial implementation was in place new memory context classes (`LocalMemoryContext` and `AggregatedMemoryContext`) were added, which models the problem better, but the existing tracking logic wasn't updated (as it's totally scattered around the code).\r\n\r\nThe high level goal is to make memory tracking more structured/unified, which hopefully will make it more reliable and easier to understand/debug (we also have plans for future changes on top of this refactoring).\r\n\r\nGoals of this PR:\r\n- Update the existing memory tracking logic that's all around the code base to use the existing memory context classes instead of the ad-hoc/one-off classes/interfaces (e.g., `OperatorSystemMemoryContext` in `OperatorContext`, `SystemMemoryUsageListener` and its various implementations, etc.)\r\n- Add more tests (`TestMemoryTracking`) to verify that memory is tracked properly.\r\n- Get rid of various unnecessary baggage/hacks (e.g., `SystemMemoryUsageListener`, `OperatorSystemMemoryContext`, Hive and ORC modules have copies of the memory context classes, `AbstratAggregatedMemoryContext` became obsolete, etc.).\r\n\r\nThe way memory tracking is structured with this PR is described in `MemoryTrackingContext`and the other contexts (operator/driver/pipeline/task/query contexts) use this class to track memory allocations.\r\n\r", "NaN"], ["9052", "Clean up PositionsPageProcessorIterator::processBatch() return value", "James Sun", "highker", "10/20/17, 03:41:35 AM", "processBatch() is responsible for telling the caller if it has produced\r\na page, yields to time, or needs to shrink the batch size. Currently\r\nthere is a local variable and a global variable to pass the information\r\nback to the caller, which is not elegant. Unify the interface to return\r\na class to informm the caller the state returned by processBatch().", "NaN"], ["9055", "Bing tile functions", "Maria Basmanova", "mbasmanova", "10/06/17, 06:13:06 PM", "Add a set of functions to covert between geometries and Bing tiles.\r\n\r\nBing tiles introduced in https://msdn.microsoft.com/en-us/library/bb259689.aspx can be used to aggregate location-specific data at different levels of detail for visualization or to join datasets on spatial relationships 'contains' or 'intersects' by approximating geometries with a set of covering tiles and performing a relation join on tile IDs.", "NaN"], ["9057", "Fix leak in running query counter for failed queries", "Martin Traverso", "martint", "09/28/17, 03:04:05 AM", "Queries that fail before they get to run cause the running queries\r\ncounter to increment but never decrement. This is a result of the\r\nchange in 516801ab482c9189344304b97ff4e4429488dfc7, which missed\r\na call to queryStopped.\r\n\r\nFixes #9056", "NaN"], ["9059", "Add partition value check before writing", "Shixuan Fan", "shixuan-fan", "10/10/17, 11:31:22 PM", "Add this check to fail early if it encounters invalid partition values.", "NaN"], ["9065", "Fix check for resource group soft memory limit", "David Phillips", "electrum", "09/28/17, 10:58:15 PM", "Previously, queries were not allowed to run if the group was already at\nthe soft memory limit, which assumed that queries would need to use\nmemory. In particular, this prevented queries from running at all if\nthe limit was set to zero, which is the case for the legacy manager.", "NaN"], ["9070", "Remove class obsoleted by Airlift upgrade", "Piotr Findeisen", "findepi", "09/29/17, 09:47:00 PM", "NaN", "NaN"], ["9071", "Deprecate query queues", "David Phillips", "electrum", "09/29/17, 09:45:05 PM", "NaN", "NaN"], ["9074", "Improve resource groups configuration documentation", "David Phillips", "electrum", "09/29/17, 10:33:30 PM", "NaN", "NaN"], ["9079", "Prevent \"FileSystem closed\" error for Hive connector", "David Phillips", "electrum", "10/02/17, 07:04:49 PM", "NaN", "NaN"], ["9083", "Update to Airbase 70", "David Phillips", "electrum", "10/03/17, 04:42:46 AM", "NaN", "NaN"], ["9092", "Change constructor interface for VariableWidthBlockBuilder", "James Sun", "highker", "10/05/17, 08:52:26 PM", "newBlockBuilderLike() of VariableWidthBlockBuilder will set a minimal\r\nentry count to be 64. If the block builder is going to add a single\r\nentry only. The initial capacity can be 64 times of the entry size,\r\nwhich leads to huge waste of memory. Pass in the expected bytes directly\r\ninstead of the average entry bytes.", "NaN"], ["9095", "Add index join support to Thrift connector", "Aleksei Statkevich", "AlekseiS", "11/03/17, 12:08:52 AM", "NaN", "NaN"], ["9098", "Small refactorings", "Piotr Findeisen", "findepi", "10/20/17, 07:05:58 PM", "Small refactorings extracted from #9026 and #9090 to make reviewing easier.", "NaN"], ["9099", "Fix analysis/planning when lambda arguments clash with relation's columns", "Piotr Findeisen", "findepi", "11/02/17, 11:15:11 AM", "Fix lambda arguments handling in `ExpressionAnalyzer` and during query planning (`TranslationMap`).\r\n\r\nFixes #9023, #7784, #9025 \r\n\r\ncc @martint @sopel39 ", "NaN"], ["9101", "Remove maven-buildtime-extension", "David Phillips", "electrum", "10/05/17, 05:44:16 AM", "For unknown reasons, this extension causes extra uploads and downloads\nof maven-metadata.xml during deployments.", "NaN"], ["9105", "Add softConcurrencyLimit to resource group config", "Raghav Sethi", "raghavsethi", "10/11/17, 09:21:12 PM", "Add 'softConcurrencyLimit' and change 'maxRunning' to be called\r\n'hardConcurrencyLimit'. Until resource groups hit\r\n'softConcurrencyLimit', they contend with all other peer groups. After\r\nthis point they contend only with other resource groups that exceed\r\n'softConcurrencyLimit'.\r\n\r\nThe specific implementation chosen required changing 'priority' in\r\nUpdatablePriorityQueue to be a long.", "NaN"], ["9107", "Avoid unnecessary shuffle before table write", "Wenlei Xie", "wenleix", "11/16/17, 09:03:59 PM", "\r\nIf the output if already partitioned as expected, there is no need for\r\na shuffle before table write.\r\n\r", "NaN"], ["9109", "Add @Override to interface methods in HttpRequestSessionContext", "Rongrong Zhong", "rongrong", "10/06/17, 04:47:27 AM", "NaN", "NaN"], ["9114", "Update to OkHttp 3.9.0", "David Phillips", "electrum", "10/28/17, 04:50:17 AM", "NaN", "NaN"], ["9119", "Bound total memory used by HiveSplitSource", "Haozhun Jin", "haozhun", "10/16/17, 10:45:58 PM", "NaN", "NaN"], ["9120", "Add elapsed time to statement stats", "Raghav Sethi", "raghavsethi", "10/12/17, 04:24:02 PM", "NaN", "NaN"], ["9122", "Move docker images to prestodb namespace", "Andrii Rosa", "arhimondr", "10/14/17, 07:19:06 PM", "NaN", "NaN"], ["9123", "Remove outdated legacy tests for FeaturesConfig", "David Phillips", "electrum", "10/13/17, 10:41:55 PM", "NaN", "NaN"], ["9126", "Add Redshift connector", "David Phillips", "electrum", "10/10/17, 12:11:09 AM", "NaN", "NaN"], ["9127", "Add support for enforcing query max execution time limits", "Nezih Yigitbasi", "nezihyigitbasi", "10/09/17, 06:34:07 PM", "A new pair of configuration and session properties are added for specifying\r\na query max execution time limit (query.max-execution-time/query_max_execution_time).\r\n\r\n(we have a query max runtime limit, which is enforced starting from the query creation time, please see discussion [here](#8794)).", "NaN"], ["9132", "Fix categorization for security errors during view analysis", "David Phillips", "electrum", "10/10/17, 10:52:46 PM", "NaN", "NaN"], ["9134", "Improve error message for invalid Hive partition values", "David Phillips", "electrum", "10/11/17, 10:03:33 PM", "NaN", "NaN"], ["9137", "Fix HiveBloomFilter::equals() method", "Nezih Yigitbasi", "nezihyigitbasi", "10/13/17, 09:55:24 PM", "`Objects.equals()` calls array object's `.equals()`, which boils down to `==`.", "NaN"], ["9138", "Add reference counting for SliceBigArray", "James Sun", "highker", "10/20/17, 03:43:24 AM", "Add a map in SliceBigArray to track the underlying data of the slice it\r\nsets. The map aims to avoid under or over counting the memory usage. In\r\nproduction, we found the original approximation can be off by 2X.", "NaN"], ["9139", "Fail on unknown file resource group properties", "David Phillips", "electrum", "10/12/17, 08:08:54 PM", "NaN", "NaN"], ["9142", "Add new locations for query output during scheduling", "Dain Sundstrom", "dain", "11/08/17, 10:57:12 PM", "Simple single stage queries will add locations throughout execution,\nso waiting until query finishs scheduling may deadlock the query.", "NaN"], ["9143", "Add environment support to db-backed resource manager", "Nezih Yigitbasi", "nezihyigitbasi", "10/26/17, 10:12:47 PM", "With the current db-backed resource manager to support multiple clusters\r\nwe need to have multiple databases (one database per cluster). With this change\r\nthe schema of the db-backed resource manager will support multiple clusters with\r\nthe new environment column. The db-backed resource manager will load the right\r\nconfiguration based on the configured environment.", "NaN"], ["9144", "Documentation improvements and 0.186 release notes", "David Phillips", "electrum", "10/14/17, 10:16:29 PM", "NaN", "NaN"], ["9150", "Force single node output by default", "David Phillips", "electrum", "10/14/17, 12:44:43 AM", "NaN", "NaN"], ["9151", "Update to Airlift 0.152 and Airbase 72", "David Phillips", "electrum", "10/14/17, 09:37:19 PM", "NaN", "NaN"], ["9152", "Support client tags in resource group selector", "Rongrong Zhong", "rongrong", "11/01/17, 06:38:43 PM", "NaN", "NaN"], ["9153", "Add page size stats to HivePageSink", "James Sun", "highker", "10/30/17, 05:42:01 PM", "We found in production users may flush a single cell with a size of 1GB.\r\nThis can cause Hive writers to a huge amount of memory. Add stats to\r\nmonitor the distribution of pages flushed.", "NaN"], ["9155", "Remove unused JSON serialization for Query and QueryPair", "David Phillips", "electrum", "03/05/18, 04:22:54 PM", "NaN", "NaN"], ["9157", "Build Javadoc in Travis CI", "David Phillips", "electrum", "10/28/17, 05:19:02 PM", "This checks that we do not have errors which will break the release.", "NaN"], ["9162", "Use getTaskStatus instead of getTaskInfo.getTaskStatus", "Haozhun Jin", "haozhun", "10/19/17, 05:33:17 AM", "Calling getTaskInfo is more expensive than calling getTaskStatus because\r\nmore task info contains a lot more fields than task status. It is wasteful\r\nto do so when only task status is needed.", "NaN"], ["9163", "Avoid unnecessary computation/locking for getStatus()", "James Sun", "highker", "10/19/17, 01:10:53 AM", "NaN", "NaN"], ["9165", "Remove vestigial notify call in StateMachine", "David Phillips", "electrum", "10/17/17, 08:14:17 PM", "NaN", "NaN"], ["9167", "Fix SHOW FUNCTIONS documentation for stddev", "Dain Sundstrom", "dain", "10/22/17, 02:57:58 AM", "NaN", "NaN"], ["9168", "Use default Java version for Travis CI", "David Phillips", "electrum", "10/17/17, 08:40:47 PM", "NaN", "NaN"], ["9169", "Add option to skip dropping temporary tables upon checksum failure", "Shixuan Fan", "shixuan-fan", "01/02/19, 06:04:40 PM", "Address #8301 ", "NaN"], ["9171", "Set thread name when removing tasks", "David Phillips", "electrum", "10/28/17, 03:47:58 AM", "Destroying splits can be expensive and take a long time, so it is\nhelpful to know the task ID when looking at thread dumps.", "NaN"], ["9174", "Update to Airlift 0.153", "Nezih Yigitbasi", "nezihyigitbasi", "10/19/17, 04:46:33 PM", "NaN", "NaN"], ["9176", "Add 0.187 release notes", "Nezih Yigitbasi", "nezihyigitbasi", "10/19/17, 04:35:38 PM", "NaN", "NaN"], ["9178", "Fix merging for JoinOperatorInfo", "Raghav Sethi", "raghavsethi", "10/26/17, 10:28:30 PM", "NaN", "NaN"], ["9182", "Write ORC output streams based on the order of sizes", "James Sun", "highker", "11/03/17, 05:45:14 AM", "Writing streams in the order of their sizes can be helpful to reduce the\r\nnumber of IOs when reading them. It gathers all streams together so we\r\ncan use a single IO to fetch all small streams.", "NaN"], ["9184", "Fix race condition in SpillAwareLookupSourceProvider.close", "Piotr Findeisen", "findepi", "10/26/17, 08:03:30 PM", "When `SpillAwareLookupSourceProvider.close` runs concurrently with\r\n`PartitionedLookupSourceFactory.closeCachedLookupSources`,\r\n`LookupSource.close` can be invoked twice concurrenctly on single\r\n`LookupSource` instance.  This is a bug, since `LookupSource` doesn't\r\nneed to be thread-safe.\r\n\r\nThanks to @haozhun for finding this and forcing me to rethink concurrency in PLSF", "NaN"], ["9185", "Fix PartitionedLookupSource.close", "Piotr Findeisen", "findepi", "10/29/17, 10:03:23 PM", "NaN", "NaN"], ["9189", "Expire cached specialized function after 1 hour", "Wenlei Xie", "wenleix", "10/24/17, 03:47:49 AM", "We have observed old cached function might trigger some JVM bug in\r\nproduction, such as repeatedly compiling MethodHandle that causes\r\nfull GC. Forcing evicting all cached function solves the issue.\r\n\r\n\r\nExample jstack output:\r\n```\r\n    \"20170926_232912_39740_3vuuu.1.79-4-76640\" #76640 prio=5 os_prio=0\r\ntid=0x00007f908006dbd0 nid=0x150a6 runnable [0x00007f8bddb1b000]\r\n       java.lang.Thread.State: RUNNABLE\r\n            at sun.misc.Unsafe.defineAnonymousClass(Native Method)\r\n            at java.lang.invoke.InvokerBytecodeGenerator.\r\nloadAndInitializeInvokerClass(InvokerBytecodeGenerator.java:284)\r\n            at java.lang.invoke.InvokerBytecodeGenerator.loadMethod(\r\nInvokerBytecodeGenerator.java:276)\r\n            at java.lang.invoke.InvokerBytecodeGenerator.\r\ngenerateCustomizedCode(InvokerBytecodeGenerator.java:618)\r\n            at java.lang.invoke.LambdaForm.compileToBytecode(LambdaForm.\r\njava:654)\r\n            at java.lang.invoke.LambdaForm.prepare(LambdaForm.java:635)\r\n            at java.lang.invoke.MethodHandle.updateForm(MethodHandle.java:\r\n1432)\r\n            at java.lang.invoke.MethodHandle.customize(MethodHandle.java:\r\n1442)\r\n            at java.lang.invoke.Invokers.maybeCustomize(Invokers.java:407)\r\n            at java.lang.invoke.Invokers.checkCustomized(Invokers.java:398)\r\n            at java.lang.invoke.LambdaForm$MH/170067652.invokeExact_MT(\r\nLambdaForm$MH)\r\n            at com.facebook.presto.operator.aggregation.MinMaxHelper.\r\ncombineStateWithState(MinMaxHelper.java:141)\r\n            at com.facebook.presto.operator.aggregation.\r\nMaxAggregationFunction.combine(MaxAggregationFunction.java:108)\r\n            at java.lang.invoke.LambdaForm$DMH/1607453282.invokeStatic_\r\nL3_V(LambdaForm$DMH)\r\n            at java.lang.invoke.LambdaForm$BMH/1118134445.reinvoke(\r\nLambdaForm$BMH)\r\n            at java.lang.invoke.LambdaForm$MH/1971758264.\r\nlinkToTargetMethod(LambdaForm$MH)\r\n            at com.facebook.presto.$gen.IntegerIntegerMaxGroupedAccumu\r\nlator_3439.addIntermediate(Unknown Source)\r\n            at com.facebook.presto.operator.aggregation.builder.\r\nInMemoryHashAggregationBuilder$Aggregator.processPage(\r\nInMemoryHashAggregationBuilder.java:367)\r\n            at com.facebook.presto.operator.aggregation.builder.\r\nInMemoryHashAggregationBuilder.processPage(InMemoryHashAggregationBuilder\r\n.java:138)\r\n            at com.facebook.presto.operator.HashAggregationOperator.\r\naddInput(HashAggregationOperator.java:400)\r\n            at com.facebook.presto.operator.Driver.processInternal(Driver.\r\njava:343)\r\n            at com.facebook.presto.operator.Driver.lambda$processFor$6(\r\nDriver.java:241)\r\n            at com.facebook.presto.operator.Driver$$Lambda$765/442308692.get(Unknown\r\nSource)\r\n            at com.facebook.presto.operator.Driver.tryWithLock(Driver.\r\njava:614)\r\n            at com.facebook.presto.operator.Driver.processFor(Driver.java:\r\n235)\r\n            at com.facebook.presto.execution.SqlTaskExecution$\r\nDriverSplitRunner.processFor(SqlTaskExecution.java:622)\r\n            at com.facebook.presto.execution.executor.\r\nPrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)\r\n            at com.facebook.presto.execution.executor.TaskExecutor$\r\nTaskRunner.run(TaskExecutor.java:485)\r\n            at java.util.concurrent.ThreadPoolExecutor.runWorker(\r\nThreadPoolExecutor.java:1142)\r\n            at java.util.concurrent.ThreadPoolExecutor$Worker.run(\r\nThreadPoolExecutor.java:617)\r\n            at java.lang.Thread.run(Thread.java:748)\r\n    ...\r\n```", "NaN"], ["9190", "Add warning message to 0.186 release notes", "Raghav Sethi", "raghavsethi", "10/20/17, 07:30:34 PM", "NaN", "NaN"], ["9194", "Minor doc style fix for Bing tiles", "Wenlei Xie", "wenleix", "10/21/17, 07:58:43 AM", "Before:\r\n![screen shot 2017-10-20 at 15 54 39](https://user-images.githubusercontent.com/799346/31844665-ff985fe2-b5ae-11e7-9bb7-220bab1653ac.png)\r\n\r\n\r\nAfter: \r\n\r\n![screen shot 2017-10-20 at 15 53 00](https://user-images.githubusercontent.com/799346/31844646-ead948a0-b5ae-11e7-9010-7943ac08af9d.png)\r", "NaN"], ["9195", "Remove duplicate method in RecordFileWriter", "David Phillips", "electrum", "10/28/17, 03:46:30 AM", "NaN", "NaN"], ["9196", "Fix: typo in exception message", "Ye Ding", "dyng", "10/26/17, 11:52:00 PM", "A trivial fix.", "NaN"], ["9201", "Report meaningful exception when no spill paths configured", "Piotr Findeisen", "findepi", "01/17/18, 09:46:45 PM", "Fixes #9197", "NaN"], ["9202", "Add flattened query plan to QueryCompletedEvent", "Raghav Sethi", "raghavsethi", "10/27/17, 06:27:26 PM", "Flattened plans contain textual representations of every fragment, the\r\nplan tree, and a flattened list of nodes. The flattened list is\r\nconstructed using a visitor with a custom serializer which produces\r\nonly ids and types for children. This format was selected to simplify\r\npost-hoc plan analysis.\r\n\r\nCloses #9016", "NaN"], ["9204", "Yield RowNumber/DistinctLimit/MarkDistinctOperators", "James Sun", "highker", "12/18/17, 07:01:18 PM", "NaN", "NaN"], ["9206", "Fix typo in method name", "Nezih Yigitbasi", "nezihyigitbasi", "10/23/17, 11:36:40 PM", "NaN", "NaN"], ["9207", "Add current memory to QueryProgressStats", "Rongrong Zhong", "rongrong", "10/25/17, 05:20:53 AM", "totalMemoryReservation in QueryStats is an aggregate of memoryReservation from all tasks.", "NaN"], ["9213", "Add eager compact option into PagesIndex", "Wenlei Xie", "wenleix", "10/29/17, 07:50:59 PM", "Currently PagesIndex compacts the blocks lazily. It is only compacted\r\nwhen the HashBuilderOperator or OrderByOperator hold it cannot reserve\r\nthe memory required.\r\n\r\nHowever, compacting blocks requires to copy them. Thus when the cluster\r\nis running low on memory and multiple operators is requesting memory,\r\nall these operators will start to compact the PagesIndex and put a huge\r\npressure on the memory, which frequently causes full GC.\r\n\r\nAdding the eager compact option into PagesIndex allows PagesIndex to\r\ncompact Immediately whenever a new page is added.", "NaN"], ["9217", "Remove copy of drivers when getting pipeline status", "James Sun", "highker", "10/26/17, 12:15:03 AM", "When iterating the drivers, a snapshot will be taken to guarantee a view\r\nconsistency. To iterate directly on drivers should be safe.", "NaN"], ["9219", "StatementResource and StatementClient refactorings", "Aleksei Statkevich", "AlekseiS", "11/20/17, 12:30:06 AM", "These changes are the first step towards v2 protocol", "NaN"], ["9222", "Add writer scaling", "David Phillips", "electrum", "12/08/17, 04:33:12 PM", "NaN", "NaN"], ["9226", "Fix unused import", "Raghav Sethi", "raghavsethi", "10/27/17, 12:04:14 AM", "NaN", "NaN"], ["9227", "Return a proper exit code from CLI", "David Phillips", "electrum", "02/01/18, 02:47:59 AM", "NaN", "NaN"], ["9229", "Minor function name fix in GeoSpatial doc", "Wenlei Xie", "wenleix", "10/27/17, 11:14:06 PM", "NaN", "NaN"], ["9232", "Reduce memory usage of HiveSplitSource", "Haozhun Jin", "haozhun", "11/07/17, 11:58:31 PM", "The first 3 commits introduces a new operation to AsyncQueue and some other preparatory work. Can you please review, @dain?\r\n\r\n@electrum, can you please review the last 3 commits?\r\n\r\n* The 4th commit (\"Reduce memory usage of HiveSplitSource\") utilize the new operation to reduce memory usage of HiveSplitSource.\r\n* The 5th commit (\"Reduce loader concurrency in BackgroundHiveSplitLoader\") fixes a minor inefficiency.\r\n* The 6th commit is a cleanup in Hive connector that @electrum asked me to do\r", "NaN"], ["9233", "Expire projection and filter cache entries after one hour", "Wenlei Xie", "wenleix", "10/31/17, 04:12:14 AM", "We have observed deoptimization storm that leads to slowness.\r\nWe suspect that it is a JVM bug that is related to stale/corrupted\r\nprofiling data associated with generated classes.", "NaN"], ["9235", "Fix usage of default charset in typeof function", "David Phillips", "electrum", "11/03/17, 08:11:32 PM", "NaN", "NaN"], ["9236", "Update to Airbase 74 and Airlift 0.154", "Dain Sundstrom", "dain", "10/29/17, 05:44:01 PM", "NaN", "NaN"], ["9238", "Improve error message when failing cast to decimal", "Yuya Ebihara", "ebyhr", "11/16/17, 07:32:47 PM", "Fix #9225", "NaN"], ["9240", "Run USE statement on server", "David Phillips", "electrum", "11/03/17, 10:53:56 PM", "NaN", "NaN"], ["9243", "Use correct non-null annotation in client", "David Phillips", "electrum", "11/03/17, 07:24:01 PM", "NaN", "NaN"], ["9246", "Support host markdown in HostLocationProvider", "Jiexi Lin", "jessesleeping", "12/07/17, 10:26:52 PM", "NaN", "NaN"], ["9248", "Do not require Thrift metastore config for file metastore", "David Phillips", "electrum", "11/03/17, 09:58:29 PM", "Also, move Thrift metastore classes to separate package", "NaN"], ["9249", "Close query runners properly in various tests", "Nezih Yigitbasi", "nezihyigitbasi", "10/31/17, 05:11:58 PM", "NaN", "NaN"], ["9252", "Fix transaction support in client and CLI", "David Phillips", "electrum", "11/01/17, 05:42:53 AM", "The transaction was marked as cleared on the statement immediately\nfollowing the START TRANSACTION statement.", "NaN"], ["9253", "Update to airlift 0.155", "Haozhun Jin", "haozhun", "10/31/17, 07:18:58 PM", "NaN", "NaN"], ["9255", "Increase test timeout in TestPrestoDriver jdbc test", "Haozhun Jin", "haozhun", "10/31/17, 11:25:47 PM", "This particular test takes 2 seconds on an idle mac. The original 4 second\r\nis too close, especially on a shared server.", "NaN"], ["9258", "Remove workaround for Jetty client Kerberos realm bug", "David Phillips", "electrum", "11/03/17, 08:53:35 PM", "NaN", "NaN"], ["9259", "Fix bound check for ArraySliceFunction", "James Sun", "highker", "11/01/17, 06:40:24 PM", "There is a corner case that we can pass -1 to Block::getRegion when\r\nfromIndex = -N, length = N, and array size is (N - 1) to array slice\r\nfunction. Fix the bound check.", "NaN"], ["9266", "Fix bing_tile_at for points close to tile edges", "Maria Basmanova", "mbasmanova", "11/08/17, 04:34:51 PM", "Fixed off-by-one errors in bing_tile_at and bing_tile_polygon functions. Added tests.\r\n\r", "NaN"], ["9267", "Correct doc for Kafka raw decoder", "\u0141ukasz Osipiuk", "losipiuk", "11/06/17, 11:24:45 AM", "The documentation of raw message/key decoder in kafka was messed up.\r\nIt stated that `mapping` represented width of data type which actually\r\nis specified via the `dataFormat` property.\r\nAlso `formatHint` was used instead `mapping`.", "NaN"], ["9268", "Add release notes for 0.188", "Haozhun Jin", "haozhun", "11/02/17, 10:42:30 PM", "NaN", "NaN"], ["9269", "Fix analysis/planning when lambda arguments clash with relation's columns (v2)", "Piotr Findeisen", "findepi", "11/03/17, 09:12:43 AM", "Fix lambda arguments handling in `ExpressionAnalyzer` and during query planning (`TranslationMap`).\r\n\r\nFixes #9023, #7784, #9025 \r\nSupersedes #9099", "NaN"], ["9270", "Update DbResourceGroupConfigurationManager to pass environment as String", "Nezih Yigitbasi", "nezihyigitbasi", "11/07/17, 10:30:48 PM", "NaN", "NaN"], ["9271", "Fix utf8 typo in Thrift connector docs", "Aleksei Statkevich", "AlekseiS", "11/03/17, 07:02:19 PM", "NaN", "NaN"], ["9272", "Customize MethodHandle for map-to-map cast during function specialization", "Shixuan Fan", "shixuan-fan", "04/10/18, 05:35:17 PM", "Addresses #9066 ", "NaN"], ["9273", "Add limits to geometry_to_bing_tiles function", "Maria Basmanova", "mbasmanova", "11/21/17, 05:44:14 PM", "Introduced 1,000,000 limit on total number of tiles produced by geometry_to_bing_tiles function to avoid \"Cannot allocate slice larger than 2147483639 bytes\" errors. This limit caps the size of the result of geometry_to_bing_tiles function at 1M * 8 bytes = 7.6 MB.\r\n\r\nAlso, introduced a limit on complexity and size of the input geometry using the following formula: number-of-vertexes * number-of-tiles-covering-bounding-box <= 25,000,000 .\r\n\r\nFinally, optimized the algorithm for identifying a set of covering tiles for zoom levels > 10. The idea is to identify large tiles which are fully covered by the geometry first. For each such tile, we can cheaply compute all the containing tiles at the right zoom level and append them to results in bulk. This way we perform a single containment check instead of 2 to the power of level delta intersection checks, where level delta is the difference between the desired zoom level and level of the large tile covered by the geometry.", "NaN"], ["9277", "Implement is_json_scalar function", "Shixuan Fan", "shixuan-fan", "02/27/18, 10:59:13 PM", "Addresses #9260 ", "NaN"], ["9279", "Update to Airlift 0.156", "David Phillips", "electrum", "11/04/17, 03:13:18 AM", "NaN", "NaN"], ["9283", "Fix exception caused by coersion in plan printer", "Dain Sundstrom", "dain", "11/04/17, 07:15:23 PM", "PlanPrinter can throw an exception when attempting to convert a constant to a String", "NaN"], ["9284", "Replace PageProjectionOutput with Work<Block>", "James Sun", "highker", "11/15/17, 06:40:31 PM", "Work<?> is designed for yielding a function call. It was introduced\r\nafter PageProjectionOutput, which has the similar purpose. Use Work as a\r\ngeneric framework for yielding functions.", "NaN"], ["9285", "Add support for DECIMAL/NUMERIC in JDBC connectors", "Piotr Findeisen", "findepi", "11/10/17, 03:29:11 PM", "NaN", "NaN"], ["9287", "Allow multiple authentication types", "David Phillips", "electrum", "11/07/17, 01:57:24 AM", "NaN", "NaN"], ["9289", " Deprecate SliceArrayBlock", "James Sun", "highker", "11/09/17, 12:31:09 AM", "`SliceArrayBlock` has `Slice` instance size overhead. This wastes memory particularly when the underlying byte array is small. We have observed frequent full GC in production due to `Slice[]` allocation. The following figure shows a comparison of heap/old gen usage between `SliceArrayBlock` and `VariableWidthBlock` in `SliceDictionaryStreamReader` under the same workload. `SliceDictionaryStreamReader` is the only class in Presto creating `SliceArrayBlock`.\r\n\r\n<img width=\"1436\" alt=\"screen shot 2017-11-05 at 11 09 54 am\" src=\"https://user-images.githubusercontent.com/2192913/32419832-4fb11590-c235-11e7-9877-a28d6ef0bef0.png\">\r", "NaN"], ["9296", "Remove DummyMetadata", "Grzegorz Kokosi\u0144ski", "kokosing", "11/07/17, 07:45:41 PM", "Remove DummyMetadata\n\nPrefer AbstractMockMetadata over DummyMetadata.", "NaN"], ["9297", "Remove unused methods from Metadata", "Grzegorz Kokosi\u0144ski", "kokosing", "07/18/18, 09:03:35 AM", "Remove unused methods from Metadata", "NaN"], ["9298", "Add tpch.q17 test query to big_query tests group", "Grzegorz Kokosi\u0144ski", "kokosing", "11/23/17, 09:26:16 AM", "Add tpch.q17 test query to big_query tests group\n\nThat query often causes JVM on travis to hit the travis memory limit.\nBecause of that this and all the subsequent tests are failing.", "NaN"], ["9301", "Add transaction and session support to JDBC driver", "David Phillips", "electrum", "11/08/17, 01:21:13 AM", "NaN", "NaN"], ["9302", "Elide empty comments in HiveColumnHandle", "Raghav Sethi", "raghavsethi", "11/07/17, 10:15:33 PM", "This makes plans less verbose.", "NaN"], ["9305", "Add Jackson serialization support to BingTile", "Maria Basmanova", "mbasmanova", "11/15/17, 07:06:55 PM", "Fixes https://github.com/prestodb/presto/issues/9294", "NaN"], ["9306", "Fix query id comparison in CreateTableOperation", "Nezih Yigitbasi", "nezihyigitbasi", "11/09/17, 06:27:41 PM", "An instance of Optional<String> was compared against a String.", "NaN"], ["9309", "Add key store configuration for JDBC driver", "David Phillips", "electrum", "11/11/17, 02:45:50 PM", "NaN", "NaN"], ["9314", "Upgrade to Airbase 75", "Dain Sundstrom", "dain", "11/08/17, 08:15:36 PM", "NaN", "NaN"], ["9315", "Move ESRI version specification to dependency management", "Wenlei Xie", "wenleix", "11/10/17, 06:49:52 AM", "NaN", "NaN"], ["9316", "Fix split scheduling for empty file in Hive connector ", "Haozhun Jin", "haozhun", "11/09/17, 12:33:50 AM", "NaN", "NaN"], ["9318", "Test handling of empty file in BackgroundHiveSplitLoader", "Piotr Findeisen", "findepi", "11/09/17, 11:41:53 AM", "Additional tests, supplementing product tests, for #9316", "NaN"], ["9319", "Update FutureStateChange to reduce listener cancellation overhead", "Nezih Yigitbasi", "nezihyigitbasi", "11/15/17, 12:13:32 AM", "Cancelling a SettableFuture can be expensive as a CancellationException is\r\ncreated and thrown under the hood. I have seen in prod that up to 5% of the\r\nCPU cycles can be spent on the cancellation code path on the coordinator.\r\n\r\nHere's the relevant hot code path:\r\n\r\n```\r\n\"Query-20171108_224523_67496_bfyte-975035\" #975035 prio=5 os_prio=0 tid=0x00007f1b34055230 nid=0xf078b runnable [0x00007f145c9e3000]\r\n   java.lang.Thread.State: RUNNABLE\r\n        at java.lang.Throwable.fillInStackTrace(Native Method)\r\n        at java.lang.Throwable.fillInStackTrace(Throwable.java:783)\r\n        - locked <0x00007f476a09e208> (a java.util.concurrent.CancellationException)\r\n        at java.lang.Throwable.<init>(Throwable.java:265)\r\n        at java.lang.Exception.<init>(Exception.java:66)\r\n        at java.lang.RuntimeException.<init>(RuntimeException.java:62)\r\n        at java.lang.IllegalStateException.<init>(IllegalStateException.java:55)\r\n        at java.util.concurrent.CancellationException.<init>(CancellationException.java:61)\r\n        at com.google.common.util.concurrent.AbstractFuture.cancellationExceptionWithCause(AbstractFuture.java:1114)\r\n        at com.google.common.util.concurrent.AbstractFuture.getDoneValue(AbstractFuture.java:500)\r\n        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:461)\r\n        at com.google.common.util.concurrent.AbstractFuture$TrustedFuture.get(AbstractFuture.java:78)\r\n        at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:142)\r\n        at com.google.common.util.concurrent.Futures.getDone(Futures.java:1174)\r\n        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1124)\r\n        at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:399)\r\n        at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:902)\r\n        at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:813)\r\n        at com.google.common.util.concurrent.AbstractFuture.cancel(AbstractFuture.java:553)\r\n        at com.google.common.util.concurrent.AbstractFuture$TrustedFuture.cancel(AbstractFuture.java:106)\r\n        at com.facebook.presto.execution.scheduler.NodeScheduler.lambda$getFirstCompleteAndCancelOthers$6(NodeScheduler.java:338)\r\n        at com.facebook.presto.execution.scheduler.NodeScheduler$$Lambda$2338/484526032.run(Unknown Source)\r\n        at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:399)\r\n        at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:902)\r\n        at com.google.common.util.concurrent.AbstractFuture.addListener(AbstractFuture.java:636)\r\n        at com.facebook.presto.execution.scheduler.NodeScheduler.getFirstCompleteAndCancelOthers(NodeScheduler.java:335)\r\n        at com.facebook.presto.execution.scheduler.NodeScheduler.toWhenHasSplitQueueSpaceFuture(NodeScheduler.java:328)\r\n        at com.facebook.presto.execution.scheduler.TopologyAwareNodeSelector.computeAssignments(TopologyAwareNodeSelector.java:197)\r\n        at com.facebook.presto.execution.scheduler.DynamicSplitPlacementPolicy.computeAssignments(DynamicSplitPlacementPolicy.java:41)\r\n        at com.facebook.presto.execution.scheduler.SourcePartitionedScheduler.schedule(SourcePartitionedScheduler.java:124)\r\n        - locked <0x00007f2096002d40> (a com.facebook.presto.execution.scheduler.SourcePartitionedScheduler)\r\n        at com.facebook.presto.execution.scheduler.SqlQueryScheduler.schedule(SqlQueryScheduler.java:396)\r\n        at com.facebook.presto.execution.scheduler.SqlQueryScheduler$$Lambda$1521/309878269.run(Unknown Source)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n```", "NaN"], ["9320", "Code style: use != to compare booleans instead of ^", "Piotr Findeisen", "findepi", "11/15/17, 07:06:10 PM", "NaN", "NaN"], ["9323", "Append timestamp to name of generated class", "Jiexi Lin", "jessesleeping", "01/05/18, 06:27:20 PM", "Generated classes will be cached in the expression cache. Having a timestamp in the class name helps us know how long we have been using the cached class. This can help us better understand the jvm deopt bug which can be related to the age of the generated code.", "NaN"], ["9333", "Allow casting JSON to row with missing fields in JSON object", "Wenlei Xie", "wenleix", "11/12/17, 10:21:57 PM", "NaN", "NaN"], ["9334", "Add weighted fair resource group scheduling policy", "Raghav Sethi", "raghavsethi", "11/14/17, 12:13:08 AM", "This policy allocates query slots amongst eligible resource groups in\r\nrealtime, weighted by shares. Of all competing groups, the group with\r\nthe lowest ratio of actual utilization (running queries/total queries)\r\nto expected utilization (share/sum of shares of all eligible groups)\r\nof the cluster will win the slot. If multiple groups have the same\r\nratio, a lottery is held where the number of tickets for each group is\r\nproportional to the share.\r\n\r\nWill add documentation if the approach/naming looks good.", "NaN"], ["9337", "Make thrift retry configurable", "Yi He", "hellium01", "11/15/17, 06:55:35 PM", "NaN", "NaN"], ["9340", "Fix dictionary offset in SliceDictionaryStreamReader", "Wenlei Xie", "wenleix", "11/14/17, 05:47:32 PM", "The last entry in dictionary block generated by\r\nSliceDictionaryStreamReader is always null. The dictionary offset\r\nof that entry should be set to the offset of the previous entry\r\n(instead of 0). Otherwise we will try to allocate negative array size\r\nwhen calling copyPositions that includes the last entry.", "NaN"], ["9344", "Add release notes for 0.189", "Raghav Sethi", "raghavsethi", "11/15/17, 04:07:43 AM", "NaN", "NaN"], ["9345", "Fix planning failure with GROUPING and implicit coercions", "Martin Traverso", "martint", "11/16/17, 06:25:54 PM", "Fixes #8499 ", "NaN"], ["9346", "Update memory revocation handling in OperatorContext ", "Nezih Yigitbasi", "nezihyigitbasi", "11/30/17, 10:07:20 PM", "The new wrapper class is used to guarantee that only a single listener is added\r\nto the wrapped future. Previously, the driver was adding a large number of\r\nlisteners to this future causing GC issues.", "NaN"], ["9347", "Reclassify JDBC connectors changes in release notes", "Piotr Findeisen", "findepi", "11/15/17, 01:50:51 PM", "NaN", "NaN"], ["9351", "Simplify implementation of GROUPING", "Martin Traverso", "martint", "12/15/17, 02:36:05 AM", "Instead of computing the group dynamically via a function call, pre-compute the mappings during planning.\r\n\r\nSupersedes https://github.com/prestodb/presto/pull/8474\r\nFixes #8109", "NaN"], ["9353", "Make Geometry values display nicely in the CLI", "Maria Basmanova", "mbasmanova", "12/08/17, 07:22:16 PM", "> presto> select st_point(1.2, 3.4);\r\n> select st_point(1.2, 3.4);\r\n>      _col0      \r\n> -----------------\r\n>  POINT (1.2 3.4) \r\n> (1 row)\r\n\r\nFixes https://github.com/prestodb/presto/issues/9193", "NaN"], ["9355", "Prune ORDER BY in aggregation when not necessary", "Rongrong Zhong", "rongrong", "11/29/17, 02:18:05 AM", "`ORDER BY` will be removed from aggregation functions except in `array_agg`, `map_agg`, `min`, `max`, `min_by`, `max_by`.\r\n\r\nI'm not sure whether we should keep them in `map_agg` and `min/max` functions. The result could be different given different input order in these functions. People complain about not having a stable min/max. This could achieve that, but it's quite expensive.", "NaN"], ["9358", "Update to Airlift 0.157", "Nezih Yigitbasi", "nezihyigitbasi", "11/17/17, 06:10:39 PM", "NaN", "NaN"], ["9359", "Add DbSourceExactMatchSelector to DbResourceGroupConfigurationManager", "Rongrong Zhong", "rongrong", "11/17/17, 08:50:19 PM", "NaN", "NaN"], ["9370", "Add config property for maximum memory for HiveSplitSource buffering", "Haozhun Jin", "haozhun", "11/18/17, 01:22:49 AM", "This commit adds config property hive.max-outstanding-splits-size.\r\nWhen a query uses more than the configured amount of memory to buffer splits\r\nfor a single table scan, the query will be failed.\r\nThe default value for this config property is higher than previous\r\nhard-coded limit.", "NaN"], ["9375", "Update to Swift 0.15.5", "Dain Sundstrom", "dain", "11/20/17, 07:53:10 PM", "NaN", "NaN"], ["9379", "Minor cleanup in AbstractTestBlock.assertRetainedSize", "Wenlei Xie", "wenleix", "11/21/17, 07:22:11 PM", "The following three classes have special handling in\r\nAbstractTestBlock.assertRetainedSize:\r\n\r\n- BlockEncoding\r\n- AtomicLong\r\n- MethodHandle\r\n\r\nBlockEncoding and AtomicLong are used in InterleavedBlock, which has\r\nbeen deprecated. MethodHandle is only used in MapBlock, and the\r\ninstances are shared among blockas created by the same MapType.\r\n\r\nThis commit removes the special handling for BlockEncoding and\r\nAtomicLong, and add comments to explain MethodHandle.", "NaN"], ["9382", "Update to Swift 0.15.6 to fix non-system classloaders", "Dain Sundstrom", "dain", "11/21/17, 06:59:27 AM", "NaN", "NaN"], ["9384", "Introduce Page.getSingleValuePage", "Piotr Findeisen", "findepi", "11/22/17, 10:56:41 AM", "NaN", "NaN"], ["9387", "Sort output of SHOW FUNCTIONS case-insensitive", "Martin Traverso", "martint", "11/21/17, 11:44:11 PM", "NaN", "NaN"], ["9388", "Fix arguments to format() call in SingleRowBlockWriter", "Nezih Yigitbasi", "nezihyigitbasi", "11/22/17, 04:59:32 PM", "NaN", "NaN"], ["9389", "Add Comparable to JDBC interval objects", "Dain Sundstrom", "dain", "11/22/17, 04:38:07 AM", "NaN", "NaN"], ["9391", "Include range end in exception message when range out of range", "Piotr Findeisen", "findepi", "11/22/17, 05:59:53 PM", "NaN", "NaN"], ["9392", "Add primitive Block#copyPositions and use it in code", "Karol Sobczak", "sopel39", "12/08/17, 09:30:08 AM", "Supersedes https://github.com/prestodb/presto/pull/8734\r\n\r\nTo be merged after: https://github.com/prestodb/presto/pull/8629\r\n\r\n@martint @losipiuk applied comments from https://github.com/prestodb/presto/pull/8734", "NaN"], ["9393", "Remove RecordSink interface", "David Phillips", "electrum", "11/27/17, 12:00:39 AM", "This interface is hard to use and has no advantages over PageSink.\r\n\r\nFixes #9329", "NaN"], ["9395", "Update equals methods to get rid of boxing/unboxing", "Nezih Yigitbasi", "nezihyigitbasi", "11/22/17, 06:19:15 PM", "NaN", "NaN"], ["9396", "Disallow window functions in HAVING clause", "Martin Traverso", "martint", "11/23/17, 05:48:40 PM", "Semantically, window functions are evaluated on the result of the\r\naggregation table expression, so it's non-sensical for the HAVING\r\nclause to contain them.\r\n\r\nFixes #3979", "NaN"], ["9397", "Change LOWEST_MAX_STANDARD_ERROR for approx_distinct function", "Shixuan Fan", "shixuan-fan", "11/25/17, 11:22:47 PM", "Maximum number of buckets in HyperLogLog changed from 8192 to 65536,\r\nso now we could support lower max standard errors.", "NaN"], ["9400", "Classify TApplicationException as external error", "Jiexi Lin", "jessesleeping", "11/28/17, 11:59:36 PM", "NaN", "NaN"], ["9401", "Fix incorrect pattern for SingleMarkDistinctToGroupBy", "Martin Traverso", "martint", "11/24/17, 06:27:50 AM", "Commit 459eb4f4d396d765a8c0b461d5c75637476b15fd refactored the rule pattern incorrectly, which causes the rule to never match.", "NaN"], ["9406", "Make sure that hadoop is up and running before running product tests", "Grzegorz Kokosi\u0144ski", "kokosing", "11/27/17, 01:07:58 PM", "NaN", "NaN"], ["9409", "Address intermittent failures of testQueryLoggingCount", "\u0141ukasz Osipiuk", "losipiuk", "11/27/17, 10:47:00 PM", "The change addresses intermittent failures of AbstractTestDistributedQueries.testQueryLoggingCount.\r\nThe failure happened because of how `beforeCompletedQueriesCount` and\r\n`beforeSubmittedQueriesCount` were determined at the beginning of tests.\r\nCode assumed that it is safe to get those values as soon as all the\r\nqueries in queryManager are completed.\r\n\r\nYet it does not mean that stats counters were already updated, as those\r\ntwo changes are not synchronized. See `SqlQueryManager.createQuery`:\r\n\r\nHere is the code which updates stats counter for completed queries.\r\n```\r\n  QueryInfo info = queryExecution.getQueryInfo();\r\n  stats.queryFinished(info);\r\n  queryMonitor.queryCompletedEvent(info);\r\n```\r\n\r\nYet according to `info.isFinalQueryInfo()` taken from\r\n`queryManager.getAllQueryInfo()` the query is already completed.\r\n\r\nfixes #8953", "NaN"], ["9417", "Minor code cleanup", "Piotr Findeisen", "findepi", "11/28/17, 01:54:35 AM", "NaN", "NaN"], ["9418", "Remove auto GC based on code-cache fill level", "Martin Traverso", "martint", "11/27/17, 08:06:23 PM", "The issue this was working around appears to no longer be\r\npresent in recent JDK versions.", "NaN"], ["9420", "Use AbstractTestFunctions in time related tests ", "\u0141ukasz Osipiuk", "losipiuk", "11/27/17, 10:45:43 PM", "Preparatory for #9385\r\n\r\ncc: @haozhun ", "NaN"], ["9423", "Remove JoinProbeCompiler ", "James Sun", "highker", "12/20/17, 06:37:32 AM", "Rebased on top of #8629. Resolves #8827\r\n\r\nBenchmark result shows little difference with bytecode gen removed:\r\n```\r\nWith bytecode gen:\r\nhash_join ::   36.741 cpu ms\r\nsql_hash_join ::  143.144 cpu ms\r\nsql_consecutive_join :: 2232.964 cpu ms\r\n\r\nWithout bytecode gen:\r\nhash_join ::   37.103 cpu ms\r\nsql_hash_join ::  134.827 cpu ms\r\nsql_consecutive_join :: 2280.834 cpu ms", "NaN"], ["9424", "Remove deprecated optimizers", "Martin Traverso", "martint", "01/22/18, 05:42:39 PM", "These have been replaced by new iterative optimizer rules.", "NaN"], ["9432", "Add ST_Buffer function", "Maria Basmanova", "mbasmanova", "11/30/17, 02:22:21 AM", "NaN", "NaN"], ["9437", "Add object_not_readable mode for Presto Hive connector", "Wenlei Xie", "wenleix", "12/22/17, 02:07:21 AM", "NaN", "NaN"], ["9440", "Classify RuntimeTApplicationException as external error", "Jiexi Lin", "jessesleeping", "11/30/17, 04:32:43 AM", "In some cases, Swift wraps `TApplicationException` to `RuntimeTApplicationException`. ", "NaN"], ["9441", "Add clientTags to queryContext", "Rongrong Zhong", "rongrong", "11/30/17, 12:17:44 AM", "NaN", "NaN"], ["9444", "Extend AbstractTestFunctions.assertInvalidFunction", "\u0141ukasz Osipiuk", "losipiuk", "11/30/17, 04:45:19 PM", "Add version of assertInvalidFunction which takes error code.\r\nAlso treat message parameter as either exact value or regex pattern.", "NaN"], ["9445", "Adapt AbstractTestQueries for connectors that do not support BIGINT", "Piotr Findeisen", "findepi", "11/30/17, 01:55:20 PM", "NaN", "NaN"], ["9446", "Skip creation of TPCH tables for type mapping tests", "Piotr Findeisen", "findepi", "11/30/17, 01:56:20 PM", "The tables are not used in these tests.", "NaN"], ["9447", "Inherit TestDateTimeOperators from AbstractTestFunctions", "\u0141ukasz Osipiuk", "losipiuk", "11/30/17, 04:41:48 PM", "NaN", "NaN"], ["9448", "Use assertInvalidFunction instead of assertThrows", "\u0141ukasz Osipiuk", "losipiuk", "11/30/17, 05:34:26 PM", "NaN", "NaN"], ["9449", "Use try-resource for FunctionAssertions in testLocale", "\u0141ukasz Osipiuk", "losipiuk", "11/30/17, 09:14:24 PM", "NaN", "NaN"], ["9451", "Add a priority column to the db resource group selectors", "Nezih Yigitbasi", "nezihyigitbasi", "12/02/17, 02:05:22 AM", "When using the json resource group configuration the selectors list\r\nhas a well-defined order (the order in which selectors are defined\r\nin the config file). The order in which selector rules are checked\r\nmatters if we want a selection rule to have a higher priority than\r\nthe other rules. This change adds support for defining an order\r\namong selectors for the db-based resource groups.", "NaN"], ["9454", "Remove unnecessary method reference", "Nezih Yigitbasi", "nezihyigitbasi", "12/01/17, 04:56:26 AM", "NaN", "NaN"], ["9455", "Mark driverBlockedFuture as final in Driver", "Nezih Yigitbasi", "nezihyigitbasi", "11/30/17, 10:19:16 PM", "NaN", "NaN"], ["9458", "Minor cleanup in presto-geospatial", "Nezih Yigitbasi", "nezihyigitbasi", "12/01/17, 06:39:15 AM", "NaN", "NaN"], ["9461", "Split time related tests into base/legacy/new", "\u0141ukasz Osipiuk", "losipiuk", "12/22/17, 10:19:28 PM", "This is a preparatory PR extracted from https://github.com/prestodb/presto/pull/9385.\r\n\r\nRationale for merging this earlier is that these commits involve moving large chunks of code from one class to the other. \r\nIt is very not nice to rebase them as any change to the source files in the upstream appears as large conflict and it is easy to \r\nlose upstream changes during resolution (diffs are not very helpful here).\r\n\r\nTo prevent those ugly conflicts it would be nice to have that already in master while work on #9385 continues.\r\n\r\nLogically this PR is no-op. \r", "NaN"], ["9462", "Ensure proper table cleanup when data setup fails", "Piotr Findeisen", "findepi", "12/01/17, 07:08:05 PM", "NaN", "NaN"], ["9463", "Fix a race condition in Driver", "Nezih Yigitbasi", "nezihyigitbasi", "12/01/17, 10:20:45 PM", "If OperatorContext::requestMemoryRevoking() is called before the driver\r\nthread calls updateDriverBlockedFuture(), the driverBlockedFuture will be completed\r\nbut right after that the driver thread will update it with a new future and\r\nreturn it. However, the returned future is not completed even though memory revocation\r\nis requested (we are missing a notification).", "NaN"], ["9464", " Remove connection parameters from JdbcSplit and JdbcOutputTableHandle ", "Piotr Findeisen", "findepi", "12/07/17, 08:24:53 PM", "NaN", "NaN"], ["9466", "Convert to Jdbi 3", "David Phillips", "electrum", "12/11/17, 05:09:57 PM", "NaN", "NaN"], ["9467", "Add query type to selectors table", "Raghav Sethi", "raghavsethi", "12/04/17, 07:59:52 PM", "NaN", "NaN"], ["9469", "Fix PluginClassLoader for Java 9", "David Phillips", "electrum", "12/15/17, 06:23:06 AM", "Packages such as java.sql are only available in the platform class loader.", "NaN"], ["9472", "Fix jvm crash during tests", "Wojciech Biela", "ilfrin", "12/12/17, 11:34:45 PM", "those two were seen to cause intermittent jvm crashes like below, during TPCDS product tests\r\n\r\n```\r\n2017-12-02 16:58:00 INFO: [1 of 10] sql_tests.testcases.tpcds.q80 (Groups: tpcds)\r\npresto-master_1       | #\r\npresto-master_1       | # A fatal error has been detected by the Java Runtime Environment:\r\npresto-master_1       | #\r\npresto-master_1       | #  SIGSEGV (0xb) at pc=0x00007f39eda79241, pid=6, tid=0x00007f39dd2f8700\r\npresto-master_1       | #\r\npresto-master_1       | # JRE version: Java(TM) SE Runtime Environment (8.0_152-b16) (build 1.8.0_152-b16)\r\npresto-master_1       | # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.152-b16 mixed mode linux-amd64 compressed oops)\r\npresto-master_1       | # Problematic frame:\r\npresto-master_1       | # V  [libjvm.so+0x692241]  KlassToOopClosure::do_klass(Klass*)+0x11\r\npresto-master_1       | #\r\npresto-master_1       | # Core dump written. Default location: /var/presto/core or core.6\r\npresto-master_1       | #\r\npresto-master_1       | # An error report file with more information is saved as:\r\npresto-master_1       | # /docker/volumes/logs/product-tests-presto-jvm-error-file.log\r\npresto-master_1       | #\r\n```", "NaN"], ["9473", "Use more specific name for exact match selector in ResourceGroupDao", "Rongrong Zhong", "rongrong", "12/04/17, 11:32:03 PM", "NaN", "NaN"], ["9474", "Broadcast spatial join", "Maria Basmanova", "mbasmanova", "03/08/18, 09:33:33 PM", "Implements broadcast spatial join using an R-Tree as described in #9890.\r\n\r\nSupports spatial joins defined using ST_Contains and ST_Intersects functions.\r\n\r\nThe implementation consists of an optimizer rule and two custom operators. The new rule changes a cross join node with a spatial filter on top into a spatial join node which is executed using new operators. SpatialIndexBuilderOperator builds an R-Tree from build side geometries (relation on the right side of the join). SpatialLookupJoinOperator processes probe geometries one record at a time looking up matching geometries in the R-Tree.\r\n\r\nFor example, the plan for the following query\r\n\r\n```\r\nwith \r\n    points as (select * from (values ('a', 1, 2)) as t(name, lat, lng)),\r\n    polygons as (select * from (values ('p1', 'POLYGON ...')) as t(name, wkt))\r\nselect count(1)\r\nfrom points, polygons\r\nwhere ST_Contains(ST_GeometryFromText(wkt), ST_Point(lat, lng))\r\n```\r\n\r\nchanges from \r\n\r\n```\r\n- FilterProject[filterPredicate = \"st_contains\"(\"st_geometryfromtext\"(CAST(\"field_23\" AS varchar)), \"st_point\"(CAST(\"field_0\" AS double), CAST(\"field_1\" AS double)))] => []\r\n    - CrossJoin => [field_0:integer, field_1:integer, field_23:varchar(11)]\r\n```\r\n\r\nto \r\n\r\n```\r\n - SpatialJoin[\"st_contains\"(\"st_geometryfromtext\", \"st_point\")] => []                     \r\n     - Project[] => [field_0:integer, field_1:integer, st_point:Geometry]                  \r\n             st_point := \"st_point\"(CAST(\"field_0\" AS double), CAST(\"field_1\" AS double))  \r\n     - Project[] => [field_23:varchar(11), st_geometryfromtext:Geometry]               \r\n             st_geometryfromtext := \"st_geometryfromtext\"(CAST(\"field_23\" AS varchar)) \r\n```\r\n\r\n<img width=\"858\" alt=\"screen shot 2018-02-28 at 9 30 36 am\" src=\"https://user-images.githubusercontent.com/27965151/36798305-3d544802-1c78-11e8-9526-7d1c73cffff6.png\">\r\n\r\nTests for the new functionality include:\r\n\r\n- unit test for the new rule: TestTransformSpatialPredicateToJoin\r\n- test for the query planner with the new rule: TestSpatialJoinPlanning\r\n- unit test for the new SpatialJoinOperator: TestSpatialJoinOperator\r\n- end-to-end test for the spatial join queries: TestSpatialJoins\r\n\r\nKnown limitations and future work:\r\n\r\n1. Only INNER JOIN is supported. \r\n\r\n1. The new optimizer rule runs after AddExchanges. Moving the rule to some place before AddExchanges requires changing PredicatePushDown rule to become aware of spatial joins and not unroll spatial join into cross join and a filter on top.\r\n\r\n1. The new optimizer rule and the new operators are defined in presto-main, although, logically, they belong to the presto-geospatial plugin. In the future, when plugin interface evolves to support custom rules and operators, the new rule and operators can be moved.\r\n\r\n1. The logic of computing memory size of a geometry object requires access to package private classes from the ESRI library. To gain access, presto-geospatial-toolkit includes a class in com.esri.core.geometry package. This split-package situation will block migration to Java 9 modules and needs to be addressed at some point. https://github.com/Esri/geometry-api-java/issues/156\r\n\r\n1. The R-Tree implementation in the JTS library is not GC-friendly as it creates a tree of Java objects. This implementation needs to be replaced.\r\n\r\n1. TODO: Use dictionary style blocks for probe columns (see LookupJoinPageBuilder)\r\n\r\nAddressing items 4 and 5 will require changes to external libraries (ESRI and JTS).", "NaN"], ["9479", "Allow passing connection properties to JdbcSqlExecutor", "Piotr Findeisen", "findepi", "12/12/17, 05:56:46 AM", "NaN", "NaN"], ["9480", "Allow Connection sources other than Driver in BaseJdbcClient", "Piotr Findeisen", "findepi", "12/08/17, 08:48:39 PM", "Based on https://github.com/prestodb/presto/pull/9464 (only last commit matters)", "NaN"], ["9481", " Use dedicated executor for running listeners in HttpPageBufferClient", "Nezih Yigitbasi", "nezihyigitbasi", "12/11/17, 09:38:58 PM", "Previously we were using a `ScheduledExecutorService` to run the listeners of HTTP response futures, which is probably not what we want. This change updates the `HttpPageBufferClient` to use a dedicated executor with a configurable size to run those listeners. The default value of this config is the same as the config (`exchange.client-threads`) that was setting the size of the `ScheduledExecutorService`.\r", "NaN"], ["9482", "Update to Airlift 0.158", "David Phillips", "electrum", "12/05/17, 09:14:25 PM", "NaN", "NaN"], ["9483", "Use explicit charset when creating JSON reader", "Martin Traverso", "martint", "12/05/17, 11:11:17 PM", "It was using the default platform charset, which might not be UTF-8", "NaN"], ["9484", "Add peakMemoryBytes to QueryStats in jdbc", "Rongrong Zhong", "rongrong", "12/06/17, 12:13:19 AM", "NaN", "NaN"], ["9487", "Remove thrift client stats module", "Aleksei Statkevich", "AlekseiS", "12/07/17, 10:24:22 PM", "NaN", "NaN"], ["9493", "Log error when Driver is left with revocable memory reservation", "Piotr Findeisen", "findepi", "12/08/17, 01:00:01 PM", "`Driver` cleans up memory reservation after closing all `Operator`s. It\r\nshould log error when there is revocable memory reservation to be\r\ncleaned up just as it does for other memory reservations.", "NaN"], ["9496", "Allow refreshing KeyStore and TrustStore in CLI", "David Phillips", "electrum", "12/06/17, 10:43:01 PM", "NaN", "NaN"], ["9497", "Add PEM support to cli", "Dain Sundstrom", "dain", "12/06/17, 09:47:33 PM", "Existing keystore and truststore properties can now be used for PEM files in the CLI", "NaN"], ["9499", "Add client tags to query detail page", "Raghav Sethi", "raghavsethi", "12/06/17, 09:37:29 PM", "NaN", "NaN"], ["9501", "Add query type to DbSourceExactMatchSelector", "Wenlei Xie", "wenleix", "12/08/17, 07:53:08 PM", "NaN", "NaN"], ["9502", "Add 0.190 release notes", "David Phillips", "electrum", "12/07/17, 12:43:15 AM", "NaN", "NaN"], ["9503", "Update thrift connector doc for retry configs", "Yi He", "hellium01", "12/14/17, 08:42:10 PM", "NaN", "NaN"], ["9508", "Add Metastore TLS support for extended Hive connector implementations", "Dain Sundstrom", "dain", "12/12/17, 03:40:08 AM", "NaN", "NaN"], ["9509", "Remove unnecessary order by fields in AggregationNode.Aggregation", "Martin Traverso", "martint", "12/08/17, 08:57:57 PM", "This same information is being tracked in FunctionCall's order-by\r\nfield. Having it in two different places requires making sure they\r\nare consistent (or at least non-conflicting), makes the code harder\r\nto reason about, the semantics harder to explain, etc.", "NaN"], ["9510", "Fix bit_count for bits between 33 and 63", "Rongrong Zhong", "rongrong", "12/09/17, 12:53:16 AM", "NaN", "NaN"], ["9512", "Estimate probe row size instead of calculating the precise value", "James Sun", "highker", "12/12/17, 04:57:37 PM", "During join, we use getRegionSizeInBytes to get the size of a row in\r\nthe probe page. However, this can be expensive if the probe is of a\r\ncomplex type. In production, we observed 50X regression in join. This\r\npatch estimates the probe row size instead.", "NaN"], ["9514", "Use page builder for array_remove", "James Sun", "highker", "01/13/18, 01:48:31 AM", "NaN", "NaN"], ["9517", "Update docker.cluster.jks in product-tests", "Piotr Findeisen", "findepi", "12/08/17, 05:02:05 PM", "This updates the TLS certificate used in product tests, the previous one\r\nexpired on Jan 4, 2017. The new certificate will expire in 2291.\r\n\r\nRelated to #9516 ", "NaN"], ["9518", "Move HiveMetastore into thrift package", "David Phillips", "electrum", "12/10/17, 07:22:54 PM", "NaN", "NaN"], ["9519", "Remove SliceArrayBlock", "James Sun", "highker", "12/18/17, 05:43:43 PM", "NaN", "NaN"], ["9526", "Fix documentation for  function", "Piotr Findeisen", "findepi", "12/10/17, 06:13:21 PM", "NaN", "NaN"], ["9531", "Update javadoc for getRegionSizeInBytes to limit its usage", "James Sun", "highker", "12/12/17, 06:57:40 PM", "NaN", "NaN"], ["9532", "Fix race between completion/fail in BackgroundHiveSplitLoader", "Haozhun Jin", "haozhun", "12/12/17, 06:42:38 PM", "NaN", "NaN"], ["9533", "Display error on stderr", "Grzegorz Kokosi\u0144ski", "kokosing", "12/12/17, 12:00:11 PM", "Display error on stderr\n\nWhen export_canonical_path or whole compose-compose.sh script was called\nwithin VAR=$(...) operator then error message was captured as VAR value\nand script exited due `exit 1` with error message swallowed.\n\nDisplaying the error message to stderr avoid the above.", "NaN"], ["9535", "Update tempto to 1.37", "Grzegorz Kokosi\u0144ski", "kokosing", "12/12/17, 08:08:39 PM", "Update tempto to 1.37\n\nTempto 1.37 displays used configuration and prevents to be called with\nwrong CLI arguments. No more changes over 1.36.", "NaN"], ["9536", "Fix unsafe publication in TaskContext", "Nezih Yigitbasi", "nezihyigitbasi", "12/14/17, 01:13:47 AM", "The state change listener added in the constructor escapes the 'this' reference.", "NaN"], ["9537", "Remove unused variable in MapSubscriptOperator", "Nezih Yigitbasi", "nezihyigitbasi", "12/12/17, 11:38:34 PM", "NaN", "NaN"], ["9539", "Improve readability of LocalExchange.LocalExchangeFactory", "Haozhun Jin", "haozhun", "12/12/17, 10:31:27 PM", "Add GuardedBy and make synchronization more obviously correct", "NaN"], ["9540", "Report physical written bytes for legacy RCFile writer", "David Phillips", "electrum", "12/13/17, 08:11:04 PM", "NaN", "NaN"], ["9543", "Add queuedTimeLimit/runningTimeLimit to resource groups doc", "Nezih Yigitbasi", "nezihyigitbasi", "12/14/17, 12:15:55 AM", "Fixes #9335.", "NaN"], ["9545", "Low memory killer based on total reservation on blocked nodes", "Haozhun Jin", "haozhun", "12/14/17, 06:26:55 AM", "NaN", "NaN"], ["9546", "Use DistributionStats for thrift connector retry latency", "Yi He", "hellium01", "12/21/17, 09:39:59 PM", "DistributionStats give more dissect on what is the latency during certain period.", "NaN"], ["9547", "Add 0.191 release notes", "Dain Sundstrom", "dain", "12/14/17, 08:40:16 PM", "NaN", "NaN"], ["9548", "Fix resource group validation for WEIGHTED_FAIR", "Raghav Sethi", "raghavsethi", "12/15/17, 01:21:58 AM", "NaN", "NaN"], ["9550", "Docker images v5", "Wojciech Biela", "ilfrin", "12/22/17, 12:55:47 PM", "Changes necessary for the v5 docker images \r\n1) Images no longer have zookeeper and historyserver so no stopping of\r\nthose services is required.\r\n2) Changed the variables used in scripts to allow usage of other\r\nkerberized images (other than hdp), they are new in v5.\r\n3) Changed default values to v5", "NaN"], ["9551", "Fix checkstyle in MemoryManagerConfig", "Haozhun Jin", "haozhun", "12/14/17, 08:02:54 PM", "NaN", "NaN"], ["9552", "Only check for ORC header magic if footer read fails", "Dain Sundstrom", "dain", "12/14/17, 09:11:38 PM", "NaN", "NaN"], ["9555", "Update 0.191 release notes", "David Phillips", "electrum", "12/14/17, 09:53:34 PM", "NaN", "NaN"], ["9556", "Update Travis build to latest image", "David Phillips", "electrum", "12/18/17, 10:51:28 PM", "NaN", "NaN"], ["9557", "Fix Java 9 incompatibilities in generated bytecode", "Martin Traverso", "martint", "12/15/17, 06:32:11 AM", "NaN", "NaN"], ["9561", "Update to ASM 6.0_BETA", "David Phillips", "electrum", "12/15/17, 06:40:55 PM", "This version is required to read Java 9 bytecode.", "NaN"], ["9562", "Reorder dependency so Presto dependencies are first", "David Phillips", "electrum", "12/16/17, 12:58:38 AM", "NaN", "NaN"], ["9563", "Update to Hadoop 2.7.4", "David Phillips", "electrum", "12/15/17, 06:58:05 PM", "NaN", "NaN"], ["9564", "Update to ASM 6.0", "David Phillips", "electrum", "12/16/17, 05:26:49 PM", "NaN", "NaN"], ["9566", "Allow spill tests to run when disk space is low", "David Phillips", "electrum", "12/18/17, 09:20:28 PM", "NaN", "NaN"], ["9567", "Improve DB resource groups operational management", "Raghav Sethi", "raghavsethi", "01/08/18, 07:01:53 PM", "NaN", "NaN"], ["9568", "Update datetime locale test for Java 9", "David Phillips", "electrum", "12/18/17, 09:21:20 PM", "Java 9 [uses the Unicode CLDR locale provider by default](https://docs.oracle.com/javase/9/intl/internationalization-enhancements-jdk-9.htm), which uses\r\ndifferent short month symbols for some locales.\r\n\r\nSee these [example differences](https://gist.github.com/electrum/ee14fe6eb6ae50484b4c015a2564bc2d).", "NaN"], ["9570", "Respect the checkSpace flag when creating ORC page sink", "Jiexi Lin", "jessesleeping", "12/19/17, 11:29:39 PM", "NaN", "NaN"], ["9571", "Report actual input size/positions in live plan view", "Martin Traverso", "martint", "12/22/17, 08:59:46 PM", "NaN", "NaN"], ["9572", "Fix ORC string statics full-ci build", "Dain Sundstrom", "dain", "12/19/17, 02:25:11 AM", "NaN", "NaN"], ["9573", "Support reading MongoDB documents as map type.", "Qi Liu", "visualage", "01/05/18, 09:20:23 AM", "The existing MongoDB connector only supports PrestoDB Map type as an array/list in MongoDB, which may not be sufficient if the MongoDB data is a document type.\r\nThis commit enhanced the MongoDB connector so that it can convert a MongoDB document directly into PrestoDB Map type.\r\nIt also enhanced the MongoDB connector so that when it converts a MongoDB document into a varchar type, it uses the BSON JSON representation instead of the simple .toString() representation. By doing this, we have the opportunity to easily write a function plugin to decode the JSON back into BSON document and do further custom processing.", "NaN"], ["9576", "Make getRegion always return a new Block for structural block", "Wenlei Xie", "wenleix", "01/13/18, 06:31:49 AM", "For structural types (array/map/row), AbstractBlock.getRegion over\r\nthe whole block currently returns `this` instead of constructing\r\na new block.\r\n\r\nAs a result, for structural type, BlockBuilder.getRegion() will return\r\na BlockBuilder instead of a Block when it is slicing the whole block,\r\nwhich is unexpected behavior.", "NaN"], ["9577", "Fix exception in ClusterMemoryManager.process", "Haozhun Jin", "haozhun", "12/19/17, 01:14:06 AM", "The result of isLastKilledQueryGone is inverted for common case. As a result,\r\nClusterMemoryManager.process could run lowMemoryKiller.chooseQueryToKill\r\nbefore it has updated the nodes and pools fields. This will lead to\r\nNoSuchElementException when ClusterMemoryManager tries to map chosen query\r\nid to the corresponding QueryExecution. This will skip updatePools and\r\nupdateNodes, causing ClusterMemoryManager to stop functioning permanently.", "NaN"], ["9579", "Improve buffer utilization calcuation for writer scaling", "David Phillips", "electrum", "12/19/17, 01:18:21 AM", "NaN", "NaN"], ["9580", "Update dependencies for Java 9", "David Phillips", "electrum", "12/19/17, 04:10:59 PM", "NaN", "NaN"], ["9581", "Replace CompletableFuture with ListenableFuture in HiveSplitSource", "Dain Sundstrom", "dain", "12/19/17, 11:39:05 PM", "NaN", "NaN"], ["9582", "Fix JMX connector to allow nulls in history table values", "David Phillips", "electrum", "12/19/17, 05:21:42 PM", "The tested MBean \"java.lang:type=Runtime\" has null values on Java 9.", "NaN"], ["9587", "Fix Accumulo for Java 9", "David Phillips", "electrum", "12/20/17, 02:06:14 AM", "Fixes #9583", "NaN"], ["9588", "Update deprecated use of AWS SDK & Guava methods in PrestoS3FileSystem", "Nezih Yigitbasi", "nezihyigitbasi", "12/21/17, 07:33:43 PM", "This PR updates the deprecated use of various AWS SDK and Guava methods in `PrestoS3FileSystem`. I also moved the config key constants from `PrestoS3FileSystem` to `S3ConfigurationUpdater` as that list is getting bigger and bigger.\r\n\r\n@z-york @avirtuos I will appreciate if you guys can also take a look and run this in your test environment as we don't have one.", "NaN"], ["9589", "Refactor and simplify Hive split generation", "Dain Sundstrom", "dain", "12/23/17, 05:25:58 PM", "NaN", "NaN"], ["9591", "Don't ask Hive for stats on the partition column", "Wojciech Biela", "ilfrin", "12/20/17, 04:03:16 PM", "Filter out the partition that is the partition column from the column\r\nlist that is sent in the partition stats request to Hive Metastore,\r\nbecause this column is not treated as a regular column and is not listed\r\non the column list, causing some assertions to fail in the JDO code\r\npaths in Hive Metastore (as opposed to the direct SQL code paths). See\r\nHive Metastore's `ObjectStore.getPartitionColumnStatisticsInternal` for\r\ndetails.", "NaN"], ["9593", "Update Maven resolver library to 1.4", "David Phillips", "electrum", "12/20/17, 04:58:47 PM", "This fixes the server starting in development mode due to Airbase\r\nnow having a profile with JDK version activation.\r\n\r\nFixes #9592", "NaN"], ["9594", "Extend getPositions with offset and length", "James Sun", "highker", "12/31/17, 10:01:39 PM", "NaN", "NaN"], ["9595", " Make double/real->decimal cast compatible with cast to varchar", "\u0141ukasz Osipiuk", "losipiuk", "12/23/17, 10:56:46 AM", "    Make double/real->decimal cast compatible with cast to varchar\r\n\r\n    Casting approximate numerics (DOUBLE and REAL) to DECIMAL used\r\n    encoding which exposed all digits from decimal representation of\r\n    values stored by floating point number (for digits before decimal point).\r\n\r\n    This resulted in somewhat unexpected results.\r\n\r\n    E.g.\r\n\r\n    cast (double '100000000000000000000000000000000' as decimal(38))\r\n    returned 100000000000000005366162204393472.\r\n\r\n    If you look at BigDecimal(double) constructor (which is discourage to be\r\n    used), it has the same behaviour.\r\n\r\n    After this change, the encoding of double will be compatible with\r\n    casting double to varchar, and also how it naturally is printed out in CLI.\r\n\r\n    The double '100000000000000000000000000000000' is printed as '1.0E32'\r\n    and cast to decimal(38) the value will match that.\r\nfixes #9575\r\n\r\ncc: @martint ", "NaN"], ["9598", "Block positions range handling minor code cleanup", "Piotr Findeisen", "findepi", "12/22/17, 11:22:36 PM", "NaN", "NaN"], ["9599", "Bump benchto version to 0.4 and fix readme file", "Karol Sobczak", "sopel39", "12/21/17, 01:34:17 PM", "FYI: @electrum ", "NaN"], ["9603", "Use ZonedDateTime for TIMESTAMP WITH TIME ZONE in tests", "Piotr Findeisen", "findepi", "12/21/17, 10:16:13 PM", "See #9527 for rationale.", "NaN"], ["9604", "Enable product tests to be executed with different JDBC driver", "Grzegorz Kokosi\u0144ski", "kokosing", "12/23/17, 08:02:51 PM", "NaN", "NaN"], ["9605", "Use OffsetTime for TIME WITH TIME ZONE in tests", "Piotr Findeisen", "findepi", "12/22/17, 07:26:45 PM", "See #9527 for rationale.", "NaN"], ["9606", "Do not add exchange locations for finished tasks", "David Phillips", "electrum", "12/29/17, 01:19:51 AM", "This prevents query hangs when using writer scaling as it can create new\ntasks with exchange locations referencing tasks that are already expired\n(and thus will be auto-created as new tasks that will never complete).", "NaN"], ["9608", "Remove driver peak memory reservation stats", "Nezih Yigitbasi", "nezihyigitbasi", "12/22/17, 08:09:06 PM", "    This change removes it for mainly two reasons: (as discussed offline)\r\n    - It's non-trivial to collect these stats with the new memory tracking framework\r\n    - These stats are not used much, so it will probably not pay off to collect them", "NaN"], ["9609", "Reuse execute method in BenchmarkQueryRunner", "Grzegorz Kokosi\u0144ski", "kokosing", "01/13/18, 07:43:47 PM", "Reuse execute method in BenchmarkQueryRunner", "NaN"], ["9611", "Minor improvements around Kerberos test infra", "Grzegorz Kokosi\u0144ski", "kokosing", "12/29/17, 09:01:50 AM", "NaN", "NaN"], ["9615", "Fail query gracefully when polling page errors out", "James Sun", "highker", "01/07/18, 09:16:24 AM", "When coordinator is pulling data from workers, pollPage can throw errors\r\ncausing returning 500 response back to the client with stacktraces.\r\nHowever, the query can still be alive given no one can kill it. The\r\npatch catches the exception, fails the query, and returns normal 200\r\nresponse to the client.", "NaN"], ["9616", "Revert the new memory tracking framework", "Nezih Yigitbasi", "nezihyigitbasi", "12/23/17, 01:02:42 AM", "We hit an issue that we need to investigate, for now it's safer to revert this change.", "NaN"], ["9617", "Fix typo KERBEROS_REMOTE_SERVICE_NAME", "Kai Sasaki", "Lewuathe", "12/23/17, 06:51:41 AM", "`KERBEROS_REMOTE_SERICE_NAME` -> `KERBEROS_REMOTE_SERVICE_NAME`", "NaN"], ["9618", "Categorize invalid offset minutes error in from_unixtime", "Ying", "yingsu00", "01/04/18, 10:03:13 PM", "Categroize invalid offset minutes error for from_unixtime\r\n\r\nThis is from task https://github.com/prestodb/presto/issues/6550\r\n\r\nWhen from_unixtime() is called but with invalid offset minutes, a java.lang.IllegalArgumentException was thrown. It was thrown in getTimeZoneKeyForOffset(). Since the error comes from an invalid argument of the fromUnixTime() functin, we want to throw a PrestoException with error code INVALID_FUNCTION_ARGUMENT. We also added a negative test testFromUnixTimeWithOffsetNegative() in TestDateTimeFunctions.java", "NaN"], ["9619", "Fix packages in presto-product-tests", "Grzegorz Kokosi\u0144ski", "kokosing", "12/27/17, 12:58:52 PM", "Fix packages in presto-product-tests\n\nQuerygrid relates to one of the Teradata product, there is no much sense\nto use it name it here. Using Teradata would be just enough.\n\nAlso there is no longer such thing like nullconnector, so renaming it to\nblackhole.", "NaN"], ["9622", "Add map_zip_with function", "Wenlei Xie", "wenleix", "01/23/18, 03:22:23 AM", "As a short-term improvement to https://github.com/prestodb/presto/issues/9554", "NaN"], ["9624", "Add Avro file format product tests", "Anu Sudarsan", "anusudarsan", "12/29/17, 08:24:02 PM", "Known limitations:\r\nAVRO file format doesn't support tinyint and smallint datatypes.\r\nAVRO file format doesn't support coercion from bigint to string datatype.", "NaN"], ["9625", "Remove unused bytecode fields for legacy lambda execution", "Wenlei Xie", "wenleix", "12/28/17, 08:20:48 PM", "Lambda execution used to use MethodHandle stored as fields in bytecode\r\ngenerated classes. However, this implementation requires to use\r\nMethodHandle.bindTo for lambda capture and cause reliability\r\nand efficiency issues.\r\n\r\nWe have changed to compile lambda as SAM (Single Abstract Method)\r\nclass instead of MethodHandle. As a result, these fields are\r\nno longer used.", "NaN"], ["9627", "Fix error categorization for empty DWRF files", "David Phillips", "electrum", "12/27/17, 09:18:57 PM", "This change is specific to DWRF, which does not allow empty files. ORC explicitly allows them (recent change in Hive, see #9354), so we'll need a different change in `OrcPageSourceFactory` to allow them.", "NaN"], ["9628", "Fix error categorization for corrupt files during read", "David Phillips", "electrum", "12/28/17, 07:41:15 AM", "NaN", "NaN"], ["9629", "Fix object_not_readable for queries with no partitions", "David Phillips", "electrum", "12/29/17, 02:02:48 AM", "NaN", "NaN"], ["9631", "Fix Hive test when using non-default schema", "David Phillips", "electrum", "12/28/17, 08:41:09 AM", "NaN", "NaN"], ["9632", " Use LocalDate for DATE in tests", "Piotr Findeisen", "findepi", "12/29/17, 02:34:48 PM", "See #9527 for rationale.", "NaN"], ["9633", "Remove unused imports", "Piotr Findeisen", "findepi", "12/28/17, 07:14:40 PM", "Checkstyle didn't catch them as unused, because the enum names are used in a switch statement -(of course this doesn't require an import, but seems enough to mislead the checkstyle)", "NaN"], ["9634", "Remove driver peak memory reservation", "Nezih Yigitbasi", "nezihyigitbasi", "12/28/17, 06:53:24 PM", "This change removes it for mainly two reasons:\r\n- It's non-trivial to collect these stats with the new memory tracking framework\r\n- These stats are not used much, so it will probably not pay off to collect them", "NaN"], ["9635", "Various cleanup for JDBC connectors", "David Phillips", "electrum", "12/29/17, 09:15:03 AM", "NaN", "NaN"], ["9636", "Refactor memory tracking ", "Nezih Yigitbasi", "nezihyigitbasi", "01/13/18, 12:01:23 AM", "This has already been reviewed as part of #9049 (it was merged and rolled back after we found a bug, which this PR fixes) and includes fixes for `OperatorContext::transferMemoryToTaskContext()` and `LocalMemoryContext::trySetBytes()`.\r\n\r\n@sopel39 the scope of the change is described in #9049.\r\n\r", "NaN"], ["9637", "Remove unused declarations", "Nezih Yigitbasi", "nezihyigitbasi", "12/28/17, 10:59:46 PM", "NaN", "NaN"], ["9640", "Allow reading empty ORC files", "David Phillips", "electrum", "12/30/17, 02:11:10 AM", "Fixes #9354", "NaN"], ["9642", "Remove deprecated getNextBatch method in SplitSource", "David Phillips", "electrum", "02/06/18, 03:27:22 PM", "NaN", "NaN"], ["9643", "Fix compilation error by restoring removed variable", "Grzegorz Kokosi\u0144ski", "kokosing", "12/29/17, 10:47:15 AM", "Fix compilation error by restoring removed variable", "NaN"], ["9645", "Update checkstyle version", "Grzegorz Kokosi\u0144ski", "kokosing", "12/30/17, 08:31:27 PM", "Update checkstyle version", "NaN"], ["9646", "TestShowPartitions cleanup", "Piotr Findeisen", "findepi", "12/29/17, 05:03:01 PM", "NaN", "NaN"], ["9647", "Use LocalDateTime/LocalTime for TIMESTAMP/TIME in tests", "Piotr Findeisen", "findepi", "01/02/18, 09:54:38 AM", "See #9527 for rationale.\r", "NaN"], ["9651", "Checkstyle improvements", "David Phillips", "electrum", "01/01/18, 09:04:20 PM", "NaN", "NaN"], ["9652", "Rule unit tests refactoring", "Grzegorz Kokosi\u0144ski", "kokosing", "01/02/18, 07:20:25 PM", "NaN", "NaN"], ["9653", " Allow SHOW PARTITIONS .. WHERE.. for table above partition limit", "Piotr Findeisen", "findepi", "01/13/18, 09:10:47 AM", "Previously, `SHOW PARTITIONS FROM <table>` would fail for Hive table\r\nhaving more partitions than `hive.max-partitions-per-scan`.\r\nHowever, the `hive.max-partitions-per-scan` setting is supposed to\r\ncontrol scans (`SELECT` queries).  In the case of `SHOW PARTITIONS` the\r\ncheck was applied after all relevant partition information was already\r\nretrieved from metastore, so it didn't give any kind of resource\r\nover-allocation counter-measure.\r\n\r\nThis commit moves the check from `HivePartitionManager` to\r\n`HiveSplitManager`, so that remains applied in `SELECT` queries but not\r\nfor `SHOW PARTITIONS`.\r\n\r\nBased on #9648, fixes #7358", "NaN"], ["9655", "Extract bytecode library to separate project", "David Phillips", "electrum", "01/19/18, 04:55:03 PM", "Depends on https://github.com/airlift/bytecode/pull/1", "NaN"], ["9658", "Update to Airbase 79", "David Phillips", "electrum", "01/03/18, 03:55:59 AM", "NaN", "NaN"], ["9660", "Update misleading comment", "Piotr Findeisen", "findepi", "01/03/18, 09:45:25 AM", "The formatting patterns defined by `SqlTimestamp`,\r\n`SqlTimestampWithTimeZone` and `SqlTimeWithTimeZone` are never used (nor\r\nare compatible) with Joda Time.", "NaN"], ["9666", "Restore protected constructor for TestHiveIntegrationSmokeTest", "David Phillips", "electrum", "01/03/18, 05:59:15 PM", "This allows the class to have subclasses with different behavior.", "NaN"], ["9667", "Remove unused methods", "Nezih Yigitbasi", "nezihyigitbasi", "01/04/18, 04:21:53 PM", "NaN", "NaN"], ["9668", "Refactor Driver to remove the initialized flag", "Nezih Yigitbasi", "nezihyigitbasi", "01/04/18, 05:29:21 AM", "NaN", "NaN"], ["9670", "Support varbinary as bytea for PostgreSQL", "Piotr Findeisen", "findepi", "01/05/18, 11:34:00 AM", "Based on @electrum's https://github.com/prestodb/presto/pull/9413", "NaN"], ["9671", "Update exception message when no page sink provided", "Piotr Findeisen", "findepi", "01/17/18, 04:51:32 PM", "Previously the exception would say `No page sink provider for connector 'catalog-name'`.\r\n\r\n  ", "NaN"], ["9672", "Remove inner StateChangeListener class", "Nezih Yigitbasi", "nezihyigitbasi", "01/04/18, 10:02:34 PM", "NaN", "NaN"], ["9674", "Reduce the scope of exception handler in from_unixtime", "Nezih Yigitbasi", "nezihyigitbasi", "01/05/18, 12:29:20 AM", "Minor change to #9618.", "NaN"], ["9677", "Make read transformations extensible in base-jdbc", "Piotr Findeisen", "findepi", "01/17/18, 04:22:41 PM", "This allows connectors extending `presto-base-jdbc` to support other\r\ndata types than supported by `BaseJdbcClient` and `JdbcRecordCursor` out\r\nof the box.", "NaN"], ["9682", "Suppress FieldAccessNotGuarded warnings in PartitionedLookupSourceFactory", "Nezih Yigitbasi", "nezihyigitbasi", "01/06/18, 12:13:09 AM", "@findepi `partitions` is annotated with `@GuardedBy(\"lock\")`, but not read with that lock held.", "NaN"], ["9683", "Support Kerberos authentication between nodes", "Anu Sudarsan", "anusudarsan", "07/20/18, 10:34:57 PM", "Supersedes https://github.com/prestodb/presto/pull/8859. ", "NaN"], ["9684", "Make CLI and JDBC driver respect cookies", "Haozhun Jin", "haozhun", "01/18/18, 01:52:33 AM", "NaN", "NaN"], ["9685", "SetBuildOperator to support yielding when memory limit is exceeded", "Ying", "yingsu00", "01/13/18, 05:31:22 AM", "SetBuildOperator didn't support yielding when tryRehash() asks for more memory than what the system currently has. To support yielding, we added the logic to handle unfinishedWork to the class, and it is done in needsInput(), which is always called by the Driver in every iteration. We didn't put it in getOutput() since SetBuildOperator is always the last operator in the plan tree.\r\n\r\nThe added test would force rehashing to fail by reserving a lot of memory, verify yielding happens\r\ncorrectly, then release the memory and verify rehashing succeeds.", "NaN"], ["9686", "Optimize TopNRowNumberOperator and TopNOperator", "James Sun", "highker", "04/28/18, 08:29:00 PM", "Resolves #5298\r\n \r\nOptimizations:\r\n* Use `GroupedTopNBuilder` to avoid copy of positions originally through `getSingleValueBlock()`\r\n* (`TopNRowNumberOperator` only) reduce complexity of flushing pages from quadratic to linear", "NaN"], ["9692", "Use explicit charset when creating InputStreamReader in HttpPageBufferClient", "Nezih Yigitbasi", "nezihyigitbasi", "01/09/18, 12:03:19 AM", "Otherwise, it uses the platform default charset, which can be anything.", "NaN"], ["9694", "Update to Airlift 0.160", "David Phillips", "electrum", "01/09/18, 12:59:13 AM", "NaN", "NaN"], ["9695", "Fix checkNotHoldsLock in HttpPageBufferClient", "Nezih Yigitbasi", "nezihyigitbasi", "01/16/18, 05:51:01 PM", "Previously this method was just logging an error.", "NaN"], ["9696", "Optimize arbitrary()", "James Sun", "highker", "01/13/18, 02:56:16 AM", "Introduced two new `AccumulatorState`s to avoid using `getObject` or `getSlice`. These two methods create new objects, which causes GC pressure.", "NaN"], ["9699", "Minor refactoring in AddExchanges", "Grzegorz Kokosi\u0144ski", "kokosing", "01/13/18, 07:29:13 PM", "NaN", "NaN"], ["9700", "Minor refactors in ExpressionUtils", "Grzegorz Kokosi\u0144ski", "kokosing", "01/13/18, 07:35:48 PM", "NaN", "NaN"], ["9701", "Execute PickTableLayout before AddExchanges", "Grzegorz Kokosi\u0144ski", "kokosing", "01/22/18, 08:18:26 PM", "Based on #9699 and #9700. Supersedes #8735.\r\n\r\nThis work is needed for making cost based decisions. We need to take our best guess at choosing a table layout before AddExchanges so that we can have more accurate statistics.  ", "NaN"], ["9702", "Add logic to check schema exists before memory&blackhole connector create table/view", "Yuya Ebihara", "ebyhr", "01/15/18, 11:31:07 AM", "Fix #9669 ", "NaN"], ["9704", "Make failAbandonedQueries method more robust", "Nezih Yigitbasi", "nezihyigitbasi", "01/09/18, 07:47:34 PM", "When we fail the abandoned queries if for some reason an exception\r\nis thrown for a query the rest of the queries will not be processed.\r\nThis change fixes that.\r\n\r\nThis PR also logs the problematic query.\r\n\r\nWe still didn't couldn't get to the bottom of the issue, but here is one instance that we saw in production:\r\n\r\n```\r\n2018-01-09T09:18:03.588-0800    WARN    query-management-2      com.facebook.presto.execution.SqlQueryManager   Error cancelling abandoned queries\r\njava.lang.IllegalArgumentException: value is negative\r\n        at io.airlift.units.Preconditions.checkArgument(Preconditions.java:26)\r\n        at io.airlift.units.Duration.<init>(Duration.java:63)\r\n        at com.facebook.presto.execution.QueryStateMachine.getQueryInfo(QueryStateMachine.java:427)\r\n        at com.facebook.presto.execution.QueryStateMachine.updateQueryInfo(QueryStateMachine.java:789)\r\n        at com.facebook.presto.execution.SqlQueryExecution.buildQueryInfo(SqlQueryExecution.java:586)\r\n        at com.facebook.presto.execution.SqlQueryExecution.getQueryInfo(SqlQueryExecution.java:552)\r\n        at com.facebook.presto.execution.SqlQueryManager.failAbandonedQueries(SqlQueryManager.java:627)\r\n        at com.facebook.presto.execution.SqlQueryManager$1.run(SqlQueryManager.java:200)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n```", "NaN"], ["9705", "Check if plan changed in PickTableLayout", "Grzegorz Kokosi\u0144ski", "kokosing", "01/13/18, 07:27:15 PM", "Check if plan changed in PickTableLayout", "NaN"], ["9706", "Remove unnecessary BoundedExecutor", "Nezih Yigitbasi", "nezihyigitbasi", "01/16/18, 06:19:00 PM", "pageBufferClientCallbackExecutor is a fixed-sized thread pool, so\r\nwrapping it with a BoundedExecutor is unnecessary.", "NaN"], ["9707", "Add error for Hive table dropped while adding partitions", "David Phillips", "electrum", "01/09/18, 10:30:38 PM", "NaN", "NaN"], ["9708", "Fix invalid config name in internal communication docs", "David Phillips", "electrum", "01/10/18, 03:33:40 AM", "NaN", "NaN"], ["9710", "Fix CLI to abort query when pager exits", "Haozhun Jin", "haozhun", "01/22/18, 11:45:44 PM", "NaN", "NaN"], ["9711", "Fix query failure when lambda is in UNNEST/VALUES", "Haozhun Jin", "haozhun", "01/18/18, 01:49:53 AM", "Fixes #9709  ", "NaN"], ["9712", "Update to Airlift 0.161", "David Phillips", "electrum", "01/10/18, 02:15:45 AM", "NaN", "NaN"], ["9715", "Remove unused variables in TestHiveMetadata", "Grzegorz Kokosi\u0144ski", "kokosing", "01/13/18, 07:14:34 PM", "Remove unused variables in TestHiveMetadata", "NaN"], ["9716", "Add support for ZSTD or LZ4 compressed ORC files", "David Phillips", "electrum", "02/09/18, 04:16:11 AM", "NaN", "NaN"], ["9719", "Fix deserialization of maps of bing tiles on the client", "Maria Basmanova", "mbasmanova", "01/16/18, 07:31:36 PM", "In most cases objects of BingTileType are serialized as JSON. However, when used as keys in a map they are serialized as strings. Hence, the client needs to handle both cases. \r\n\r\nHere is what the client receives in response to `select map(array[bing_tile(1, 2, 10)], array[bing_tile(3, 4, 11)]);`\r\n\r\n`\"data\":[[{\"BingTile{x=1, y=2, zoom_level=10}\":{\"x\":3,\"y\":4,\"zoom\":11}}]]`\r\n\r\nFixes #9693.\r\n\r\n```\r\npresto> select bing_tile(1,2,3);\r\nselect bing_tile(1,2,3);\r\n       _col0        \r\n--------------------\r\n {x=1, y=2, zoom=3} \r\n(1 row)\r\n\r\npresto> select array[bing_tile(1, 2, 10)];\r\nselect array[bing_tile(1, 2, 10)];\r\n         _col0         \r\n-----------------------\r\n [{x=1, y=2, zoom=10}] \r\n(1 row)\r\n\r\npresto> select map(array[bing_tile(1, 2, 10)], array[bing_tile(3, 4, 11)]);\r\nselect map(array[bing_tile(1, 2, 10)], array[bing_tile(3, 4, 11)]);\r\n                          _col0                          \r\n---------------------------------------------------------\r\n {BingTile{x=1, y=2, zoom_level=10}={x=3, y=4, zoom=11}} \r\n(1 row)\r\n```", "NaN"], ["9720", "Simplify array and map unnesters", "Nezih Yigitbasi", "nezihyigitbasi", "01/16/18, 05:50:22 PM", "NaN", "NaN"], ["9721", "Optimize min_by and max_by", "James Sun", "highker", "02/23/18, 06:54:42 PM", "rebased on top of #9696; resolves #9553", "NaN"], ["9722", "Make order of Authenticators to be determined", "Grzegorz Kokosi\u0144ski", "kokosing", "01/21/18, 09:54:31 AM", "Make order of Authenticators to be determined\n\nThis commits make that AuthenticationFilter will apply Authenticators in\norder of how they were specified in http-server.authentication.type.\n\nSo far order was hard-coded causing CERTIFICATE authorization to have the\nhighest priority, however it may depends of the use case. Sometimes user\nmay prefer KERBEROS or LDAP over CERTIFICATE.", "NaN"], ["9723", "Add script to stop all product tests containers", "Grzegorz Kokosi\u0144ski", "kokosing", "01/13/18, 07:22:34 PM", "NaN", "NaN"], ["9725", "Fix freeing revocable memory after query moved to reserved pool", "Piotr Findeisen", "findepi", "01/17/18, 03:03:47 PM", "Fixes #9488", "NaN"], ["9726", "Fix update of prompt after USE statement in CLI", "David Phillips", "electrum", "01/11/18, 07:09:16 PM", "NaN", "NaN"], ["9727", "Update MemoryPool to return futures that are not cancellable", "Nezih Yigitbasi", "nezihyigitbasi", "01/15/18, 07:57:21 PM", "It's possible to cancel the ListenableFutures that the MemoryPool\r\nreturns (and unblock the callers waiting on those futures to allocate\r\nmore memory), so this change makes sure that the returned futures do not\r\nsupport cancellation just to be safe. (as discussed in #9636)", "NaN"], ["9728", "Update verifier doc", "Wenlei Xie", "wenleix", "01/12/18, 03:39:35 AM", "NaN", "NaN"], ["9730", "Enforce CPU time limits in SqlQueryManager", "Nezih Yigitbasi", "nezihyigitbasi", "01/15/18, 07:57:06 PM", "Previously it was enforced in `ClusterMemoryManager`. Given that `SqlQueryManager`\r\nis already enforcing a bunch of other limits and given that this limit is not\r\nrelated to memory, enforcing it in `SqlQueryManager` makes more sense.", "NaN"], ["9731", "Fix performance regression in SourcePartitionedScheduler", "Haozhun Jin", "haozhun", "01/12/18, 12:59:47 AM", "`AbstractSet.removeAll(c)` performs terribly when `this.size() <= c.size()`\r\nand `c.contains` is not `O(1)`.", "NaN"], ["9734", "Remove display-only stats tests", "Grzegorz Kokosi\u0144ski", "kokosing", "01/13/18, 07:21:12 PM", "Remove display-only stats tests\n\nRemove statistics tests which just display statistics for\nwhole tree for manual analysis. Those proved to not be very useful\nand mechanism it uses (getting stats from EXPLAIN plan) would not work\nfor more detailed column statistics anyway.", "NaN"], ["9736", "Add missed finishInsert method in MongoDB connector", "Yuya Ebihara", "ebyhr", "01/13/18, 07:20:40 PM", "Fix #9724 ", "NaN"], ["9737", " Fix version in the memory context module pom", "Nezih Yigitbasi", "nezihyigitbasi", "01/13/18, 12:25:50 AM", "NaN", "NaN"], ["9741", "Rename cost concept to stats", "Piotr Findeisen", "findepi", "01/17/18, 03:02:49 PM", "1. `PlanNodeCost` that currently exists in master represents statistics (currently: row count, total size in bytes)\r\n2. this commit gives it a more appropriate name (`PlanNodeStatsEstimate`)\r\n3. a class representing cost (true cost) will be introduced later", "NaN"], ["9742", "Change argument type in TpchMetadata.getPrestoType to TpchColumn", "Piotr Findeisen", "findepi", "01/15/18, 03:15:00 PM", "Minor refactor to make code shorter", "NaN"], ["9743", "Improve exception handling in BasePlanTest", "Piotr Findeisen", "findepi", "01/15/18, 03:05:11 PM", "(very minorest refactorlet)", "NaN"], ["9745", "Use specified schema in presto-memory's rename table", "Yuya Ebihara", "ebyhr", "01/18/18, 10:32:38 AM", "Fix #9688 \r\n\r\nPreviously, RENAME TABLE in presto-memory always renames table to same schema even if the target schema is specified. \r\nThis commit changes the behavior to use the specified schema. \r\nIf the schema does not exist, throw SchemaNotFoundException.", "NaN"], ["9747", "Migrate determine join distribution type", "Piotr Findeisen", "findepi", "01/17/18, 03:04:05 PM", "This is alternative approach to https://github.com/prestodb/presto/pull/9744 -- it doesn't try to migrate selecting semi-join distribution type to iterative optimizer (it currently cannot be done without hacks).", "NaN"], ["9750", "Fix hive's drop column causes ConcurrentModificationException", "Yuya Ebihara", "ebyhr", "01/17/18, 01:32:10 PM", "Fix #9733 ", "NaN"], ["9751", "Remove unused local variable in CursorProcessorCompiler", "Nezih Yigitbasi", "nezihyigitbasi", "01/18/18, 10:16:13 PM", "NaN", "NaN"], ["9752", "Construct \"proper literal\" for CHAR in LiteralInterpreter.toExpression", "Piotr Findeisen", "findepi", "01/18/18, 11:04:17 AM", "NaN", "NaN"], ["9755", "Fix formatting - remove blank line before {", "Grzegorz Kokosi\u0144ski", "kokosing", "01/17/18, 12:53:41 PM", "Fix formatting - remove blank line before {", "NaN"], ["9759", "Add global bucket access support to S3 filesystem", "Nezih Yigitbasi", "nezihyigitbasi", "01/26/18, 08:53:06 PM", "Fixes #9758.", "NaN"], ["9760", "Prevent cyclic failure references in toFailure", "James Sun", "highker", "01/17/18, 11:58:25 PM", "toFailure(s) can create cyclic suppression given it is recursive. Use a\r\nhash set to detect seen failures.", "NaN"], ["9761", "Make default JDBC read mappings reusable", "David Phillips", "electrum", "01/18/18, 01:28:36 AM", "NaN", "NaN"], ["9762", "Introduce benchmark for planner", "Grzegorz Kokosi\u0144ski", "kokosing", "01/19/18, 12:29:35 PM", "Introduce benchmark for planner\r\n\r\nInitial version of planner benchmark.\r\n\r\n```\r\nBenchmark                     (iterativeOptimizerEnabled)  (queryPrefix)\r\nMode                                               Cnt    Score    Error  Units\r\nBenchmarkPlanner.planQueries  true          avgt   20  391.363 \u00b1 10.498  ms/op\r\nBenchmarkPlanner.planQueries  true  EXPLAIN avgt   20  438.380 \u00b1 21.655  ms/op\r\nBenchmarkPlanner.planQueries  false         avgt   20  330.634 \u00b1 11.935  ms/op\r\nBenchmarkPlanner.planQueries  false EXPLAIN avgt   20  359.638 \u00b1 11.664  ms/op\r\n```\r\n\r\nWith optimizer called two times:\r\n```\r\nBenchmark                     (iterativeOptimizerEnabled)  (queryPrefix)\r\nMode                                               Cnt    Score    Error  Units\r\nBenchmarkPlanner.planQueries  true          avgt   20  646.199 \u00b1 19.304  ms/op\r\nBenchmarkPlanner.planQueries  true EXPLAIN  avgt   20  792.024 \u00b1 57.285  ms/op\r\nBenchmarkPlanner.planQueries  false         avgt   20  560.272 \u00b1 21.037  ms/op\r\nBenchmarkPlanner.planQueries  false EXPLAIN avgt   20  649.590 \u00b1 26.306  ms/op\r\n```", "NaN"], ["9764", "Remove left over throws clause", "Nezih Yigitbasi", "nezihyigitbasi", "01/18/18, 12:40:35 AM", "It causes compilation failure.", "NaN"], ["9767", "Test for cost/stats calculation done by CoefficientBasedStatsCalculator (v2)", "Grzegorz Kokosi\u0144ski", "kokosing", "01/19/18, 08:07:05 AM", "NaN", "NaN"], ["9768", "Fix MemoryPoolInfo queryMemoryRevocableReservations initialization", "Dongmin Yu", "miniway", "01/18/18, 06:41:11 PM", "NaN", "NaN"], ["9770", "Remove unused EmptyDeleteOptimizer", "Grzegorz Kokosi\u0144ski", "kokosing", "01/19/18, 07:52:10 AM", "Remove unused EmptyDeleteOptimizer\n\nIt looks like a mistake that during migration of EmptyDeleteOptimizer no\nlegacy rule was used IterativeOptimizer. But since there were no bug\nreported for that for a long time it seems that it is safe to remove\nit.", "NaN"], ["9771", "Fix testNormalCdf unit test", "Nezih Yigitbasi", "nezihyigitbasi", "01/18/18, 11:42:58 PM", "```\r\n[ERROR] Tests run: 3643, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 692.581 s <<< FAILURE! - in TestSuite\r\n[ERROR] testNormalCdf(com.facebook.presto.operator.scalar.TestMathFunctions)  Time elapsed: 0.335 s  <<< FAILURE!\r\njava.lang.AssertionError: expected [true] but found [false]\r\n\tat com.facebook.presto.operator.scalar.TestMathFunctions.testNormalCdf(TestMathFunctions.java:1330)\r\n\r\n[INFO] \r\n[INFO] Results:\r\n[INFO] \r\n[ERROR] Failures: \r\n[ERROR]   TestMathFunctions.testNormalCdf:1330->AbstractTestFunctions.assertInvalidFunction:129 expected [true] but found [false]\r\n[INFO] \r\n[ERROR] Tests run: 3643, Failures: 1, Errors: 0, Skipped: 0\r\n```", "NaN"], ["9774", "Update documentation for spiller-max-used-space-threshold", "Piotr Findeisen", "findepi", "01/19/18, 05:32:19 PM", "NaN", "NaN"], ["9775", "Enfore TableScanNode's currentConstraint is set only when layout chosen", "Piotr Findeisen", "findepi", "01/20/18, 05:25:21 PM", "NaN", "NaN"], ["9777", "Optimize tableScan with layout with predicate TupleDomain:none", "Grzegorz Kokosi\u0144ski", "kokosing", "03/26/18, 05:43:13 AM", "Optimize tableScan with layout with predicate TupleDomain:none\r\n\r\nFixes #9776 ", "NaN"], ["9784", "Use SerdeProperties when adding Hive partition", "Yuya Ebihara", "ebyhr", "01/23/18, 02:29:11 PM", "Fix #7301 ", "NaN"], ["9789", "Update tests for simba JDBC driver", "Grzegorz Kokosi\u0144ski", "kokosing", "02/06/18, 11:39:53 AM", "NaN", "NaN"], ["9790", "Inline TableLayoutRewriter", "Grzegorz Kokosi\u0144ski", "kokosing", "01/23/18, 09:02:45 AM", "Inline TableLayoutRewriter", "NaN"], ["9791", "Add the peak of total memory reservation to query stats", "Nezih Yigitbasi", "nezihyigitbasi", "01/23/18, 09:23:30 PM", "This value is the peak of the user + system memory reservation over time.\r\nWhen we remove the system memory pool and start enforcing memory limits to\r\nsystem memory reservations, this value will tell us whether a particular\r\nquery may hit the memory limit.", "NaN"], ["9793", "Fix RowNumber optimization bug in planner for filters with negative or zero values", "Ying", "yingsu00", "01/25/18, 03:32:16 AM", "We recently found two issues in RowNumber optimization in planner:\r\nhttps://github.com/prestodb/presto/issues/9779 Planner fails to create correct TopNRowNumber Node with invalid filters on row_number() \r\nhttps://github.com/prestodb/presto/issues/9792 row_number() produces wrong results with some inequality filters \r\n\r\nThe problem was that the planner didn't validate the upperbound. The fix was to return an empty upperbound for filters like row_number() = -1 or row_number() <= -1.", "NaN"], ["9795", "Use io.prestodb.tempto 1.42", "\u0141ukasz Osipiuk", "losipiuk", "01/24/18, 04:49:49 PM", "NaN", "NaN"], ["9797", "Add Hive storage format session property", "James Sun", "highker", "01/31/18, 09:14:49 PM", "Add Hive storage format and respect table format session properties.\r\nThese two properties are experimental and should be reverted in the\r\nfuture.", "NaN"], ["9798", "Optimize the ST_Contains function", "Maria Basmanova", "mbasmanova", "01/26/18, 07:04:07 PM", "It turns out that 60% of execution time for ST_Contains is spent deserializing geometries from slices. This PR adds a benchmark to show that and optimizes ST_Contains to avoid costly deserialization when possible.\r\n\r\nThe benchmark runs ST_Contains for a large polygon and 3 points:\r\n* inside the polygon, \r\n* outside of the polygon, but inside the polygon's bounding box,\r\n* outside of the polygon's bounding box. \r\n\r\nThe benchmark runs ST_Contains using both slices and geometries. Here are the results:\r\n\r\n```\r\nBenchmark                                                            Mode  Cnt       Score       Error  Units\r\nBenchmarkSTContains.deserialize                                      avgt   20  131919.856 \u00b1  4921.112  ns/op\r\nBenchmarkSTContains.deserializeSimpleGeometry                        avgt   20     486.274 \u00b1    13.489  ns/op\r\nBenchmarkSTContains.stContainsInnerPoint                             avgt   20  227284.106 \u00b1 18774.481  ns/op\r\nBenchmarkSTContains.stContainsInnerPointDeserialized                 avgt   20   66938.920 \u00b1  1570.299  ns/op\r\nBenchmarkSTContains.stContainsInnerPointSimpleGeometry               avgt   20     813.359 \u00b1    34.580  ns/op\r\nBenchmarkSTContains.stContainsOuterPointInEnvelope                   avgt   20  218173.265 \u00b1  9345.754  ns/op\r\nBenchmarkSTContains.stContainsOuterPointInEnvelopeDeserialized       avgt   20   69004.912 \u00b1  2625.112  ns/op\r\nBenchmarkSTContains.stContainsOuterPointInEnvelopeSimpleGeometry     avgt   20     837.565 \u00b1    58.022  ns/op\r\nBenchmarkSTContains.stContainsOuterPointNotInEnvelope                avgt   20  134204.161 \u00b1  5208.045  ns/op\r\nBenchmarkSTContains.stContainsOuterPointNotInEnvelopeDeserialized    avgt   20      30.272 \u00b1     1.232  ns/op\r\nBenchmarkSTContains.stContainsOuterPointNotInEnvelopeSimpleGeometry  avgt   20     643.869 \u00b1    15.003  ns/op\r\n```\r\n\r\nFor points within the polygon's bounding box, ST_Contains on geometries is almost 3 times faster than on slices. For points outside of the polygon's bounding, the difference is even larger.\r\n\r\nST_Contains runs in two phases: (1) check if bounding box of the left geometry contains bounding box of the right geometry; if not, return false; (2) check if the left geometry contains the right geometry. This is what makes ST_Contains for points outside of polygon's bounding box run faster.\r\n\r\nPhase 1 doesn't require full geometries, only their bounding boxes. Hence, it is sufficient to deserialize just that part (read 4 doubles). Making this change greatly speeds up ST_Contains for cases when computation completes in phase 1 and doesn't proceed to phase 2 (e.g. point is outside polygon's bounding box). Benchmark results for the optimized version of the\r\nST_Contains show 6x improvement for simple geometry and 1000x improvement for\r\ncomplex geometry.\r\n\r\n```\r\nBenchmark                                                            Mode  Cnt       Score       Error  Units\r\nBenchmarkSTContains.deserialize                                      avgt   20  138402.816 \u00b1 11643.730  ns/op\r\nBenchmarkSTContains.deserializeEnvelope                              avgt   20      59.143 \u00b1     2.541  ns/op\r\nBenchmarkSTContains.deserializeEnvelopeSimpleGeometry                avgt   20      61.340 \u00b1     3.933  ns/op\r\nBenchmarkSTContains.deserializeSimpleGeometry                        avgt   20     499.509 \u00b1    22.847  ns/op\r\nBenchmarkSTContains.stContainsInnerPoint                             avgt   20  231484.522 \u00b1 17018.650  ns/op\r\nBenchmarkSTContains.stContainsInnerPointDeserialized                 avgt   20   67700.512 \u00b1  2490.978  ns/op\r\nBenchmarkSTContains.stContainsInnerPointSimpleGeometry               avgt   20     920.532 \u00b1    21.465  ns/op\r\nBenchmarkSTContains.stContainsOuterPointInEnvelope                   avgt   20  206741.486 \u00b1  3162.731  ns/op\r\nBenchmarkSTContains.stContainsOuterPointInEnvelopeDeserialized       avgt   20   66869.945 \u00b1  1507.815  ns/op\r\nBenchmarkSTContains.stContainsOuterPointInEnvelopeSimpleGeometry     avgt   20     919.686 \u00b1    28.572  ns/op\r\nBenchmarkSTContains.stContainsOuterPointNotInEnvelope                avgt   20     109.417 \u00b1     2.514  ns/op\r\nBenchmarkSTContains.stContainsOuterPointNotInEnvelopeDeserialized    avgt   20      29.153 \u00b1     0.734  ns/op\r\nBenchmarkSTContains.stContainsOuterPointNotInEnvelopeSimpleGeometry  avgt   20     109.529 \u00b1     2.475  ns/op\r\n```\r", "NaN"], ["9804", "Turn on new local scheduler by default", "Raghav Sethi", "raghavsethi", "01/27/18, 12:21:41 AM", "NaN", "NaN"], ["9806", "Consider boxing when checking whether value matches type", "Piotr Findeisen", "findepi", "01/30/18, 07:49:44 PM", "This fixes index join in TPCH connector (`IndexedTpchConnectorFactory`)", "NaN"], ["9807", " Fix static import of format method", "Piotr Findeisen", "findepi", "01/30/18, 07:50:33 PM", "With `MessageFormat.format` being imported instead of `String.format`,\r\nthe code would produce exception with `%s` not being replaced:\r\n\r\n```\r\n   Query failed: All properties [%s, %s, %s] must be set if any are set\r\n```", "NaN"], ["9812", "Small fixes", "Piotr Findeisen", "findepi", "01/30/18, 07:49:08 PM", "NaN", "NaN"], ["9816", "Fix \"Tuple domain handles must have assigned symbols\" error", "Martin Traverso", "martint", "01/25/18, 12:44:45 AM", "Fixes https://github.com/prestodb/presto/issues/9800\r\n\r\nNo test for now. It requires a connector that supports predicate pushdown and index joins.", "NaN"], ["9818", "Fix query hang when partitions are offline for retention", "German Gil", "ggilfb", "02/05/18, 08:23:19 PM", "NaN", "NaN"], ["9820", "Enhance error message when session property used unknown catalog", "Yi He", "hellium01", "01/30/18, 08:39:03 PM", "When user set a session property with unknown catalog, current message is misleading. This makes debug hard since the session property won't appear in UI or query event log if validity check fails first. \r\n\r\nWhat happened in production is that we see a lot of catalog does not exist error but the queries do not use the catalog at all. There is almost no way to immediately figure out what caused the queries to fail. ", "NaN"], ["9823", "Remove predicate pushdown switch in TPCH connector (expose and test for #9800 bug)", "Grzegorz Kokosi\u0144ski", "kokosing", "02/05/18, 09:10:50 AM", "NaN", "NaN"], ["9826", "Fix memory accounting in TopNRowNumberOperator", "Nezih Yigitbasi", "nezihyigitbasi", "01/25/18, 07:55:16 PM", "Fixes an issue I found during verification:\r\n\r\n```\r\njava.sql.SQLException: Query failed (#20180125_152704_03745_hb4s3): bytes cannot be negative\r\n\tat com.facebook.presto.jdbc.PrestoResultSet.resultsException(PrestoResultSet.java:1793)\r\n\tat com.facebook.presto.jdbc.PrestoResultSet$ResultsPageIterator.computeNext(PrestoResultSet.java:1781)\r\n\tat com.facebook.presto.jdbc.PrestoResultSet$ResultsPageIterator.computeNext(PrestoResultSet.java:1747)\r\n\tat com.facebook.presto.jdbc.internal.guava.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:145)\r\n\tat com.facebook.presto.jdbc.internal.guava.collect.AbstractIterator.hasNext(AbstractIterator.java:140)\r\n\tat com.facebook.presto.jdbc.internal.guava.collect.TransformedIterator.hasNext(TransformedIterator.java:42)\r\n\tat com.facebook.presto.jdbc.internal.guava.collect.MultitransformedIterator.hasNext(MultitransformedIterator.java:50)\r\n\tat com.facebook.presto.jdbc.internal.guava.collect.MultitransformedIterator.hasNext(MultitransformedIterator.java:50)\r\n\tat com.facebook.presto.jdbc.PrestoResultSet.next(PrestoResultSet.java:140)\r\n\tat com.facebook.presto.verifier.Validator.convertJdbcResultSet(Validator.java:525)\r\n\tat com.facebook.presto.verifier.Validator.lambda$getResultSetConverter$4(Validator.java:499)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\r\n\tat com.google.common.util.concurrent.SimpleTimeLimiter.callWithTimeout(SimpleTimeLimiter.java:128)\r\n\tat com.facebook.presto.verifier.Validator.executeQuery(Validator.java:430)\r\n\tat com.facebook.presto.verifier.Validator.executeQueryTest(Validator.java:307)\r\n\tat com.facebook.presto.verifier.Validator.validate(Validator.java:218)\r\n\tat com.facebook.presto.verifier.Validator.valid(Validator.java:190)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: bytes cannot be negative\r\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:122)\r\n\tat com.facebook.presto.memory.context.SimpleLocalMemoryContext.setBytes(SimpleLocalMemoryContext.java:55)\r\n\tat com.facebook.presto.operator.OperatorContext$DecoratedLocalMemoryContext.setBytes(OperatorContext.java:632)\r\n\tat com.facebook.presto.operator.TopNRowNumberOperator.getPage(TopNRowNumberOperator.java:341)\r\n\tat com.facebook.presto.operator.TopNRowNumberOperator.getOutput(TopNRowNumberOperator.java:252)\r\n\tat com.facebook.presto.operator.Driver.processInternal(Driver.java:379)\r\n\tat com.facebook.presto.operator.Driver.lambda$processFor$8(Driver.java:278)\r\n\tat com.facebook.presto.operator.Driver.tryWithLock(Driver.java:645)\r\n\tat com.facebook.presto.operator.Driver.processFor(Driver.java:272)\r\n\tat com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:975)\r\n\tat com.facebook.presto.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:162)\r\n\tat com.facebook.presto.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:492)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n```", "NaN"], ["9830", "Ensure tables initially not analyzed in TestHiveTableStatistics", "Piotr Findeisen", "findepi", "01/30/18, 07:51:20 PM", "`TestHiveTableStatistics` needs control over whether tables are\r\ninitially analyzed, because it tests both situations: analyzed and\r\nnon-analyzed tables (and deleting stats in Hive is not possible).", "NaN"], ["9831", "Simplify stats calculation code", "Piotr Findeisen", "findepi", "01/30/18, 07:52:15 PM", "NaN", "NaN"], ["9832", "Update Airlift to 0.162", "Raghav Sethi", "raghavsethi", "01/26/18, 10:25:31 PM", "NaN", "NaN"], ["9838", "Add missing 'alwaysRun' in @AfterClass", "Piotr Findeisen", "findepi", "01/30/18, 07:51:43 PM", "We do always `@AfterClass(alwaysRun = true)` except for two cases. I updated those two.", "NaN"], ["9839", "Fix predicate push-down in TPCH connector", "Piotr Findeisen", "findepi", "01/30/18, 07:48:37 PM", "Previously, for `TupleDomain` on column other than `orderstatus`, it\r\nwould incorrectly conclude no `orderstatus` value matches and return\r\nempty `TupleDomain`.", "NaN"], ["9840", "Remove unused variable from LocalExchange", "Karol Sobczak", "sopel39", "01/31/18, 10:23:08 AM", "NaN", "NaN"], ["9842", "Make local exchanger responsible for blocking writes", "Karol Sobczak", "sopel39", "02/05/18, 12:43:09 PM", "It is more natural if local exchange is responsible for blocking writes. This allows to customize local exchangers even more (e.g: for distributed sort).\r\n\r\nFor distributed sort I will create an exchanger that manages memory pool slightly differently than other exchanges. It will give each source 1/N of total local exchange memory.", "NaN"], ["9843", "Minor code cleanup in TestStateCompiler", "Piotr Findeisen", "findepi", "01/30/18, 07:52:31 PM", "NaN", "NaN"], ["9844", "Expose low/high value in statistics for Hive tables", "Piotr Findeisen", "findepi", "01/31/18, 11:29:29 AM", "\r\ncc @losipiuk ", "NaN"], ["9845", "Add 0.193 release notes", "Nezih Yigitbasi", "nezihyigitbasi", "01/30/18, 05:09:07 PM", "NaN", "NaN"], ["9847", "Use whenAnyCompleteCancelOthers in Airlift", "Haozhun Jin", "haozhun", "01/30/18, 06:18:46 PM", "NaN", "NaN"], ["9848", "Short-circuit inner and right join when right side is empty", "German Gil", "ggilfb", "04/04/18, 05:49:31 PM", "This is a fix for: #9769. After looking more at the problem, I don't think we can just do the check at planning (as in a large percentage of the cases, a table scan is involved). I discussed the issue with with @dain & @rongrong, they pointed me to an implementation of the fix, so that was my starting point (original PR: https://github.com/prestodb/presto/pull/7275).", "NaN"], ["9849", "Approximate object references for ReferenceCountMap", "James Sun", "highker", "02/09/18, 11:24:57 PM", "ReferenceCountMap maps from objects to reference counts. Its keys are\r\nstored in a single object array. In some extreme cases, the array grows\r\nto hundreds of millions of objects. This can cause G1GC to push huge\r\namounts of object pointers to a task queue, which can both lead to GC\r\noverhead as well as queue overflow. The problem can be solved by using\r\nbig arrays to store objects. However, the performance overhead (other than\r\nGC) can still be high. This patch combines both identity hash code and\r\nthe size of the objects as the key to identity an object. Collision can\r\nhappen but according to benchmark is neglectable.", "NaN"], ["9851", "Change StatsCalculator API and cache stats in Memo", "Piotr Findeisen", "findepi", "01/31/18, 11:29:45 AM", "This introduces support for on-demand Stats calculation in Rules, keeping calculated stats in Memo.\r\n\r\nInitial commits (`Reorder fields to match constructor arguments`, `Remove Filter above TableScan logic from CoefficientBasedStatsCalculator`, `Use doubles instead Estimates in PlanNodeStatsEstimate`) were already reviewed in https://github.com/prestodb/presto/pull/9812, https://github.com/prestodb/presto/pull/9831.", "NaN"], ["9852", "Add close() method to ConnectionFactory interface", "Piotr Findeisen", "findepi", "01/30/18, 07:48:09 PM", "NaN", "NaN"], ["9853", "Remove LocalExchangeMemoryManager#setNoBlockOnFull", "Karol Sobczak", "sopel39", "02/06/18, 10:20:12 AM", "This methods is not needed as pages are drained\r\nfrom LocalExchangeSink in LocalExchangeSource#close.\r\nThis causes LocalExchangeMemoryManager#notFullFuture\r\nto unblock.", "NaN"], ["9857", "Fix flaky test TestQueryManager.testQueryCpuLimit", "Nezih Yigitbasi", "nezihyigitbasi", "01/31/18, 05:27:12 PM", "We now run a query that will run for a longer time to give more time to the\r\nSqlQueryManager thread to kill it. We also explicitly wait for the final\r\nquery state after running the query instead of waiting for the SqlQueryManager\r\nthread implicitly.\r\n\r\nFixes #9856.", "NaN"], ["9862", "Fix peak memory in query details UI", "Nezih Yigitbasi", "nezihyigitbasi", "02/05/18, 05:56:21 PM", "The name of this field changed in the QueryStats json causing it to be empty\r\nin the query details UI.", "NaN"], ["9864", "Add refresh failure counter to DB resource group manager", "Raghav Sethi", "raghavsethi", "02/02/18, 05:19:53 AM", "NaN", "NaN"], ["9865", "Move LDAP authentication to password authenticator plugin", "David Phillips", "electrum", "02/09/18, 02:33:24 AM", "Fixes #7632", "NaN"], ["9866", "Composable Stats Calculator (phase 1)", "Piotr Findeisen", "findepi", "02/09/18, 07:57:27 AM", "This introduces `ComposableStatsCalculator`, a `StatsCalculator` that is meant to replace `CoefficientBasedStatsCalculator` with better statistic calculation rules.\r\n\r\ncc @losipiuk @fiedukow \r\n\r\n@kokosing review list (@findepi please do not mess with commits):\r\n\r\n - [x] Support column stats in StatisticsAssertion\r\n - [x] Change type of ResultWithQueryId.queryId to QueryId\r\n - [x] Add createPlan() method to QueryRunner interface\r\n - [x] Support LocalQueryRunner in StatisticsAssertion\r\n - [x] Add PlanNodeStatisticsAssertion\r\n - [x] Add ComposableStatsCalculator\r\n - [x] Allow using ComposableStatsCalculator instead of CoefficientBasedStatsCalculator\r\n - [x] Add stats calculator unit testing framework\r\n - [x] Add TestTpchLocalStats stub\r\n - [x] Add OutputStatsRule\r\n - [x] Add TableScanStatsRule\r\n - [x] Add FilterStatsRule\r\n - [x] Add stats calculation for FilterNode comparisons\r\n - [x] Add stats calculation for FilterNode boolean expressions\r\n - [x] Add stats calculation for FilterNode logical operations\r\n - [x] Add stats calculation for FilterNode is (not) null expression\r\n - [x] Add stats calculation for FilterNode comparison related operators\r\n - [x] Assume unknown value domain for types not representable as double\r\n - [x] Replace PlanStatsMatcher with StatsOutputRowCountMatcher\r\n - [x] TestStatsCalculator tests new version of calculator", "NaN"], ["9867", "Compare nanos delta instead of direct comparison", "Karol Sobczak", "sopel39", "02/01/18, 10:24:18 AM", "NaN", "NaN"], ["9868", "Introduce OrderingScheme", "Karol Sobczak", "sopel39", "02/05/18, 11:46:39 AM", "Part of: https://github.com/prestodb/presto/pull/9854", "NaN"], ["9869", "Fix deadlock in QueryContext::setMemoryPool", "Nezih Yigitbasi", "nezihyigitbasi", "01/31/18, 10:19:34 PM", "    Previously when setMemoryPool is called while an operator is allocating memory\r\n    it's possible that these two threads deadlock as they acquire the monitors in\r\n    different orders. The thread calling setMemoryPool was acquiring the monitor of\r\n    the QueryContext instance/this first and then the monitor of the user/revocable\r\n    memory contexts in the queryMemoryContext. However, the driver threads reserving memory\r\n    were acquiring the monitors of the user/revocable memory contexts first, and\r\n    then the monitor of the QueryContext instance.\r\n\r\n    This change solve the issue by preventing locking of the user/revocable memory contexts\r\n    in the queryMemoryContext by getting the same information from the MemoryPool.", "NaN"], ["9872", "Support lambda function for regex replacement", "Ying", "yingsu00", "03/01/18, 02:04:12 AM", "There are some uses cases of string manipulation that can be efficiently supported if regexp_replace can take a lamdba function to define the replacement. The lambda function will take an array of match groups. For example, INITCAP in Oracle could be written as regexp_replace(string, '(\\w)(\\w*)', x -> upper(x[1]) || lower(x[2]))\r\n\r\nUpdate:\r\nThe document description for this new regexp_replace() function is put into regexp.rst. The function is still named as regexp_replace().\r\n\r\nWhen lambda returns null, the whole replace string becomes null. This is because in SQL the null represents \"not known\". If we replace a matched group with unknown value, the whole result becomes unknown. The current presto regexp_replace follows the same way:\r\nselect REGEXP_REPLACE('abc', 'b', null);\r\nResult is null.\r\n\r\nThe first capturing group(the entire match) for each match is not passed to lambda. There are two main reasons for this decision:\r\n  1) performance considerations: Since the entire match can always be passed if the user uses () for the whole pattern, we don't need to pass the whole match when it's not necessary. Passing it always incurs extra cost.\r\n 2) To follow the current numbering convention in existing regexp_replace function: $1 represents the first match, $2 the second. E.g.\r\n    SELECT regexp_replace('1a 2b 14m', '(\\d+)([ab]) ', '3c$2 '); -- '3ca 3cb 14m'\r\nSo the INITCAP example would be regexp_replace('new york', '(\\w)(\\w*)', x -> upper(x[1]) || lower(x[2])) = 'New York'\r\n\r\nhttps://github.com/prestodb/presto/issues/9772", "NaN"], ["9874", "Minor CLI and JDBC improvements", "David Phillips", "electrum", "02/01/18, 07:55:12 PM", "NaN", "NaN"], ["9876", "Create distributed plans in BenchmarkPlanner", "Piotr Findeisen", "findepi", "02/05/18, 10:20:37 AM", "`BenchmarkPlanner` runs both iterative optimizer enabled and disabled.\r\nWithout iterative optimizer, and with `forceSingleNode=true`, the\r\ncreated plan is invalid and eventually fails.\r\n\r\nThis disables `forceSingleNode` so that we always produce a plan without\r\nfailure.", "NaN"], ["9877", "Add a test that Presto works without iterative optimizer", "Piotr Findeisen", "findepi", "02/05/18, 10:20:22 AM", "NaN", "NaN"], ["9879", "Fix planning performance regression for tables with many partitions", "Martin Traverso", "martint", "02/02/18, 08:34:50 PM", "This fixes a performance regression due to a recent change to make\r\nPickTableLayout run before AddExchanges (e6d963b).\r\n\r\nFor queries containing complex expression over partition keys, PickTableLayout was\r\nnot using this information when asking the connector for a layout. If a Hive\r\ntable has many partitions (several thousand), this causes the TableScan node to\r\ncontain a \"current constraint\" that enumerates all the possible values of the key.\r\n\r\nLater, when AddExchanges.planTableScan executes, it builds an expression out of the\r\ncurrent constraint and any additional filters, which the Hive connector uses to prune\r\nuninteresting partitions. Due to historical reasons, evaluating that expression can be\r\nvery expensive. It requires analyzing the expression, doing type inference, finding\r\nfunction implementations, etc.\r\n\r\nWith this change, we avoid handing getLayouts() an expression that's expensive to evaluate by simplifying TupleDomains with too many discrete terms.\r\n\r\n(redo of https://github.com/prestodb/presto/pull/9871)", "NaN"], ["9882", "Lazy load buffer for large ORC streams", "James Sun", "highker", "02/06/18, 11:14:07 PM", "An 8 MB buffer will be created for a large ORC stream when reading a\r\nnew stripe. However the buffer may not be used given the data will not\r\nnecessarily be loaded. In production we found frequent full GC due to\r\nfast allocation and release of such buffers. This patch delays the\r\nbuffer creation to when the data is to be loaded.", "NaN"], ["9886", "Make TPCH to support predicate pushdown form PART.container and type and apply Layout Constarint.predicate", "Grzegorz Kokosi\u0144ski", "kokosing", "02/06/18, 08:44:22 AM", "This is based on https://github.com/prestodb/presto/pull/9823.\r\n\r\nThe aim is to provide coverage for `AddExchanges` for tables with multiple partitions so in future we could avoid regressions in planner. See related fix https://github.com/prestodb/presto/pull/9879.\r\n\r\nNow TPCH connector support predicate pushdown for PART container (40 values) and type (150 values). #9879 is about case where table has thousands of partitions, here we have only 40 +150, but anyway it was possible to see the impact in `BenchmarkPlanner` where with this change applied:\r\n\r\n```\r\nCurrently:\r\nBenchmark                     Mode  Cnt     Score    Error  Units\r\nBenchmarkPlanner.planQueries  avgt   20  2987.712 \u00b1 89.916  ms/op\r\n\r\nWith PictTableLayout commented out from PlanOptimizers:\r\nBenchmark                     Mode  Cnt    Score    Error  Units\r\nBenchmarkPlanner.planQueries  avgt   20  634.585 \u00b1 22.911  ms/op\r\n\r\nWith https://github.com/prestodb/presto/pull/9879:\r\nBenchmark                     Mode  Cnt    Score    Error  Units\r\nBenchmarkPlanner.planQueries  avgt   20  893.612 \u00b1 35.967  ms/op\r\n```\r\n\r\nNotice that `BenchmarkPlanner` is not focused only to test the case of partition pruning. It could be seen as general planner impact.", "NaN"], ["9889", "Add SignatureBinder test involving function<..., varchar>", "Haozhun Jin", "haozhun", "02/14/18, 12:33:26 AM", "NaN", "NaN"], ["9893", "Fix Cassandra listTables denies empty table_name", "Yuya Ebihara", "ebyhr", "02/06/18, 08:15:28 AM", "Fix #8470 ", "NaN"], ["9895", "Allow overriding temporary table name generation", "Piotr Findeisen", "findepi", "02/05/18, 12:02:32 PM", "The currently generated name is 43-char length, which is too long in\r\ncertain circumstances (databases).", "NaN"], ["9898", "Remove debug flag from ClientSession", "David Phillips", "electrum", "02/06/18, 03:30:16 PM", "This flag is specific to the CLI.", "NaN"], ["9899", "Fix typo in PlanMatchPattern.Ordering#getSortOrder", "Karol Sobczak", "sopel39", "02/06/18, 09:52:42 AM", "NaN", "NaN"], ["9900", "Stream after sort is only ordered if sort wasn't parallelised", "Karol Sobczak", "sopel39", "02/09/18, 09:31:02 AM", "NaN", "NaN"], ["9901", "Limit the number of values for min/max n functions", "James Sun", "highker", "02/06/18, 11:14:09 PM", "min_n/max_n or min_by_n/max_by_n functions do not check the value of n\r\nbefore creating the corresponding heaps for the states. This led to\r\nproduction failure due to OOM when n is big enough. This patch enforces\r\nn to be no greater than 10000.", "NaN"], ["9906", "Make information_schema tests reusable", "Piotr Findeisen", "findepi", "02/09/18, 07:56:44 AM", "`information_schema` needs to be tested on a per-connector basis. This\r\ncommit moves existing connector-specific information schema tests to\r\nshared superclass so that they are run for all connectors.", "NaN"], ["9910", "Remove redundant tests", "Grzegorz Kokosi\u0144ski", "kokosing", "02/09/18, 08:53:07 AM", "Fixes: #9907 ", "NaN"], ["9912", "Add word stem function", "David Phillips", "electrum", "02/07/18, 05:42:59 AM", "NaN", "NaN"], ["9914", "Improve state tracking in StatementClient", "Haozhun Jin", "haozhun", "02/08/18, 05:56:22 AM", "StatementClient previously use 3 booleans to track 4 possible states.\r\nThis redundancy introduced confusion and bugs.\r\nFor example, a completed query may still be aborted.\r\nAn enum is used instead.\r\n\r\nStatementClient previously had isValid and isClosed.\r\nisClosed is confusingly named because it represents whether the client\r\nis aborted instead of whether the client can potentially produce more data.\r\nIt is renamed to isAborted.", "NaN"], ["9915", "Fix NULLIF for map type when type coercion is required", "Wenlei Xie", "wenleix", "02/11/18, 01:56:24 AM", "The compilation of NULLIF will perform necessary cast into common type.\r\nIt uses \"dup\" as the expression to generate the first value when invoke\r\nthe cast function, assuming the first value is on the stack. This\r\ndoesn't work when the invocation framework need to push other stuff\r\n(e.g. ConnectorSession) before invoke the cast function.", "NaN"], ["9918", "Check validity of SSL key certificates in client", "David Phillips", "electrum", "02/09/18, 04:34:55 PM", "NaN", "NaN"], ["9919", "Add release notes for 0.194", "Haozhun Jin", "haozhun", "02/09/18, 12:10:52 AM", "NaN", "NaN"], ["9920", "Remove interpreter fallback on compiler failure", "David Phillips", "electrum", "02/09/18, 03:57:58 PM", "The code no longer works and is not exercised anywhere.", "NaN"], ["9921", "Introduce CostCalculator", "Piotr Findeisen", "findepi", "02/21/18, 07:45:10 AM", "NaN", "NaN"], ["9922", "Composable Stats Calculator (phase 2)", "Piotr Findeisen", "findepi", "02/13/18, 12:52:33 PM", "NaN", "NaN"], ["9923", "Updated Benchto schema sizes and query categorization", "Wojciech Biela", "ilfrin", "02/15/18, 01:41:49 PM", "supersedes #9887", "NaN"], ["9924", "Fix SymbolStatsEstimate.equals when low/high NaN", "Piotr Findeisen", "findepi", "02/09/18, 08:02:56 AM", "NaN", "NaN"], ["9927", "Fix CLI delay on exit", "David Phillips", "electrum", "02/09/18, 12:35:11 AM", "NaN", "NaN"], ["9928", "Use state object to cache output for array_join", "Wenlei Xie", "wenleix", "02/21/18, 09:33:18 PM", "The current implementation creates a DynamicSliceOutput for each\r\ninvocation. It estimates the output size by Block.getSizeInBytes().\r\nThis is inaccurate and inefficient when the underlying block\r\nis DictionaryBlock.", "NaN"], ["9929", "Measure communications timeouts from first failure", "Dain Sundstrom", "dain", "02/13/18, 08:19:06 PM", "Backoff fails queries early because it measures duration from the end of\nthe last successful request.  In cases where there is a large gap between\nrequest, a query may fail after a single attempt. Instead the failure\nduration is measured from the end of the first failed request in a sequence\nof failures.  With this change the silding failure window is no longer\npossible, so all of the code related to it has been removed.", "NaN"], ["9931", "Minor refactorings in ATQ", "Grzegorz Kokosi\u0144ski", "kokosing", "02/09/18, 11:52:34 AM", "NaN", "NaN"], ["9934", "Add support for Glue Hive metastore", "Rentao", "rentaow", "03/27/18, 04:39:11 AM", "Allows users to use AWS Glue Data Catalog as metastore in Hive connector", "NaN"], ["9937", "Polish: replace this lambda with a method reference.", "Igor Suhorukov", "igor-suhorukov", "01/02/19, 06:01:13 AM", "NaN", "NaN"], ["9938", "Fix JOIN ... USING semantics", "Martin Traverso", "martint", "02/21/18, 06:48:33 PM", "Given T(k1, .., kn, a1, ..., an) and U(k1, ... kn, b1, ..., bn),\r\nthe output of t JOIN u USING (k1, ..., kn) is expected to be a\r\ntable with the following schema:\r\n\r\n(k1, ..., kn, a1, ..., an, b1, ..., bn)\r\n\r\nThe fields a_i and b_i are resolvable using the aliases for the\r\nleft and right tables. k_i are derived from coalesce(t.k_i, u.k_i)\r\n\r\nFixes #2311", "NaN"], ["9944", "Remove test suppression for PostgreSQL", "Piotr Findeisen", "findepi", "02/13/18, 09:26:51 AM", "`testLargeIn` passes for PostgreSQL when run locally on Mac, no need to\r\ndisable it.\r\n\r\nIf this gets merged, https://github.com/prestodb/presto/issues/5752 should be closed.\r", "NaN"], ["9947", "Allow non-enumerable schemas for JDBC connectors", "David Phillips", "electrum", "02/12/18, 10:38:36 PM", "NaN", "NaN"], ["9948", "Fixed Hive split discovery to respect hive.max-outstanding-splits-size", "Haozhun Jin", "haozhun", "02/12/18, 09:27:22 PM", "The configuration was not respected in Hive split discovery for scans where\r\ngrouped execution is on.", "NaN"], ["9949", "Fix race condition when listing columns in MySQL connector", "David Phillips", "electrum", "02/13/18, 05:19:53 PM", "The MySQL JDBC driver has a race condition where it lists all tables, then\nfetches column metadata for each table, which throws a SQL exception if a\ntable is dropped during this process. This error does not occur if the\ndriver is set to use INFORMATION_SCHEMA for metadata.", "NaN"], ["9950", "Remove redundant casts", "Martin Traverso", "martint", "02/13/18, 10:46:59 PM", "If the type of the argument is the same as the target type or\r\nthe cast is a \"type-only\" conversion, remove the cast.\r\n\r\nFixes #9946", "NaN"], ["9952", "Fix thread safety for stats tests", "David Phillips", "electrum", "02/13/18, 09:52:41 PM", "NaN", "NaN"], ["9953", "Optimize ST_Envelope", "Maria Basmanova", "mbasmanova", "02/21/18, 07:55:48 PM", "Similar to #9798, ST_Envelope spent most of the time deserializing geometry from Slice. This PR updates ST_Envelope to limit deserialization to just the envelope.\r\n\r\nBenchmark results before the change:\r\n\r\n```\r\nBenchmark                                      Mode  Cnt       Score       Error  Units\r\nBenchmarkSTEnvelope.complexGeometry  avgt   20  173054.303 \u00b1 24902.172  ns/op\r\nBenchmarkSTEnvelope.simpleGeometry   avgt   20    1866.689 \u00b1   388.987  ns/op\r\n```\r\n\r\nand after:\r\n\r\n```\r\nBenchmark                                      Mode  Cnt     Score    Error  Units\r\nBenchmarkSTEnvelope.complexGeometry  avgt   20   977.294 \u00b1 55.636  ns/op\r\nBenchmarkSTEnvelope.simpleGeometry   avgt   20  1016.174 \u00b1 57.284  ns/op\r\n```\r\n\r\nAlso, fixes #9814 ", "NaN"], ["9956", "Fix thread safety and warnings in TestValueStore", "David Phillips", "electrum", "02/14/18, 02:16:34 AM", "NaN", "NaN"], ["9957", "Skip reserved hash slot in ORC dictionary builder", "James Sun", "highker", "02/14/18, 05:05:30 AM", "ORC dictionary builder by default uses position 0 to denote null values.\r\nHowever, hash function of the builder can return 0 as a legit empty\r\nposition. Need to skip the reserved slot for null.", "NaN"], ["9960", "Fix typo resouceGroup -> resourceGroup", "Kai Sasaki", "Lewuathe", "02/21/18, 06:47:59 PM", "`resouceGroup` -> `resourceGroup`", "NaN"], ["9961", "Simplify Constraint construction in TableScanStatsRule", "Piotr Findeisen", "findepi", "02/21/18, 08:10:08 AM", "Since `predicate` being added to `node.getCurrentConstraint()` is always\r\n`TRUE_LITERAL`, the code constructing `DomainTranslator`, `TupleDomain`\r\nand `TupleDomain`'s intersection was not doing anything useful.", "NaN"], ["9963", "Statistics calculations for comparisons of expressions", "Piotr Findeisen", "findepi", "02/21/18, 01:44:18 PM", "NaN", "NaN"], ["9968", "Extract aggregation tests to a separate class", "Piotr Findeisen", "findepi", "02/26/18, 10:00:59 AM", "This moves the aggregation tests from `AbstractTestQueries` to a\r\nseparate class. These tests cover Presto's engine behavior, so there is\r\nno point in running them against different connectors.\r\n\r\nTest methods were moved verbatim, with inevitable slight reordering\r\n(attempting to actually introduce more order). There were two exceptions:\r\n\r\n- `testGroupByRepeatedField` and `testGroupByRepeatedField2` were\r\n  merged\r\n- `testAggregationWithProjection` and `testAggregationWithProjection2`\r\n  were merged", "NaN"], ["9969", "Disable optimize-mixed-distinct-aggregations in tests by default", "Piotr Findeisen", "findepi", "02/26/18, 06:53:47 PM", "`optimizer.optimize-mixed-distinct-aggregations` is false by default, so\r\nit should not be turned on in tests by default.", "NaN"], ["9970", "Remove unused field", "Karol Sobczak", "sopel39", "02/21/18, 03:41:42 PM", "NaN", "NaN"], ["9971", "Run TestMemoryTracking as single threaded", "Nezih Yigitbasi", "nezihyigitbasi", "02/15/18, 06:06:06 PM", "```\r\njava.lang.AssertionError: expected [100000000] but found [600000000]\r\n\tat com.facebook.presto.memory.TestMemoryTracking.assertStats(TestMemoryTracking.java:397)\r\n\tat com.facebook.presto.memory.TestMemoryTracking.testRevocableMemoryAllocations(TestMemoryTracking.java:260)\r\n```", "NaN"], ["9972", "Fix or mark single-threaded all tests with @BeforeMethod", "Piotr Findeisen", "findepi", "02/21/18, 08:10:54 AM", "NaN", "NaN"], ["9973", "Add truncated field to HiveInputInfo", "Haozhun Jin", "haozhun", "02/22/18, 02:00:45 AM", "NaN", "NaN"], ["9974", "Add file path to RcFilePageSource exceptions", "Haozhun Jin", "haozhun", "02/24/18, 12:45:04 AM", "NaN", "NaN"], ["9976", "Improve memory tracking of PartitionedOutputOperator", "Nezih Yigitbasi", "nezihyigitbasi", "02/21/18, 10:01:54 AM", "If the memory usage is updated only in the constructor and not in\r\naddInput the usage can be off by several orders of magnitude. However,\r\nwe cannot update the memory usage at every addInput call by calculating\r\nthe retained size of the PageBuilders in PagePartitioner as that can be\r\nexpensive especially for complex types. Therefore, this change\r\napproximates the memory usage by using getSizeInBytes() instead of\r\nthe more expensive getRetainedSizeInBytes() method.\r\n\r\nFor now this PR fixes the accuracy of the system allocations done by the `PartitionedOutputOperator`. We have work in progress that will unify the system/user pools, and that will address the problem of tracking `PartitionedOutputOperator` memory in user pool, which will then address the GC problems mentioned in #9858.", "NaN"], ["9977", "Track GC count and time in TaskStatus", "Dain Sundstrom", "dain", "03/13/18, 06:24:04 AM", "Track number and duration of full GCs encountered in a task.", "NaN"], ["9978", "Prevent accidental treatment of GroupReference as real PlanNode", "Piotr Findeisen", "findepi", "02/21/18, 08:07:57 AM", "NaN", "NaN"], ["9980", "Remove legacy DetermineJoinDistributionType", "Grzegorz Kokosi\u0144ski", "kokosing", "02/21/18, 07:47:47 PM", "Remove legacy DetermineJoinDistributionType\n\nLegacy DetermineJoinDistributionType was not written just couple months\nbefore it was migrated, so it was had a chance to become a real legacy.\nThere is no point of having legacy DetermineJoinDistributionType as risk\nof having a bug in migrated rule is the same as in legacy.", "NaN"], ["9981", "Fix setting timeout for Hive metastore Thrift client", "David Phillips", "electrum", "02/16/18, 07:32:28 PM", "This was lost in 0.191 when SSL support was added.", "NaN"], ["9982", " Close join probe before creating outer position iterator ", "Haozhun Jin", "haozhun", "02/16/18, 10:58:03 PM", "Fixes #9766.", "NaN"], ["9983", "Add release notes for 0.195", "Raghav Sethi", "raghavsethi", "02/19/18, 11:22:57 PM", "NaN", "NaN"], ["9984", "Add support for casts in InPredicate to TupleDomain", "Grzegorz Kokosi\u0144ski", "kokosing", "03/02/18, 07:46:11 PM", "Add support for casts in InPredicate to TupleDomain\r\n\r\nFixes #9979 \r", "NaN"], ["9986", "Use type.appendTo() to serialize BlockPositionState", "Wenlei Xie", "wenleix", "02/27/18, 06:43:06 PM", "Use type.appendTo() to serialize BlockPositionState,\r\ninstead of using type.get/write method which requires to distinguish\r\nSlice/Object in the method name.\r\n\r\nThis has the following benefits:\r\n  * Avoids new block/slice allocation.\r\n  * Only one BlockPositionState class is needed.\r\n\r\n\r\nFollow-up on https://github.com/prestodb/presto/pull/9696. Relates to https://github.com/prestodb/presto/issues/9553", "NaN"], ["9989", "Add a configuration option to disable creating non-managed Hive table", "Matt Fuller", "mattsfuller", "02/21/18, 04:49:59 PM", "Add a configuration option using which one can disable creating\r\na non-managed (external) table. This will prevent the current\r\ncase where one can create an external table to query any data\r\nin S3.", "NaN"], ["9991", "Print Join distribution explicitly in EXPLAIN", "Piotr Findeisen", "findepi", "02/21/18, 09:58:54 AM", "Based on https://github.com/prestodb/presto/pull/9921, first few commits already reviewed.", "NaN"], ["9992", "Add tests for reading temporal types with Presto JDBC", "Piotr Findeisen", "findepi", "02/21/18, 07:45:26 AM", "Extracted from https://github.com/prestodb/presto/pull/9951", "NaN"], ["9993", "Ensure plan correctly printed in RuleTester", "Piotr Findeisen", "findepi", "02/21/18, 02:49:34 PM", "Co-authored by: @kokosing", "NaN"], ["9994", "Support wildcard `*` in JMX table name to match several MBeans data", "Grzegorz Kokosi\u0144ski", "kokosing", "03/22/18, 08:37:51 PM", "Add cumulative schema to JMX connector\r\n\r\nThanks to this change user would not need to query each JMX table to find out which IterativeOptimizer rule takes the most time, instead they could:\r\n```\r\nselect * from jmx.cumulative.\"com.facebook.presto.sql.planner.iterative\";\r\n```", "NaN"], ["9996", "Fix query peak user/total memory tracking", "Nezih Yigitbasi", "nezihyigitbasi", "02/23/18, 10:32:18 PM", "Previously, the query peak user/total memory was incorrectly calculated\r\nas the peak of task user/total memory.\r\n\r\nThis is similar to the approach used by the original commit that added the query peak memory tracking: https://github.com/prestodb/presto/commit/60127c66529d1a12981607d06658ccc9a0411868\r\n\r\nVerified the correctness with some prod query.", "NaN"], ["9998", "Exclude coordinator from nodes count when calculating cost", "Piotr Findeisen", "findepi", "02/21/18, 09:53:52 PM", "This was previously done for `CostCalculatorWithEstimatedExchanges`\r\nonly. This change applies same behavior to\r\n`CostCalculatorUsingExchanges` as well.", "NaN"], ["9999", "Rename DataType.decimalType to decimalDataType", "Piotr Findeisen", "findepi", "02/21/18, 07:39:05 PM", "Because all other factory methods end with \"DataType\".", "NaN"], ["10001", "Minor changes to OutputBufferMemoryManager", "Nezih Yigitbasi", "nezihyigitbasi", "02/22/18, 12:19:17 AM", "- Rename isFull() to isOverutilized() as isFull() was only used for\r\nchecking whether the buffer is overutilized.\r\n- Mark isFull() as synchronized.", "NaN"], ["10007", "Clean up resource groups", "Raghav Sethi", "raghavsethi", "03/07/18, 02:36:49 AM", "This removes the redundant ResourceGroupStateInfo class and cleans up\r\nseveral tests.", "NaN"], ["10011", "Prevent testShowTablesLike() failure when tests are running concurrently", "Piotr Findeisen", "findepi", "02/25/18, 09:03:25 PM", "Narrow patter in tests to prevent failures when tests are executing\r\nconcurrently.", "NaN"], ["10012", " Fix reading DECIMAL when JDBC returns BigDecimal with lower scale", "Piotr Findeisen", "findepi", "02/26/18, 06:57:31 PM", "NaN", "NaN"], ["10015", "Properly mark the request start time in RequestErrorTracker", "Nezih Yigitbasi", "nezihyigitbasi", "02/27/18, 06:58:26 AM", "Otherwise Backoff.failureRequestTimeTotal will not be calculated\r\ncorrectly.\r\n\r\nCurrently `failureRequestTimeTotal` can be zero:\r\n```\r\ncom.facebook.presto.spi.PrestoTransportException: Encountered too many errors talking to a worker node. The node may have crashed or be under too much load. This is probably a transient issue, so please retry your query in a few minutes. (getting task status http://[2401:db00:1020:6212:face:0:5:0]:7777/v1/task/20180222_034236_01849_hf6pb.2.47 - 461 failures, failure duration 300.07s, total failed request time 0.00s)\r\n\tat com.facebook.presto.server.remotetask.RequestErrorTracker.requestFailed(RequestErrorTracker.java:130)\r\n\tat com.facebook.presto.server.remotetask.ContinuousTaskStatusFetcher.failed(ContinuousTaskStatusFetcher.java:187)\r\n\tat com.facebook.presto.server.remotetask.SimpleHttpResponseHandler.onFailure(SimpleHttpResponseHandler.java:86)\r\n\tat com.google.common.util.concurrent.Futures$4.run(Futures.java:1126)\r\n\tat io.airlift.concurrent.BoundedExecutor.drainQueue(BoundedExecutor.java:78)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```", "NaN"], ["10019", "Miscellaneous minor fixups and refactors", "Grzegorz Kokosi\u0144ski", "kokosing", "03/02/18, 07:49:42 PM", "NaN", "NaN"], ["10022", "Introduce WorkProcessor and use it in MergeHashSort", "Karol Sobczak", "sopel39", "04/03/18, 03:42:26 PM", "NaN", "NaN"], ["10023", "Fix decimal ROUND_N return type and rounding for inputs with 0 scale", "Piotr Findeisen", "findepi", "02/26/18, 06:54:08 PM", "NaN", "NaN"], ["10024", " Use primitives in MathFunctions", "Piotr Findeisen", "findepi", "02/26/18, 08:27:20 PM", "NaN", "NaN"], ["10026", "Fix partial container cleanup in run_on_docker.sh", "Piotr Findeisen", "findepi", "02/27/18, 03:03:04 PM", "`cleanup()` function may be called before `HADOOP_LOGS_PID`,\r\n`PRESTO_LOGS_PID` variables are defined. This ensures the script doesn't\r\nterminate (due to `set -u` being used).\r\n\r\nNote: cannot use `test -v varname` nor `[[ -v varname ]]`, because on\r\nMac, `/bin/bash` is Bash 3.x, doesn't support `-v` for checking if\r\nvariable is defined.", "NaN"], ["10029", "Rename various fields to clarify they are related to user memory", "Nezih Yigitbasi", "nezihyigitbasi", "02/27/18, 04:18:13 AM", "Fields such as `memoryReservation`, `totalMemoryReservation`, `cumulativeMemory`, `peakMemory` are renamed to have the word `user` in them to make it clear that they are user memory related.\r\n\r\nI just wanted to get this in as I am working on unifying the system/user pool right now.", "NaN"], ["10030", "Acknowledge output buffer pages immediately", "James Sun", "highker", "03/01/18, 07:21:55 AM", "\r\nAn exchange client round robins each output buffer and acknowledges\r\npreviously received pages. However, this pattern of fetch pages can be\r\nslow when the fan-out is large. It is possible an output buff holds onto\r\na received page for a long while before the next reqest to acknowledge\r\nthe sent pages.\r\n#7987 can solve this problem but it relies on HTTP/2 to be enabled.\r\nThis patch is a quick fix to mitigate the issue. It should be reverted\r\nonce #7987 is landed.\r", "NaN"], ["10032", "Fix race in OutputBufferMemoryManager::updateMemoryUsage()", "Nezih Yigitbasi", "nezihyigitbasi", "03/02/18, 05:17:11 PM", "It's important that setBytes(bufferedBytes.addAndGet(bytesAdded)) is\r\nsynchronized as it's a compund action. Otherwise, when pages are\r\ndereferenced we can end up with incorrect accounting.\r\n\r\nHere is an example. Assume that bufferedBytes is 100 bytes and\r\ntwo pages are dereferenced, say with bytesAdded = -5 and -10, so the\r\nexpected reserved memory in the end is 100 - (5 + 10) = 85 bytes.\r\nAssume that thread T1 runs bufferedBytes.addAndGet(-5) and gets 95,\r\nthen T2 runs bufferedBytes.addAndGet(-10) and gets 85\r\n(as bufferedBytes is atomic). Immediately after that T2 can execute\r\nsetBytes(85) and then T1 can execute setBytes(95) after T1, and we end\r\nup with 95 bytes in the end, which is incorrect.", "NaN"], ["10034", "Change NULL IN treatment in ExpressionInterpreter so it's \"more correct\"", "Piotr Findeisen", "findepi", "03/09/18, 10:43:00 AM", "Apparently it was not possible to reach `ExpressionInterpreter` with\r\n`InPredicate` having empty values. However, if it was possible, the\r\nlogic to return `null` for `NULL IN ...` would not be correct.\r\n\r\nThis commit changes the code so that it's more obvious why it's correct.", "NaN"], ["10035", "Reset resetMemoryRevocationRequestListener when driver is closed", "Karol Sobczak", "sopel39", "02/28/18, 10:06:08 AM", "resetMemoryRevocationRequestListener should be reset on driver\r\nclose. Otherwise OperatorContext holds reference to\r\nDriver and Operator instances thus consuming memory.", "NaN"], ["10037", "Reorganize lambda function documentation", "David Phillips", "electrum", "03/01/18, 12:52:17 AM", "NaN", "NaN"], ["10039", "Clear OrderByOperator pagesIndex on close", "Karol Sobczak", "sopel39", "02/28/18, 10:02:13 AM", "NaN", "NaN"], ["10040", "Various cleanups for row type signature handling", "Martin Traverso", "martint", "04/05/18, 12:23:10 PM", "NaN", "NaN"], ["10041", "Optimize the ST_XMin, ST_XMax, ST_YMin, ST_YMax functions", null, "mehrdad-honarkhah", "03/05/18, 04:07:32 PM", "Similar to [#9953](https://github.com/prestodb/presto/pull/9953), ST_XMin, ST_XMax, ST_YMin, and ST_YMax geospatial functions spend most of the time deserializing geometry from Slice. This PR updates them to limit deserialization to just the envelope.\r\n\r\nThis change fixes the NullPointerException reported in [#10027](https://github.com/prestodb/presto/pull/10027) as well.\r\n\r\nMicro-benchmarking shows a big performance improvement.\r\n\r\nBefore:\r\n\r\n```\r\nBenchmark                       Mode  Cnt Score     Error    Units\r\nBenchmarkSTXmin.complexGeometry avgt 20 123830.375 \u00b1 4063.732 ns/op\r\nBenchmarkSTXmin.simpleGeometry avgt 20 535.399 \u00b1 56.854 ns/op`\r\n```\r\n\r\nand after:\r\n\r\n```\r\nBenchmark                       Mode  Cnt Score   Error  Units\r\nBenchmarkSTXmin.complexGeometry avgt 20 59.393 \u00b1 5.843 ns/op\r\nBenchmarkSTXmin.simpleGeometry avgt 20 54.984 \u00b1 1.887 ns/op\r\n```", "NaN"], ["10042", "Fix connector session properties for information_schema", "David Phillips", "electrum", "03/03/18, 03:26:46 AM", "Session properties were not passed to the metadata calls used by\ninformation_schema (including the SHOW statements which also use it).", "NaN"], ["10043", "Fix string stats trunction for original orc writer", "James Sun", "highker", "02/28/18, 05:35:33 AM", "We need to check if the orc writer version before trying to truncate the\r\nstring to get the stats given there could be invalid code points.", "NaN"], ["10044", "Make is_json_scalar throw on invalid input", "Wenlei Xie", "wenleix", "02/28/18, 09:01:06 PM", "is_json_scalar returns null on invalid input. However, this can be\r\ntoo lenient. Make it more strict for now and we can consider relax\r\nit later if necessary.\r\n\r\nFix the behavior in https://github.com/prestodb/presto/pull/9277", "NaN"], ["10047", "Extract TPCH_COLUMN_NAMING_PROPERTY constant", "Grzegorz Kokosi\u0144ski", "kokosing", "02/28/18, 07:58:09 PM", "Extract TPCH_COLUMN_NAMING_PROPERTY constant", "NaN"], ["10050", "Add state query parameter to getAllQueryInfo endpoint", "Nezih Yigitbasi", "nezihyigitbasi", "03/02/18, 05:16:31 PM", "This change adds a state query parameter to the getAllQueryInfo\r\nendpoint to filter queries by the specified state as this endpoint can\r\nreturn a large number of queries.\r\n\r\nThe reason for this change is to aid debugging, sometimes I curl this endpoint from the command line  and do a bunch of other stuff, and the data this endpoint returns is huge.", "NaN"], ["10051", "Optimize min/max aggregation with BlockPositionState", "Wenlei Xie", "wenleix", "03/10/18, 04:59:39 AM", "min/max have the same issue with arbitrary when it's aggregating over varchar/structural types (#9696). While min/max over array/map is rare, we see increasing usage case for min/max over varchar/row, which causes GC pressure.", "NaN"], ["10054", "Improve client protocol latency at query completion", "David Phillips", "electrum", "03/05/18, 03:09:57 PM", "The statement resource was waiting the maximum time at query completion\r\nfor the exchange client to have data, which never occurs if there is no\r\ndata for the query.", "NaN"], ["10056", "Fix reserved slots in ORC dictionary builder", "James Sun", "highker", "03/01/18, 03:55:17 AM", "position 0 is a reserved slot to indicate a null position in `SliceDictionaryColumnWriter`.\r\n\r\nPR #9957 avoids the hash collision to `key = 0`; because, by then, we believed that `key = 0` is a reserved position to indicate a null value. However, after carefully reading the code, it turns out that\r\n- `value = 0` is actually the reserved position and\r\n- `DictionaryBuilder::rehashBlock` will assign some position `blockPositionByHash[pos] = 0` given the loop starts from 0; however, `block.equals` does not distinguish between null position vs empty slice due to `getSliceLength` returns 0 for nulls (in `VariableWidthBlock`) as well.\r\n\r\nSo the real fix is not to avoid hash collision to `key = 0` but to avoid assigning 0 to any position in `blockPositionByHash`.\r\n\r\nEssentially, `rehashBlock` function is the one to fix.", "NaN"], ["10057", "Change ROUND_N so it accepts N being INTEGER rather than BIGINT", "Piotr Findeisen", "findepi", "04/05/18, 02:13:08 PM", "Implementations of `ROUND_N` do unchecked `N \u2192 int` conversion. This is\r\nsafe only for `N` being `INTEGER` or smaller integral data type.", "NaN"], ["10059", "Use more portable expression in QueryBuilder", "Piotr Findeisen", "findepi", "03/05/18, 08:32:31 PM", "Not all databases understand `TRUE` and `FALSE`, so let's use more\r\nportable expression.\r\n\r\nIn fact, this code is not currently reachable, since planner eliminates\r\ntable scan when condition is known to be false. Also, it eliminates\r\n`TupleDomain`'s domains which contain all values. However,\r\n`QueryBuilder` doesn't need to rely on that behavior.", "NaN"], ["10060", "Set missing hive stats to unknown instead of zero", "Rebecca Schlussel", "rschlussel", "03/06/18, 04:36:46 PM", "Previously we weren't checking that thrift fields were set before using their\r\nvalues.  As a result, unset fields were reported as having a value of\r\nzero.  This fixes that issue to appropriately set those fields as unknown.", "NaN"], ["10061", "Various ExpressionInterpreter cleanups", "Martin Traverso", "martint", "04/03/18, 07:52:39 PM", "This is handled by SimplifyExpressions during optimization,\r\nso it's unnecessary.", "NaN"], ["10062", "Add index size for probe during join", "James Sun", "highker", "03/01/18, 09:07:33 PM", "Estimated size for probe should have both dictionary size and id size.\r\nOtherwise, in the extreme case where nothing is appended on the build\r\nand same row appended on the probe can lead to the builder increasing\r\ncontinuously.", "NaN"], ["10063", "Log acknowledge response only for non-successful status code", "James Sun", "highker", "03/01/18, 09:07:33 PM", "We should not log acknowledge response if the status code is 200, 204,\r\netc.", "NaN"], ["10066", "Add 0.196 release notes", "Dain Sundstrom", "dain", "03/01/18, 10:19:12 PM", "NaN", "NaN"], ["10067", "Remove extra format call for log statement", "David Phillips", "electrum", "03/01/18, 11:08:43 PM", "NaN", "NaN"], ["10069", "Disable log function with config flag", "Elon Azoulay", "elonazoulay", "04/25/18, 01:06:13 AM", "Add FeatureConfig flag to disable log and eventually remove it.\r\nThe 2 argument form of the log function does not conform to the latest sql standard.\r\n\r\nResolves #9939 ", "NaN"], ["10070", "Push SemiJoin predicate inferred from filter side to source side (v2)", "\u0141ukasz Osipiuk", "losipiuk", "03/13/18, 08:30:17 PM", "Followup of #10048\r\n\r\nNow with guarding that semi-join output is actually used for filtering out rows above semi join.\r\nStill a point change. Could be extended with restructuring PP for SemiJoin to also move predicates from source side to filter side.\r\nAnd eventually migrated to Iterative optimizer as discussed elsewhere.\r\n\r\nWait till tests pass before rewiew.\r\n\r\ncc: @kokosing, @sopel39, @martint, @findepi \r\n\r\nAlso fixes #10085", "NaN"], ["10071", "Use AstUtils in SubExpressionExtractor", "Grzegorz Kokosi\u0144ski", "kokosing", "03/03/18, 07:37:41 PM", "Use AstUtils in SubExpressionExtractor", "NaN"], ["10072", "Reuse node IDs in optimizers", "Karol Sobczak", "sopel39", "03/02/18, 08:24:55 PM", "Reusing Node IDs simplifies tracking of plan nodes throughout the planning phase", "NaN"], ["10073", "Add a GeoQueryRunner for testing geospatial functions", "Maria Basmanova", "mbasmanova", "03/05/18, 05:51:30 PM", "Also, \r\n\r\n- add support for installing plugins to LocalQueryRunner;\r\n- rename TestGeoQueries to TestGeoFunctions for clarity.", "NaN"], ["10074", "Acknowledge only when have received at least one page", "James Sun", "highker", "03/03/18, 12:13:11 AM", "NaN", "NaN"], ["10075", "Show only differing rows in QueryAssertions", "Piotr Findeisen", "findepi", "03/05/18, 10:11:03 AM", "When only few rows differ among many, printing just 100 first actual\r\nrows may not be helpful at all. Showing differing rows is more useful.", "NaN"], ["10079", "First commits for client protocol v2", "David Phillips", "electrum", "03/03/18, 05:58:03 PM", "Extracted from #9607 and ready to merge. I already reviewed this and only created the PR to run Travis tests.", "NaN"], ["10080", "Add catalog level access control to Raptor", "Jiexi Lin", "jessesleeping", "03/06/18, 08:35:48 PM", "This PR adds read-only and file based catalog level access control to Raptor. The approach is similar to what we did in the Hive connector.", "NaN"], ["10086", "Lower the number of buckets in TestHiveBucketedTables", "Grzegorz Kokosi\u0144ski", "kokosing", "03/06/18, 05:48:59 AM", "Lower the number of buckets in TestHiveBucketedTables\n\nWith 4 buckets INSERT INTO used up to 6 yarn containers at the same\ntime. Such huge resource consumption for such small query causes this\ntest to flaky on travis.\n\nWith 2 buckets there is up to 4 yarn containers at the same time used.", "NaN"], ["10088", "Test InPredicate with narrowing cast in DomainTranslator", "Grzegorz Kokosi\u0144ski", "kokosing", "03/06/18, 05:40:19 AM", "Test InPredicate with narrowing cast in DomainTranslator", "NaN"], ["10091", "Update to Airbase 80", "David Phillips", "electrum", "04/03/18, 07:31:35 AM", "NaN", "NaN"], ["10093", "Remove unnecessary null check", "Nezih Yigitbasi", "nezihyigitbasi", "03/06/18, 01:16:46 AM", "Checked value is a primitive.", "NaN"], ["10095", "Enforce minimum number of workers at scheduling time", "Nezih Yigitbasi", "nezihyigitbasi", "03/07/18, 12:36:02 AM", "The config parameter \"query-manager.initialization-required-workers\"\r\nis only enforced at query admission time. However, we don't check\r\nwhether we have enough workers during scheduling, which can cause\r\ninteresting issues, such as queries failing by hitting memory limits\r\ndue to running on a small number of nodes. With this change, we enforce\r\nthis limit also during scheduling and fail such queries fast, which\r\nwill reduce waste of cluster resources.\r\n\r\nThis PR:\r\n- Reuses the existing \"query-manager.initialization-required-workers\" parameter as the threshold.\r\n- Creates a new internal error code: `NOT_ENOUGH_ACTIVE_NODES`.", "NaN"], ["10096", "Fix indefinite hang in Presto client", "Haozhun Jin", "haozhun", "03/06/18, 06:53:49 PM", "When Presto client fails to fetch data, time out check is skipped.\r\nAs a result, the client may hang indefinitely.", "NaN"], ["10098", "Add trace token support to scheduler and exchange HTTP clients", "Nezih Yigitbasi", "nezihyigitbasi", "03/20/18, 09:57:43 PM", "Depends on https://github.com/airlift/airlift/pull/616.\r\n\r\n`GenerateTraceTokenRequestFilter` doesn't extend or use `TraceTokenManager` in order not to mess with the thread locals. It uses the same approach to generate the tokens.\r\n\r\nAlso validated with the HTTP client/server logs locally.", "NaN"], ["10104", "Add ORC max stripe size session property", "James Sun", "highker", "03/07/18, 12:44:50 AM", "NaN", "NaN"], ["10105", " Use OutputStreamSliceOutput directly in OrcWriter", "James Sun", "highker", "03/07/18, 03:45:19 AM", "This patch depends on https://github.com/airlift/slice/pull/96", "NaN"], ["10106", "Fix lost/delayed split scheduling update for tasks ", "Haozhun Jin", "haozhun", "03/07/18, 07:26:25 PM", "NaN", "NaN"], ["10108", "Current user fn", "Elon Azoulay", "elonazoulay", "04/27/18, 06:06:01 PM", "Add current_user function\r\nMinor refactor/update of https://github.com/prestodb/presto/pull/6895", "NaN"], ["10112", "Add option to drop ORC string stats if exceeding limit", "James Sun", "highker", "03/10/18, 02:47:23 AM", "NaN", "NaN"], ["10113", "Introduce ClientSession.Builder", "Grzegorz Kokosi\u0144ski", "kokosing", "03/08/18, 12:21:19 PM", "Introduce ClientSession.Builder\n\nThis removes the a bit of burden of constructor calls which takes a lot of\narguments and it hard to understand what it does and verify if it is correct.", "NaN"], ["10114", "DATE/TIME/TIMESTAMP/ZONE improvements in base-jdbc and tests", "Piotr Findeisen", "findepi", "03/09/18, 08:06:37 AM", "NaN", "NaN"], ["10115", "Allow including a comment when adding a column to a table", "David Phillips", "electrum", "04/23/18, 09:40:16 PM", "The grammar previously allowed specifying a comment, but it was neither\ndocumented nor implemented.", "NaN"], ["10118", "Prefer toDataProvider instead of toArgumentsArrays", "Grzegorz Kokosi\u0144ski", "kokosing", "03/08/18, 09:02:23 PM", "Prefer toDataProvider instead of toArgumentsArrays", "NaN"], ["10120", "Fix username extraction sample json from built-in sys access control docs", "Andrea Tommaso Bonanno", "andreatbonanno", "03/10/18, 06:04:31 AM", "The mistyped `catalog` in the json snippet made Presto return access denied for every catalog.\r\n\r\nNow the sample json implements an exact matching of the full principal name for LDAP and Kerberos authentication, as already described in the docs, allowing access to all the catalogs.", "NaN"], ["10121", "Remove system pool", "Nezih Yigitbasi", "nezihyigitbasi", "04/25/18, 12:12:03 AM", "This PR makes several changes to remove the system pool. I have verified this patch with the verifier tool using several thousand production queries.\r\n\r\nTo be specific, this PR:\r\n- Removes the system pool, we now have two pools: general and reserved.\r\n- Adds a new memory limit (`query.max-total-memory-per-node`) that will fail queries when the total (user + system) memory reservation hits this limit. Based on offline discussions, we do not want to fail system reservations with the existing per-node local memory limit as it will change the current behavior (and users have already built their mental model of how things work in the engine and it would change that). This new limit will be set higher than the existing per-node local memory and will be a final safeguard to kill queries if they allocate crazy amount of system memory (e.g., a query with a large number of `UNION ALL`s). The existing per-node local memory (`query.max-memory-per-node`) will continue to behave the same.\r\n- Add support for blocking to various operators and to `OutputBufferMemoryManager` when they allocate system memory. Previously, they were ignoring the blocked future returned from the memory pool when the pool was full.\r\n- Update the OOM killer (`ClusterMemoryManager`) as it was only looking at the user memory, now it looks at the total memory reservation of queries as that's what's being reserved now from the general pool.\r\n- Add a new heap headroom config parameter to `NodeMemoryConfig` to leave some headroom in the heap when sizing the memory pools (for untracked allocations and to have some headroom when the pools get full). I added this as an additional knob to reduce the memory pressure on workers as with the current configuration we just fill up the heap when the pools are full, which is not good from the garbage collector's point of view.\r\n- We still keep track of user/system reservations in stats separately as we care about both types of allocation, so no changes to that.", "NaN"], ["10124", "Add ST_IsValid and geometry_invalid_reason functions", "Maria Basmanova", "mbasmanova", "03/20/18, 07:49:07 PM", "Add functions to check whether a given geometry is well formed and understand why if it is not.\r\n\r\n- ST_IsValid function returns true iff input geometry value is well formed\r\n- geometry_invalid_reason returns the reason why geometry is not well formed or null if it is\r\n\r\nST_IsValid function follows SQL/MM Part 3 standard. The standard doesn't have a function to return a reason for why geometry is not well formed, hence, geometry_invalid_reason name follows Presto convention.\r\n\r\nhttps://postgis.net/docs/using_postgis_dbmanagement.html#OGC_Validity provides a good explanation of what it means for a geometry to be well formed.\r\n\r\nThe implementation is based on functionality provided by the Java Topology Suite (JTS). ESRI library is missing this functionality.\r\n\r\nExisting geospatial functions encode geometry objects into Slices using ESRI shapefile format described in https://www.esri.com/library/whitepapers/pdfs/shapefile.pdf. This PR includes a new deserialization method that converts geometry Slice into JTS geometry object.", "NaN"], ["10126", "Minor improvements in SingleBlock for Array/Map/Row", "Wenlei Xie", "wenleix", "03/13/18, 07:35:11 PM", "- Change return type for AbstractSingleArrayBlock.getBlock() as Block.\r\n- Minor fix in SingleRowBlockWriter.toString()\r\n- Implement SingleMapBlock.toString() to help debug.", "NaN"], ["10128", "Change JVM time zone in tests to better test corner cases", "Piotr Findeisen", "findepi", "10/30/18, 12:37:23 PM", "This changes default JVM time zone in tests from `Asia/Katmandu` to `America/Bahia_Banderas`. The latter zone has more 'nice' features, see #10078 for context and explanation.\r\n\r\nNote: **this doesn't fix  #10078 fully**. We should change default session zone as well (`TestingSession` defaults to UTC).", "NaN"], ["10130", "Parse decimal literals as DECIMAL by default", "Piotr Findeisen", "findepi", "03/20/18, 02:50:40 PM", "https://github.com/prestodb/presto/pull/9369 introduce possibility to parse literals as `0.1` as `DECIMAL` rather than `DOUBLE`.\r\n\r\nNow, this becomes the default behavior.", "NaN"], ["10131", "Update Simba error expected messages", "Grzegorz Kokosi\u0144ski", "kokosing", "03/09/18, 02:32:47 PM", "Update Simba error expected messages", "NaN"], ["10133", "Add named custom variable support to resource group templates", "Raghav Sethi", "raghavsethi", "03/14/18, 04:46:47 AM", "NaN", "NaN"], ["10136", "Move TestHiveBucketedTables tests to big_query group", "Grzegorz Kokosi\u0144ski", "kokosing", "03/12/18, 10:58:31 AM", "Move TestHiveBucketedTables tests to big_query group\n\nThis test is running two map-reduce jobs schedulled from hive.\nEach of them is using up to 6 yarn containers.\nSuch huge resource consumption cases this test be unreliable in travis\nenvironments.", "NaN"], ["10137", "Add ST_IsSimple geospatial function", "Zhehui Zhou", "grayy921013", "03/12/18, 05:52:28 PM", "Added ST_IsSimple geospatial function and corresponding tests.", "NaN"], ["10138", "Dynamic allocating ORC output buffer", "James Sun", "highker", "03/10/18, 04:20:10 AM", "ORC output buffer is used by all the output streams in the ORC writer.\r\nHowever, each stream can vary in sizes due to different data types. A\r\nbuffer size should be adaptive based on data types. The patch starts\r\nwith a small buffer size and gradually grow it if necessary.", "NaN"], ["10139", "Upgrade to Airlift 0.164", "Dain Sundstrom", "dain", "03/10/18, 10:51:37 PM", "NaN", "NaN"], ["10140", "Add retained size of stats to ORC writers", "James Sun", "highker", "03/13/18, 06:58:05 AM", "An ORC writer maintains all the stripe statistics until the writer is\r\nclosed. The stats can take GBs of memory when the output table is large.\r\nAlso, some of the stats contain boxed types, which can cause object\r\noverhead. This patch does the following things:\r\n- Use primitive types in stats if possible\r\n- Track stripe statistics for ORC writers\r\n- Track row group statistics for ORC writer validators", "NaN"], ["10141", "Optimize array_agg with flattened group state", "Wenlei Xie", "wenleix", "03/28/18, 07:59:47 PM", "Currently, a BlockBuilder is maintained for each group for array_agg function \r\n(stored in BlockBigArray), this causes the following issues:\r\n\r\n* There is a large number of Java objects and incur overhead both for memory\r\nand GC.\r\n* Since aggregation state supports random access, it can cause excessive\r\ncross-region overhead.\r\n\r\nThis commit stores all the states across different groups with a flattened group state\r\nto avoid such issues. \r\n\r\nPart of fix to #9553", "NaN"], ["10148", "Warnings System", "Elon Azoulay", "elonazoulay", "10/16/18, 12:23:56 AM", "Updated #8611 according to the notes in the review.\r\nAlso looked at notes from #7852.\r\n\r\nAdded warnings to QueryCompletedEvent and renamed/moved classes according to comments in the review.\r\nAdd query id to WarningCollector to prefix messages.", "NaN"], ["10150", "Use view owner from connector if present", "David Phillips", "electrum", "03/13/18, 02:17:31 AM", "NaN", "NaN"], ["10151", "Expose counters for ORC writer memory usage", "James Sun", "highker", "03/13/18, 11:11:21 AM", "NaN", "NaN"], ["10152", "Optimize serialization of Geometry types", "Zhehui Zhou", "grayy921013", "03/20/18, 03:41:56 PM", "Optimize serialization of Geometry types by using 1 byte type rather than 4 bytes length.\r\nSRID is no longer included in serialized form. \r\nSerialized size for a Point decreased 25% from 28 to 21 bytes.", "NaN"], ["10155", "Remove duplicate dependency in presto-geospatial", "David Phillips", "electrum", "03/14/18, 03:13:48 AM", "NaN", "NaN"], ["10157", "Move implementation of DISTINCT aggregates to optimizer rule", "Martin Traverso", "martint", "03/20/18, 02:35:55 AM", "The planner now does a more straightforward translation of DISTINCT\r\naggregates. Their implementation (either by using MarkDistinct or\r\na pre-aggregation) is done as optimizer rules.\r\n\r\nAlso, fixes #10156", "NaN"], ["10161", "Broadcast distance query", "Maria Basmanova", "mbasmanova", "03/23/18, 08:03:34 PM", "Implements broadcast distance query as described in #10163 and includes the following:\r\n\r\n- Changes `TransformSpatialPredicateToJoin` optimizer rule to recognize `ST_Distance(left, right) <= R` query pattern and transform corresponding cross join node with a filter on top into a spatial join node. \r\n- `ST_Distance(left, right) < R`, `R > ST_Distance(left, right)` and `R >= ST_Distance(left, right)` query patterns are supported as well.\r\n- Changes `SpatialIndexBuilderOperator` to build an R-Tree using envelopes expanded by radius in four directions (up, down, left and right) and use radius to perform final spatial relationship test on pairs of geometries with intersecting envelopes.\r\n\r\nOptimizer rule applies if all of the following is true:\r\n\r\n- comparison type is either (1) `<` or `<=` with `radius` expression on the right side or (2) `>` or `>=` with `radius` expression on the left side;\r\n- radius is either a scalar expression or uses symbols only from the build (right) side of the join\r\n- arguments of `ST_Distance` are non-scalar expressions; one of the arguments uses symbols from left side of the join, the other from right.\r\n\r", "NaN"], ["10166", "Ignore string stats validation if the expected one is null", "James Sun", "highker", "03/16/18, 12:41:15 AM", "The expected string stats is calcuated by merging all row group stats.\r\nThis can produce nulls for min/max because we have limitation on string\r\nstats length. However, the actual stats read from disk is calculated by\r\nscanning all the rows in the stripe so that min/max can have actual\r\nvalues within limit.", "NaN"], ["10167", "Destroy buffers sooner when client buffers are destroyed", "Nezih Yigitbasi", "nezihyigitbasi", "03/20/18, 07:49:46 PM", "In checkFlushComplete() we should destroy the buffers if they are\r\nalso in the NO_MORE_BUFFERS state. This will help releasing the\r\nresources faster.", "NaN"], ["10170", "Fix 'integer overflow' in PagesRTreeIndex", "Maria Basmanova", "mbasmanova", "03/20/18, 03:48:41 PM", "`PagesRTreeIndex#isJoinPositionEligible` threw \"integer overflow\" from `toIntExact(joinAddress)` conversion performed before invoking `JoinFilterFunction`. `joinAddress` is a `SynteticAddress` that encodes pageIndex and position within the page. `toIntExact(joinAddress)` succeeds only if `pageIndex == 0` and throws \"integer overflow\" when `pageIndex > 0`.\r\n\r\nThis issue affects spatial joins with additional filters applied to columns from both sides of the join.\r\n\r\nSpatial joins without additional filters are not affected.\r\n\r\nFixes #10169 \r", "NaN"], ["10173", "Add simplify_geometry function", "Yizhe CHEN", "jijichen", "03/21/18, 10:11:36 AM", "NaN", "NaN"], ["10176", "Minor Cleanup of Histogram and HistogramState", "sr", "rash67", "03/19/18, 11:28:30 PM", "Histogram had dead code in that state.get() could not return\r\nnull any more, so the if/else checks in Histogram.combine() were\r\nsuperfluous and removed. I did leave in assertions\r\nthat state.get() != null as the current code is not written to\r\nhandle such a case.\r\n\r\nChange also removes the Histogram.set() method entirely as\r\nit is not used any longer.\r\n\r\nLastly, In order to make sure I was getting consistent testing,\r\nI made TestHistogram use a single code path of functions to get\r\nit's InternalAggregationFunctions.", "NaN"], ["10177", "Inline AddExchanges#Context", "Grzegorz Kokosi\u0144ski", "kokosing", "03/19/18, 10:34:46 PM", "Inline AddExchanges#Context", "NaN"], ["10178", " Make sure every Rule has a name", "Piotr Findeisen", "findepi", "03/19/18, 10:51:13 PM", "NaN", "NaN"], ["10180", "Construct proper literal for DECIMAL in LiteralInterpreter.toExpression", "Piotr Findeisen", "findepi", "03/19/18, 10:50:55 PM", "NaN", "NaN"], ["10182", "[geospatial] Optimize st-intersects", "Andrii Rosa", "arhimondr", "03/20/18, 08:59:57 PM", "Try to deserialize and check envelopes first. If envelopes intersect than do the\r\nfull deserialization and a fair slow check.\r\n\r\nBelow are the becnhmark results. Score is a number of operations per second.\r\nBigger is better.\r\n\r\nOriginal:\r\n\r\n```\r\nBenchmark                                                                Mode  Cnt       Score       Error  Units\r\nBenchmarkSTIntersects.stIntersectsCrossingLine                          thrpt   15    3283.622 \u00b1   167.239  ops/s\r\nBenchmarkSTIntersects.stIntersectsCrossingLineSimpleGeometry            thrpt   15  400249.628 \u00b1  6911.584  ops/s\r\nBenchmarkSTIntersects.stIntersectsInnerLine                             thrpt   15    2389.466 \u00b1   108.045  ops/s\r\nBenchmarkSTIntersects.stIntersectsInnerLineSimpleGeometry               thrpt   15  378110.330 \u00b1  5722.562  ops/s\r\nBenchmarkSTIntersects.stIntersectsOuterLineInEnvelope                   thrpt   15    2467.781 \u00b1   165.310  ops/s\r\nBenchmarkSTIntersects.stIntersectsOuterLineInEnvelopeSimpleGeometry     thrpt   15  374974.848 \u00b1 11822.304  ops/s\r\nBenchmarkSTIntersects.stIntersectsOuterLineNotInEnvelope                thrpt   15    5817.151 \u00b1   205.039  ops/s\r\nBenchmarkSTIntersects.stIntersectsOuterLineNotInEnvelopeSimpleGeometry  thrpt   15  766485.338 \u00b1 12150.271  ops/s\r\n```\r\n\r\nModified:\r\n\r\n```\r\nBenchmark                                                                Mode  Cnt        Score        Error  Units\r\nBenchmarkSTIntersects.stIntersectsCrossingLine                          thrpt   15     3312.916 \u00b1    182.780  ops/s\r\nBenchmarkSTIntersects.stIntersectsCrossingLineSimpleGeometry            thrpt   15   385701.159 \u00b1  12637.764  ops/s\r\nBenchmarkSTIntersects.stIntersectsInnerLine                             thrpt   15     2425.224 \u00b1    111.010  ops/s\r\nBenchmarkSTIntersects.stIntersectsInnerLineSimpleGeometry               thrpt   15   362100.862 \u00b1   9446.807  ops/s\r\nBenchmarkSTIntersects.stIntersectsOuterLineInEnvelope                   thrpt   15     2378.565 \u00b1    106.855  ops/s\r\nBenchmarkSTIntersects.stIntersectsOuterLineInEnvelopeSimpleGeometry     thrpt   15   359020.076 \u00b1   6955.090  ops/s\r\nBenchmarkSTIntersects.stIntersectsOuterLineNotInEnvelope                thrpt   15  7726620.862 \u00b1 186465.089  ops/s\r\nBenchmarkSTIntersects.stIntersectsOuterLineNotInEnvelopeSimpleGeometry  thrpt   15  7742866.387 \u00b1 156396.092  ops/s\r\n```\r\n\r\nFor a negative case, when a full deserialization is required to be made\r\nanyway (intersects, evelope intersects) small regression is noticed. ~4% performance\r\nhit for simple geometries, where deserialization time is neligible. And <1% performance\r\nhit for complex geometries, where deserialization takas most of the time.\r\n\r\nHowever for a positive case, when full deserialization is not needed performance gain\r\nis much significant. For the complex geometry case `intersects` operation is 1300 times\r\nfaster.", "NaN"], ["10185", "Add SignatureBinder test case for lambda that returns orderable", "Haozhun Jin", "haozhun", "03/22/18, 11:09:38 PM", "NaN", "NaN"], ["10187", "Fix epoch day to java.sql.Date conversion", "Piotr Findeisen", "findepi", "03/19/18, 10:50:34 PM", "The original conversion returned wrong result in JVM with time zone that\r\nhad shift on 1970-01-01 00:00:00 (e.g. `America/Bahia_Banderas`).\r\n\r\nThis problem was found thanks to https://github.com/prestodb/presto/pull/10128", "NaN"], ["10188", "Add 0.197 release notes", "David Phillips", "electrum", "03/19/18, 08:15:47 PM", "NaN", "NaN"], ["10189", "Revert \"Use appropriate initial state object in generated aggregation\u2026", "Wenlei Xie", "wenleix", "03/19/18, 11:32:25 PM", "\u2026 code\"\r\n\r\nThis reverts commit 3fc4a3e18acab7233033a71ab94bf21bfef416d6.\r\n\r\nHistorically, the scratchState is always a SingleState no matter the\r\naggregated state is a grouped state or not. Commit 3fc4a3e changes\r\nthe behavior to use the same type of States for combine function.\r\n\r\nHowever, for aggregations use flattened grouped state (e.g. histogram),\r\ndeserializing into a grouped state can be significantly more expensive\r\nthan an single state.", "NaN"], ["10190", "Cetegorize error creating Hive imput format", "Dain Sundstrom", "dain", "03/21/18, 02:29:41 AM", "NaN", "NaN"], ["10193", "TIME/TIMESTAMP W/O TIME ZONE semantics fix - continuation (v3)", "Piotr Findeisen", "findepi", "06/27/18, 07:23:00 PM", "Part of #7122, supersedes #9385", "NaN"], ["10195", "Small StreamPreferredProperties cleanups", "Karol Sobczak", "sopel39", "03/20/18, 11:08:52 AM", "One of which is making sure that order of partitioning properties is preserved and deterministic when preferences are constrained.", "NaN"], ["10196", "Extract validateValueIsPowerOfTwo method", "Karol Sobczak", "sopel39", "03/20/18, 11:08:15 AM", "NaN", "NaN"], ["10197", "Make local exchange buffer size configurable", "Karol Sobczak", "sopel39", "03/20/18, 08:40:46 AM", "For some clusters it's beneficial to adjust/increase local exchange memory size (as well as other buffer sizes, e.g: exchange client, sink). Currently, local exchange buffer wasn't configurable.", "NaN"], ["10198", "Fix benchmark queries", "Piotr Findeisen", "findepi", "03/19/18, 10:52:45 PM", "NaN", "NaN"], ["10200", "Fix computing averages in WindowOperatorStats", "\u0141ukasz Osipiuk", "losipiuk", "03/22/18, 09:42:44 AM", "The code failed if all the `WindowInfo` objects we iterated over had `getTotalRowsCount() == 0`.\r\nThis change fixes the behaviour but I am not exactly sure why we need the `.filter(windowInfo -> windowInfo.getTotalRowsCount() > 0)` alltogether.\r\n@arhimondr can you take a look if this is a proper fix and if original code is right?\r\n\r\ncc: @findepi \r", "NaN"], ["10201", "Allow nulls in exact match selectors table", "Raghav Sethi", "raghavsethi", "03/19/18, 08:15:30 PM", "NULL is interpreted as a wildcard match.", "NaN"], ["10202", "Fix ordinal_position when adding new column", "Jiexi Lin", "jessesleeping", "03/28/18, 07:10:01 PM", "Use the `max(ordinal_position)  + 1` as the `ordinal_position` value for newly added columns.", "NaN"], ["10203", "Add great_circle_distance function", "Maria Basmanova", "mbasmanova", "03/22/18, 08:56:35 PM", "NaN", "NaN"], ["10204", "Improve correlated subqueries support and try to avoid count(*) when rewriting exists from apply to lateral node", "Karol Sobczak", "sopel39", "05/09/18, 05:24:14 AM", "count bigints were produced and sent over network. This caused\r\nunnecessary overhead.\r\n\r\nThis will help for TPCH/q04, TPCH/q22.\r\n\r\nProblematic stages:\r\n```\r\n Fragment 7 [SOURCE]\r\n     CPU: 55.89m, Input: 4500000000 rows (37.72GB); per task: avg.: 562500000.00 std.dev.: 42329961.37, Output: 4497181626 rows (75.40GB)\r\n     Output layout: [custkey_13, count_81]\r\n     Output partitioning: HASH [custkey_13]\r\n     Execution Flow: UNGROUPED_EXECUTION\r\n     - Aggregate(PARTIAL)[custkey_13] => [custkey_13:bigint, count_81:bigint]\r\n\r\n...\r\n\r\n Fragment 5 [SOURCE]\r\n     CPU: 35.17m, Input: 3793363900 rows (31.80GB); per task: avg.: 474170487.50 std.dev.: 18241715.82, Output: 1375801657 rows (23.05GB)\r\n     Output layout: [orderkey_0, count_82]\r\n     Output partitioning: HASH [orderkey_0]\r\n     Execution Flow: UNGROUPED_EXECUTION\r\n     - Aggregate(PARTIAL)[orderkey_0] => [orderkey_0:bigint, count_82:bigint]\r\n             Cost: {rows: ? (?), cpu: 419999279630.00, memory: ?, network: 0.00}\r\n             CPU fraction: 38.70%, Output: 1375801657 rows (23.05GB)\r\n             Input avg.: 1334282.06 rows, Input std.dev.: 23.20%\r\n             Collisions avg.: 3794927.73 (246.85% est.), Collisions std.dev.: 18.69%\r\n             count_82 := \"count\"(*)\r\n```", "NaN"], ["10205", "Improve local aggregation parallelism", "Karol Sobczak", "sopel39", "03/26/18, 07:28:25 PM", "local exchange type for aggregation can be governed by similar rules as remote exchange. See `com/facebook/presto/sql/planner/optimizations/AddExchanges.java:251`", "NaN"], ["10206", "Fix current_date when session zone had gap at 1970-01-01 00:00:00", "Piotr Findeisen", "findepi", "03/21/18, 10:17:55 AM", "Computation done by the implementation of `current_date` was failing if\r\nsession zone had a gap at 1970-01-01 00:00:00. This commit changes the\r\nAPIs used so that the code works in those cases too.\r\n\r\nThis problem was found thanks to https://github.com/prestodb/presto/pull/10128", "NaN"], ["10207", "Analyze query before enqueuing", "Martin Traverso", "martint", "03/20/18, 04:08:44 AM", "This helps catch semantic and access control errors much sooner.\r\nOtherwise, queries can queue for a long time only to fail with\r\ntrivial errors.", "NaN"], ["10208", "Remove dead code in SqlQueryManager", "Wenlei Xie", "wenleix", "04/02/18, 09:22:39 PM", "QueryQueueFullException can only be thrown when submitting the\r\nQueryExecution. The resource group is never assigned when\r\ncreating QueryExecution.", "NaN"], ["10210", "Change path to root to be separate method", "Raghav Sethi", "raghavsethi", "03/23/18, 12:16:37 AM", "NaN", "NaN"], ["10213", "Add profile for running Hive S3 tests", "Karol Sobczak", "sopel39", "03/23/18, 08:50:46 AM", "NaN", "NaN"], ["10217", "Add profile to skip Accumulo tests", "David Phillips", "electrum", "03/21/18, 12:29:32 AM", "These tests do not work on machines that only have IPv6.", "NaN"], ["10219", "Do not use Streams in Block/Pages implementations", "Piotr Findeisen", "findepi", "03/21/18, 08:40:20 PM", "While Streams often offer more readable code, they have certain\r\nperformance implications and therefore should be avoided in execution.", "NaN"], ["10220", "Remove Page.getBlocks()", "Dain Sundstrom", "dain", "03/22/18, 05:03:54 AM", "NaN", "NaN"], ["10221", "Inline com.facebook.presto.matching.Util", "Piotr Findeisen", "findepi", "03/21/18, 04:09:33 PM", "NaN", "NaN"], ["10224", "Alternative execution strategy for multiple DISTINCT aggregates", "Martin Traverso", "martint", "03/27/18, 04:45:38 AM", "This includes two changes:\r\n- Support for DISTINCT in aggregation accumulators to avoid multiple-shuffles when using MarkDistinct\r\n- Support for aggregations with DISTINCT + FILTER (WHERE...)", "NaN"], ["10226", "Switch to ESRI's estimateMemorySize API for geometry objects", "Maria Basmanova", "mbasmanova", "03/22/18, 03:21:31 PM", "Upgraded esri-geometry-api to 2.1.0 (from 1.2.1). This version offers an API to estimate memory size of a given geometry object and provides proper `equals` methods on geometry objects.\r\n\r\nReplaced custom memory estimation with the new API. \r\n\r\nUpdated `assertEquals` calls in tests to pass geometry objects instead of WKTs.\r", "NaN"], ["10227", "Use simple non-partitioned output for scaled table writer", "Dain Sundstrom", "dain", "03/22/18, 02:41:53 AM", "NaN", "NaN"], ["10229", "Add peak per-node total memory usage to QueryStats", "Nezih Yigitbasi", "nezihyigitbasi", "03/29/18, 12:01:41 AM", "To enforce limits against the total memory usage we need to know the\r\nmax total memory a particular query uses across the nodes.\r\n\r\nNeeded for setting limits when https://github.com/prestodb/presto/pull/10121 is merged.", "NaN"], ["10231", "Use better pattern in PushPartialAggregation rules", "Grzegorz Kokosi\u0144ski", "kokosing", "04/09/18, 09:44:58 AM", "Use better pattern in PushPartialAggregation rules", "NaN"], ["10232", "Remove unused PlanRewriter", "Karol Sobczak", "sopel39", "03/22/18, 01:50:59 PM", "NaN", "NaN"], ["10233", "Accept system properties in RuleTester", "\u0141ukasz Osipiuk", "losipiuk", "03/22/18, 03:23:11 PM", "Extracted from https://github.com/prestodb/presto/pull/9453", "NaN"], ["10234", "Reformat code of EqualityInference class", "\u0141ukasz Osipiuk", "losipiuk", "04/02/18, 01:46:39 PM", "Import static DeterminismEvaluator.isDeterministic and NullabilityAnalyzer.mayReturnNullOnNonNullInput\r\n\r\n\r\nExtracted from #9453", "NaN"], ["10235", "Remove unused code in InCodeGenerator", "Piotr Findeisen", "findepi", "03/23/18, 11:39:12 AM", "NaN", "NaN"], ["10236", "Remove unused preGeneratedExpressions variables", "Wenlei Xie", "wenleix", "03/22/18, 08:20:03 PM", "preGeneratedExpressions used to be required in more places\r\nto compile lambda and try expressions.", "NaN"], ["10239", "Fix NPE in Raptor shard cleaner", "David Phillips", "electrum", "03/23/18, 12:23:40 AM", "NaN", "NaN"], ["10241", "Use explain plan in QueryCompletedEvent if possible", "Dain Sundstrom", "dain", "03/28/18, 05:25:44 PM", "NaN", "NaN"], ["10242", "Multiple fixes to ORC writer", "James Sun", "highker", "03/23/18, 08:30:44 PM", "NaN", "NaN"], ["10246", "Optimize DECIMAL constants for byte code generation", "Piotr Findeisen", "findepi", "03/23/18, 09:44:02 PM", "NaN", "NaN"], ["10247", "Create Ordering in CostComparator", "Piotr Findeisen", "findepi", "03/23/18, 03:40:17 PM", "`Ordering` has added bonus that it supports predictable min/max\r\nselection from a `Collection`.", "NaN"], ["10248", "Run Hive S3 tests on Travis", "Karol Sobczak", "sopel39", "03/26/18, 10:00:24 AM", "NaN", "NaN"], ["10253", "Prevent NullPointerException in PartitionedLookupSourceFactory", "Piotr Findeisen", "findepi", "04/04/18, 03:03:41 PM", "When query is aborted and `PartitionedLookupSourceFactory` gets\r\ndestroyed before `HashBuilderOperator` is stopped, then\r\n`PartitionedLookupSourceFactory.freePartitions()` may have been called\r\nbefore `lendPartitionLookupSource()` (and `supplyLookupSources()`) are\r\ncalled.  When this happened, `PartitionedLookupSourceFactory.partitions`\r\nwould have null entries, leading to `NullPointerException` in\r\n`supplyLookupSources()`.\r\n\r\nFixes #10238", "NaN"], ["10255", "Support MIN/MAX over UNKNOWN type", "Wenlei Xie", "wenleix", "03/28/18, 05:52:28 PM", "NaN", "NaN"], ["10259", "Expose \"$partitions\" system tables for partitioned Hive tables", "\u0141ukasz Osipiuk", "losipiuk", "04/04/18, 10:33:50 AM", "@electrum, @findepi \r\n**Please just do high level review to validate if approach is generally OK**\r\n\r\nThis is an try to workaround the problem with listing Hive partitions when there are more of them than configuration set hard limit.\r\nWe decided that want to try approach when for each partitioned Hive table we expose a counterpart system table (suffixed with `$partitions`) which \r\nwill expose all the partitioning keys.\r\n\r\nThis PR achieves that by exposing new `Optional<SystemTable> ConnectorMetadata.getSystemTable()` method.\r\nThe method is then plugged to per-connector SystemMemory connector via `SystemTablesProvider` (which used to be just `List<SystemTable>` before).\r\nThe `SystemTablesProvider` is responsible for listing and resolving system tables.\r\nThe used implementation is provided with `List<SystemTable>` (for sake of static system tables) and `Metadata` which is used to delegate calls to `ConnectorMetadata.getSystemTable` for dynamic ones.\r\n\r\nThe approach generally seems to work but please let me know what you think about it.\r\n\r\nOne limitation I see with current implementation is that we are only listing the static system tables. This leads to some weirdnesses. E.g you will get empty table when query `SHOW COLUMNS FROM \"table$partitions\"`, because it is rewritten to `information_schema` query which depends on fact that queried tables are listed via `ConnectorMedata.listTables`. \r\nPotentially this can be extended by adding `ConnectorMetadata.listSystemTables` method. Yet for Hive we cannot cheaply test if table is partitioned or not (I think).\r\nAnd without that check it gets problematic if the `$partitions` table is queried for table which is not actually partitioned as we do not have any columns to return. But generally it should be doable - we just need to think about what is exact expected behaviour.\r\n\r\nEdit: addressed limitation with `Use SystemTablesProvider.getSystemTable in listTableColumns` commit.\r", "NaN"], ["10260", "Increase max arguments to zip function to 5", "Rebecca Schlussel", "rschlussel", "03/27/18, 03:11:06 PM", "There was a user request to increase the number of arrays that can be\r\npassed to the zip function.", "NaN"], ["10261", "Validator for aggregations that produce default values", "Karol Sobczak", "sopel39", "03/28/18, 01:48:21 PM", "Validator ensures that no duplicates of default values will be produced by final aggregations.", "NaN"], ["10262", "Convert Thrift connector to Drift", "David Phillips", "electrum", "04/02/18, 05:03:41 PM", "NaN", "NaN"], ["10264", "Add resourceGroupId to SessionConfigurationContext", "Wenlei Xie", "wenleix", "04/04/18, 02:22:46 AM", "NaN", "NaN"], ["10266", "Do not require table creation for SHOW TABLES test", "David Phillips", "electrum", "03/27/18, 12:09:39 AM", "NaN", "NaN"], ["10267", "Allow listing missing schemas in Accumulo", "David Phillips", "electrum", "03/27/18, 03:42:17 AM", "The schema name passed to listing APIs is merely a hint.\nImplementations are expected to succeed whether or not it exists.", "NaN"], ["10268", "[geospatial] Optimize [not only] point serialization", "Andrii Rosa", "arhimondr", "04/02/18, 05:08:21 PM", "Improve `geospatial` serialization mechanisms.\r\n\r\n- Serialize point in a custom way (4 bytes)\r\n- Support enclosed geometry collections\r\n- Make serialization to do not loose the actual multi type\r\n- Improve serialization/deserealization performance ( <10% performance boost in general case, up to 100% performance boost in case of point)\r\n- Improve envelope deserealization performance (up to 50% boost)\r\n- ... and many many more ... \r\n\r\nPlease refer to commit messages for more details", "NaN"], ["10272", "Add support for ORDER BY aggregations with grouping sets", "Martin Traverso", "martint", "04/05/18, 12:27:54 PM", "NaN", "NaN"], ["10273", "Fix thread safety in Glue metastore tests", "David Phillips", "electrum", "03/27/18, 08:04:26 PM", "NaN", "NaN"], ["10274", "Don't reuse WKTReader across threads", "Maria Basmanova", "mbasmanova", "03/27/18, 04:59:29 PM", "WKTReader is not threadsafe.", "NaN"], ["10275", "Add doc for cast between JSON and ROW", "Wenlei Xie", "wenleix", "03/28/18, 05:24:10 AM", "NaN", "NaN"], ["10277", "Validate null supression in structural blocks", "Dain Sundstrom", "dain", "03/29/18, 12:31:42 AM", "Structural blocks assume that null map, array, and rows have no entries\nin the nested blocks.  If nested blocks do have entries exceptions or\nincorrect results can occur.  There are no known uses of this behavior,\nand this change ensures none are added in the future.", "NaN"], ["10278", "Use explicit Optional in ConnectorMetadata", "Grzegorz Kokosi\u0144ski", "kokosing", "07/18/18, 09:01:03 AM", "NaN", "NaN"], ["10280", "Drop tables at the end of testScaleWriters test", "Piotr Findeisen", "findepi", "03/28/18, 12:01:37 PM", "This follows `testGroupedJoin` example", "NaN"], ["10283", "Fix overflow", "Maria Basmanova", "mbasmanova", "04/03/18, 09:03:17 PM", "Fixes #10276", "NaN"], ["10284", "Add missing parameters to exception message", "Martin Traverso", "martint", "03/28/18, 07:18:36 PM", "NaN", "NaN"], ["10286", "Minor MapBlock and RowBlock improvements", "Dain Sundstrom", "dain", "04/03/18, 01:39:17 AM", "NaN", "NaN"], ["10287", "Update to Airlift 0.165", "Nezih Yigitbasi", "nezihyigitbasi", "03/29/18, 06:35:37 PM", "NaN", "NaN"], ["10289", "Add Json Web Token (JWT) authenticator", "Dain Sundstrom", "dain", "04/06/18, 03:18:05 AM", "NaN", "NaN"], ["10290", "Minor fixups in product tests script", "Grzegorz Kokosi\u0144ski", "kokosing", "04/02/18, 07:47:16 AM", "NaN", "NaN"], ["10291", "Add a newline before query in INSERT INTO formatting", "Leiqing Cai", "caithagoras", "03/30/18, 08:51:35 PM", "NaN", "NaN"], ["10293", "Revert check enforcing minimum number of workers during scheduling", "Nezih Yigitbasi", "nezihyigitbasi", "03/29/18, 06:36:07 PM", "This reverts commit 8695d5d591d3ad36ea02f197a0a4a86d9bca4ed2.", "NaN"], ["10295", "Change WEIGHTED_FAIR to use FCFS to pick between winners", "Raghav Sethi", "raghavsethi", "03/29/18, 09:28:27 PM", "This mechanism also guarantees no starvation, is easier to understand\r\nand more deterministic, and leads to shorter queue sizes overall.", "NaN"], ["10297", "Update to Accumulo 1.7.4", "David Phillips", "electrum", "03/31/18, 07:05:01 AM", "This version should work the latest Guava.", "NaN"], ["10299", "Refactor Structural Block and Avoid Building Hash Table when Copying Map Block", "Wenlei Xie", "wenleix", "04/12/18, 07:40:48 AM", "Benchmark:\r\n\r\nBefore\r\n```\r\nBenchmark                          (mapSize)  Mode  Cnt     Score    Error  Units\r\nBenchmarkMapCopy.benchmarkMapCopy          1  avgt  100   141.925 \u00b1  0.830  ns/op\r\nBenchmarkMapCopy.benchmarkMapCopy          2  avgt  100   277.466 \u00b1  2.169  ns/op\r\nBenchmarkMapCopy.benchmarkMapCopy          4  avgt  100   541.838 \u00b1  5.143  ns/op\r\nBenchmarkMapCopy.benchmarkMapCopy          8  avgt  100  1127.118 \u00b1 39.768  ns/op\r\nBenchmarkMapCopy.benchmarkMapCopy         16  avgt  100  2212.371 \u00b1 84.363  ns/op\r\n```\r\n\r\nAfter\r\n```\r\nBenchmark                          (mapSize)  Mode  Cnt     Score    Error  Units\r\nBenchmarkMapCopy.benchmarkMapCopy          1  avgt  100    89.487 \u00b1  0.736  ns/op\r\nBenchmarkMapCopy.benchmarkMapCopy          2  avgt  100   157.988 \u00b1  1.185  ns/op\r\nBenchmarkMapCopy.benchmarkMapCopy          4  avgt  100   291.660 \u00b1  1.935  ns/op\r\nBenchmarkMapCopy.benchmarkMapCopy          8  avgt  100   589.937 \u00b1 23.140  ns/op\r\nBenchmarkMapCopy.benchmarkMapCopy         16  avgt  100  1178.167 \u00b1 45.828  ns/op\r\n```\r\n\r\nIt's 37% to 48% improvement.", "NaN"], ["10300", "Fix example in map transform_values documentation", "Haozhun Jin", "haozhun", "03/30/18, 05:36:48 PM", "NaN", "NaN"], ["10302", "Remove empty constructor to BlockBuilderStatus", "Wenlei Xie", "wenleix", "04/03/18, 12:28:05 AM", "When this constructor is used, this class has no observable\r\ninternal state, and this constructor essentially constructs\r\na black hole.\r\n\r\nSince BlockBuilder now allows taking null as BlockBuilderStatus,\r\nthis constructor and all its usage should probably be removed.", "NaN"], ["10307", "Close and destroy operators as soon as finished", "Dain Sundstrom", "dain", "04/10/18, 10:36:09 PM", "Close and destroy operators as soon as finished. When an operator is finished close, destroy and remove the operator and the source operator change in the Driver.\r", "NaN"], ["10309", "Add the ability to specify resource estimates for queries", "Raghav Sethi", "raghavsethi", "04/09/18, 10:39:50 PM", "cc @highker ", "NaN"], ["10310", "Move CLI_ARGUMENTS to docker compose files", "Grzegorz Kokosi\u0144ski", "kokosing", "04/03/18, 05:54:03 AM", "Move CLI_ARGUMENTS to docker compose files\r\n\r\nThat way configuration related to given environment is stored in this\r\nenvironment configuration files not in a script which is just using that\r\nenvironment.\r\n\r\nAlso that way it is easier to use Presto cli manually to connect to presto in\r\nenvironment specific Presto server by something like:\r\n```\r\n./conf/docker/singlenode-kerberos-hdfs-impersonation/compose.sh run application-runner /docker/volumes/conf/docker/files/presto-cli.sh\r\n```\r", "NaN"], ["10311", "Cast null to output column types for INSERT", "James Sun", "highker", "04/03/18, 02:19:31 AM", "ORC writer uses ColumnarRow/Map/Array to inspect the structure of a\r\nblock. The down cast can fail if the input is of unknown type, which by\r\ndefault creates fixed width blocks that cannot be decoded by either row,\r\nmap or array. This can happen when the INSERT misses the output alias.\r\nWe force a cast from null to the output column types to workaround this.", "NaN"], ["10312", "Support bucket number evolution in Hive connector", "Haozhun Jin", "haozhun", "04/10/18, 07:00:21 PM", "This allows mismatch between partition bucket count and table bucket count\r\nif the ratio is a power of two.\r", "NaN"], ["10313", "Remove unused classes", "David Phillips", "electrum", "04/06/18, 01:21:05 AM", "NaN", "NaN"], ["10316", "Extract tpch PredicateUtils", "Grzegorz Kokosi\u0144ski", "kokosing", "04/05/18, 10:35:49 AM", "Extract tpch PredicateUtils\n\nThis extraction removes unnecessary coupling between TpchMetadata and\nTestTpchMetadata.", "NaN"], ["10318", "Allow emulating worker count in RuleTester", "Piotr Findeisen", "findepi", "04/03/18, 08:11:15 PM", "NaN", "NaN"], ["10319", "Restore predicate pushdown switch in TPCH connector", "Piotr Findeisen", "findepi", "04/05/18, 02:28:32 PM", "This reverts removal of predicate pushdown switch in TPCH connector.\r\nAdditionally, this uses the switch also when querying table statistics,\r\nso that TPCH connector can mimic a partitioned table (or not).", "NaN"], ["10321", "Simplify expressions every time PredicatePushDown is run", "Piotr Findeisen", "findepi", "04/05/18, 01:08:20 PM", "`PredicatePushDown` may make new expressions simplifiable. It should be\r\nalways followed by optimizers that do the simplification.", "NaN"], ["10322", "Some minor CBO-related refactors", "Piotr Findeisen", "findepi", "04/10/18, 08:27:54 AM", "NaN", "NaN"], ["10323", "Support skip.header/footer.line.count", "Grzegorz Kokosi\u0144ski", "kokosing", "04/11/18, 10:25:37 AM", "NaN", "NaN"], ["10324", "Spatial left join", "Maria Basmanova", "mbasmanova", "04/04/18, 11:08:43 PM", "Addresses #10282", "NaN"], ["10327", "Optimize the ST_Intersection function for envelopes", "Maria Basmanova", "mbasmanova", "04/09/18, 05:30:54 PM", "Optimized version is 400 times faster.\r\n\r\n```\r\nBenchmark                                 Mode  Cnt      Score      Error  Units\r\nBenchmarkEnvelopeIntersection.envelopes   avgt   20    128.189 \u00b1    9.407  ns/op\r\nBenchmarkEnvelopeIntersection.geometries  avgt   20  58120.260 \u00b1 9739.057  ns/op\r\n```\r", "NaN"], ["10328", "Support OpenJDK with server RPM", "Anu Sudarsan", "anusudarsan", "04/05/18, 01:14:21 PM", "replacing https://github.com/prestodb/presto/pull/5541", "NaN"], ["10329", "Avoid asserting fixed catalog name in TestHiveIntegrationSmokeTest", "Wenlei Xie", "wenleix", "04/05/18, 04:41:23 AM", "NaN", "NaN"], ["10330", "Add line_locate_point function", "Maria Basmanova", "mbasmanova", "04/06/18, 07:32:14 PM", "`line_locate_point` function returns a float between 0 and 1 representing the location of the closest point on the LineString to the given Point, as a fraction of total 2d line length. It is useful to order points on a \"road\" LineString to follow the \"road\".\r\n\r\nHere is a corresponding PostGIS function for reference: https://postgis.net/docs/ST_LineLocatePoint.html", "NaN"], ["10331", "Avoid meaningless expression evaluation in LayoutConstraintEvaluator", "Grzegorz Kokosi\u0144ski", "kokosing", "07/18/18, 08:57:34 AM", "Avoid meaningless expression evaluation in LayoutConstraintEvaluator\n\nWhen bindings do not provide any values used in expression there is no\npoint to evaluate such expression.", "NaN"], ["10334", "Add support for JWT public key signing", "Dain Sundstrom", "dain", "04/10/18, 03:39:57 AM", "NaN", "NaN"], ["10335", "Disable join reordering in tests expecting concrete execution plan", "Piotr Findeisen", "findepi", "04/06/18, 09:33:02 AM", "NaN", "NaN"], ["10336", "Improve queries on information_schema.columns with filter on table_schema", "Grzegorz Kokosi\u0144ski", "kokosing", "04/12/18, 08:53:16 AM", "Improve queries on information_schema.columns with filter on table_schema\r\n\r\nIn case of catalogs with high number of schemas and tables in them,\r\nsimple queries like:\r\n```\r\nSELECT * FROM information_schema WHERE table_schema LIKE 'pattern'\r\n```\r\ntakes a really long time to complete.\r\n\r\nAbove queries are typical from BI tools, which commonly do not to escape\r\nwildcards in LIKE pattern, even if the pattern goal is to match\r\nsingle schema or single table.\r", "NaN"], ["10338", "Redact value of internal-communication.https.keystore.key", "Rentao", "rentaow", "04/06/18, 05:28:35 PM", "Prevent sensitive value of internal-communication.https.keystore.key from showing in server.log", "NaN"], ["10340", "Fix ORC writer present stream writing incorrect checkpoints", "James Sun", "highker", "04/06/18, 11:02:35 PM", "NaN", "NaN"], ["10341", "Refactor SqlQueryManager to get query types without down cast", "James Sun", "highker", "04/07/18, 07:42:28 AM", "NaN", "NaN"], ["10342", "Optimize ExpressionInterpreter for predicate expressions", "Grzegorz Kokosi\u0144ski", "kokosing", "04/09/18, 11:18:48 AM", "Optimize ExpressionInterpreter for predicate expressions", "NaN"], ["10347", "Test that delegation in PartitionsAwareAccessControl is complete", "Piotr Findeisen", "findepi", "04/09/18, 09:04:04 PM", "`PartitionsAwareAccessControl` must explicitly implement all interface\r\nmethods.  This adds a test for that.", "NaN"], ["10348", "Rethrow remote exception with wrapping in tests", "Piotr Findeisen", "findepi", "04/10/18, 06:55:13 AM", "This is important for an IDE to be able to say in which line a test\r\nfailed.", "NaN"], ["10350", "Publish exchange summary on query completion", "Nezih Yigitbasi", "nezihyigitbasi", "04/09/18, 07:39:12 PM", "For analyzing exchange memory usage we need to publish this information, which is already available in `ExchangeClientStatus`.", "NaN"], ["10351", "Only check view creation permissions at query time", "David Phillips", "electrum", "04/10/18, 08:16:36 PM", "NaN", "NaN"], ["10352", " Short-circuit Join when probe side empty", "Piotr Findeisen", "findepi", "04/10/18, 09:44:54 AM", "NaN", "NaN"], ["10354", "Fix flaky TestQueryStateInfoResource", "James Sun", "highker", "04/10/18, 07:01:55 PM", "NaN", "NaN"], ["10356", "Make Constraint::predicate to be Optional", "Grzegorz Kokosi\u0144ski", "kokosing", "07/18/18, 07:17:26 AM", "Make Constraint::predicate to be Optional\n\nPassing Constaint::predicate to a connector means for connector that\nthere is some external (planner) utility which can filter out not needed\ndata.\n\nPassing `bindings -> true` is very misleading. Because no\nexternal (to connector) knowledge is injected and any data filtering on\nthe connector side is just pointless.\nOptional.empty() is much better because connector can avoid such\nfiltering which in some cases could be very expensive.", "NaN"], ["10359", "Publish OutputBuffer peak memory usage on query completion", "Nezih Yigitbasi", "nezihyigitbasi", "04/10/18, 08:29:50 PM", "NaN", "NaN"], ["10360", "Run Hive Glue test on Travis", "Rentao", "rentaow", "04/17/18, 10:18:03 AM", "NaN", "NaN"], ["10361", "Short circuit Driver.firstFinishedFuture with one future", "Dain Sundstrom", "dain", "04/27/18, 03:24:33 AM", "NaN", "NaN"], ["10364", "Update \"Hive partitions are immutable\" error message", "Nezih Yigitbasi", "nezihyigitbasi", "04/11/18, 03:07:43 AM", "NaN", "NaN"], ["10365", "Remove driver special handling for single operator", "Dain Sundstrom", "dain", "04/30/18, 04:47:34 PM", "NaN", "NaN"], ["10366", "Close query runner in TestQueryPlansDeterministic", "Haozhun Jin", "haozhun", "04/11/18, 12:51:05 AM", "NaN", "NaN"], ["10367", "Fix thread-unsafe usage of CharsetEncoder", "Raghav Sethi", "raghavsethi", "04/11/18, 12:15:15 AM", "NaN", "NaN"], ["10372", "Use directExecutor() instead of newDirectExecutorService()", "Piotr Findeisen", "findepi", "04/11/18, 08:08:22 PM", "`newDirectExecutorService()` creates new instance on each call.", "NaN"], ["10373", "Cleanup legacy plugin.config dir property", "Piotr Findeisen", "findepi", "04/11/18, 08:06:40 PM", "NaN", "NaN"], ["10375", "Add support for column level access control", "Rebecca Schlussel", "rschlussel", "05/02/18, 03:19:13 PM", "NaN", "NaN"], ["10376", "Minor refactor to OperatorContext.DecoratedLocalMemoryContext", "Nezih Yigitbasi", "nezihyigitbasi", "04/12/18, 05:37:52 PM", "NaN", "NaN"], ["10378", "Fix thread interruption in WorkProcessorUtils", "Nezih Yigitbasi", "nezihyigitbasi", "04/12/18, 05:37:25 PM", "NaN", "NaN"], ["10384", "Stats for testing planner decision against considerable data size", "Piotr Findeisen", "findepi", "04/13/18, 10:02:41 AM", "NaN", "NaN"], ["10385", "Optimize more geo functions", "Andrii Rosa", "arhimondr", "04/12/18, 06:02:06 PM", "NaN", "NaN"], ["10387", "Add ST_GeometryType function", "Jacob Wasserman", "jwass", "04/20/18, 07:12:14 PM", "Example:\r\n\r\n```\r\npresto> select ST_GeometryType(ST_GeometryFromText('POLYGON ((1 1, 1 4, 4 4, 4 1))'));\r\n   _col0\r\n------------\r\n ST_Polygon\r\n(1 row)\r\n```", "NaN"], ["10388", "Add Kudu connector", "Martin Weindel", "MartinWeindel", "08/15/18, 05:17:35 AM", "Add connector to Apache Kudu.\r\n\r\nIt supports\r\n- querying a Kudu table with full support of Kudu scanner predicates and Kudu partitionings/splits\r\n- inserting, updating, deleting rows\r\n- creating and dropping tables and schemas\r\n- copying tables\r\n- managing range partitions\r\n\r\nNote: This pull request is based on the source code of [https://github.com/MartinWeindel/presto-kudu](https://github.com/MartinWeindel/presto-kudu) with adaptions for Presto build, package names, and coding styles.", "NaN"], ["10389", "Add 0.199 release notes", "Nezih Yigitbasi", "nezihyigitbasi", "04/18/18, 12:33:18 AM", "#10306", "NaN"], ["10390", "Remove TypeManager parameter from BlockEncodingFactory.readEncoding", "Haozhun Jin", "haozhun", "04/20/18, 07:40:56 PM", "The bulk of this PR is in \"Remove hack to serialize a Block without using BlockEncodingSerde\".\r\n\r\nThis makes option 2 in #10386 feasible.", "NaN"], ["10391", "Update docker images for hive tests", "Grzegorz Kokosi\u0144ski", "kokosing", "04/24/18, 07:51:55 PM", "Update docker images for hive tests", "NaN"], ["10392", "Use IS_NOT_DISTINCT_FROM operator in Group By", "Elon Azoulay", "elonazoulay", "04/27/18, 06:20:57 PM", "This PR also fixed the behavior of IS_NOT_DISTINCT_FROM operator for NaN\r\nResolves #1508", "NaN"], ["10395", "Properly propagate SQLException and VerifierException in Verifier", "Nezih Yigitbasi", "nezihyigitbasi", "04/16/18, 04:54:53 PM", "getResultSetConverter() method can throw SQLException and\r\nVerifierException. However, those exceptions will be wrapped in\r\nan ExecutionException by the TimeLimiter. That may cause the verifier\r\nrun to fail completely instead of failing a single query (e.g., when\r\nthe query result set has more than maxRowCount rows).\r\n\r\n\r\n```\r\n00:58:11 2018-04-13T00:58:11.227-0700\tERROR\tmain\tBootstrap\tUncaught exception in thread main\r\n00:58:11 java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.facebook.presto.verifier.VerifierException: More than '10000' rows, failing query\r\n00:58:11 \tat com.facebook.presto.verifier.Verifier.takeUnchecked(Verifier.java:247)\r\n00:58:11 \tat com.facebook.presto.verifier.Verifier.run(Verifier.java:133)\r\n00:58:11 \tat com.facebook.presto.verifier.VerifyCommand.run(VerifyCommand.java:150)\r\n00:58:11 \tat com.facebook.presto.verifier.PrestoFacebookVerifier.main(PrestoFacebookVerifier.java:22)\r\n00:58:11 Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.facebook.presto.verifier.VerifierException: More than '10000' rows, failing query\r\n00:58:11 \tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n00:58:11 \tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\r\n00:58:11 \tat com.facebook.presto.verifier.Verifier.takeUnchecked(Verifier.java:244)\r\n00:58:11 \t... 3 more\r\n00:58:11 Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.facebook.presto.verifier.VerifierException: More than '10000' rows, failing query\r\n00:58:11 \tat com.google.common.base.Throwables.propagate(Throwables.java:241)\r\n00:58:11 \tat com.facebook.presto.verifier.Validator.executeQuery(Validator.java:467)\r\n00:58:11 \tat com.facebook.presto.verifier.Validator.executeQueryControl(Validator.java:346)\r\n00:58:11 \tat com.facebook.presto.verifier.Validator.validate(Validator.java:207)\r\n00:58:11 \tat com.facebook.presto.verifier.Validator.valid(Validator.java:192)\r\n00:58:11 \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n00:58:11 \tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n00:58:11 \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n00:58:11 \tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n00:58:11 \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n00:58:11 \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n00:58:11 \tat java.lang.Thread.run(Thread.java:748)\r\n00:58:11 Caused by: java.util.concurrent.ExecutionException: com.facebook.presto.verifier.VerifierException: More than '10000' rows, failing query\r\n00:58:11 \tat com.google.common.util.concurrent.SimpleTimeLimiter.wrapAndThrowExecutionExceptionOrError(SimpleTimeLimiter.java:274)\r\n00:58:11 \tat com.google.common.util.concurrent.SimpleTimeLimiter.callWithTimeout(SimpleTimeLimiter.java:161)\r\n00:58:11 \tat com.facebook.presto.verifier.Validator.executeQuery(Validator.java:433)\r\n00:58:11 \t... 10 more\r\n00:58:11 Caused by: com.facebook.presto.verifier.VerifierException: More than '10000' rows, failing query\r\n00:58:11 \tat com.facebook.presto.verifier.Validator.convertJdbcResultSet(Validator.java:554)\r\n00:58:11 \tat com.facebook.presto.verifier.Validator.lambda$getResultSetConverter$4(Validator.java:505)\r\n00:58:11 \t... 4 more\r\n```", "NaN"], ["10399", "Remove redundant code from ClusterMemoryManager", "Raghav Sethi", "raghavsethi", "04/25/18, 03:54:29 AM", "Presto memory pools are not dynamic, so ClusterMemoryManager can\r\nassert that the GENERAL, RESERVED and SYSTEM pools exist and act\r\naccordingly.", "NaN"], ["10400", "User lower case when listing schemas", "Grzegorz Kokosi\u0144ski", "kokosing", "04/16/18, 05:19:32 PM", "User lower case when listing schemas\n\nSchemaTableName or QualifiedObjectName either make sure that name parts\nare in lower case or make them to be in lower case. However when listing\nschemas, schema is kept in plain String which does not enforce name to\nbe in lower case.\n\nTo make schema consistent with other name related objects it is\ntransformed to lower case in MetadataManager. That way it will work for\nall connectors.\n\nAlternatively, a SchemaName could be introduced. But this is much bigger\nchange and may need SPI to be changed as well.", "NaN"], ["10401", "Prefer String.toLowerCase(ENGLISH) over String.toLowerCase()", "Grzegorz Kokosi\u0144ski", "kokosing", "07/09/18, 11:18:22 AM", "Prefer String.toLowerCase(ENGLISH) over String.toLowerCase()", "NaN"], ["10405", "Support correlated no aggregation scalar subqueries (v2)", "Grzegorz Kokosi\u0144ski", "kokosing", "06/06/18, 10:02:24 AM", "Support correlated no aggregation scalar subqueries\n\nAdding support correlated non aggregation scalar subqueries like:\nSELECT a FROM rel WHERE b = correlation", "NaN"], ["10407", "Make s3 fs rename fail if it is already renamed", "Grzegorz Kokosi\u0144ski", "kokosing", "04/20/18, 07:10:19 AM", "Make s3 fs rename fail if it is already renamed\n\nIf file is already named foo.txt it cannot be renamed to foo.txt.", "NaN"], ["10411", "Fix timeout handling in verifier", "David Phillips", "electrum", "04/16/18, 07:30:57 PM", "This also cleans up exception handling by using the proxy version of\nTimeLimiter with an interface that declares checked exceptions.", "NaN"], ["10413", "Fix verifier startup", "David Phillips", "electrum", "04/16/18, 08:44:25 PM", "Interfaces used with TimeLimiter proxies need to be public.", "NaN"], ["10414", "Add ST_NumGeometries and ST_GeometryN functions", null, "dongqingx", "05/08/18, 10:17:42 PM", "NaN", "NaN"], ["10416", "Remove unused method in SqlQueryManager", "Nezih Yigitbasi", "nezihyigitbasi", "04/23/18, 03:43:01 PM", "NaN", "NaN"], ["10418", "Fix planning failure for mixed join and group by with grouped execution", "Haozhun Jin", "haozhun", "04/19/18, 12:11:14 AM", "Planning failed when\r\n* Build side of a join contains group by (or join) that can benefit from grouped execution\r\n* The join itself cannot take advantage of grouped execution", "NaN"], ["10420", "Use target path for table location", "Grzegorz Kokosi\u0144ski", "kokosing", "04/20/18, 07:11:05 AM", "Use target path for table location\r\n\r\nlocationService::writePathRoot returns a location where data was staged\r\nduring the insert. It is not guaranteed that this location will still\r\nexists after the insert commit. Moreover this location contains data\r\nfrom single insert.\r\n\r\nUsing locationService.targetPath returns a real table location where\r\ndata staged data is moved.\r", "NaN"], ["10422", "Revert \"Run Hive Glue test on Travis\"", "Karol Sobczak", "sopel39", "04/17/18, 10:20:38 AM", "This reverts commit ae7d8aa89003c7c896148fe1018e2b0b62b5d72e.\r\n\r\nRelease in progress", "NaN"], ["10423", "AWS Glue Travis tests ", "Karol Sobczak", "sopel39", "04/20/18, 09:57:43 AM", "To be merged after release is complete\r\n\r\nalready reviewed: https://github.com/prestodb/presto/pull/10360", "NaN"], ["10424", "Ignore BLOOM_FILTER_UTF8 ORC streams", "Karol Sobczak", "sopel39", "04/20/18, 09:48:55 AM", "For now I just added a code that ignores `bloom filters utf_8` stream. However, I wonder maybe we can just add code:\r\n```\r\n            if (stream.getStreamKind() == BLOOM_FILTER_UTF8) {\r\n                OrcInputStream inputStream = streamsData.get(entry.getKey());\r\n                bloomFilters.put(stream.getColumn(), metadataReader.readBloomFilterIndexes(inputStream));\r\n            }\r\n```\r\nto\r\n```\r\nStripeReader#readBloomFilterIndexes\r\n```\r", "NaN"], ["10426", "Add a statistics rule for estimating Join (CBO)", "Piotr Findeisen", "findepi", "05/17/18, 07:48:48 AM", "NaN", "NaN"], ["10429", "Remove maxBlockSizeInBytes in BlockBuilderStatus", "Wenlei Xie", "wenleix", "04/27/18, 02:25:34 AM", "Historically, we limit the size of each individual BlockBuilder since\r\nwe cannot limit the size of PageBuilder.  This is no longer the case\r\nsince 2a59617c44305975b960f9d0c013f72b7a21f209 as we can limit the PageBuilder size.\r\n\r\nThe BlockBuilder size in PageBulder is limited in an average way, i.e.\r\nmaxPageBytes / numBlocks. This can be problematic when there are\r\nmost BlockBuilder in a PageBuilder store primitive types,\r\nwhile one BlockBuilder in the PageBuilder stores structural types.\r\nThe PageBuilder will be considered as full prematurely.", "NaN"], ["10430", "Fix tests for BlockBuilder.appendStructure()", "Wenlei Xie", "wenleix", "04/20/18, 05:27:33 PM", "This is done via using different ways to copy the block in\r\nAbstractTestBlock.assertBlockPosition.\r\n\r\nTests for appendStructure() is missed in\r\n2ccfa9b3bc500dbcd501022c5c806badfcccf88c.", "NaN"], ["10431", "Fix decoding min/max REAL value in Hive stats", "Piotr Findeisen", "findepi", "04/20/18, 10:52:50 AM", "NaN", "NaN"], ["10433", "Extend the username extraction to principal user matching rules", "Yaliang Wang", "Yaliang", "04/27/18, 09:57:34 AM", "This is a follow-up patch to the #9943. \r\n+cc @losipiuk, @findepi, @dain, @luohao \r\n\r\nThe patch will give the ability to check either a principal and username pair can match regex or a principal can be substituted to exactly match username. \r\n\r\nWhy: \r\nWe want a service can run queries on behalf of the user with a shared principal to a group of users, whose names may not be relative to the principal name.\r\n\r\nHow: \r\n* take `principal`\r\n* match `principal` pattern, if it matches\r\n* match `user` pattern, if it matches, returns the `allow` value\r\n* produce some string using `principal_to_user` as substitution (substitution pattern)\r\n* take this string and check the equality of it against requested username. If it matches, return the `allow` value\r\n* If no rules are specified, it won't make any check.\r\n* If the principal misses all rules, the access will be declined.\r", "NaN"], ["10437", "Fix running JDBC driver on Java 9+", "David Phillips", "electrum", "04/18/18, 06:22:33 PM", "The shade plugin was producing class files that would fail at runtime\r\nwith `IncompatibleClassChangeError`.", "NaN"], ["10441", "Fix flaky test TestQueryStateInfoResource", "James Sun", "highker", "04/18/18, 10:07:49 PM", "NaN", "NaN"], ["10443", "Update 0.199 release notes", "Wenlei Xie", "wenleix", "04/19/18, 12:34:58 AM", "NaN", "NaN"], ["10444", "Add legacy row field ordinal access config and session property", "Haozhun Jin", "haozhun", "04/19/18, 12:05:48 AM", "NaN", "NaN"], ["10445", "Update 0.199 release notes", "Nezih Yigitbasi", "nezihyigitbasi", "04/19/18, 12:08:38 AM", "NaN", "NaN"], ["10446", "Fix partition offline check in HiveSplitManager", "Wenlei Xie", "wenleix", "04/19/18, 03:50:09 PM", "It is checking the table parameters rather than partition parameters.\r\nThe bug is introduced in f8824b16d5cd33cdeb95fad9f84ae074dc4c5531.", "NaN"], ["10451", "Update 0.199 release notes", "Wenlei Xie", "wenleix", "04/19/18, 10:05:12 PM", "NaN", "NaN"], ["10453", "Allow running Presto with Java 10 or above", "Haozhun Jin", "haozhun", "04/19/18, 06:51:17 PM", "NaN", "NaN"], ["10456", "Collect basic hive table statistics on table write", "Andrii Rosa", "arhimondr", "05/09/18, 04:31:00 PM", "`rowCount`, `fileCount`, `rawSize`, `totalSize`", "NaN"], ["10457", "Fix infinite loop due to MultipleDistinctAggregationToMarkDistinct rule", "Martin Traverso", "martint", "04/20/18, 12:58:37 AM", "The rule pattern accepted aggregations with DISTINCT and FILTER/mask, but the\r\nbody of the rule can't handle that scenario.\r\n\r\nThis changes adjusts the pattern to reject any aggregation node that contains\r\na DISTINCT aggregate with a mask of FILTER clause.\r", "NaN"], ["10458", "Add TableWriterInfo to keep track of peak writer memory usage", "Nezih Yigitbasi", "nezihyigitbasi", "04/27/18, 05:06:43 AM", "NaN", "NaN"], ["10459", "Support resource group selection based on resource estimates", "Wenlei Xie", "wenleix", "04/30/18, 08:07:02 PM", "NaN", "NaN"], ["10463", "Use MySQL, PostgreSQL and MSSQL images version in product tests", "Karol Sobczak", "sopel39", "04/20/18, 02:20:11 PM", "NaN", "NaN"], ["10466", "Simplify Block serialization/deserialization", "Haozhun Jin", "haozhun", "04/20/18, 10:49:49 PM", "Previously, there are 3 sections in a serialized Block:\r\n* Name - written by BlockEncodingSerde\r\n  * e.g. \"ARRAY\" indicating it's an ArrayBlock\r\n* Metadata - written by BlockEncodingFactory\r\n  * e.g. \"INT_ARRAY\" indicating the inner Block type of an ArrayBlock\r\n  * e.g. 4 indicating the entry size of FixedWidthBlock\r\n* Data - written by BlockEncoding\r\n\r\nThis 3-section scheme makes things more complicated:\r\n* Not recursive - For an ArrayBlock with IntArrayBlock inside, the encoding\r\n  of IntArrayBlock isn't the same as a stand-alone IntArrayBlock. Specifically,\r\n  part of it is put in the metadata section, and part of it in the data section.\r\n  This requires each invididual encoding to handle nested blocks,\r\n  which introduces both code duplication and complexity.\r\n* It is harder to learn and understand the implementations with\r\n  two interfaces (BlockEncodingFactory and BlockEncoding),\r\n  which don't have a very clear separation of responsibility, for each Block.\r\n\r\nThis commit solves both problems:\r\n* Only a single interface (BlockEncoding) is now needed.\r\n* A block inside another block now serializes the exact same way as a standalone one.\r\n  The outer block no longer need to handle any of the details of the inner one.\r\n\r\nIt also resolves the TODO item that replacementBlockForWrite was not respected for\r\nnested Blocks.", "NaN"], ["10467", "Include full path to password auth config file when not found", "Piotr Findeisen", "findepi", "04/23/18, 07:31:45 AM", "Previously, when `PasswordAuthenticatorManager`'s config file was not\r\nfound, the following would get logged, which doesn't tell where Presto\r\nis looking for the config file:\r\n\r\n```\r\n2018-04-20T22:01:53.887+0200\tERROR\tmain\tcom.facebook.presto.server.PrestoServer\tetc/password-authenticator.properties (No such file or directory)\r\njava.io.FileNotFoundException: etc/password-authenticator.properties (No such file or directory)\r\n\tat java.io.FileInputStream.open0(Native Method)\r\n\tat java.io.FileInputStream.open(FileInputStream.java:195)\r\n\tat java.io.FileInputStream.<init>(FileInputStream.java:138)\r\n\tat com.facebook.presto.util.PropertiesUtil.loadProperties(PropertiesUtil.java:33)\r\n\tat com.facebook.presto.server.security.PasswordAuthenticatorManager.loadPasswordAuthenticator(PasswordAuthenticatorManager.java:63)\r\n```", "NaN"], ["10470", "Remove getType from Operator and OperatorFactory", "Dain Sundstrom", "dain", "05/10/18, 12:15:48 AM", "The types on Operator and OperatorFactory are not needed and make LocalExecutionPlanner more complex.", "NaN"], ["10477", "Do not derive all grouping sets when traversing tree", "Martin Traverso", "martint", "04/24/18, 08:47:53 PM", "The only node that has visitable expressions is SimpleGroupBy. There's\r\nno point in synthesizing the grouping sets from nodes like Cube or Rollup\r\njust for the purpose of visiting the resulting expressions.\r\n\r\nOne effect of doing so is that query creation can fail due to OOMs\r\nwhen allocating too many objects.", "NaN"], ["10481", "Fix bing_tile_coordinates function to produce row block", "Wenlei Xie", "wenleix", "04/24/18, 11:39:25 PM", "Previously, bing_tile_coordinates produces a IntArrayBlock with length 2\r\nto represent Row(INTEGER, INTEGER). This no longer works since\r\nRowBlockBuilder.appendStructure now require row block in\r\n630ec959c4e29d09c3b46cba2085e320d815f1ac.", "NaN"], ["10483", "Prevent MySQL connector from reading rows after close", "David Phillips", "electrum", "04/24/18, 09:34:30 PM", "NaN", "NaN"], ["10485", "Disable optimization to short circuit on empty tables", "German Gil", "ggilfb", "04/25/18, 12:12:42 AM", "We found out a problem with this change when you have a large table joined with a small table (not an empty table) causing the query to hang.\r\ncc: @findepi \r\n\r\nreverts https://github.com/prestodb/presto/pull/9848", "NaN"], ["10487", "Revert removing system memory pool", "Nezih Yigitbasi", "nezihyigitbasi", "04/25/18, 03:46:19 AM", "Reverting because some more thought needs to go into about how to roll this back (or disable this) if/when we have issues in prod. ", "NaN"], ["10489", "Fix warnings in ThriftHiveMetastore", "David Phillips", "electrum", "04/27/18, 08:03:04 PM", "NaN", "NaN"], ["10490", "Cleanup exception handling", "David Phillips", "electrum", "04/30/18, 08:43:11 PM", "This removes all usages of the deprecated `Throwables.propagate()` method.\r\n\r\nNote that this is not a mechanical refactoring. I investigated every usage to determine the proper exception handling strategy. Many of the problems described in [this article](https://github.com/google/guava/wiki/Why-we-deprecated-Throwables.propagate) were present in our code base. Examples:\r\n\r\n* Only catching a checked exception (rather than mixed checked/unchecked). These are directly wrapped with `RuntimeException` or specializations such as `UncheckedIOException`.\r\n* Catching checked exceptions in places where they cannot be thrown. These are removed entirely.\r\n* Overly broad catches. For example, catching `Exception` when the only checked exception is `IOException`.\r\n* Re-wrapping exceptions that were already wrapped. For example, if the cause of `UncheckedExecutionException` is checked, the original can be re-thrown directly, rather than re-wrapping the cause.", "NaN"], ["10491", "Remove system pool", "Nezih Yigitbasi", "nezihyigitbasi", "04/30/18, 07:56:26 PM", "Only 714578a needs a review, the other commits have already been reviewed.\r\n\r\nCommit 714578a adds a new config property `deprecated.legacy-system-pool-enabled` to enable the system memory pool (by default the system pool is disabled). This flag will help us roll this change out in a safer way. To support the system pool I added a new class `LegacyQueryContext`, which has the old behavior, and the `DefaultQueryContext` has the new behavior (system allocations go to general pool). There is some code duplication among these context classes, but this is a temporary (and a somewhat clean) solution and once we have more confidence in this change I am going to delete the `LegacyQueryContext` and we will have a single `QueryContext`. So, once the system pool is removed completely I am going to clean up these classes.\r\n\r\nOpen issues:\r\n- This patch also added support for blocking for system allocations to various operators and output buffers. When `deprecated.legacy-system-pool-enabled=true` (old behavior), do we want to also disable this blocking behavior? I guess an easy way to do it is to return `NOT_BLOCKED` in `LegacyQueryContext::updateSystemMemory()`. If we want to have the config to rollback this change 100% then it makes sense to also revert the blocking behavior.", "NaN"], ["10492", "Add release notes for 0.200", "Haozhun Jin", "haozhun", "04/26/18, 10:00:43 PM", "NaN", "NaN"], ["10493", "Add support for trace token", "Yi He", "hellium01", "05/11/18, 09:28:03 PM", "Allow user to pass a trace token to presto to trace performance.", "NaN"], ["10494", "Add session property prefer-partial-aggregation", "Rongrong Zhong", "rongrong", "04/27/18, 09:58:52 PM", "NaN", "NaN"], ["10495", "Allow thrift connector to provide header to service", "Yi He", "hellium01", "05/11/18, 09:27:05 PM", "NaN", "NaN"], ["10496", "Add monitoring for the executor in HeartbeatFailureDetector", "Nezih Yigitbasi", "nezihyigitbasi", "04/26/18, 02:00:33 AM", "NaN", "NaN"], ["10499", "Update page retained size with instance size after compact", "James Sun", "highker", "04/27/18, 02:12:05 AM", "NaN", "NaN"], ["10501", "Add Presto protocol proxy service", "David Phillips", "electrum", "06/14/18, 06:59:18 PM", "NaN", "NaN"], ["10504", "Use direct Executor instead of ExecutorService", "David Phillips", "electrum", "04/27/18, 08:02:43 PM", "NaN", "NaN"], ["10505", "Update to ASM 6.1.1", "Haozhun Jin", "haozhun", "04/27/18, 07:19:44 PM", "This expression fails with ASM 6.0 in ClassReader constructor:\r\n`new ClassReader(classLoader.getResourceAsStream('java/lang/Object.class'))`.\r\nIt throws IllegalArgumentException in Java 10.0.1.\r\n\r\nDowngrading Java to 9.0.4 or upgrading ASM to 6.1.1 fixes the problem.", "NaN"], ["10510", "Fix potential overflow in BingTileFunctions::mapSize", "Nezih Yigitbasi", "nezihyigitbasi", "04/27/18, 08:37:20 PM", "Fixes #10502.", "NaN"], ["10511", "Use aggregates from order by expressions in order by scope", "Karol Sobczak", "sopel39", "04/27/18, 01:44:33 PM", "Proposed fix for: https://github.com/prestodb/presto/issues/10508", "NaN"], ["10513", "Update to Airlift 0.168", "Nezih Yigitbasi", "nezihyigitbasi", "04/27/18, 08:37:36 PM", "NaN", "NaN"], ["10514", "Update to aircompressor 0.10", "Martin Traverso", "martint", "04/27/18, 08:35:03 PM", "Improves zstd decompression performance for some datasets", "NaN"], ["10515", "Handle NaN hash for real and double types", "James Sun", "highker", "04/28/18, 06:13:02 AM", "NaN", "NaN"], ["10517", "Change ORC writer validation from boolean to percentage", "Dain Sundstrom", "dain", "04/28/18, 10:24:03 PM", "Sampling a small portion of writes for validation can reduce the time to discovery for corruption.", "NaN"], ["10518", "Remove ORC min row count wtih min stripe size", "Dain Sundstrom", "dain", "04/30/18, 06:16:29 PM", "Instead of enforcing a min row count use a stripe size.  This is better heuristic for large\nrows, and results in better compression since large rows can still use dictionary encoding.", "NaN"], ["10519", "Fix dictionary start offset of columnar array and map", "James Sun", "highker", "04/30/18, 12:43:28 AM", "NaN", "NaN"], ["10521", "Write file header and footer with stripe data", "Dain Sundstrom", "dain", "06/04/18, 07:00:41 PM", "Reorganize ORC code to write full stripe, with optional file header and footer,\nin one chunk.  This allows the underlying storage system to rely on large fully\nbuffered writes to improve performance.", "NaN"], ["10522", "Add cache for parameterized type", "Wenlei Xie", "wenleix", "05/01/18, 02:39:41 AM", "To avoid repeatedly resolving operator and creating BoundMethodHandle\r\nwhen deserializing MapBlock, we use the MethodHandles store in\r\nMapType. Caching MapType is required to accomplish this.", "NaN"], ["10523", "Fix Hive $partitions table for null partition values", "David Phillips", "electrum", "04/30/18, 06:57:13 PM", "Fixes #10520", "NaN"], ["10524", "Remove support for SHOW PARTITIONS statement", "David Phillips", "electrum", "05/10/18, 07:17:08 PM", "The Hive $partitions tables should be queried directly instead.", "NaN"], ["10525", "Completely remove SHOW PARTITIONS", "David Phillips", "electrum", "08/20/18, 03:25:29 PM", "This should be merged at a later date, after we no longer need the\ndeprecation hint for users that still try to use the old syntax.", "NaN"], ["10527", "Avoid repeated construction of MethodHandles during map deserialization", "Haozhun Jin", "haozhun", "05/01/18, 01:17:53 AM", "NaN", "NaN"], ["10528", "Improve error message for certain Hive delete queries", "Haozhun Jin", "haozhun", "05/01/18, 01:14:47 AM", "When an auto-commit Hive delete query fails, the error message previously\r\nmentioned transaction, which could be confusing to the users.\r\nThis commit simplifies the error message in the most common use case.", "NaN"], ["10531", "Set expected rows when creating VALUES pages", "Dain Sundstrom", "dain", "05/09/18, 06:34:36 PM", "NaN", "NaN"], ["10533", "Set mapred.min.split.size property for skip.header.line.count", "Haozhun Jin", "haozhun", "05/01/18, 12:34:20 AM", "`mapred.min.split.size` is the legacy name of `mapreduce.input.fileinputformat.split.minsize`.\r\n\r\ncc @kokosing ", "NaN"], ["10534", "Ignore validation of sum for ORC integer stats if it is null", "James Sun", "highker", "05/01/18, 02:31:11 PM", "NaN", "NaN"], ["10537", "move memory connector warnings to a limitations sections", "Matt Fuller", "mattsfuller", "05/16/18, 12:25:43 PM", "Create a limitations sections to make the memory connector documentation more readable.", "NaN"], ["10538", "Revert \"Update to aircompressor 0.10\"", "Raghav Sethi", "raghavsethi", "05/02/18, 05:36:42 PM", "This reverts commit 50d13df01d20ed1f4a8a9131c09d1301e9ac963b.\r\n\r\nReverting because this version appears to cause segfaults in test runs.", "NaN"], ["10539", "Revert \"Add support for column-level access control\"", "Raghav Sethi", "raghavsethi", "05/02/18, 08:57:11 PM", "This reverts commit 92142545c5a8e9be88d0b8ee3dff320b1a00d389.\r\n\r\nReverting because the original commit caused integration tests to fail.", "NaN"], ["10540", "Fix line_locate_point function", "Maria Basmanova", "mbasmanova", "05/08/18, 10:15:35 PM", "line_locate_point function is supposed to return a float between 0 and 1 representing the location of the closest point on the LineString to the given Point, as a fraction of total 2d line length. Instead, this function was returning the length from the start of the LineString to the closest point. ", "NaN"], ["10541", "Update to Drift 1.9", "David Phillips", "electrum", "05/03/18, 02:06:44 AM", "NaN", "NaN"], ["10545", "Add support for Parquet datasource stats", "Piyush Narang", "piyushnarang", "05/08/18, 10:19:59 PM", "Related issue: https://github.com/prestodb/presto/issues/10484\r\nThis change adds support to keep track of file format stats for Parquet's new Parquet reader as well as some limited stats while using the old reader (just the stats to read metadata related to predicates). Hooking up the full stats for the old reader isn't straightforward as the Parquet library doesn't seem to expose hooks where we can set up and update the `FileFormatDataSourceStats` object. ", "NaN"], ["10546", "Fix ST_IsValid and geometry_invalid_reason functions", "Maria Basmanova", "mbasmanova", "05/09/18, 03:06:05 PM", "Re-implement ST_IsValid and geometry_invalid_reason functions using ESRI library.\r\n\r\nThe original implementation was failing with \"shell is null but hole found\" error for some self-intersecting geometries when deserializing geometry Slice into JTS Geometry object. A polygon consists of an exterior shell and zero or more interior holes. A multi-polygon can have multiple shells, each with zero or more holes. Slice representation of a geometry contains a list of exterior and interior rings, but doesn't identify exterior rings explicitly. It is only known that points of exterior rings are listed in clockwise order, while points of interior rings in counterclockwise order. In some cases, e.g. hourglass shaped polygon `POLYGON ((0 0, 1 1, 0 1, 1 0, 0 0))`, it is not possible to determine whether a set of points is listed in clockwise or counterclockwise order. In these cases, deserialization to JTS geometry object fails.\r\n\r\nThis PR updates ST_IsValid and geometry_invalid_reason functions to use the functionality from the ESRI library and avoids deserialization to JTS Geometry object.\r\n\r\nFixes #10544", "NaN"], ["10548", "Limit grouping elements", "Elon Azoulay", "elonazoulay", "05/12/18, 06:20:17 AM", "Resolves #10547 ", "NaN"], ["10551", "Fix distinct from operator for map", "Haozhun Jin", "haozhun", "05/04/18, 05:31:40 PM", "All implementations must be implemented with null flag convention due to deficiencies in Presto engine implementation.\r\n\r\ncc @elonazoulay ", "NaN"], ["10552", "Add distinct from operator for BingTile type", "Haozhun Jin", "haozhun", "05/04/18, 06:15:35 PM", "Types with \"equal\" operator should also have \"distinct from\" operator,\r\njust like they do for \"not equal\" operator.", "NaN"], ["10553", "Add column properties to SQL parser", "Martin Weindel", "MartinWeindel", "07/24/18, 08:55:08 AM", "The Kudu connector needs to specify additional properties for columns on creating tables (or altering a table by adding a new column).\r\nFollowing the existing syntax for table properties, this PR adds column properties to the parser and all related infrastructure to manage them similar to table properties.\r\nFormatting SQL code for `SHOW CREATE TABLE` has also been updated accordingly.\r\n\r\nExample for new column properties syntax from the Kudu connector:\r\n```sql\r\nCREATE TABLE kudu.default.sample (\r\n  serialno varchar WITH ( primary_key = true )\r\n  col1 varchar WITH ( encoding = 'dictionary', compression = 'LZ4' ),  \r\n  col2 varchar WITH ( nullable = true )\r\n);\r\n```\r\nThe concrete column properties are defined in the connectors.", "NaN"], ["10554", "Optimize array equals for bigint/double types", "Nezih Yigitbasi", "nezihyigitbasi", "05/10/18, 11:19:11 PM", "Using `ArrayComparisonBenchmark`, before:\r\n```\r\narray_equals ::  110.131 cpu ms ::    0B peak memory :: in   15K,      0B,    136K/s,      0B/s :: out     1,      9B,       9/s,     81B/s\r\n```\r\nAfter:\r\n```\r\narray_equals ::   99.742 cpu ms ::    0B peak memory :: in   15K,      0B,    150K/s,      0B/s :: out     1,      9B,      10/s,     90B/s\r\n```\r\n\r\nResults in roughly 10% better performance -- decrease in CPU time (from 110.131 cpu ms to 99.742 cpu ms) and increase in throughput (from 136K rows/s to 150K rows/s).\r\n\r\n\r", "NaN"], ["10555", "Minor refactor for ClusterMemoryManager", "Nezih Yigitbasi", "nezihyigitbasi", "05/08/18, 10:19:38 PM", "NaN", "NaN"], ["10556", "Update deprecated use of Futures.addCallback", "Nezih Yigitbasi", "nezihyigitbasi", "05/08/18, 10:19:28 PM", "NaN", "NaN"], ["10557", "Return true in canCoerce for row types with field name mismatches ", "Haozhun Jin", "haozhun", "05/10/18, 07:36:53 PM", "NaN", "NaN"], ["10558", "Update to aircompressor 0.11", "Martin Traverso", "martint", "05/09/18, 05:40:24 PM", "Improves zstd decompression performance for some datasets.", "NaN"], ["10560", "Add ORC write validation low memory mode", "Dain Sundstrom", "dain", "05/21/18, 08:00:50 PM", "In low memory mode, row group statistics are not full tracked and instead\nonly a checksum is used.\n\nRowGroupStatistics should alway have an exact match in the verfiication\nstep because they are either directly loaded from metadata or calculated\nover the same rows in the same order.", "NaN"], ["10564", "Test CTAS for Cassandra", "Piotr Findeisen", "findepi", "05/09/18, 06:50:07 AM", "NaN", "NaN"], ["10568", "Add IGNORE NULLS clause to various Window functions", "Michael Styles", "ptkool", "12/03/19, 10:20:11 PM", "See https://github.com/prestodb/presto/issues/4554.\r\n\r\nThis PR adds the IGNORE/RESPECT NULLS clause to window functions LAG, LEAD, FIRST_VALUE, LAST_VALUE, and NTH_VALUE.", "NaN"], ["10569", "Refactor Hive write path, target path and write mode decision", "Wenlei Xie", "wenleix", "05/11/18, 09:38:27 PM", "NaN", "NaN"], ["10572", "Fixes to scheduling for bucketed execution", "Haozhun Jin", "haozhun", "05/11/18, 06:44:19 PM", "NaN", "NaN"], ["10575", "Bind /ui as a static resource instead of /", "Nezih Yigitbasi", "nezihyigitbasi", "05/08/18, 10:18:16 PM", "Previously, the path / was bound as a static resource, which was causing\r\nall HTTP requests (whether to a static or a non-static resource) to go\r\nthrough the io.airlift.http.server.ClassPathResourceHandler. This handler\r\nupon receiving a request looks up that path through its class loader\r\nusing the getClassLoader().getResource() method. In Java 9 the\r\nBuiltinClassLoader caches these lookups eventually resulting in a huge\r\ncache, which may cause GC issues (see BuiltinClassLoader::findMiscResource\r\nand the resourceCache field).\r\n\r\nThis change binds only the /ui path as a static resource. Therefore,\r\nrequests to other paths will not go through the ClassPathResourceHandler.\r", "NaN"], ["10577", "Add the ST_ConvexHull function", null, "tpeng-fb", "05/14/18, 04:45:25 PM", "Add `ST_ConvexHull(geometry)` function to return the minimum convex geometry that encloses all points in the input geometry.\r\n\r\nDue to a bug in `OGCGeometry#convexHull` API, https://github.com/Esri/geometry-api-java/issues/172, current implementation doesn't support geometry collections.", "NaN"], ["10579", "Add support for column-level access control", "Rebecca Schlussel", "rschlussel", "05/09/18, 06:50:06 PM", "No need for re-review.  This is exactly the same as before (approved at #10375), just rebased on master.  ", "NaN"], ["10581", "Add release notes for 0.201", "Raghav Sethi", "raghavsethi", "05/09/18, 09:39:17 PM", "NaN", "NaN"], ["10582", "Limit the size of local buffers for stream readers", "Ying", "yingsu00", "07/02/18, 10:25:16 PM", "In the past we have seen large number of humongous allocations on the\r\ncluster worker nodes. The size of each allocated buffer could be as high\r\nas 80MB, and the total allocation could be as high as 100GB per worker\r\nnode. This situation happens when reading map or array types. The reason\r\nof such high memory consumption is two folds:\r\n\r\n1) When we read the nested types like map and array, we didn't take the\r\nnumber of elements within each cell into account when caculating the\r\nbatch size. A single map cell could contain 20K entries in some cases.\r\nCurrently we read the first block and bound the batch size accordingly,\r\nbut the first block might already contain too many (e.g. millions of)\r\nentries.\r\n\r\n2) The number of stream readers Presto creates for each task involving\r\nscan could be a lot. It is the product of the following numbers:\r\n\r\n    a) The number of drivers that was assigned with a split to read.\r\n    b) The number of columns that need to be scanned.\r\n    c) The number of streams for each column. This depend on the column\r\n       type, e.g. the MapStreamReader contains keyStreamReader and\r\n       valueStreamReader.\r\n\r\n    This could be up to hundreds stream readers. Each stream reader hold\r\na few of local buffers depending on the underlying streams. These local\r\nbuffers were of the same size of the batch size, so if we mis-estimated\r\nthe batch size to be some large number, these local buffers would be\r\nhumongous too.\r\n\r\nThe fix is 3 steps:\r\n\r\n1) In each stream reader, we limit the size of the local buffer to 1K\r\nrows at maximum no matter how large the batch size is. The blocks\r\nbeing built would still be humongous but we can reduce the local buffer\r\nsizes.\r\n\r\n2) Read the stats for map and array columns, and use it as a heuristic\r\nto determine the batch size for stream readers. Currently we only did\r\nthis for string type but not map and array types.\r\n\r\n3) If the stats is not available, we could try to start from very few\r\nrows to determine how many rows to read for the first block.\r\n\r\nThis commit is only for step 1).", "NaN"], ["10585", "Use getMetastoreClient method instead of using it directly", "Andrii Rosa", "arhimondr", "05/09/18, 10:46:58 PM", "Some TestHiveClient implementations do override this method", "NaN"], ["10589", "Raptor assignment limiter fixes", "David Phillips", "electrum", "05/10/18, 04:56:00 AM", "NaN", "NaN"], ["10593", "Require more data only if source is unfinished", "Karol Sobczak", "sopel39", "05/10/18, 09:39:09 AM", "Fixes: https://github.com/prestodb/presto/issues/10587", "NaN"], ["10596", "Fix incorrect pushdown of aggregation through outer join", "Martin Traverso", "martint", "05/18/18, 05:26:42 PM", "PushAggregationThroughOuterJoin was incorrectly inferring that\r\na plan of the following shape produces distinct outputs on the\r\nouter branch of the join:\r\n\r\n- Aggregate\r\n        group by x\r\n   - LeftJoin\r\n       - Project x = x\r\n           - Aggregate\r\n                group by x, y\r\n       - ...\r\n\r\nIt was only testing whether the projection had identities, but it wasn't\r\nensuring that all inputs to the projection were present in the output.\r\nIn the example above, given:\r\n\r\n(1, 10)\r\n(1, 20)\r\n(1, 20)\r\n\r\nthe pairs that come out the the aggregation will be unique, but the values\r\nof each individual column will not. This breaks the assumption made by\r\nthat optimizer.\r\n\r\nWe fix the issue by removing projections from consideration. If there\r\nis a no-op identity projection, it will be taken care of by\r\nRemoveRedundantIdentityProjections.\r\n\r\nFixes #10592", "NaN"], ["10598", "Remove legacy DesugaringOptimizer", "Martin Traverso", "martint", "05/18/18, 04:42:20 PM", "This has been replaced with new-style rules.", "NaN"], ["10600", "Improve error message for anonymous row type in Hive", "Haozhun Jin", "haozhun", "05/10/18, 08:03:31 PM", "Supersedes #10599 ", "NaN"], ["10602", "Nested loop join in the context of grouped execution", "Haozhun Jin", "haozhun", "05/23/18, 06:45:08 PM", "This PR generalizes the bridge mechanism for lookup join (between probe and build), and applies it to nested loop join.", "NaN"], ["10603", "Make client query submission idempotent", "Dain Sundstrom", "dain", "05/24/18, 10:59:00 PM", "When a query posted via the StatementResource, we delay the actual submission of the query to the QueryManager until the first get data request from the client.  This means that the client has received the query ID and has started the protocol.\n\nAdditionally, the query submission is moved to a background thread so the client is not blocked during query parsing and analysis.", "NaN"], ["10604", "Destroy exchange sources for finished tasks on the coordinator", "Dain Sundstrom", "dain", "07/10/18, 10:55:15 PM", "A task can sometimes finish before the sources of the task are even scheduled, so\nthere is no active system that is resposible for destroying the finished task's\noutput buffers on the source task.  This patch changes the coordinator to attempt\ndestruction of all exchange sources when a task finishes and any future exchange\nsources added during scheduling.", "NaN"], ["10605", "Support Overwriting Partition for Hive Connector", "Wenlei Xie", "wenleix", "06/05/18, 05:51:12 AM", "Revise according to comments in progress :) ", "NaN"], ["10606", "Fix query failure when a Hive bucket has no splits", "Haozhun Jin", "haozhun", "05/15/18, 12:52:07 AM", "NaN", "NaN"], ["10607", "Disallow duplicate keys to map constructor", "Wenlei Xie", "wenleix", "05/16/18, 09:26:27 PM", "Currently map constructor does not check duplicate keys and will\r\nreturn an illegal MapBlock on input with duplicate keys.", "NaN"], ["10608", "Support Hive bucketed sorted tables", "David Phillips", "electrum", "05/22/18, 12:27:13 AM", "NaN", "NaN"], ["10610", "Implement missing operators", "Elon Azoulay", "elonazoulay", "05/15/18, 06:01:00 PM", "Verify all operator types are implemented\r\n\r\nVerify that operators for comparable types implement the following:\r\n* EQUALS\r\n* NOT_EQUALS\r\n* IS DISTINCT FROM\r\n\r\nVerify that operators for orderable types implement the following:\r\n* GREATER_THAN\r\n* GREATER_THAN_OR_EQUAL\r\n* LESS_THAN\r\n* LESS_THAN_OR_EQUAL\r\n\r\nResolves #10591 ", "NaN"], ["10613", "Fix flaky scale writers test", "David Phillips", "electrum", "05/17/18, 10:40:40 PM", "NaN", "NaN"], ["10619", "Increase default page size minimum for merging pages", "Karol Sobczak", "sopel39", "05/16/18, 02:25:34 PM", "Pages are merged when both they don't have enough rows\r\nor their size is too small. Default page size minumum for\r\nmerging pages is 25Kb. This mimumum is fine for short rows\r\nas 25Kb pages usually contain many rows (more than minumum\r\n250). However for large rows 25Kb minumum is met when\r\npage contains only few rows. This causes Presto performance\r\ndegradation and memory overhead of internal Presto\r\nstructures (e.g: PagesIndex). Incrising minumum page size\r\nlimit to 500Kb should eliminate the problem of pages with\r\nsize around 25Kb but only 20-30 rows (average rows size\r\nof 1000).", "NaN"], ["10622", "Make PageSorter and PageIndexer supported interfaces", "David Phillips", "electrum", "05/16/18, 10:01:21 PM", "NaN", "NaN"], ["10626", "Fix ST_NumPoints function for geometry collections", "Maria Basmanova", "mbasmanova", "05/21/18, 03:29:12 PM", "Fix NPE in the ST_NumPoints function when input is a geometry collection.\r\n\r\nFixes #10625", "NaN"], ["10627", "Fix geometry_to_bing_tiles function for geometry collections", "Maria Basmanova", "mbasmanova", "05/18/18, 08:04:09 PM", "Fix NPE in geometry_to_bing_tiles function when input is a geometry collection.\r\n\r\nFixes #10623", "NaN"], ["10632", "Make TestIpAddressType a subclass of AbstractTestType", "Elon Azoulay", "elonazoulay", "06/01/18, 07:19:54 PM", "Resolves #10631 ", "NaN"], ["10635", "Add peak memory usage to presto-cli output", "Cole Bowden", "bowdencm", "05/21/18, 05:47:07 PM", "Tested locally, terminal output was as follows:\r\n```\r\nSplits: 100 total, 0 done (0.00%)\r\nCPU Time: 270.9s total,  157K rows/s, 6.47MB/s, 34% active\r\nPer Node: 1.3 parallelism,  208K rows/s,  8.6MB/s\r\nParallelism: 5.3\r\nPeak Memory: 1.97GB\r\n0:51 [42.4M rows, 1.71GB] [833K rows/s, 34.4MB/s]", "NaN"], ["10637", "Respect X-Forwarded-Proto header in protocol responses", "David Phillips", "electrum", "05/17/18, 11:07:41 PM", "Squashed version of #10402 to run Travis tests", "NaN"], ["10638", "Add memory aware query execution", "Raghav Sethi", "raghavsethi", "06/08/18, 06:37:13 AM", "If peak memory estimates are provided and the\r\nexperimental.memory-aware-execution flag is enabled, the coordinator\r\nwill not schedule a query for execution unless it can 'pre-allocate'\r\nenough memory for the query to complete. This is currently designed\r\nspecifically for the scenario when there are a small number of\r\nextremely memory-intensive queries and the goal is to not have them\r\ntrip over each other.", "NaN"], ["10639", "Fixes for web UI", "David Phillips", "electrum", "05/18/18, 01:05:59 AM", "NaN", "NaN"], ["10640", "Fix RowTypeSignature parsing", "Leiqing Cai", "caithagoras", "06/20/18, 12:21:38 AM", "NaN", "NaN"], ["10641", "Support multiple ranges in ShardPredicate", "Jiexi Lin", "jessesleeping", "05/25/18, 05:36:50 PM", "Construct a SQL filter clause for column domain that has multiple ranges. For a domain of `col1[[1, 3], [4, 6]], col2[[2, 5], [7, 8]]` ,the constructed predicates should look like:\r\n```\r\n...\r\n(\r\n    (c1_max >= ? OR c1_max IS NULL) AND (c1_min <= ? OR c1_min IS NULL) \r\n    OR \r\n    (c1_max >= ? OR c1_max IS NULL) AND (c1_min <= ? OR c1_min IS NULL)\r\n) \r\nAND \r\n(\r\n    (bucket_number >= ? OR bucket_number IS NULL) AND (bucket_number <= ? OR bucket_number IS NULL) \r\n    OR \r\n    (bucket_number >= ? OR bucket_number IS NULL) AND (bucket_number <= ? OR bucket_number IS NULL)\r\n)\r\n```", "NaN"], ["10642", " Fix ORC Writer for TIMESTAMP before 1970-01-01", "Wenlei Xie", "wenleix", "05/19/18, 07:41:07 PM", "NaN", "NaN"], ["10645", "Fix formatting and remove unused static variable", "Karol Sobczak", "sopel39", "05/18/18, 11:08:36 AM", "NaN", "NaN"], ["10646", "Remove unused catalogs loaded variable", "Karol Sobczak", "sopel39", "05/18/18, 11:08:14 AM", "NaN", "NaN"], ["10647", "Add total number of partitions to TaskUpdateRequest", "Maria Basmanova", "mbasmanova", "06/01/18, 07:22:27 PM", "Extend TaskUpdateRequest to include total number of partitions.\r\n\r\nIn the distributed spatial join, spatial partitioning duplicates input rows on probe and build sides into multiple partitions when a geometry's bounding box intersects multiple partitions. Therefore, each join task needs to know which spatial partition it is processing to perform inline de-deduplication to\r\nprevent duplicate join results.\r\n\r\nFurther more, in the distributed spatial join the input rows are partitioned using 2-tier partitioning scheme: \r\n\r\n(1) spatial partitioning; \r\n\r\n(2) hash partitioning on spatial partition ID.\r\n\r\nTask ID provides the final partition number, e.g. the result of applying hash function to the spatial partition ID: task_id = hash(spatial_partition_id). Total number of hash partitions allows join task to figure out the list of spatial partition IDs for a given task ID.\r\n\r\nSee #10454 for additional context.", "NaN"], ["10652", "Report whether server has finished starting in /v1/info", "Haozhun Jin", "haozhun", "05/18/18, 10:38:02 PM", "cc @sopel39 ", "NaN"], ["10653", "Allow nulls to be appended in zip_with", "Leiqing Cai", "caithagoras", "06/02/18, 02:01:44 AM", "This simplifies the query when zip arrays with different lengths.\r\n\r\nIn the current syntax, there is no way to distinguish between an appended `NULL` and an actual `NULL` in the lambda function. If this is required in future use cases, we'll be able to support it by implementing a `zip_with` that takes in a ternary lambda expression.", "NaN"], ["10655", "Add tiny stripe threshold config and session property", "Haozhun Jin", "haozhun", "05/21/18, 07:26:16 PM", "NaN", "NaN"], ["10656", "Change multimap_agg to be order sensitive", "Dain Sundstrom", "dain", "05/19/18, 04:22:29 AM", "NaN", "NaN"], ["10658", "Add multimap_from_entries function", "Wenlei Xie", "wenleix", "05/22/18, 08:44:53 PM", "NaN", "NaN"], ["10659", "Add statistics rules for aggregation and semi-join", "Piotr Findeisen", "findepi", "06/04/18, 09:09:52 AM", "NaN", "NaN"], ["10660", "Remove extra Hive S3 test configuration", "Grzegorz Kokosi\u0144ski", "kokosing", "05/22/18, 04:35:10 AM", "Remove extra Hive S3 test configuration", "NaN"], ["10662", " Run different hive tests in separate travis branches", "Andrii Rosa", "arhimondr", "05/23/18, 03:53:14 PM", "Travis runs a bash block without the '-e' flag. Any failures before the latest\r\ncommand are ignored. That may result in false positive test runs.", "NaN"], ["10663", "Disallow creating Hive tables with unsupported partition types", "Cole Bowden", "bowdencm", "06/05/18, 03:15:23 PM", "NaN", "NaN"], ["10665", "Change type of currentSize to long", "Nezih Yigitbasi", "nezihyigitbasi", "05/21/18, 09:43:02 PM", "This change is for preventing potential overflow.", "NaN"], ["10666", "Remove support for legacy ORDER BY semantics", "Martin Traverso", "martint", "05/23/18, 04:50:59 PM", "This behavior has been disabled by default for more than a year.\r\nIt is time to remove it.", "NaN"], ["10667", "Remove redundant method", "David Phillips", "electrum", "05/22/18, 06:47:25 PM", "NaN", "NaN"], ["10668", "Use USE_NULL_FLAG in RowDistinctFromOperator", "Elon Azoulay", "elonazoulay", "05/22/18, 06:05:35 PM", "RowDistinctFromOperator should conform to the calling convention\r\nfor IS DISTINCT FROM.\r\n\r\nResolves #10664 ", "NaN"], ["10670", "Fix rpm dependency", "Piotr Findeisen", "findepi", "05/23/18, 06:47:47 AM", "The rpm install script uses `uuidgen` to generate `node.id`.", "NaN"], ["10672", "Reopening #10648 to merge it", "Rebecca Schlussel", "rschlussel", "05/22/18, 03:19:13 PM", "I accidentally didn't merge it before, so merging now.  ", "NaN"], ["10673", "Add basic JDBC driver support for prepared statements", "David Phillips", "electrum", "06/16/18, 06:34:54 PM", "NaN", "NaN"], ["10675", "Remove unnecessary use of Visitor", "Martin Traverso", "martint", "05/23/18, 04:50:06 PM", "It was a generic implementation for any node (visitPlan) that did not\r\neven walk the plan tree.\r\n\r\ncc: @findepi ", "NaN"], ["10676", "Add 0.202 release notes", "Dain Sundstrom", "dain", "05/23/18, 01:26:17 AM", "NaN", "NaN"], ["10681", "Improve support for AssignUniqueId in PropertyDerivations", "Karol Sobczak", "sopel39", "05/31/18, 08:55:13 AM", "NaN", "NaN"], ["10683", "Check also hive metastore port when checking hadoop", "Grzegorz Kokosi\u0144ski", "kokosing", "05/23/18, 12:24:29 PM", "Check also hive metastore port when checking hadoop\n\nThis was applied to product test in ffa7c2a5c81, now it also used for\nhive tests.", "NaN"], ["10685", "Fix unguarded access to a field", "Piotr Findeisen", "findepi", "05/31/18, 07:39:13 PM", "NaN", "NaN"], ["10686", "Hive column statistics small refactorings", "Andrii Rosa", "arhimondr", "05/29/18, 02:21:27 PM", "These refactorings are the pre-requisite for the #10456 \r\n\r\nAlong with tiny trivial refactorings there are few bigger ones:\r\n\r\n- Do not store NDV for Boolean in HiveColumnStatistics. Since there is no NDV statistic stored in the Metastore for the `boolean` type, we compute NDV based on (`false`, `true` and `null`) counts statistics. `HiveColumnStatistics` is rather a simple storage class. So, it must hold the exact representation of the statistics stored in the Metastore. The NDV computation must be done in the `MetastoreHiveStatisticsProvider.java`, where all the other translations take place. As part of this refactoring the way we process null is changed, to be consistent with the rest of the statistics (see #10674). \r\n- Remove optional from the Metastore statistics interfaces. `Optional.empty()` and `Optional.of(ImmutableMap.of())` is basically the same. It introduces a lot of confusion of when to return an empty map, and when to return an empty optional.\r\n- Remove column granularity from the `getStatistics*` metastore methods. Currently we never query statistics for a subset of columns. It is not supported by the `ConnectorMetadata` interface. So this code just complicates the interfaces, and increases the amount of memory used by the statistics cache.", "NaN"], ["10687", "Remove time-based expiration of page function bytecode", "David Phillips", "electrum", "05/23/18, 09:55:02 PM", "The JVM bug can affect newly generated code, so expiring it\nbased on time does not help.", "NaN"], ["10688", "Run presto-main tests as a separate travis job", "Andrii Rosa", "arhimondr", "05/23/18, 07:38:25 PM", "The TEST_OTHER_MODULES branch takes almost an hour, being a bottleneck for the\r\nentire build. All the other builds take 30 - 40 minutes.\r\nSince tests for the presto-main module take ~20 minutes it is a good candidate for\r\nextraction.", "NaN"], ["10690", "Remove LocalMemoryContext::transferMemory method", "Nezih Yigitbasi", "nezihyigitbasi", "05/23/18, 09:30:56 PM", "This method was only used by the nested loop join operator. Now that\r\nnested loop join operator doesn't depend on this method it can be\r\nremoved.", "NaN"], ["10691", "Fix IntelliJ warnings in SqlQueryScheduler", "Dain Sundstrom", "dain", "05/24/18, 05:59:59 PM", "NaN", "NaN"], ["10692", "Collect statistics for AddExchanges", "Grzegorz Kokosi\u0144ski", "kokosing", "05/31/18, 07:29:50 PM", "Collect statistics for AddExchanges\r", "NaN"], ["10695", "Remove hardcoded query max memory in TestingPrestoServer", "Nezih Yigitbasi", "nezihyigitbasi", "05/24/18, 03:14:31 AM", "With the removal of the sytem pool and the introduction of the new query\r\ntotal memory limit, this hardcoded value may cause failures depending\r\non the JVM heap size used for testing.\r\n\r\nThis change was introduced in https://github.com/prestodb/presto/commit/cc65c4b626d311bced1344172091fc4a539827fd. I have run the tests in another test PR (https://github.com/prestodb/presto/pull/10693, [build](https://travis-ci.org/prestodb/presto/builds/382920031)) and they all look good, so it seems that we don't need this hardcoded config anymore. ", "NaN"], ["10697", "Fix sending challenge for multiple authenticators", "David Phillips", "electrum", "05/24/18, 05:28:03 AM", "When using multiple authentication methods such as PASSWORD and JWT,\nthe WWW-Authenticate challenge was only sent for the last method\nrather than all of them. This breaks authentication for clients\nthat do not pre-authenticate requests.", "NaN"], ["10698", "Add support for limit column pruning + fix failing correlate subquery", "Karol Sobczak", "sopel39", "05/24/18, 06:03:23 PM", "Fixes: https://github.com/prestodb/presto/issues/10696", "NaN"], ["10701", "Correct offset calculation in MapBlockBuilder.buildHashTableStrict", "Wenlei Xie", "wenleix", "05/24/18, 05:58:33 PM", "NaN", "NaN"], ["10702", "Fix quoting in error message for SHOW PARTITIONS", "David Phillips", "electrum", "05/24/18, 06:23:41 PM", "NaN", "NaN"], ["10703", "Only check column access once", "Rebecca Schlussel", "rschlussel", "05/24/18, 09:19:08 PM", "Only check column access permissions when we're in the outer query scope\r\nso that the check happens only once for all the tables/columns in the\r\nquery.", "NaN"], ["10704", "Update to Drift 1.12", "David Phillips", "electrum", "05/24/18, 11:56:59 PM", "NaN", "NaN"], ["10705", "Delegate output operator blocking to output buffer", "Dain Sundstrom", "dain", "05/25/18, 01:25:37 AM", "Previously, output operator maintained a private blocked state which, which was\nsynchronized to the output buffer only when flushing data.  This patch removes\nthat private blocked state simply delegates blocked state to the output buffer.", "NaN"], ["10706", "Upgrade to Airlift 0.169", "Dain Sundstrom", "dain", "05/25/18, 06:18:34 AM", "NaN", "NaN"], ["10708", "Define all parameters in single place", "Grzegorz Kokosi\u0144ski", "kokosing", "05/28/18, 04:45:34 AM", "Define all parameters in single place\n\nIt is easier when parameters are set in single place.\nAlso pom.xml is not the best place to set default values for parameters,\nbecause test can called from IDE and then defaults are not set. When\nparameters are set in script then it is easier to manually copy them to IDE.", "NaN"], ["10709", "Represent all-nulls with RLE block", "Praveen Krishna", "Praveen2112", "06/15/18, 12:49:44 AM", "We have created a NullValueBlock which effectively represents a block in which **all the entries are null**. If we have a LongArrayBlock of size 7168 in which all the entries are null, we have a long array of same size which we don't require.  When such type of blocks are replaced with a NullValueBlock (which maintains only the positionCount) we can reduce the memory consumed by those blocks. Serialization/DeSerialization of the NullValueblocks are also simple as we use only the positionCount. In future we can also add tweak the project and filter operators when working on the NullValueBlocks(Similar to RunLengthEncodedBlock and DictionaryBlock).", "NaN"], ["10710", "Do not use replicated join if probe side is naturally partitioned", "Karol Sobczak", "sopel39", "05/31/18, 01:31:43 PM", "NaN", "NaN"], ["10711", "Add support for assign unique id node and filter in our join stats", "Karol Sobczak", "sopel39", "06/18/18, 10:57:20 AM", "Q21 before:\r\n```\r\n- LeftJoin[(\"orderkey\" = \"orderkey_10\") AND (\"suppkey_12\" <> \"suppkey_0\")][$hashvalue_204, $hashv\r\n        Distribution: REPLICATED\r\n        Cost: {rows: 2450757425 (228.24GB), cpu: 2391276411339.29, memory: 255027000671.95, netwo\r\n    - Project[] => [orderkey:bigint, suppkey_0:bigint, commitdate:date, receiptdate:date, name:va\r\n            Cost: {rows: 296052124 (27.57GB), cpu: 1276596897180.48, memory: 15027412311.95, netw\r\n            $hashvalue_204 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orderkey\r\n        - AssignUniqueId => [orderkey:bigint, suppkey_0:bigint, commitdate:date, receiptdate:date\r\n                Cost: {rows: 296052124 (24.81GB), cpu: 1246991684800.55, memory: 15027412311.95,\r\n            - Project[] => [orderkey:bigint, suppkey_0:bigint, commitdate:date, receiptdate:date,\r\n                    Cost: {rows: 296052124 (22.06GB), cpu: 1244623267810.15, memory: 15027412311.\r\n                - InnerJoin[(\"orderkey\" = \"orderkey_3\")][$hashvalue_201, $hashvalue_202] => [orde\r\n                        Distribution: REPLICATED\r\n                        Cost: {rows: 296052124 (24.81GB), cpu: 1220939097906.21, memory: 15027412\r\n                        ...\r\n    - LocalExchange[HASH][$hashvalue_205] (\"orderkey_10\") => suppkey_12:bigint, orderkey_10:bigin\r\n            Cost: {rows: 5999989709 (223.52GB), cpu: 599998970900.00, memory: 0.00, network: 0.00\r\n        - ScanProject[table = hive:tpch_sf1000_orc:lineitem, originalConstraint = true] => [suppk\r\n                Cost: {rows: 5999989709 (111.76GB), cpu: 119999794180.00, memory: 0.00, network:\r\n                non_null_188 := true\r\n                $hashvalue_206 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orde\r\n                LAYOUT: tpch_sf1000_orc.lineitem\r\n                suppkey_12 := HiveColumnHandle{name=suppkey, hiveType=bigint, hiveColumnIndex=2,\r\n```\r\nafter:\r\n```\r\n- RightJoin[(\"orderkey_10\" = \"orderkey\") AND (\"suppkey_12\" <> \"suppkey_0\")][$hashvalue_195, $hash\r\n        Distribution: PARTITIONED\r\n        Cost: {rows: 2450757425 (228.24GB), cpu: 2181375456974.36, memory: 44879335499.45, networ\r\n    - ScanProject[table = hive:tpch_sf1000_orc:lineitem, originalConstraint = true] => [suppkey_1\r\n            Cost: {rows: 5999989709 (111.76GB), cpu: 119999794180.00, memory: 0.00, network: 0.00\r\n            non_null_188 := true\r\n            $hashvalue_195 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orderkey\r\n            LAYOUT: tpch_sf1000_orc.lineitem\r\n            suppkey_12 := HiveColumnHandle{name=suppkey, hiveType=bigint, hiveColumnIndex=2, colu\r\n            orderkey_10 := HiveColumnHandle{name=orderkey, hiveType=bigint, hiveColumnIndex=0, co\r\n    - LocalExchange[HASH][$hashvalue_196] (\"orderkey\") => orderkey:bigint, suppkey_0:bigint, comm\r\n            Cost: {rows: 296052124 (27.57GB), cpu: 1306695531175.55, memory: 15274123119.52, netw\r\n        - Project[] => [orderkey:bigint, suppkey_0:bigint, commitdate:date, receiptdate:date, nam\r\n                Cost: {rows: 296052124 (27.57GB), cpu: 1277090318795.61, memory: 15274123119.52,\r\n                $hashvalue_206 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$hash_code\"(\"orde\r\n            - AssignUniqueId => [orderkey:bigint, suppkey_0:bigint, commitdate:date, receiptdate:\r\n                    Cost: {rows: 296052124 (24.81GB), cpu: 1247485106415.68, memory: 15274123119.\r\n                - Project[] => [orderkey:bigint, suppkey_0:bigint, commitdate:date, receiptdate:d\r\n                        Cost: {rows: 296052124 (22.06GB), cpu: 1245116689425.29, memory: 15274123\r\n                    - InnerJoin[(\"orderkey\" = \"orderkey_3\")][$hashvalue_203, $hashvalue_204] => [\r\n                            Distribution: PARTITIONED\r\n                            Cost: {rows: 296052124 (24.81GB), cpu: 1221432519521.34, memory: 1527\r\n                        - Project[] => [orderkey:bigint, suppkey_0:bigint, commitdate:date, recei\r\n                                Cost: {rows: 296052124 (22.06GB), cpu: 1066103658475.45, memory:\r\n                                $hashvalue_203 := \"combine_hash\"(BIGINT '0', COALESCE(\"$operator$\r\n```\r\n\r\nBasically outer join sides were not flipped.", "NaN"], ["10712", "Correct rowInputRate to be count/s", "qqibrow", "qqibrow", "06/14/18, 03:37:42 PM", "The current unit for rowInputRate is actually count/ms. we should convert totalWallTime from ms to s to correct this. ", "NaN"], ["10713", "Fix LocalFileRecordCursor timestamp parser", "Nezih Yigitbasi", "nezihyigitbasi", "05/25/18, 10:45:15 PM", "If the JVM default timezone is UTC then HTTP request log timestamp\r\nwill not be parseable by the LocalFileRecordCursor (e.g.,\r\n2017-12-21T00:12:27Z and 2017-12-21T00:12:27.000Z).\r\n\r\nThis change uses the same formatter as in [HttpLogLayout](https://github.com/airlift/airlift/blob/8231839017159b70029dda31fababdc672348c55/http-server/src/main/java/io/airlift/http/server/HttpLogLayout.java#L13).\r\n\r\nFixes #9601.\r\n\r\nGiven the log (the last two timestamps are not parseable with the current code):\r\n```\r\n2018-05-24T23:59:59.006-07:00\t2401:db00:21:7095:face:0:2d:0\tGET\t/v1/statement/20180525_065921_13647_iwv5x/32\tnull\tnull\t200\t0\t54\t1002\tnull\r\n2017-12-21T00:12:27Z\t2401:db00:21:7095:face:0:2d:0\tGET\t/v1/statement/20180525_065921_13647_iwv5x/32\tnull\tnull\t200\t0\t54\t1002\tnull\r\n2017-12-21T00:12:27.000Z\t2401:db00:21:7095:face:0:2d:0\tGET\t/v1/statement/20180525_065921_13647_iwv5x/32\tnull\tnull\t200\t0\t54\t1002\tnull\r\n```\r\n\r\nWe get:\r\n\r\n```\r\n select * from localfile.logs.http_request_log;\r\n server_address |        timestamp        |        client_address         | method |                 request_uri                  | user | agent | response_code | request_size | res\r\n----------------+-------------------------+-------------------------------+--------+----------------------------------------------+------+-------+---------------+--------------+----\r\n 127.0.0.1:8080 | 2018-05-24 23:59:59.006 | 2401:db00:21:7095:face:0:2d:0 | GET    | /v1/statement/20180525_065921_13647_iwv5x/32 | NULL | NULL  |           200 |            0 |\r\n 127.0.0.1:8080 | 2017-12-21 00:12:27.000 | 2401:db00:21:7095:face:0:2d:0 | GET    | /v1/statement/20180525_065921_13647_iwv5x/32 | NULL | NULL  |           200 |            0 |\r\n 127.0.0.1:8080 | 2017-12-21 00:12:27.000 | 2401:db00:21:7095:face:0:2d:0 | GET    | /v1/statement/20180525_065921_13647_iwv5x/32 | NULL | NULL  |           200 |            0 |\r\n(3 rows)\r\n```", "NaN"], ["10714", "Update PlanPrinter to distinguish between inner and left spatial joins", "Maria Basmanova", "mbasmanova", "05/25/18, 08:47:19 PM", "Modified output of `EXPLAIN <query>` to distinguish between inner and left spatial joins. \r\n\r\n`SpatialInnerJoin[\"st_contains\"(\"st_geometryfromtext\", \"st_point\")] => [st_point:Geometry, st_geometryfromtext:Geometry]`\r\n\r\nvs.\r\n\r\n`SpatialLeftJoin[\"st_contains\"(\"st_geometryfromtext\", \"st_point\")] => [st_point:Geometry, st_geometryfromtext:Geometry]`", "NaN"], ["10715", "Fix execution failure for distinct with effective aggregation filter", "Haozhun Jin", "haozhun", "05/26/18, 06:11:40 AM", "NaN", "NaN"], ["10717", "Improve correlates EXISTS subqueries tests", "Karol Sobczak", "sopel39", "05/31/18, 08:54:45 AM", "Tests were lacking validation of aggregation location when correlated EXISTS is resolved.\r\n\r", "NaN"], ["10718", " Lower initial number of entries in PageBuilder constructor", "Wenlei Xie", "wenleix", "06/06/18, 07:08:24 PM", "PageBuilder constructor has no estimation about bytes per entry.\r\nWith the previous default initial expected entries set to 1024,\r\none could construct PageBuilder consumes too much memory when\r\nthe number of columns is huge, which is problematic.\r\n\r\nAs an concrete example, in PartitionedOutputOperator, the initial\r\nmemory holding by `pageBuilders` in constructor will be\r\n\r\n```\r\npartitionCount * columnCount * DEFAULT_INITIAL_EXPECTED_ENTRIES\r\n    * sum(columnExpectedBytesPerEntry)\r\n```\r\n\r\nWith hundreds of partitions and thousands of columns, a single\r\nPartitionedOutputOperator can hold multiple GBs memory.\r\n\r\nWith PageBuilder constructor only being used to create\r\n\"seed\" PageBuilder, a more conservative DEFAULT_INITIAL_EXPECTED_ENTRIES\r\ncan be used to avoid such issues. The penalty of resizing will only\r\naffect the first PageBuilder.", "NaN"], ["10719", "Update closedSlicesRetainedSize in ChunkedSliceOutput", "Wenlei Xie", "wenleix", "05/29/18, 06:14:54 PM", "NaN", "NaN"], ["10720", "Optimize cassandra in queries", "Abhishek", "aandis", "06/06/18, 09:55:06 AM", "Fixes #10700", "NaN"], ["10721", "Make equality inference deterministic", "Karol Sobczak", "sopel39", "05/31/18, 08:54:27 AM", "NaN", "NaN"], ["10722", "Fix S3 tests", "Grzegorz Kokosi\u0144ski", "kokosing", "05/31/18, 07:28:27 PM", "NaN", "NaN"], ["10723", "Merge two classes testing plan determinism", "Piotr Findeisen", "findepi", "05/31/18, 07:38:49 PM", "NaN", "NaN"], ["10726", "NPE check for parameters field in Glue db, table, partition objects", "Rentao", "rentaow", "05/31/18, 02:33:33 PM", "NaN", "NaN"], ["10727", "Improve error message for Hive max open writers", "David Phillips", "electrum", "05/29/18, 08:43:37 PM", "NaN", "NaN"], ["10728", "Support LazyBlock in ColumnarArray/Map/Row", "Wenlei Xie", "wenleix", "05/29/18, 11:25:11 PM", "LazyBlock will be passed to the OrcWriter if the TableWriterOperator\r\nis directly next to the TableScanOperator.\r\n\r\nIn the future, we should make operators not producing LazyBlocks\r\nby explicitly unwrap them.", "NaN"], ["10729", "Check column access for view identities", "Rebecca Schlussel", "rschlussel", "06/19/18, 02:29:30 AM", "Check column access for view identities.  The check was being skipped\r\nwhen we moved column access checks to the Analyzer. With this change,\r\ncreateViewWithSelectFromColumns can replace\r\ncreateViewWithSelectFromTable/View", "NaN"], ["10730", "Add 0.203 release notes", "David Phillips", "electrum", "05/30/18, 09:59:13 PM", "NaN", "NaN"], ["10731", "Use streaming aggregation for a correlated scalar subquery", "Maria Basmanova", "mbasmanova", "06/20/18, 08:33:36 PM", "Optimize aggregation in a correlated scalar subquery to remove unnecessary local exchange and partial aggregation and enable streaming mode.\r\n\r\nCorrelated subquery like\r\n\r\n`select name, (select max(name) from region where regionkey = nation.regionkey) from nation;`\r\n\r\nis rewritten to a left join with an aggregation over synthetic row ID column generated by AssignUniqueId node:\r\n\r\n```\r\n- Aggregation (unique, probe.*)\r\n  - LeftJoin\r\n     - AssignUniqueId (unique)\r\n        - probe\r\n     - build\r\n```\r\n\r\nSince \"unique\" column is unique, the aggregation input is partitioned on grouping keys and therefore aggregation can be executed in a single step without any exchanges. Furthermore, since the output of the join is grouped on \"unique\" column, the aggregation doesn't need to accumulate all of the input in memory before emitting results. It can generate results every time the value of the \"unique\" column changes. \r\n\r\nThis PR implements streaming support in `HashAggregationOperator`, updates `StreamPropertyDerivations` and `PropertyDerivations` to set `paritioned_on(unique)` and `grouped(unique)` properties for the output of the `AssignUniqueId` node, and uses these properties in `AddLocalExchanges` to enable streaming aggregation.\r\n\r\nFixes #8171 ", "NaN"], ["10732", "Report physical written bytes for Parquet writer", "David Phillips", "electrum", "08/04/18, 07:12:09 PM", "This allows using writer scaling with Parquet.", "NaN"], ["10734", "Enable optimized Parquet reader and predicate pushdown", "David Phillips", "electrum", "05/31/18, 06:49:28 PM", "NaN", "NaN"], ["10737", "Parquet Cleanup", "Zhenxiao Luo", "zhenxiao", "10/15/18, 04:43:46 PM", "@dain @electrum @nezihyigitbasi now New Parquet Reader is production ready\r\nthis patch is to remove the old Parquet reader, and a cleanup of Parquet code", "NaN"], ["10744", "Remove workaround for ANTLR bug", "Martin Traverso", "martint", "06/02/18, 12:40:59 AM", "Issue https://github.com/antlr/antlr4/issues/781 has already been fixed, so\r\nthe workaround of extracting the pattern into a separate rule is no longer\r\nneeded.", "NaN"], ["10746", "Add missing space after comma in TestVarbinaryFunction", "Haozhun Jin", "haozhun", "06/02/18, 02:43:19 AM", "Fixes checkstyle failure", "NaN"], ["10747", "Fix build error in presto-docs due to lpad/rpad collision", "Haozhun Jin", "haozhun", "06/02/18, 06:01:46 AM", "lpad and rpad are functions for both string and binary.\r\nOne of them must be marked as noindex to avoid index collision.", "NaN"], ["10748", "Make ComposableStatsCalculator.Rule a generic type", "Maria Basmanova", "mbasmanova", "06/04/18, 08:36:39 PM", "NaN", "NaN"], ["10750", "Fix silent clamping in double to long cast", "Gerlou Shyy", "gerlou", "06/07/18, 05:41:52 PM", "Fixed silent clamping from double to long cast because the code did not check if the input was in the valid range of a long. Also added test cases for valid and invalid input. ", "NaN"], ["10754", "Add peak total memory to query details UI", "Nezih Yigitbasi", "nezihyigitbasi", "07/20/18, 02:53:55 AM", "![sc](https://user-images.githubusercontent.com/1223839/40943401-c4bce00a-6805-11e8-93a1-ff264c3dc037.png)\r", "NaN"], ["10756", "Upgrade to Slice 0.34", "Dain Sundstrom", "dain", "06/05/18, 12:26:50 AM", "NaN", "NaN"], ["10757", "OrcDataOutput cleanup", "Dain Sundstrom", "dain", "06/05/18, 01:54:29 AM", "NaN", "NaN"], ["10759", "Add ST_EnvelopeAsPts spatial function", "Keyao Pan", "keyaopan", "06/06/18, 03:45:19 AM", "NaN", "NaN"], ["10760", "Change Hive OrcFileWriter to report flushed and buffered bytes for witten size", "Dain Sundstrom", "dain", "06/06/18, 01:53:36 AM", "NaN", "NaN"], ["10763", "Translate HiveMetastore NDV to Presto NDV", "Andrii Rosa", "arhimondr", "06/06/18, 03:41:21 PM", "The \"number of distinct values\" statistic in the HiveMetastore assumes\r\n`null` as a \"distinct value\", when the Presto CBO doesn't", "NaN"], ["10766", "Improve memory usage and accounting for Hive sort buffer", "David Phillips", "electrum", "06/07/18, 04:19:11 AM", "NaN", "NaN"], ["10767", "Base PageProcessor on WorkProcessor", "Karol Sobczak", "sopel39", "12/18/18, 10:34:59 AM", "This is initial (Stage 1) PR for effort to support:\r\n* cross operator lazy pages (starting from source operators)\r\n* cleanup/simplify contract between operators via `WorkProcessor` pipelines\r\n* provide base for further improvements (e.g: on stack rows without `Page` materialization, Graal)\r\n\r\nThe advantage of cross operator lazy pages is that we can avoid IO when queries are highly selective. This requires that significant processing happens in source stage, but this becomes more and more the case with improvements like CBO (\"broadcast joins\") or grouped execution.\r\n\r\nKeep in mind that this is my side effort so I would like to divide it into smaller chunks. \r\n\r\nStages are:\r\n**Stage 1**\r\n* base `PageProcessor` on `WorkProcessor`\r\n\r\n**Stage 2**\r\n* internally base `ScanFilterAndProject` on `WorkProcessor`. The pipeline would look like follows:\r\n```\r\nsplit singleton -> [flatMap] -> pages source\r\n                -> [transform] -> page processor \r\n                -> [transform] -> merge pages\r\n```\r\nor if split is cursor based\r\n```\r\nsplit singleton -> [flatMap] -> cursor source -> [transform] -> merge pages\r\n```\r\n* internally base `FilterAndProject` on `WorkProcessor`. The pipeline would look like follows:\r\n```\r\npage buffer -> [transform] -> page processor -> [transform] -> [merge pages]\r\n```\r\n\r\n**Stage 3**\r\n* create interface for operators that are based on `WorkProcessor` pipelines\r\n* create standarized abstract operator class for operators that internally are based on `WorkProcessor` pipelines\r\n* combine operators that are based on `WorkProcessors` via dedicated \"gluing\" operator\r\n* base `TopNOperator` on `WorkProcessor` pipelines (fast data exploration!)\r\n\r", "NaN"], ["10768", "Small fixups work processor", "Karol Sobczak", "sopel39", "06/07/18, 02:36:09 PM", "NaN", "NaN"], ["10769", "Improve visibility of resource estimates", "Raghav Sethi", "raghavsethi", "06/07/18, 02:09:39 PM", "NaN", "NaN"], ["10770", "Minor refactor to TestHiveIntegrationSmokeTest", "Wenlei Xie", "wenleix", "06/07/18, 05:47:00 AM", "NaN", "NaN"], ["10773", "Improve parser error reporting", "Martin Traverso", "martint", "06/08/18, 02:41:14 AM", "NaN", "NaN"], ["10777", "Expose metastore access table column stats as MBean", "Grzegorz Kokosi\u0144ski", "kokosing", "06/08/18, 05:44:39 AM", "Expose metastore access table column stats as MBean", "NaN"], ["10779", "Code style fixes in Cassandra tests", "Piotr Findeisen", "findepi", "06/07/18, 07:13:02 PM", "NaN", "NaN"], ["10780", "Enable ST_Area for all geometry types", "Jacob Wasserman", "jwass", "06/08/18, 09:35:38 PM", "For Point and LineString, the area is 0.0.\r\nFor GeometryCollection, the area is the sum of the areas of the\r\nindividual geometries.\r\n\r\n@mbasmanova ", "NaN"], ["10782", "Ensure locally created memory contexts are closed in MergeSortedPages", "Karol Sobczak", "sopel39", "06/14/18, 10:54:27 AM", "Dist sort rebase work", "NaN"], ["10784", "Change ORC validation-percentage to be [0, 100)", "Dain Sundstrom", "dain", "06/08/18, 12:26:36 AM", "Change the hive.orc.writer.validation-percentage configuration property\nto align better with the usage of percentage in Presto, so the range\nis between [0, 100) instead of [0, 1).", "NaN"], ["10785", "Avoid repeated BlockBuilder construction in ArrayDistinctFunction", "Gerlou Shyy", "gerlou", "06/20/18, 06:44:17 PM", "Use same mechanism as ArrayShuffleFunction to avoid repeated BlockBuilder construction in ArrayDistinctFunction", "NaN"], ["10786", "Fix value count method in SliceDictionaryColumnWriter", "Wenlei Xie", "wenleix", "06/08/18, 02:22:10 AM", "Methods getValueCount() and getNonNullValueCount() in\r\nSliceDictionaryColumnWriter should return the value count of the\r\nwhole stripe. However, currently it returns the value count of\r\nthe row group.\r\n\r\nThis can lead to\r\n- Wrong decision for dictionary optimizer\r\n- Write too much data in one stripe for highly dictionary encoded\r\ncolumns.", "NaN"], ["10787", "Upgrade to Slice 0.35", "Nezih Yigitbasi", "nezihyigitbasi", "06/08/18, 03:38:33 PM", "NaN", "NaN"], ["10789", "Set default soft memory limit to Long.MAX_VALUE", "Raghav Sethi", "raghavsethi", "06/08/18, 06:17:20 PM", "This fixes #10383, where deployments using the legacy resource group\r\nconfiguration manager were unable to run more than a single query that\r\nused memory concurrently.", "NaN"], ["10790", "Remove legacy task/split scheduler", "Raghav Sethi", "raghavsethi", "06/14/18, 03:41:37 PM", "Since 0.193, the default configuration has been to use the new\r\nscheduler.", "NaN"], ["10791", "Remove unused legacyOrderBy field", "Martin Traverso", "martint", "06/15/18, 03:27:23 AM", "We missed it when we removed the feature.", "NaN"], ["10792", "Fix memory accounting in OrcInputStream", "Nezih Yigitbasi", "nezihyigitbasi", "06/08/18, 05:53:10 PM", "Previously, sliceInput.length() was used as an approximation of the\r\nretained size for memory accounting. However, that value can be\r\nsignificantly larger than the actual retained size. For example,\r\nfor ChunkedSliceInput the length() method returns the total length\r\nof the backing stream instead of the actual buffer size that's\r\nretained. That causes significant over-accounting for the scan operators.\r\n\r", "NaN"], ["10794", "Allow at most one public constructor in @ScalarFunction class", "Haozhun Jin", "haozhun", "06/15/18, 01:05:08 AM", "I have talked to @martint about the design choice here.", "NaN"], ["10795", "Improve performance and simplify map equal and distinct from", "Gerlou Shyy", "gerlou", "06/12/18, 12:47:42 AM", "Map Equal and Distinct From no longer implemented by making Java maps", "NaN"], ["10796", " Add session properties related to ORC writer", "Wenlei Xie", "wenleix", "06/09/18, 05:48:47 AM", "NaN", "NaN"], ["10797", "Fix cast from row(json) to row(...)", "Wenlei Xie", "wenleix", "06/09/18, 12:51:20 AM", "Resolves https://github.com/prestodb/presto/issues/9113", "NaN"], ["10800", "Upgrade to Airlift 0.170", "Nezih Yigitbasi", "nezihyigitbasi", "06/09/18, 01:44:17 AM", "NaN", "NaN"], ["10802", "Avoid constructing default values in Map IS DISTINCT FROM", "Piotr Findeisen", "findepi", "06/27/18, 07:24:19 PM", "NaN", "NaN"], ["10806", "Distributed sort v2", "Karol Sobczak", "sopel39", "07/09/18, 10:26:07 AM", "Design doc: https://docs.google.com/document/d/1-dIzyak0z-D13Kq7eVIDPYQFnEHpbsdq5Du4RJRwQmc/edit?usp=sharing\r\n\r", "NaN"], ["10808", "Add explict memory accounting to OrcDataSource stream", "Dain Sundstrom", "dain", "06/12/18, 07:55:36 PM", "NaN", "NaN"], ["10810", "Add parsing and config for path implementation", "Cole Bowden", "bowdencm", "06/27/18, 06:53:32 PM", "No changes to functionality (except for throwing an error on a \"SET PATH\" query), but building out the base for utilizing the path in function resolution.", "NaN"], ["10813", "Change Hive's default storage format to ORC", "Piotr Findeisen", "findepi", "10/12/18, 10:13:54 PM", "As ORC is the generally recommended storage format, defaults should\r\nreflect that.", "NaN"], ["10817", "Add 0.204 release notes", "Martin Traverso", "martint", "06/13/18, 08:14:17 PM", "Initial set of notes from https://github.com/prestodb/presto/issues/10742. Some items need rephrasing.", "NaN"], ["10818", "Add config for min/max drivers per task", "Nezih Yigitbasi", "nezihyigitbasi", "06/14/18, 10:33:24 PM", "This will let us tune the min/max number of drivers to run per task to keep the memory usage under control (we observed that a large number of drivers that start at the same time can retain a significant amount of memory).\r\n\r\n**Question:** I set the default value of `maxDriversPerTask` to `Integer.MAX_VALUE` to keep the current behavior. Is this what we want? ", "NaN"], ["10819", "Update jline to 2.14.6", "Andrii Rosa", "arhimondr", "06/15/18, 01:56:24 AM", "On Ubuntu 18.04 the `infocmp` (command that prints terminal capabilities)\r\noutputs the `colors` property in hex format. The jline library up\r\nto 2.14.4 didn't support such representation.\r\n\r\nhttps://github.com/jline/jline2/commit/c1b1676de1803278289af0622ad202f1c7a526ec", "NaN"], ["10821", "Implement approx_distinct under the annotation framework", "Jiexi Lin", "jessesleeping", "06/20/18, 07:18:54 PM", "This is part of the work to support `approx_distinct` for all data types. The goal is that any type that implements the `XX HASH 64` operator can have its `approx_distinct` implementation via the parametric implementation of `approx_distinct`.\r\n\r\nWhat we have done:\r\n(1) support a single aggregation function to have multiple `input` function implementations. \r\n(2) convert existing implementation of `approx_distinct` into the annotation framework. \r\n\r\nWhat's next (separate PR):\r\n(1) implement `XX_HASH_64` operators for other data types", "NaN"], ["10825", "Fix version in 0.204 release notes", "Nezih Yigitbasi", "nezihyigitbasi", "06/13/18, 08:39:52 PM", "NaN", "NaN"], ["10826", "Add config for concurrent lifespans in a task during grouped execution", "Haozhun Jin", "haozhun", "06/21/18, 01:23:42 AM", "NaN", "NaN"], ["10827", "Fix invariant violation in GroupedExecutionTagger", "Haozhun Jin", "haozhun", "06/15/18, 07:23:06 PM", "Query would fail during distributed planning due to a sanity check that\r\nverifies the invariants.\r\n\r\nThe bug happens for plan nodes that satistify the following conditions:\r\n1) at least one child can benefit from grouped execution, and\r\n2) at least one child is incompatible with grouped execution.", "NaN"], ["10831", "Small fixes in init.d script", "Piotr Findeisen", "findepi", "06/14/18, 02:56:38 PM", "NaN", "NaN"], ["10833", "Close QueryAssertions", "Piotr Findeisen", "findepi", "06/15/18, 12:40:35 PM", "related to https://github.com/prestodb/presto/issues/10812", "NaN"], ["10835", "Remove unused COALESCE token", "David Phillips", "electrum", "06/14/18, 06:11:19 PM", "NaN", "NaN"], ["10838", "Remove unused integer data type tokens", "David Phillips", "electrum", "06/14/18, 09:54:49 PM", "NaN", "NaN"], ["10839", "Close TestMemoryWorkerCrash.queryRunner after the test", "Nezih Yigitbasi", "nezihyigitbasi", "06/15/18, 07:22:29 PM", "NaN", "NaN"], ["10841", "Add context object for SessionPropertyConfigurationManager", "David Phillips", "electrum", "06/15/18, 09:55:17 PM", "NaN", "NaN"], ["10842", "Elide Block null arrays if all values non-null", "Dain Sundstrom", "dain", "06/19/18, 05:28:58 PM", "For fixed witdth data this saves ~10% of memory", "NaN"], ["10843", "Fix varchar column display size", "Sagar Sumit", "codope", "05/01/19, 11:56:24 PM", "Fixes #10807", "NaN"], ["10846", "Add getLoadedBlock/Page in Block/Page to supersede assureLoaded ", "Wenlei Xie", "wenleix", "07/11/18, 05:44:56 AM", "NaN", "NaN"], ["10850", "Fix incorrectly allowed filter clause in conditional expressions", "Sagar Sumit", "codope", "06/17/18, 03:30:29 AM", "Fixes #10834 \r\n@martint I added a check for filter, and few more test cases.", "NaN"], ["10851", "Remove unused field", "Piotr Findeisen", "findepi", "06/18/18, 09:39:10 AM", "NaN", "NaN"], ["10852", "Fix backward-compatibility processing for array with one group field", "Kate Galieva", "kgalieva", "06/18/18, 05:32:31 PM", "According to Parquet spec:\r\nIf the repeated field is a group with one field and is named either array\r\nor uses the LIST-annotated group's name with _tuple appended\r\nthen the repeated type is the element type and elements are required.\r\n\r\nhttps://github.com/apache/parquet-format/blob/master/LogicalTypes.md#lists", "NaN"], ["10854", "Optimize GroupByHash over RLE page", "Wenlei Xie", "wenleix", "06/19/18, 04:36:01 PM", "HivePageSink leverages GroupByHash to get the writer index. However,\r\nthis will causes calculating hash code and equality per each row even\r\nfor queries writing into static partition.", "NaN"], ["10856", "Support skip.footer.line.count", "Grzegorz Kokosi\u0144ski", "kokosing", "06/18/18, 11:58:25 AM", "Support skip.footer.line.count", "NaN"], ["10857", "Handle DictionaryBlocks from different sources in dictionary aggregation", "Piotr Findeisen", "findepi", "06/27/18, 07:24:02 PM", "In an \"Aggregation over Join\" query, the `LookupJoinOperator` can\r\nproduce `DictionaryBlock`s. Later, a projection adding a hash channel\r\nmay or may not create `DictionaryBlock`s, as\r\n`DictionaryAwarePageProjection` chooses its strategy adaptively, but the\r\ndata block will be always passed-thru. Thus, downstream Aggregation\r\noperator cannot assume that data block being a dictionary implies hash\r\nblock is a dictionary too.\r\n\r\nFixes #9756", "NaN"], ["10858", "Add support for a global total memory limit", "Nezih Yigitbasi", "nezihyigitbasi", "06/21/18, 02:09:29 AM", "Since the  local total memory limit is only enforced when the system pool is disabled, to be consistent with that we also enforce the global total memory limit only when the system pool is disabled.", "NaN"], ["10859", "Fix formatting for zip_with documentation", "David Phillips", "electrum", "06/19/18, 12:41:24 AM", "NaN", "NaN"], ["10860", "Inline only expressions for filters that contains only ininling targe\u2026", "Ying", "yingsu00", "08/02/18, 07:59:43 PM", "This is to fix github issue #10455. \r\n\r\nWe used to inline all predicate when doing push down predicates. This could have problems when the projection columns contains complex expressions like the following:\r\n\r\nWITH\r\nt1 (v) AS (VALUES 1),\r\nt2 AS( select if(v = 0, v, v) v from t1 ),\r\nt3 AS( select if(v = 0, v, v) v from t2 ),\r\nt4 AS( select if(v = 0, v, v) v from t3 ),\r\nt5 AS( select if(v = 0, v, v) v from t4 ),\r\nt6 AS( select if(v = 0, v, v) v from t5 ),\r\nt7 AS( select if(v = 0, v, v) v from t6 ),\r\nt8 AS( select if(v = 0, v, v) v from t7 ),\r\nt9 AS( select if(v = 0, v, v) v from t8 ),\r\nt10 AS( select if(v = 0, v, v) v from t9 ),\r\nt11 AS( select if(v = 0, v, v) v from t10 ),\r\nt12 AS( select if(v = 0, v, v) v from t11 ),\r\nt13 AS( select if(v = 0, v, v) v from t12 ),\r\nt14 AS( select if(v = 0, v, v) v from t13 ),\r\nt15 AS( select if(v = 0, v, v) v from t14 ),\r\nt16 AS( select if(v = 0, v, v) v from t15 )\r\nselect *\r\nfrom t16\r\nwhere v = 0\r\n\r\nThis short-term fix is to adjust the inlining heuristics to only do it if the expressions are trivial or appear only once (similar to how the InlineProjections rule works)\r\n\r", "NaN"], ["10863", "Improve ORC stripe size estimation", "Wenlei Xie", "wenleix", "06/20/18, 04:43:23 AM", "Currently buffered bytes are used to estimate ORC strip size and\r\ndeciding flush. This can overestimate stripe size when compression\r\nis enabled.", "NaN"], ["10865", "Speedup TestHiveFileBasedSecurity", "Piotr Findeisen", "findepi", "06/19/18, 02:53:14 PM", "Instead of creating the entire TPCH table set it is enough to create\r\none small table, such as nation.\r\n\r\nAfter closing a query executor reference must be nullified to prevent\r\nmemory leaks.\r\n\r\ncc @arhimondr ", "NaN"], ["10866", "Close and let GC QueryRunners in tests", "Piotr Findeisen", "findepi", "06/19/18, 08:06:28 PM", "Related to https://github.com/prestodb/presto/issues/10812\r\n\r\nFor most tests this is done automatically in `AbstractTestQueryFramework` (https://github.com/prestodb/presto/blob/master/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java#L85-L89). For others, we need to take care manually.", "NaN"], ["10869", "Use VARCHAR and VARBINARY types for JDBC driver", "David Phillips", "electrum", "06/20/18, 09:03:12 PM", "Presto \"varchar\" is now VARCHAR rather than LONGNVARCHAR\nand \"varbinary\" is now VARBINARY rather than LONGVARBINARY.", "NaN"], ["10871", "Refactor AbstractTestHiveClientS3 to become file system generic", "Grzegorz Kokosi\u0144ski", "kokosing", "06/21/18, 06:18:54 AM", "Refactor AbstractTestHiveClientS3 to become file system generic\n\nIn order to make AbstractTestHiveFileSystem to be able to test other\nfile systems than s3, there had be used includes/excludes instead of\ngroups for test selection. It is because test group annotated at child\nclass does not propagate to parent class.", "NaN"], ["10872", "Fix typo", "Piotr Findeisen", "findepi", "06/27/18, 07:23:43 PM", "Extracted from https://github.com/prestodb/presto/pull/9241", "NaN"], ["10874", "Remove unused dependencies", "Grzegorz Kokosi\u0144ski", "kokosing", "06/20/18, 06:13:19 PM", "Remove unused dependencies", "NaN"], ["10876", "Fix type check for ST_ExteriorRing", "Jacob Wasserman", "jwass", "06/20/18, 04:11:01 PM", "Remove MultiPolygons as acceptable type for ST_ExteriorRing, which requires Polygons.\r\n\r\nFixes #10875", "NaN"], ["10878", "Update to Airbase 82", "David Phillips", "electrum", "08/02/18, 08:03:17 PM", "This version guarantees the JVM will terminate on OOM during tests.", "NaN"], ["10879", "Exclude broadcast exchanges from PushRemoteExchangeThroughAssignUniqueId rule", "Maria Basmanova", "mbasmanova", "06/21/18, 04:11:19 PM", "NaN", "NaN"], ["10880", "Fix TypeSignature parsing of row type with field name starting with \"_\"", "Leiqing Cai", "caithagoras", "06/21/18, 04:50:17 PM", "NaN", "NaN"], ["10881", "Add per table column_ranges system table in Raptor", "Jiexi Lin", "jessesleeping", "08/24/18, 10:31:15 PM", "For every Raptor table, we add a system table `<table_name>$column_ranges` which shows `min` and `max` values of all `BIGINT`, `DATE`, `TIMESTAMP`, `BOOLEAN` columns in that table. \r\n\r\nWe don't have ranges for `INTEGER` type because Raptor does not collect `INTEGER` column stats. We don't have ranges for `VARCHAR` because Raptor may truncate `VARCHAR` in the stats. We don't have ranges for `DOUBLE` because in current Raptor conversion from Presto double to MySQL double is not a one-one mapping. ", "NaN"], ["10882", "Fix error categorization for Hive split loader", "David Phillips", "electrum", "07/09/18, 07:45:19 PM", "NaN", "NaN"], ["10883", "Unnest array of row to multiple columns", "Rongrong Zhong", "rongrong", "07/24/18, 06:45:51 PM", "This partially resolves #8151 (for array type).", "NaN"], ["10885", "Avoid Class#newInstance()", "Piotr Findeisen", "findepi", "06/21/18, 10:51:51 AM", "This method propagates checked exceptions uncleanly and generally should\r\nnot be used.  In many places we already did\r\n`.getConstructor().newInstance()`, this fixes the remaining ones.", "NaN"], ["10886", "Test read snappy compressed parquet table loaded in hive", "Grzegorz Kokosi\u0144ski", "kokosing", "07/03/18, 11:11:01 AM", "NaN", "NaN"], ["10889", "Provide more natural char predicates semantics", "Karol Sobczak", "sopel39", "07/03/18, 08:58:24 AM", "FYI: @kbajda ", "NaN"], ["10891", "Avoid double counting dictionary when estimating stripe size", "Wenlei Xie", "wenleix", "06/21/18, 06:18:43 PM", "SliceDictionaryColumnWriter.getBufferedBytes() already contains\r\ndictionary bytes.", "NaN"], ["10893", "Add UDF split_to_multimap", "Rongrong Zhong", "rongrong", "06/28/18, 05:43:03 PM", "Resolves #10892", "NaN"], ["10895", "Add 0.205 release notes", "Nezih Yigitbasi", "nezihyigitbasi", "06/27/18, 06:28:01 AM", "Fixes #10828.", "NaN"], ["10897", "Update simba drivers tests results", "Grzegorz Kokosi\u0144ski", "kokosing", "07/03/18, 12:04:40 PM", "Update simba drivers tests results\n\nSimba drivers does not support complex types.\nThey are currently saying that any complex type is a VARCHAR.", "NaN"], ["10898", "Add ST_PointN()", "Jacob Wasserman", "jwass", "06/28/18, 03:25:44 PM", "Implement ST_PointN to return the nth ST_Point in the input LineString.", "NaN"], ["10899", "Implement approx_distinct for more types", "Jiexi Lin", "jessesleeping", "07/11/18, 09:54:16 PM", "- Previously implemented: \r\n`BIGINT`, `VARCHAR`, `VARBINARY`, `DOUBLE`\r\n- Newly added in this commit (10 more): \r\n`INTEGER`, `SMALLINT`, `TINYINT`, `DECIMAL`, `REAL`,\r\n`DATE`, `TIMESTAMP`, `TIMESTAMP WITH TIMEZONE`, `TIME, TIME WITH TIMEZONE`, \r\n`IPADDRESS`\r\n\r\nI didn't support `BOOLEAN` because exact `COUNT DISTINCT` is better for boolean. \r\nI didn't support `CHAR` because it's not clear to me what's the right behavior when a CHAR value is padded. \r\nI didn't support `INTERVAL YEAR TO MONTH` and `INTERVAL DAY TO SECOND` because they are not very useful. \r\n\r\nSupports for geometry types (i.e `BING_TILE` and `GEOMETRY`), complex types (i.e. `ROW`, `ARRAY` and `MAP`) and `JSON` type can be added in the future if they are useful. \r", "NaN"], ["10906", "Add ST_Geometries function", "Matias Hanco", "yamatias", "06/28/18, 05:22:50 PM", "NaN", "NaN"], ["10907", "Combine view access tests", "Rebecca Schlussel", "rschlussel", "06/26/18, 12:13:22 AM", "Reusing the same view name was a problem for tests executed in parallel,\r\nand there was no need for separate tests.", "NaN"], ["10908", "Indeterminate operator", "Andrii Rosa", "arhimondr", "06/29/18, 10:14:18 PM", "INDETERMINATE operator is a scalar operator which would detect if the\r\ngiven value is indeterminate. A value is indeterminate if one of the\r\nfollowing conditions are met:\r\n\r\n- This value is null\r\n- If this value is of strutural type, such as ArrayType, MapType,\r\n  and RowType, it contains one or more indeterminate values.\r\n\r\nMaps with indetermnate keys shouldn't be allowed", "NaN"], ["10909", "EQUAL + NOT EQUAL + IN operators for complex type values that include NULL", "Andrii Rosa", "arhimondr", "08/23/18, 08:19:09 PM", "NaN", "NaN"], ["10912", "Add resource group id to queries table", "Elon Azoulay", "elonazoulay", "06/29/18, 10:45:39 PM", "This is related to issue #4310\r\nResolves #10901 ", "NaN"], ["10913", "Allow re-running TestSqlStandardAccessControlChecks", "Piotr Findeisen", "findepi", "06/27/18, 07:29:22 PM", "NaN", "NaN"], ["10914", "Add test for sql-standard check of SELECT from VIEW", "Piotr Findeisen", "findepi", "07/18/18, 08:50:27 PM", "NaN", "NaN"], ["10916", "Log Presto version during server start", "Piotr Findeisen", "findepi", "06/27/18, 07:24:50 PM", "By defaulting `ServerConfig.prestoVersion` to introspected Presto\r\nversion, Presto version gets logged along with all other configuration\r\nproperties.", "NaN"], ["10920", "Add ST_InteriorRings", "Matias Hanco", "yamatias", "06/29/18, 01:44:26 AM", "NaN", "NaN"], ["10921", "Add ST_Union geospatial function", "Xin Yao", "yaoxin226", "06/28/18, 11:03:18 PM", "ST_Union function takes two geometries and returns a new geometry that represents the point set union of the input geometries.", "NaN"], ["10924", "Expose debug port for Presto server in product tests", "Grzegorz Kokosi\u0144ski", "kokosing", "06/28/18, 05:47:26 AM", "Expose debug port for Presto server in product tests\n\nBy default Presto server does not expose 5005 port for debugger, it has\nto be manually changed in jvm.config. But when jvm.config is changed to\nexpose that debug port, then docker compose configuration has to be also\nchanged.\n\nWhen only docker compose is changed that 5005 is forwarded from\nPresto container and jvm.config is not changed, then 5005 port\ncan be still used for other purpouses (probably to debug the other java process).\n\nThanks to this change, it is just a bit easier to debug Presto server\nprocess in product tests. One file less to edit (remember).", "NaN"], ["10926", "Start Presto and Hadoop at the same time in product tests", "Grzegorz Kokosi\u0144ski", "kokosing", "06/28/18, 05:53:12 AM", "Start Presto and Hadoop at the same time in product tests\n\nPresto does not much depend on Hadoop (Hive). Using hive connector\ndepends on Hive being up and running.\n\nThat way it is a bit faster to run product tests.", "NaN"], ["10927", "Support V2 Cassandra clusters", "Yuya Ebihara", "ebyhr", "01/03/19, 02:18:52 PM", "This PR is a continuation of https://github.com/prestodb/presto/pull/7955.", "NaN"], ["10930", "Implement SET PATH statement & add client capabilities header", "Cole Bowden", "bowdencm", "07/10/18, 07:13:15 PM", "Implementation for the SET PATH statement, which sets the path field in the session to later be used for function resolution.", "NaN"], ["10931", "Introduce new TIME/TIMESTAMP semantics to date_add, date_diff", "Piotr Findeisen", "findepi", "07/09/18, 09:04:35 PM", "(found when working on https://github.com/prestodb/presto/pull/10128)", "NaN"], ["10932", "Implement CURRENT_PATH expression", "Cole Bowden", "bowdencm", "07/02/18, 09:27:07 PM", "The parsing for CURRENT_PATH has already been added, and this adds a scalar session function to give it functionality - the function connects to DesugarCurentPath.", "NaN"], ["10934", "Make TestLegacyLogFunction single threaded", "Haozhun Jin", "haozhun", "06/28/18, 12:43:38 AM", "This avoids creating multiple query runners at once.", "NaN"], ["10935", "Fix listener object retention issue in SourcePartitionedScheduler ", "Haozhun Jin", "haozhun", "06/30/18, 04:54:40 AM", "NaN", "NaN"], ["10938", "Remove redundant TypeRegistry constructor", "Piotr Findeisen", "findepi", "06/29/18, 10:11:49 AM", "In all cases, types passed to the constructor were registered by\r\ndefault.  The constructor was ignoring its only argument anyway.", "NaN"], ["10943", "Allow missing fields in structs in ORC", "Shreyas Joshi", "shreyasjoshis", "07/23/18, 06:37:57 PM", "### Summary\r\n\r\nStructs can have missing fields if the the table schema has changed to add nested fields, but the underlying ORC files are not updated. Earlier Presto would fail with an `ArrayIndexOutOfBounds` exception. Now, the missing fields will be marked as null.\r\n\r\nThis behavior will allow Presto to support addition of fields to structs without needing to change the underlying data.\r\n\r\n### Steps to reproduce the problem\r\nWhen the table schema has struct fields that the underlying ORC file does not have, Presto errors out and fails to read the ORC file. Here are steps to reproduce the problem:\r\n\r\n```shell\r\n# Clean table location\r\nbin/hdfs dfs -rm -r -f /user/hive/warehouse/test_schema.db/test_tbl/\r\nbin/hdfs dfs -rm -r -f /user/hive/warehouse/test_schema.db/test_tbl2/\r\n```\r\n\r\nCreate tables and insert data\r\n```sql\r\nDROP TABLE test_schema.test_tbl;\r\n\r\nCREATE EXTERNAL TABLE `test_schema.test_tbl`(\r\n  `a` string,  \r\n  `b` struct<ts:STRING, topic:STRING>)\r\nROW FORMAT SERDE\r\n  'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\r\nSTORED AS INPUTFORMAT\r\n  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\r\nOUTPUTFORMAT\r\n  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';\r\n\r\nINSERT INTO table test_schema.test_tbl SELECT 'a_val1', named_struct('ts', 'b1ts','topic', 'b1topic');\r\n\r\nDROP TABLE test_schema.test_tbl2;\r\nCREATE EXTERNAL TABLE `test_schema.test_tbl2`( \r\n  `a` string,\r\n  `b` struct<ts:STRING>)\r\nROW FORMAT SERDE\r\n  'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\r\nSTORED AS INPUTFORMAT\r\n  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\r\nOUTPUTFORMAT\r\n  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';\r\n\r\nINSERT INTO table test_schema.test_tbl2 SELECT 'a_val2', named_struct('ts', 'b2ts');\r\n```\r\n\r\nCopy data over\r\n```\r\nbin/hdfs dfs -cp /user/hive/warehouse/test_schema.db/test_tbl2/000000_0 /user/hive/warehouse/test_schema.db/test_tbl/from_test_tbl2\r\n```\r\n\r\nQuery from Hive (this works as expected):\r\n```\r\nhive> select * from test_schema.test_tbl;\r\na_val1\t{\"ts\":\"b1ts\",\"topic\":\"b1topic\"}\r\na_val2\t{\"ts\":\"b2ts\",\"topic\":null}\r\n```\r\n\r\nQuery from Presto (This fails with the following):\r\n```java\r\njava.lang.ArrayIndexOutOfBoundsException: 1\r\n\tat com.facebook.presto.orc.reader.StructStreamReader.readBlock(StructStreamReader.java:100)\r\n\tat com.facebook.presto.orc.OrcRecordReader.readBlock(OrcRecordReader.java:370)\r\n\tat com.facebook.presto.hive.orc.OrcPageSource$OrcBlockLoader.load(OrcPageSource.java:240)\r\n\tat com.facebook.presto.hive.orc.OrcPageSource$OrcBlockLoader.load(OrcPageSource.java:216)\r\n\tat com.facebook.presto.spi.block.LazyBlock.assureLoaded(LazyBlock.java:256)\r\n\tat com.facebook.presto.spi.Page.assureLoaded(Page.java:230)\r\n\tat com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:265)\r\n\tat com.facebook.presto.operator.Driver.processInternal(Driver.java:337)\r\n\tat com.facebook.presto.operator.Driver.lambda$processFor$6(Driver.java:241)\r\n\tat com.facebook.presto.operator.Driver.tryWithLock(Driver.java:614)\r\n\tat com.facebook.presto.operator.Driver.processFor(Driver.java:235)\r\n\tat com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)\r\n\tat com.facebook.presto.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)\r\n\tat com.facebook.presto.execution.executor.LegacyPrioritizedSplitRunner.process(LegacyPrioritizedSplitRunner.java:23)\r\n\tat com.facebook.presto.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:485)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)```\r\n```\r\n\r\n### The solution\r\nWith the changes in this PR Presto will output:\r\n```\r\npresto> SELECT * FROM hive.test_schema.test_tbl;\r\n   a    |            b\r\n--------+--------------------------\r\n a_val2 | {ts=b2ts, topic=null}  <-- This is the important row, with the topic set to null\r\n a_val1 | {ts=b1ts, topic=b1topic}\r\n(2 rows)\r\n\r\nQuery 20180625_221954_00001_94ydd, FINISHED, 1 node\r\nSplits: 18 total, 18 done (100.00%)\r\n0:03 [2 rows, 795B] [0 rows/s, 292B/s]\r\n```\r\nhttps://github.com/prestodb/presto/issues/10941", "NaN"], ["10945", "Update to docker images version 6", "Piotr Findeisen", "findepi", "06/29/18, 08:47:57 AM", "Includes JDK update (https://github.com/prestodb/docker-images/pull/24). \r\nDoes not include TZ change (https://github.com/prestodb/docker-images/pull/22), this will go in 7.", "NaN"], ["10946", "Small cleanup in TeradataDateFunctions", "Piotr Findeisen", "findepi", "07/09/18, 08:17:23 PM", "extracted from https://github.com/prestodb/presto/pull/10128", "NaN"], ["10948", "Fail FileBasedSystemAccessControl on uknown rules", "Grzegorz Kokosi\u0144ski", "kokosing", "07/18/18, 08:59:31 AM", "Fail FileBasedSystemAccessControl on uknown rules\r\n\r\nFixes #10947 ", "NaN"], ["10950", "Rename misleading variable name hasOrderBy to noOrderBy in AggregationNode's constructor", "James Xu", "xumingming", "07/02/18, 08:03:29 AM", "minor fix for #10949 ", "NaN"], ["10952", "Add applicationNamePrefix property to JDBC driver", "David Phillips", "electrum", "07/02/18, 04:13:20 PM", "NaN", "NaN"], ["10958", "Add BasePlanTest#assertDistributedPlan method", "Karol Sobczak", "sopel39", "07/03/18, 08:26:37 AM", "NaN", "NaN"], ["10961", "Add BLOCK_AND_POSITION calling convention to annotation framework", "Gerlou Shyy", "gerlou", "08/02/18, 06:25:00 PM", "Move @BlockPosition and @BlockIndex to SPI", "NaN"], ["10962", "Refactor LongInputStream", "Ying", "yingsu00", "07/16/18, 09:59:35 PM", "Moving the repeating code from the classes implementing LongInputStream to LongInputStream\r\nas default methods.", "NaN"], ["10966", "Fix reading table with headers in kerberized environments", "Grzegorz Kokosi\u0144ski", "kokosing", "07/04/18, 09:01:44 AM", "Fix reading table with headers in kerberized environments\n\nPreviously when table had non zero value set for header or footer line\ncount, then Presto was unable to generate splits for it, raising:\nUnable to query tables due to Can't get Master Kerberos principal for\nuse as renewer.\n\nMentioned above error was raised from internals of FileInputFormat. This\nchange avoids using FileInputFormat.getSplits.", "NaN"], ["10967", "Add missing CHAR function so that fixed TPCDS queries can pass", "Karol Sobczak", "sopel39", "07/04/18, 08:21:29 AM", "NaN", "NaN"], ["10969", "Take into account nulls fraction when computing data size", "Karol Sobczak", "sopel39", "07/04/18, 10:02:15 AM", "NaN", "NaN"], ["10970", "Invalidate statistics caches", "Andrii Rosa", "arhimondr", "07/03/18, 04:31:29 PM", "NaN", "NaN"], ["10972", "Preliminary column statistics refactorings", "Andrii Rosa", "arhimondr", "07/10/18, 03:35:02 AM", "Refactor part of https://github.com/prestodb/presto/pull/10617", "NaN"], ["10973", "Add peak memory reservation to operator summaries", "Nezih Yigitbasi", "nezihyigitbasi", "07/13/18, 08:06:56 PM", "Having the peak user, system, and total memory reservation in\r\noperator summaries in the query stats becomes useful when debugging\r\ncases where the reported peak task memory is high.", "NaN"], ["10974", "Cleanup of various classes", "Nezih Yigitbasi", "nezihyigitbasi", "07/05/18, 04:32:23 PM", "NaN", "NaN"], ["10977", "Further remove trailing spaces from char predicates in TPC-DS", "Karol Sobczak", "sopel39", "07/04/18, 10:41:58 AM", "NaN", "NaN"], ["10978", "Compute average row size by using non-null rows only", "Karol Sobczak", "sopel39", "07/05/18, 09:18:01 AM", "NaN", "NaN"], ["10979", "Add block compact test cases", "Wenlei Xie", "wenleix", "08/17/18, 04:35:46 PM", "NaN", "NaN"], ["10980", "Remove support for Hadoop 1.x", "David Phillips", "electrum", "07/06/18, 04:36:31 PM", "NaN", "NaN"], ["10982", "Make sure to close AWS credentials provider", "Nezih Yigitbasi", "nezihyigitbasi", "07/06/18, 06:35:09 AM", "Some AWS credentials providers need to be closed explicitly. For example, the `STSAssumeRoleSessionCredentialsProvider` being added in #10864 or `InstanceProfileCredentialsProvider` when async refresh is enabled, etc.\r", "NaN"], ["10983", "Use Optional to represent missing LIKE escape character", "Martin Traverso", "martint", "07/07/18, 10:05:36 PM", "Extracted from https://github.com/prestodb/presto/pull/10816 with comments addressed.", "NaN"], ["10984", "Remove support for legacy JOIN USING behavior", "Martin Traverso", "martint", "07/18/18, 10:58:41 PM", "Extracted from https://github.com/prestodb/presto/pull/10816", "NaN"], ["10985", "Generalize grouping set rewriter", "Martin Traverso", "martint", "07/09/18, 03:35:35 PM", "Break its dependency on Analysis and QuerySpecification node by passing\r\nthe required data directly. This is to prepare for the split between AST -> IR,\r\nwhich requires getting rid of expression rewriters (AST -> AST) during planning\r\n and replace them with AST -> IR translations.\r\n\r\nExtracted from https://github.com/prestodb/presto/pull/10816", "NaN"], ["10986", "Preserve original window frame expressions in plan instead of AST", "Martin Traverso", "martint", "07/06/18, 07:15:37 PM", "Instead of recording the expressions prior to being transformed into\r\nsymbols in the AST node, store the information in the corresponding\r\nWindowNode. This information is only used for printing the plan\r\nand is a more natural place to do it.\r\n\r\nExtracted from https://github.com/prestodb/presto/pull/10816 with comments addressed.", "NaN"], ["10987", "Remove unnecessary use of addIntermediateMappings", "Martin Traverso", "martint", "07/09/18, 03:32:11 PM", "Extracted from https://github.com/prestodb/presto/pull/10816 with comments applied.", "NaN"], ["10988", "Add memory tracking for StreamReader local buffers", "Ying", "yingsu00", "08/13/18, 07:27:04 PM", "We use some vectors as local buffers in the StreamReaders, but they were\r\nnot tracked in the system pool. This amount is inegligible, because\r\nthere is one thread created for each stage/task/split, and for each\r\nthread, multiple StreamReaders would be created depending on the number\r\nof columns and column type. The number of StreamReaders could be very\r\nbig. And each StreamReader could hold buffers of a few KB. Altogher\r\nthis can be a large amount.\r\n\r\nThis commit is to track this part of memory usage as part of the\r\nPageSource system memory usage.\r\n\r\nWe also try to include the instance sizes for the StreamReaders that\r\nhave at least one local buffer. The StreamReaders that don't have a\r\nlocal buffer were not counted, i.e. all non-leaf level ones, like\r\nListStreamReader, MapStreamReader, and SliceDirectStreamReader, which\r\nis a leaf level StreamReader but does not use a local buffer. This is\r\ntrying to follow the MemoryContext usage convention and not to confuse\r\npeople by updating the MemoryContext in each constructors. This missing\r\npart could be a couple of dozens of MB on each worker node and not\r\nsignificant.", "NaN"], ["10989", "Heuristically account for auxiliary structures in getOutputSizeInBytes", "Karol Sobczak", "sopel39", "07/09/18, 12:30:41 PM", "    Auxiliary structures size was ignored when output size\r\n    was computed for symbol. This causes large underestimation\r\n    for VARCHAR/CHAR columns if average row length extracted from\r\n    Hive stats is small (e.g: 1 or 2 bytes). Even though actual\r\n    row data is small Presto still processes Blocks with large\r\n    auxiliary structures that cause CPU overhead. Not including\r\n    those in stats causes wrong join orders to be selected.", "NaN"], ["10990", "Simplify group id node", "Martin Traverso", "martint", "07/09/18, 03:34:58 PM", "Don't create an artificial mapping for pass-through columns. This\r\nsimplifies planning and other transformations slightly.\r\n\r\nDepends on https://github.com/prestodb/presto/pull/10987", "NaN"], ["10992", "Drop streaming when splitting streaming aggregation over exchange", "Maria Basmanova", "mbasmanova", "07/10/18, 12:58:58 AM", "Fixes #10971", "NaN"], ["10993", "Rename AST operator type classes and fields", "Martin Traverso", "martint", "07/09/18, 04:49:51 PM", "To make it possible to introduce expression data type when we split IR from AST.\r\n\r\nExtracted from https://github.com/prestodb/presto/pull/10816", "NaN"], ["10994", "Encapsulate type map in TypeProvider", "Martin Traverso", "martint", "07/10/18, 01:17:30 AM", "This will make it easier to swap out and eventually remove the provider once we add types to the expression IR.\r\n\r\nExtracted from https://github.com/prestodb/presto/pull/10816 with comments addressed.", "NaN"], ["10995", "Extract ConstantExpressionVerifier", "Martin Traverso", "martint", "07/09/18, 03:12:27 PM", "The functionality doesn't really belong in ExpressionInterpreter, which is already\r\ndoing too much.\r\n\r\nExtracted from https://github.com/prestodb/presto/pull/10816 with comments addressed.", "NaN"], ["11000", "Use PlanNodeSercher for plan node searching", "Grzegorz Kokosi\u0144ski", "kokosing", "07/09/18, 11:17:57 AM", "Use PlanNodeSercher for plan node searching", "NaN"], ["11004", "Fix compilation failure", "Andrii Rosa", "arhimondr", "07/09/18, 05:19:22 PM", "NaN", "NaN"], ["11006", "Add support for Linux ppc64le", "David Phillips", "electrum", "08/03/18, 04:01:58 PM", "The Hadoop dependency is a snapshot. I will release it and update to the\nrelease version before merging this PR.\nSee https://github.com/prestodb/presto-hadoop-apache2/pull/29\n\nThe full build with tests will not run on ppc64le due to various missing\nnative binaries used by the tests.", "NaN"], ["11007", "Remove PlanBuilder method which was not following convention", "Grzegorz Kokosi\u0144ski", "kokosing", "07/10/18, 10:54:44 AM", "Remove PlanBuilder method which was not following convention\n\nConvention in PlanBuilder is that source argument is passed as last.\nThat way one can structure code like:\npb.node(\n    ...\n    pb.node(\n      ...\n      pb.node()))", "NaN"], ["11009", "Cost-based join reordering v2", "Rebecca Schlussel", "rschlussel", "07/20/18, 03:09:59 PM", "Supersedes #10753  \r\n\r\nCC @findepi @mbasmanova @arhimondr ", "NaN"], ["11011", "Add support for tagging memory allocations", "Nezih Yigitbasi", "nezihyigitbasi", "07/25/18, 11:05:46 PM", "This will help us understand and monitor how much memory is allocated\r\nfor each active query at a fine granularity -- how much memory is being\r\nused by table scan, exchange buffers, and the join operators, etc.\r\n\r\nEach worker can be monitored through the /v1/memory/{pool_id} endpoint\r\nand the coordinator has an aggregate cluster view at /v1/cluster/memory.\r\n\r\nHere is a sample output from `/v1/cluster/memory` with two running queries:\r\n\r\n```\r\n...\r\n\r\n\"queryMemoryAllocations\": {\r\n\t\"20180710_233941_00005_77z72\": [\r\n                {\r\n\t\t\t\"tag\": \"InMemoryHashAggregationBuilder\",\r\n\t\t\t\"allocation\": 4570656\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"tag\": \"PartitionedOutputOperator\",\r\n\t\t\t\"allocation\": 12288\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"tag\": \"ScanFilterAndProjectOperator\",\r\n\t\t\t\"allocation\": 11799696\r\n\t\t}\r\n\t],\r\n\t\"20180710_233934_00004_77z72\": [\r\n                {\r\n\t\t\t\"tag\": \"PartitionedOutputOperator\",\r\n\t\t\t\"allocation\": 68099003\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"tag\": \"ScanFilterAndProjectOperator\",\r\n\t\t\t\"allocation\": 34089607\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"tag\": \"ExchangeOperator\",\r\n\t\t\t\"allocation\": 28066530\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"tag\": \"HashBuilderOperator\",\r\n\t\t\t\"allocation\": 56311250\r\n\t\t}\r\n\t]\r\n}\r\n...\r\n```", "NaN"], ["11012", "Add /v1/cluster/workerMemory endpoint", "Nezih Yigitbasi", "nezihyigitbasi", "07/11/18, 09:05:52 PM", "This new endpoint provides the coordinator's view of the worker\r\nmemory pools. This info can be useful when debugging cluster ooms.\r\n\r\n```\r\n\"5f3cbd7b-d15f-4b4e-9ab7-e42ad412e519 [127.0.0.1]\": {\r\n\t\t\"totalNodeMemory\": \"2672505652B\",\r\n\t\t\"pools\": {\r\n\t\t\t\"reserved\": {\r\n\t\t\t\t\"maxBytes\": 1145359564,\r\n\t\t\t\t\"reservedBytes\": 0,\r\n\t\t\t\t\"reservedRevocableBytes\": 0,\r\n\t\t\t\t\"queryMemoryReservations\": {},\r\n\t\t\t\t\"queryMemoryRevocableReservations\": {},\r\n\t\t\t\t\"freeBytes\": 1145359564\r\n\t\t\t},\r\n\t\t\t\"general\": {\r\n\t\t\t\t\"maxBytes\": 1527146088,\r\n\t\t\t\t\"reservedBytes\": 0,\r\n\t\t\t\t\"reservedRevocableBytes\": 0,\r\n\t\t\t\t\"queryMemoryReservations\": {},\r\n\t\t\t\t\"queryMemoryRevocableReservations\": {},\r\n\t\t\t\t\"freeBytes\": 1527146088\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n```", "NaN"], ["11013", "HDFS wire encryption", "Grzegorz Kokosi\u0144ski", "kokosing", "07/20/18, 05:49:24 AM", "NaN", "NaN"], ["11017", "Support dropping schema in memory connector", "Grzegorz Kokosi\u0144ski", "kokosing", "07/18/18, 08:58:07 AM", "Support dropping schema in memory connector", "NaN"], ["11029", "Minor improvements to ORC writer memory accounting", "Nezih Yigitbasi", "nezihyigitbasi", "07/19/18, 12:50:13 AM", "NaN", "NaN"], ["11030", "Grow the batch size for OrcRecordReader from 1 row", "Ying", "yingsu00", "08/02/18, 09:33:53 PM", "In the past we have seen large number of humongous allocations on the\r\ncluster worker nodes. The size of each block could be as high\r\nas 80MB, and the total allocation could be as high as 100GB per worker\r\nnode. This situation happens when reading map or array types. One of the\r\nreason is that when we read the nested types like map and array, we\r\ndidn't take the number of elements within each cell into account when\r\ncaculating the batch size. A single map cell could contain 10K entries\r\nin some cases and the total data entries read could be 10M. Currently\r\nwe read the first block and bound the batch size accordingly, but the\r\nfirst block might already contain too many (e.g. millions of) entries.\r\n\r\nThis commit is to change the first read from 1024 rows to 1 row, then\r\nexponentially grow the batch size until it hits the 1024-row bound or\r\nthe max block size bound (16MB), then adjust the batch size for each\r\nbatch later on. The growth factor BATCH_SIZE_GROWTH_FACTOR is defined\r\nas 2 for now.\r\n\r\nTo show the result of this change, we did an experiment reading a\r\ncolumn of type MAP<INT,ARRAY<BIGINT>> where each row contains around\r\n10K entries. Before the change the block sizes were around 80MB, and\r\nafter this change the block sizes were bounded around 16MB.\r\n\r\nThe number of rows read and block sizes before the change:\r\nReading 979 rows. block size: 76853564\r\nReading 979 rows. block size: 75622553\r\nReading 1011 rows. block size: 80844513\r\nReading 1024 rows. block size: 81683180\r\nReading 70 rows. block size: 5939036\r\nReading 1022 rows. block size: 77635114\r\nReading 997 rows. block size: 75353930\r\nReading 989 rows. block size: 81052672\r\n\r\nThe number of rows read and block sizes after the change:\r\n(suppose the growth factor is 32)\r\nReading one column:\r\nReading 1 rows. block size: 81185\r\nReading 32 rows. block size: 2560714\r\nReading 206 rows. block size: 16128454\r\nReading 206 rows. block size: 16150072\r\nReading 206 rows. block size: 16120201\r\nReading 206 rows. block size: 15966427\r\nReading 145 rows. block size: 11342435\r\n\r\nReading two columns:\r\nReading 1 rows. block size 80141 block size 64409\r\nReading 32 rows. block size 2423968 block size 2821975\r\nReading 99 rows. block size 7631865 block size 7972065\r\nReading 99 rows. block size 7481655 block size 8050815\r\n\r\n(suppose the growth factor is 2)\r\nReading 1 rows Block size 78956\r\nReading 2 rows Block size 110110\r\nReading 4 rows Block size 250065\r\nReading 8 rows Block size 424828\r\nReading 16 rows Block size 949918\r\nReading 32 rows Block size 2493616\r\nReading 64 rows Block size 5577162\r\nReading 128 rows Block size 12501611\r\nReading 190 rows Block size 18751822\r\nReading 190 rows Block size 18767870\r\nReading 190 rows Block size 18718009\r\nReading 170 rows Block size 12666075", "NaN"], ["11031", "Add unit test to cover hash code operator in REAL type", "Jiexi Lin", "jessesleeping", "07/18/18, 06:07:42 PM", "Add similar test as #10515 to the `HASH_CODE` operator in `REAL` type. ", "NaN"], ["11036", "Fix SqlTime, SqlTimeWithTimeZone construction in tests", "Piotr Findeisen", "findepi", "07/20/18, 08:56:49 PM", "`DateTime` being used to represent TIME, TIME WITH TIME ZONE  in tests\r\nmakes only sense from internal representation's perspective, but it also makes\r\ntests less clear and requiring normalization routines.", "NaN"], ["11037", "Validate OrderingScheme symbols against Exchange partitioning scheme", "Karol Sobczak", "sopel39", "07/13/18, 11:31:51 AM", "In Exchange OrderingScheme symbols are exchange output symbols\r\nand not symbols from source nodes.\r\n\r\nFixes: #11032", "NaN"], ["11040", "Mention config property names when validating memory configuration", "Piotr Findeisen", "findepi", "07/18/18, 06:14:58 AM", "This validates aforementioned settings, optionally producing something like this:\r\n\r\n```\r\n2018-07-13T14:38:03.600+0200\tINFO\tmain\tBootstrap\tnode.internal-address                                                 null                              null\r\n2018-07-13T14:38:03.600+0200\tINFO\tmain\tBootstrap\tnode.pool                                                             general                           general\r\n2018-07-13T14:38:04.257+0200\tERROR\tmain\tcom.facebook.presto.server.PrestoServer\tUnable to create injector, see the following errors:\r\n\r\n1) Error: Invalid configuration property with prefix '': query.max-total-memory-per-node must be greater than query.max-memory-per-node (for class com.facebook.presto.memory.NodeMemoryConfig.queryMaxTotalMemoryPerNodeGreaterThanMaxMemoryPerNode)\r\n\r\n1 error\r\ncom.google.inject.CreationException: Unable to create injector, see the following errors:\r\n\r\n1) Error: Invalid configuration property with prefix '': query.max-total-memory-per-node must be greater than query.max-memory-per-node (for class com.facebook.presto.memory.NodeMemoryConfig.queryMaxTotalMemoryPerNodeGreaterThanMaxMemoryPerNode)\r\n\r\n1 error\r\n\tat com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:543)\r\n\tat com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:159)\r\n\tat com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:106)\r\n\tat com.google.inject.Guice.createInjector(Guice.java:87)\r\n\tat io.airlift.bootstrap.Bootstrap.initialize(Bootstrap.java:241)\r\n\tat com.facebook.presto.server.PrestoServer.run(PrestoServer.java:115)\r\n\tat com.facebook.presto.server.PrestoServer.main(PrestoServer.java:67)\r\n```", "NaN"], ["11041", "Remove usages of deprecated parseTimestampInLocalTime", "Piotr Findeisen", "findepi", "07/18/18, 06:14:30 AM", "NaN", "NaN"], ["11042", "Minor changes to ClusterMemoryManager", "Nezih Yigitbasi", "nezihyigitbasi", "07/13/18, 11:39:59 PM", "- Simplifies `isLastKilledQueryGone ` method.\r\n- Add some logs to give more details about the state/flow.", "NaN"], ["11044", "Remove unnecessary remote exchange from scalar correlated subquery plan", "Maria Basmanova", "mbasmanova", "08/10/18, 05:49:52 PM", "Fixes #10830", "NaN"], ["11045", "Minor test code cleanup", "Piotr Findeisen", "findepi", "07/18/18, 06:14:16 AM", "NaN", "NaN"], ["11046", "Make sure to release task-level memory on completion", "Nezih Yigitbasi", "nezihyigitbasi", "07/14/18, 06:40:42 AM", "This is another safety mechanism in addition to what we have at the driver level. I guess it's possible to have a racy allocation at the task level (from the buffers) after the drivers are destroyed resulting in a positive memory reservation in the memory pools (which can cause issues when the cluster runs out of memory as the `ClusterMemoryManager` waits until the last killed query is dropped from the entire cluster).", "NaN"], ["11049", "Implement EXPLAIN (TYPE IO, FORMAT JSON) statement", "Shixuan Fan", "shixuan-fan", "08/14/18, 11:16:07 PM", "This will print the input tables with column constraints as well as output table in JSON format. ", "NaN"], ["11050", "Limit number of stages in a query", "Haozhun Jin", "haozhun", "08/02/18, 09:16:59 PM", "Fixes #10867.", "NaN"], ["11051", "Add release notes for 0.206", "Haozhun Jin", "haozhun", "07/17/18, 10:32:20 PM", "NaN", "NaN"], ["11053", "Fix TestTime, TestDateTimeFunctions to always test non-legacy timestamp", "Piotr Findeisen", "findepi", "07/18/18, 06:14:03 AM", "When base class was calling `testSessionBuilder()`, it was using default\r\nvalue of `legacy_timestamp` session toggle, which was not appropriate\r\nfor subclasses testing non-legacy semantics.", "NaN"], ["11054", "Collect column statistics on write [v2]", "Andrii Rosa", "arhimondr", "08/02/18, 12:58:42 PM", "Important changes since the version 1:\r\n\r\n- Commits from `Return HiveColumnStatistics from the HiveMetastore interface` to `Move createPartitionValues method to a utility class ` were extracted into a separate PR and merged: #10972\r\n- Added `Implement AutoCloseableCloser` commit.\r\n- SPI bits extracted to `Collect column statistics on table write: SPI`\r\n- `getInsertStatisticsMetadata ` and `getNewTableStatisticsMetadata ` method were merged into the single `getStatisticsCollectionMetadata ` method\r\n- `AggregationOperator` integration with the `TableWriterOperator` has changed\r\n- `ExtendedHiveMetastore#isColumnStatistitcsSupported()` replaced with the `ExtendedHiveMetastore#getSupportedColumnStatistics`. That eliminates a need of `CollectibleStatisticsProvider`. Commit: `Replace supportsColumnStatistics with getSupportedColumnStatistics`\r\n- `ENABLED_FOR_MARKED_TABLES ` option has been removed as well as a related table property.\r\n- `Migrate column statistics on drop and rename column` commit has been dropped\r\n- Added `Collect column statistics on table write: Smoke Tests` commit", "NaN"], ["11060", " Remove references to session from PropertyMetadata", "Piotr Findeisen", "findepi", "07/18/18, 06:13:48 AM", "`PropertyMetadata` is used to declare e.g. table properties, so not\r\nlimited to session properties, as code would previously indicate.", "NaN"], ["11064", "Remove deprecated table access control methods", "Rebecca Schlussel", "rschlussel", "07/18/18, 06:14:32 PM", "NaN", "NaN"], ["11065", "Revert \"Destroy exchange sources for finished tasks on the coordinator\"", "Haozhun Jin", "haozhun", "07/17/18, 08:31:40 PM", "This reverts commit 36a60a6f6e029ddd890d0eb10de01f339eedfebb.\r\n\r\nIt introduced too many coordinator to worker HTTP requests in a bursty manner.\r\nThis leads to communication failures due to\r\n\"Max requests queued per destination 1000 exceeded for HttpDestination\",\r\nThis affects not only the query that is triggering the issue, but also\r\nother queries running at the same time.", "NaN"], ["11067", "Improve Stripe Size Estimation for SliceDictionaryColumnWriter", "Wenlei Xie", "wenleix", "08/07/18, 01:11:32 AM", "When writing dictionary encoded columns, the ORC write estimates\r\nthe buffered size based on (1) estimated index bytes per entry,\r\nand (2) number of entries.\r\n\r\nHowever, this doesn't account for the compression, which can be\r\nvery efficient when the dictionary indices are highly skewed.\r\nFor an extreme example, thinking about dictionary has 1M entires,\r\nyet one value appeared all the time, while all the other (1M - 1)\r\nentires only appeared once.\r\n\r\nWe have observed written stripe size is only 10M ~ 20M when the\r\nestimated stripe size is full at 64MB for real workloads..\r\n\r\nThis commit makes SliceDictionaryColumnWriter also writes the\r\ndictionary index to a temporary data stream, and use the buffered\r\nsize of that stream to estimate stripe size.\r\n\r\nNote the uncompressed dictionary IDs can be too large to store in\r\nmemory. Thus we uncompress and read the dictionary ids from the\r\ntemporary data stream.", "NaN"], ["11072", "Distributed spatial join", "Maria Basmanova", "mbasmanova", "10/16/18, 02:53:32 AM", "Distributed spatial join uses spatial partitioning scheme based on a K-D-B-tree https://en.wikipedia.org/wiki/K-D-B-tree to partition both sides of the join, then processes individual pairs of partitions using R-Tree formed from the geometries in the build partition.\r\n\r\nK-D-B-tree describing the partitioning scheme is provided by the user via `spatial_partitioning_table_name` session property. The value is the name of the table having one row and one column storing the JSON representation of the K-D-B-tree. The name of the column doesn't matter. To create such a table, run a query: \r\n\r\n`CREATE TABLE p AS SELECT spatial_partitioning(ST_GeometryFromText(wkt)) as v FROM t`\r\n\r\n`spatial_partitioning` is a new aggregate function that takes a geometry, builds a KDB tree and returns its JSON representation as a string. The above query will compute minimum bounding box containing all of the geometries and partition it into non-overlapping rectangles using a sample of the data. For each partition, the number of geometries overlapping partition rectangle will be roughly the same. The implementation uses reservoir sampling with maximum sample size of 1M geometries.\r\n\r\nThe presence of the `spatial_partitioning_table_name` session property turns a broadcast spatial join into distributed spatial join. Distributed join is available for inner joins using ST_Contains, ST_Intersects or ST_Distance functions to define join condition.\r\n\r\n```\r\nCREATE TABLE t_partitioning AS \r\nSELECT spatial_partitioning(ST_Point(t.longitude, t.latitude)) as v FROM t;\r\n\r\nSET SESSION spatial_partitioning_table_name = 't_partitioning';\r\n\r\nSELECT * \r\nFROM t, u\r\nWHERE ST_Contains(ST_GeometryFromText(u.wkt), ST_Point(t.longitude, t.latitude))\r\n```\r\n\r\n`TransformSpatialPredicates` optimizer rule reads KDB tree from the table specified in `spatial_partitioning_table_name` session property and plans the distributed spatial join.\r\n\r\n```\r\n- filter (ST_Contains(a, b)) \r\n   - cross-join \r\n      - probe\r\n      - build\r\n```\r\n\r\nbecomes\r\n\r\n```\r\n- spatial_join(ST_Contains(a, b))\r\n  - unnest(pids)\r\n     - project(pids := spatial_partitions(kdbTree, a))\r\n        - probe\r\n  - unnest(pids)\r\n     - project(pids := spatial_partitions(kdbTree, b))\r\n         - build\r\n```\r\n\r\nand \r\n\r\n```\r\n- filter (ST_Distance(a, b) <= r) \r\n   - cross-join \r\n      - probe\r\n      - build\r\n```\r\n\r\nbecomes\r\n\r\n```\r\n- spatial_join(ST_Distance(a, b) <= r)\r\n  - unnest(pids)\r\n     - project(pids := spatial_partitions(kdbTree, a))\r\n        - probe\r\n  - unnest(pids)\r\n     - project(pids := spatial_partitions(kdbTree, b, r))\r\n        - build\r\n```\r\n\r\n<img width=\"914\" alt=\"screen shot 2018-08-09 at 1 15 20 pm\" src=\"https://user-images.githubusercontent.com/27965151/43914606-51a9a904-9bd6-11e8-8050-9ef0030bd24b.png\">\r\n\r\nSince the number of spatial partitions may not match the number of nodes available for the join stage, `AddExchanges` adds hash-based exchanges over partition ID columns. Therefore, a single node may process multiple spatial partitions.\r\n\r\nUnlike hash partitioning, spatial partitioning may place a single row into multiple partitions (e.g. a polygon may intersect multiple partitions) and cause multiple nodes to produce identical pairs of matching geometries. To avoid duplicate results, the spatial join operator uses inline deduplication technique. It works as follows:\r\n\r\n- on each node, a join operator figures a set of spatial partitions it is processing using task ID and a mapping between all spatial partition IDs and corresponding extents: `task_id = hash(spatial_partition_id) % total_tasks`\r\n- when considering a potential match (e.g. a pair of geometries with intersecting bounding boxes), the join operator runs a sequence of checks:\r\n  - check if `probe.spatial_partition_id == build.spatial_partition_id` and skip the match if not;\r\n  - if neither probe nor build geometry is a point, compute the intersection of the bounding boxes and check whether the reference point of that intersection belongs to the spatial partition; the reference point is defined as lower left corner (it can be anything, but must be consistent across all join operators and must belong to the intersection); if the reference point located outside of the spatial partition, skip the potential match; \r\n\r\nThis technique relies on (1) non-overlapping spatial partitions (provided by the K-D-B-tree) and (2) containment check that considers partition extent as a rectangle open on the right and upper sides (e.g. any given point belongs to only one partition).\r\n\r\nThis change is part of #9834", "NaN"], ["11073", "Allow debug in KerberosAuthentication", "Piotr Findeisen", "findepi", "07/23/18, 05:20:04 PM", "Allow debug the same way `KerberosAuthenticator` does.", "NaN"], ["11074", "Make sure to free output buffer memory allocations", "Nezih Yigitbasi", "nezihyigitbasi", "07/18/18, 11:36:50 PM", "This change makes sure to release the allocated buffer memory in\r\nPartitionedOutputOperator and TaskOutputOperator. We introduce a new\r\nmethod OutputBuffer::forceFreeMemory() so that these operators can force\r\nfree that memory.\r\n\r\nAnother alternative considered is to free the memory in\r\nOutputBuffer.destroy/fail methods on task completion. However, that\r\nwould race with driver threads that may be enqueueing pages to the\r\nbuffers resulting in updating closed memory contexts, which will throw\r\nan ISE.", "NaN"], ["11075", "Verify aggregated context is not closed in updateBytes", "Nezih Yigitbasi", "nezihyigitbasi", "07/23/18, 05:15:16 PM", "NaN", "NaN"], ["11083", "Fix approx_distinct(NULL)", "Jiexi Lin", "jessesleeping", "07/20/18, 09:25:53 AM", "Explicitly support `UNKNOWN` type in approximate count distinct\r\nimplementation.", "NaN"], ["11088", "Kafka connector enhancements and cleanup", "Grzegorz Kokosi\u0144ski", "kokosing", "07/23/18, 09:10:04 AM", "NaN", "NaN"], ["11092", "Canonicalize ValuesNode as it may contain correlated symbols", "Grzegorz Kokosi\u0144ski", "kokosing", "07/21/18, 04:44:15 AM", "Canonicalize ValuesNode as it may contain correlated symbols\n\nFixes #11047", "NaN"], ["11097", "Optimizing ValidateDependenciesChecker for JoinNode", "Praveen Krishna", "Praveen2112", "08/02/18, 07:41:56 AM", "The planning time of presto is bit high when compared with the previous presto version (Issue : #11024). So we profiled and figured out that `ValidateDependenciesChecker.java` is taking more to validate a IntermediatePlan. This PR optimizes `ValidateDependenciesChecker.java` so that the planning time can be reduced for queries. \r\n\r\n**Benchmark for initial version**\r\nBenchmark (iterativeOptimizerEnabled) (stage) Mode Cnt Score Error Units\r\nBenchmarkPlanner.planQueries true optimized avgt 20 431.245 \u00b1 15.984 ms/op\r\nBenchmarkPlanner.planQueries true created avgt 20 142.711 \u00b1 2.069 ms/op\r\nBenchmarkPlanner.planQueries false optimized avgt 20 437.861 \u00b1 27.142 ms/op\r\nBenchmarkPlanner.planQueries false created avgt 20 142.856 \u00b1 2.397 ms/op\r\n\r\n**Benchmark after optimizing the `JoinNode` validation**\r\nBenchmark (iterativeOptimizerEnabled) (stage) Mode Cnt Score Error Units\r\nBenchmarkPlanner.planQueries true optimized avgt 20 313.418 \u00b1 16.652 ms/op\r\nBenchmarkPlanner.planQueries true created avgt 20 14.891 \u00b1 1.257 ms/op\r\nBenchmarkPlanner.planQueries false optimized avgt 20 298.709 \u00b1 11.603 ms/op\r\nBenchmarkPlanner.planQueries false created avgt 20 14.491 \u00b1 0.460 ms/op", "NaN"], ["11098", "Disallow closing of memory contexts managed by OperatorContext", "Karol Sobczak", "sopel39", "07/20/18, 08:02:46 PM", "NaN", "NaN"], ["11100", "Disable aggregation pushdown through outer join if aggregation contai\u2026", "Atri Sharma", "atris", "08/02/18, 07:44:58 AM", "\u2026ns probe symbols\r\n\r\nFixes #10724", "NaN"], ["11101", "Implement approx_distinct for CHAR type", "Andrii Rosa", "arhimondr", "07/20/18, 07:53:49 PM", "NaN", "NaN"], ["11103", "Fix alter table drop column in raptor", "Yi He", "hellium01", "08/02/18, 06:00:49 PM", "When we delete part of shard that contains dropped column, we will have\r\nNPE because a mismatch between orc file and metadata.", "NaN"], ["11104", "Cleanup legacy handling for TRY", "Wenlei Xie", "wenleix", "08/02/18, 11:13:14 PM", "TRY expression used to be handled in a special way similar to lambda\r\nexpressions. #8889 remove this special handling by making TRY expression\r\na syntactic sugar using lambda.\r\n\r\nHowever we are still keeping the name `LambdaAndTryExpression`\r\nin many places, and keeps the `tryMethodMap` although it is never used.\r\n\r\nThis commit cleanup these legacy handling.\r", "NaN"], ["11106", "Disable testPartitionStatisticsSampling for Glue metastore tests", "Grzegorz Kokosi\u0144ski", "kokosing", "07/23/18, 11:04:50 AM", "Disable testPartitionStatisticsSampling for Glue metastore tests", "NaN"], ["11108", "Avro support in Kafka connector", "Anu Sudarsan", "anusudarsan", "08/16/18, 02:39:37 PM", "NaN", "NaN"], ["11113", "Make ClusterMemoryManager resilient to memory accounting leaks", "Nezih Yigitbasi", "nezihyigitbasi", "08/02/18, 05:29:08 PM", "We have seen that if a finished query has non-zero memory reservation\r\non some worker (e.g., \"leak\") it will effectively disable the oom killer\r\nas the lastKilledQuery will never be gone from the system\r\n(isLastKilledQueryGone will return false). This change makes the\r\nClusterMemoryManager resilient to such memory leaks.\r\n\r\nThe change addds a ClusterMemoryLeakDetector and updates\r\nClusterMemoryManager to use the leak detector to determine whether the\r\nlastKilledQuery has a leak, and if it has the ClusterMemoryManager\r\nconsiders the lastKilledQuery as gone from the system, and continues\r\nfunctioning as usual.", "NaN"], ["11114", "Update product tests config.properties for local environment", "Andrii Rosa", "arhimondr", "07/23/18, 08:47:08 PM", "Add kafka plugin.\r\n\r\nRelated to: https://github.com/prestodb/presto/pull/11088", "NaN"], ["11116", "Change java type of UNKNOWN from void to boolean", "Jiexi Lin", "jessesleeping", "08/03/18, 09:21:10 PM", "Using boolean to represent `UNKNOWN` helps remove all special handling\r\nfor void/Void in functions/operators implementations and bytecode\r\ngenerations.\r\n\r\nThe changes were done by:\r\n- Change `UnknownType.java` and `UnknownOperators.java`, followed by all fixes to pass all tests under `presto-main/src/test/java/com/facebook/presto/type` and `presto-main/src/test/java/com/facebook/presto/operator`\r\n- Search for `void.class` and check if it's related to `UNKNOWN` handling\r\n- Search for `Void.class` and check if it's related\r\n- Search for `public static Void.class` and check if it's return value of certain SQL functions/operators\r\n- Search for `@SqlType\\([^\\)]*\\) Void` and check if it's parameters of certain SQL functions/operators", "NaN"], ["11119", "Append original error message to thrift connector remote exception", "Jiexi Lin", "jessesleeping", "08/02/18, 05:04:39 PM", "This PR append the original error message from the remote server to the generic remote server exception thrown in the thrift connector. In some application (e.g. command line interface), it's convenient to display server side customized error message to user.\r\n\r\nFor example, remote thrift server can fail a query because it expects filters on certain column in all queries. It's not very informative by just saying \"Exception raised by remote server\" in the error message.", "NaN"], ["11121", "Migrate TransformCorrelatedSingleRowSubqueryToProject to rule and allow it to work with multiple projections", "Atri Sharma", "atris", "08/03/18, 08:56:13 AM", "\u2026roject\r\n\r\nThe commit adds a new rule for $subject, and removes the current optimizer\r\nthat performs the task. The new rule works in the same optimizer as InlineProjections\r\nand can deal with duplicate identity projections, allowing subqueries with multiple\r\nprojections to work, something the previous optimizer lacked\r\n\r\nFixes #11026 and supersedes #11099", "NaN"], ["11122", "Deallocate prepared statement on close", "Piotr Findeisen", "findepi", "08/31/18, 01:22:14 PM", "Fixes https://github.com/prestodb/presto/issues/11110", "NaN"], ["11123", "Use names to map struct field to underlying data in ORC", "Shreyas Joshi", "shreyasjoshis", "08/15/18, 02:41:19 AM", "Fixes #11001. A summary for completeness:\r\n\r\nPreviously, presto used the ordinal position of a struct field to map a field to its corresponding data. The method does not work if fields are removed from the struct.\r\n\r\nLet's say we have `struct <a:string, b string, c:string>` that changes to `struct <a:string, c:string>` (Field `b` is dropped). In this case the value of `c` from the old ORC files would be incorrect. It would read in the value of `b` from the underlying ORC files and misrepresent it as the values for `c`.\r\n\r\nWith this change, the old values of `b` will be ignored. `c` will be null if it is missing from an ORC file, or it will have the right value assigned to it.\r", "NaN"], ["11125", "Upgrade to Airlift 0.172", "Nezih Yigitbasi", "nezihyigitbasi", "07/24/18, 11:27:23 PM", "NaN", "NaN"], ["11126", "Fix correctness issue with DISTINCT grouping sets", "Martin Traverso", "martint", "08/30/18, 12:10:12 AM", "Fixes https://github.com/prestodb/presto/issues/11120.\r\n\r", "NaN"], ["11127", "Remove size limit for writing Hive bucketed sorted tables", "David Phillips", "electrum", "08/02/18, 03:08:09 PM", "NaN", "NaN"], ["11130", "Move discovery server to coordinator module", "David Phillips", "electrum", "08/13/18, 08:25:20 PM", "NaN", "NaN"], ["11132", "Update units to 1.3", "Nezih Yigitbasi", "nezihyigitbasi", "08/02/18, 05:28:18 PM", "NaN", "NaN"], ["11133", "Skip currentConstraint field when serializing TableScanNode", "Haozhun Jin", "haozhun", "08/02/18, 08:25:24 PM", "currentConstraint can be pretty complex. As a result, it may incur\r\na significant cost to serialize, to hold in memory as a JSON,\r\nand to transport over network.", "NaN"], ["11136", "Remove duplicate method", "Piotr Findeisen", "findepi", "08/11/18, 07:55:46 PM", "NaN", "NaN"], ["11137", "Report SemanticException for invalid TIME literal", "Piotr Findeisen", "findepi", "08/11/18, 08:09:19 PM", "NaN", "NaN"], ["11138", "Enable history for presto-cli.sh", "Grzegorz Kokosi\u0144ski", "kokosing", "08/02/18, 07:38:20 AM", "Enable history for presto-cli.sh\n\n./conf/docker/<env>/compose.sh run application-runner /docker/volumes/conf/docker/files/presto-cli.sh\n\nAbove command executes presto-cli which is capable to connect to product\ntest Presto server in given test environment. However notice that it\nalways executes new docker container for that process. Because of that\nit does not maintain history between sessions.\n\nThanks to this commit history will be saved.", "NaN"], ["11139", "Expose debug port for Hadoop processes in product tests", "Grzegorz Kokosi\u0144ski", "kokosing", "08/02/18, 07:38:08 AM", "Expose debug port for Hadoop processes in product tests\n\nThanks to this change it is a bit easier (one place less to change) when\nenabling debugging for any process running on hadoop-master container.\n\nNotice that we already expose 5005 on presto-master to debug Presto\nserver.", "NaN"], ["11141", "Move tagged memory allocations on memory pool update", "Nezih Yigitbasi", "nezihyigitbasi", "07/26/18, 10:04:59 PM", "When a query is moved from one pool to another we need to also move\r\nthe tagged memory allocations properly to the new memory pool.\r\n\r\nFollow up for #11011", "NaN"], ["11142", "Minor cleanup", "Nezih Yigitbasi", "nezihyigitbasi", "08/02/18, 05:27:58 PM", "NaN", "NaN"], ["11144", "Make possible to overwrite default history path with env variable", "Grzegorz Kokosi\u0144ski", "kokosing", "08/02/18, 07:37:53 AM", "Make possible to overwrite default history path with env variable\n\nBy setting PRESTO_CLI_HISTORY_FILE user can overwrite the path where\npresto-cli history commands are stored.", "NaN"], ["11145", "Remove unnecessary Lookup::resolve usage", "Grzegorz Kokosi\u0144ski", "kokosing", "08/02/18, 07:36:49 AM", "Remove unnecessary Lookup::resolve usage", "NaN"], ["11146", "Fix cli history initialization", "Nezih Yigitbasi", "nezihyigitbasi", "08/02/18, 05:27:09 PM", "Previously, cli would just use the MemoryHistory if the history file\r\nis not readable/writeable. However, that breaks the case where the cli\r\nis freshly installed and there is no .presto_history file.", "NaN"], ["11147", "Update documentation about link to query's JSON", "Piotr Findeisen", "findepi", "08/14/18, 06:49:25 AM", "NaN", "NaN"], ["11149", " Remove Optional from tagged memory allocation support API", "Nezih Yigitbasi", "nezihyigitbasi", "08/09/18, 09:04:20 PM", "Follow up for https://github.com/prestodb/presto/pull/11011 cleaning the API.", "NaN"], ["11150", "Add internal aggregations over estimated in-memory data size for stats", "Andrii Rosa", "arhimondr", "08/03/18, 12:44:51 AM", "These aggregation are needed for `MAX_VALUE_SIZE` and `TOTAL_VALUES_SIZE` statistics computation. The PR that adds this 2 statistics will be opened once the #11054 and the #11107 are merged.", "NaN"], ["11151", "Add release notes for 0.207", "Raghav Sethi", "raghavsethi", "08/01/18, 07:10:46 PM", "Closes https://github.com/prestodb/presto/issues/11068", "NaN"], ["11153", "Optimizing DependenciesChecker for TableScanNode", "Praveen Krishna", "Praveen2112", "08/02/18, 07:39:20 AM", "This PR skips the dependencies check performed for a `TableScanNode` , because `TableScanNode` has no dependencies. ", "NaN"], ["11157", "Upgrade to esri-geometry-api version 2.2.0", "Timothy Meehan", "tdcmeehan", "08/03/18, 06:16:33 PM", "Updating the esri-geometry-api to leverage better support for geometry collections.", "NaN"], ["11158", "Migrate Web UI to new javascript workflow + improvements", "Raghav Sethi", "raghavsethi", "10/18/18, 05:28:28 PM", "Fixes #11337\r\nFixes #11679", "NaN"], ["11162", "Rename values for join distribution type property", "Raghav Sethi", "raghavsethi", "08/01/18, 07:09:36 PM", "NaN", "NaN"], ["11163", "Fix bottleneck in InternalResourceGroup::internalGenerateCpuQuota", "Nezih Yigitbasi", "nezihyigitbasi", "08/06/18, 08:32:50 PM", "internalGenerateCpuQuota() is called from\r\nInternalResourceGroupManager::refreshAndStartQueries(), which is called\r\nevery millisecond to start queries. Under a high query submission rate\r\ninternalGenerateCpuQuota() becomes a bottleneck when Math.multiplyExact()\r\nthrows an exception (which is possible with the default value of\r\ncpuQuotaGenerationMillisPerSecond) while holding the monitor of the root\r\nresource group. Plenty of methods in the query starting path grabs the\r\nmonitor of the root resource group, and since internalGenerateCpuQuota()\r\nlocks the root for extended amount of time (due to filling in the stack\r\ntraces when exceptions are thrown) the rate at which the coordinator\r\nstarts queries drops significantly. This change fixes that by using the\r\nguava math methods, which don't throw exceptions on overflow.\r\n\r\nIn a performance test before the fix, given a config that allows 200\r\nconcurrent queries the coordinator was only able to run < 50 queries\r\nin the cluster. After this fix it was able to saturate the entire cluster.", "NaN"], ["11171", "Add the geometry_union_agg spatial aggregation function", "Timothy Meehan", "tdcmeehan", "08/09/18, 03:13:07 PM", "This change will add aggregated form of scalar ST_Union, which will iteratively union the input geometries.  For single geometries it will not simplify collections or multi types.\r\n\r\nThis change is marked as WIP to allow for early feedback, however it has a dependency on #11157 to allow for safe unions of geometry collections, and other bug fixes.", "NaN"], ["11172", "Add SpookyHashV2 hash functions", "Martin Traverso", "martint", "08/02/18, 06:39:48 PM", "NaN", "NaN"], ["11175", "Remove TableScanNode originalConstraint", "Grzegorz Kokosi\u0144ski", "kokosing", "08/22/18, 11:02:01 AM", "Remove TableScanNode originalConstraint\n\nOriginal constraint was just a hint. Its aim was to tell to the user\nwas predicate was pushed down to the connector.\nHowever it was not uncommon to have incorrect information. For example:\n - when connector was supporting only part of predicate\n originalConstraint was storing whole predicate\n - when multiple iterations were performed to make predicate more\n specific only, the information about the first predicate was stored", "NaN"], ["11176", "Pull dataSize statistics from Hive for VARCHAR columns", "Andrii Rosa", "arhimondr", "08/02/18, 03:12:43 PM", "Supersedes https://github.com/prestodb/presto/pull/11107", "NaN"], ["11178", "Add bing_tiles_around variant taking radius in kilometers", null, "avinashidnani", "08/16/18, 11:48:34 PM", "Note: Not sure if we want to restrict the radiusInKm to a MAX value (large radius compared to earth_radius could cause issues in the calcs)", "NaN"], ["11179", "Update to Airbase 83", "David Phillips", "electrum", "08/13/18, 07:50:19 PM", "NaN", "NaN"], ["11180", "Log the leaked queries at DEBUG level", "Nezih Yigitbasi", "nezihyigitbasi", "08/03/18, 04:14:54 AM", "NaN", "NaN"], ["11181", "Add FunctionInvokerProvider for requesting invocation convention", "Gerlou Shyy", "gerlou", "08/13/18, 11:44:15 PM", "Make it possible to request a specific invocation convention when asking for a MethodHandle of a function signature.", "NaN"], ["11182", "Move tagged memory allocations on memory pool update", "Nezih Yigitbasi", "nezihyigitbasi", "08/09/18, 06:33:47 PM", "This change fixes the LegacyQueryContext. ebc86e3 already fixes the issue\r\nfor the DefaultQueryContext.\r\n\r\nFollow up for https://github.com/prestodb/presto/pull/11141", "NaN"], ["11183", "Make sure to close scheduled executor in various tests", "Nezih Yigitbasi", "nezihyigitbasi", "08/03/18, 09:07:22 PM", "NaN", "NaN"], ["11185", "Collect data size statistic ", "Andrii Rosa", "arhimondr", "08/06/18, 02:56:46 PM", "In the Hive connector for CHAR, VARCHAR, VARBINARY columns.", "NaN"], ["11191", "Make sure to free output buffer memory on task completion", "Nezih Yigitbasi", "nezihyigitbasi", "08/07/18, 02:55:00 AM", "Follow up for https://github.com/prestodb/presto/pull/11188. We now close the output buffer memory manager on task completion, and ignore any subsequent calls.", "NaN"], ["11192", "correcting documentation for percent_rank window function return type", null, "phcnq", "08/03/18, 11:41:39 PM", "NaN", "NaN"], ["11194", "Remove unnecessary copies in PruneUnreferencedOutputs", "Praveen Krishna", "Praveen2112", "08/09/18, 05:47:30 AM", "Currently in `PruneUnreferencedOutputs` optimizer, when we are processing a `Project` node\r\n\r\n```\r\nfor (int i = 0; i < node.getOutputSymbols().size(); i++) {\r\n    Symbol output = node.getOutputSymbols().get(i);\r\n    Expression expression = node.getAssignments().get(output);\r\n```\r\nthis can be replaced by a `Map.Entry` ", "NaN"], ["11196", "Optimizing replaceChildren for JoinNode", "Praveen Krishna", "Praveen2112", "09/07/18, 06:08:14 AM", "When we execute `replaceChildren` for a `JoinNode` \r\n\r\n```\r\nList<Symbol> newOutputSymbols = Stream.concat(newLeft.getOutputSymbols().stream(), newRight.getOutputSymbols().stream())\r\n                .filter(outputSymbols::contains)\r\n```\r\n\r\nTime taken for executing `outputSymbols#contains` increases as the list size increases, so we can we replace that with a `HashSet` ", "NaN"], ["11197", "Do not create transaction manager on workers", "Dain Sundstrom", "dain", "08/09/18, 06:30:29 PM", "NaN", "NaN"], ["11198", "Cleanup binding of failure detector", "David Phillips", "electrum", "08/13/18, 08:09:36 PM", "Move binding to `CoordinatorModule` and reuse `NoOpFailureDetector` on workers.", "NaN"], ["11199", "Remove unused statement resource thread pools from workers", "David Phillips", "electrum", "08/13/18, 08:05:57 PM", "The statement resource is only bound on the coordinator.", "NaN"], ["11200", "Only bind stats calculator on coordinator", "David Phillips", "electrum", "08/13/18, 07:57:10 PM", "NaN", "NaN"], ["11201", "Only bind plan optimizers on coordinator", "David Phillips", "electrum", "08/05/18, 10:14:37 PM", "NaN", "NaN"], ["11207", "Add missing word in \"too many stages\" error code", "David Phillips", "electrum", "08/06/18, 04:37:53 PM", "NaN", "NaN"], ["11209", "Minor cleanup & refactor", "Nezih Yigitbasi", "nezihyigitbasi", "08/06/18, 08:39:20 PM", "NaN", "NaN"], ["11211", "Fix explain analyze for index join", "Yi He", "hellium01", "09/14/18, 12:18:20 PM", "Currently if we run explain analyze on queries that uses index join, we will fail with null pointer error. IndexBuild has input/output stats populated, we just need to mark it in the PipelineContext to allow PlanNodeStatsSummarizer to pick it up. ", "NaN"], ["11212", " Fix handling of Object type in annotation-based scalar", "Haozhun Jin", "haozhun", "08/15/18, 09:45:49 PM", "NaN", "NaN"], ["11214", "Statistics collection fixes and optimizations", "Andrii Rosa", "arhimondr", "08/07/18, 09:10:53 PM", "NaN", "NaN"], ["11215", "Fix NPE in MemoryPool::getInfo()", "Nezih Yigitbasi", "nezihyigitbasi", "08/07/18, 07:48:12 PM", "```\r\njava.lang.NullPointerException\r\n        at com.facebook.presto.memory.MemoryPool.lambda$getInfo$1(MemoryPool.java:87)\r\n        at java.base/java.util.HashMap.forEach(HashMap.java:1341)\r\n        at com.facebook.presto.memory.MemoryPool.getInfo(MemoryPool.java:85)\r\n        at com.facebook.presto.memory.LocalMemoryManager.getInfo(LocalMemoryManager.java:109)\r\n        at com.facebook.presto.memory.MemoryResource.getMemoryInfo(MemoryResource.java:54)\r\n        at jdk.internal.reflect.GeneratedMethodAccessor248.invoke(Unknown Source)\r\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n```", "NaN"], ["11217", "Add 0.208 releasse notes", "Dain Sundstrom", "dain", "08/07/18, 10:07:00 PM", "NaN", "NaN"], ["11218", "Fix memory accounting leak in AggregationOperator", "Nezih Yigitbasi", "nezihyigitbasi", "08/08/18, 08:45:33 PM", "If an AggregationOperator is used inside a TableWriterOperator\r\nfor stats collection the AggregationOperator will leak, as the driver\r\nthread will only destroy the TableWriterOperator, and operators\r\nrely on getting destroyed by the driver threads to free up their\r\nallocations.\r\n\r\nThis change also closes the memory context of TableWriterOperator\r\nin close(), however that's just for testing as the memory reserved\r\nby that context will be released when the driver thread destroys its\r\noperator context.", "NaN"], ["11222", "Don't poll/update the coordinator memory pools if not necessary", "Nezih Yigitbasi", "nezihyigitbasi", "08/09/18, 05:17:53 PM", "If work isn't scheduled on the coordinator, there is no point in\r\npolling or updating its memory pools (which happens every 1s).", "NaN"], ["11224", "Update 0.208 release notes", "Dain Sundstrom", "dain", "08/08/18, 08:49:13 PM", "NaN", "NaN"], ["11225", "Fix statistics planner", "Andrii Rosa", "arhimondr", "08/08/18, 10:49:55 PM", "NaN", "NaN"], ["11230", "Remove redundent is decimal check", "Karol Sobczak", "sopel39", "08/09/18, 12:50:35 PM", "Check for decimal type is already performed\r\nat: com/facebook/presto/rcfile/RcFileEncoding.java:89", "NaN"], ["11234", "Fix information schema predicate pushdown for views", "Nezih Yigitbasi", "nezihyigitbasi", "08/10/18, 09:55:23 PM", "For queries of the form:\r\n\r\n```\r\nSELECT view_definition\r\nFROM information_schema.views\r\nWHERE table_catalog = 'catalog' AND\r\ntable_schema = 'schema_name'\r\nAND table_name = 'view_name';\r\n```\r\n\r\nthat specifies both the schema name and view name, the current\r\nimplementation calculates the prefix \"schema_name.*\" to get\r\nall the views under the given schema. However, it should calculate the\r\nprefix \"schema_name.view_name\" to get only the specified view.\r\nThis change fixes that.\r\n\r\n@kokosing `isTablesEnumeratingTable()` method returns false when we query the `views` table. Is there a reason why we don't include `views` in that method? That contradicts with the `calculatePrefixesWithTableName` method where we also call `metadata.getView()`. Including the `views` table in that method fixes the problem.", "NaN"], ["11235", "Add SpatialJoinNode", "Maria Basmanova", "mbasmanova", "09/21/18, 03:21:54 PM", "Add a special node to represent spatial join to accommodate distributed spatial join #11072 that needs to add 3 more spatial join-specific properties (KDB tree and partition ID symbols for build and probe).", "NaN"], ["11236", "Refactor BooleanInputStream", "Ying", "yingsu00", "08/13/18, 07:10:46 PM", "Refactor BooleanInputStream", "NaN"], ["11237", "Store column statistics as part of addPartition", "Andrii Rosa", "arhimondr", "09/06/18, 12:37:03 PM", "Pass the statistics as a parameter", "NaN"], ["11238", "Fix the race between close and destroy in the Driver", "Nezih Yigitbasi", "nezihyigitbasi", "08/10/18, 09:54:38 PM", "There is a race condition where a driver thread may not destroy the\r\noperators even though it's closed. This is not desirable as there are\r\nparts of the code that rely on the operators to be destroyed by the\r\ndriver, e.g., memory tracking related code.\r\n\r\nThe race occurs when a driver thread T1 is in the tryWithLock method and\r\nholds the exclusiveLock, and it has already called the\r\ndestroyIfNecessary() method. At this point T1 hasn't destroyed the operators\r\nyet as the driver hasn't been closed. Now if the task owning those splits\r\ngets aborted (say due to a LIMIT query), another thread T2 will call\r\ndriver.close(), and in close() it will try to acquire the lock to destroy the\r\noperators, but T1 still holds that lock. Then, T1 releases the lock and checks\r\nthe condition `while (pendingTaskSourceUpdates.get() != null\r\n&& state.get() == State.ALIVE && exclusiveLock.tryLock())`, and this condition\r\nis false as the state is NEED_DESTRUCTION (as the driver is closed by T2). At this\r\npoint T1 just exits without destroying the operators.", "NaN"], ["11240", "Add UnknownType.writeBoolean because some logic relies on it ", "Haozhun Jin", "haozhun", "08/10/18, 12:34:22 AM", "NaN", "NaN"], ["11246", "Log queryId when a small stripe get MAX_BYTES flush", "Wenlei Xie", "wenleix", "08/10/18, 11:06:52 PM", "NaN", "NaN"], ["11248", "Add MockConnectorFactory.Builder to simplify construction", "Nezih Yigitbasi", "nezihyigitbasi", "08/16/18, 09:57:36 PM", "This is a minor followup for https://github.com/prestodb/presto/pull/11234", "NaN"], ["11249", "Add Hive connector procedure call to create empty partition", "Wenlei Xie", "wenleix", "08/24/18, 01:12:38 AM", "NaN", "NaN"], ["11255", "Fix formatting in PushAggregationThroughOuterJoin", "David Phillips", "electrum", "08/13/18, 07:40:18 PM", "NaN", "NaN"], ["11260", "Rename pre-existing FunctionInvoker to InterpretedFunctionInvoker", "Gerlou Shyy", "gerlou", "08/13/18, 11:54:51 PM", "NaN", "NaN"], ["11261", "Fix delay in sending noMoreSplits for scan node in colocated join", "Haozhun Jin", "haozhun", "08/15/18, 09:57:47 PM", "When a fragment/stage contains multiple scan nodes. None of the scan nodes\r\nwill receive a TaskSource where TaskSource.noMoreSplits = true until\r\nscheduling for splits for all the scan nodes in the stage finishes.\r\nInstead, each scan node should receive noMoreSplits indepedently.\r\n\r\nThe bug affects scheduling for fragment/stage with more than one scan nodes.\r\nOnly colocated join can create such fragment/stage at this time.\r\n\r\nThis fixes bug 1 in #11253.", "NaN"], ["11262", "Merge partition preference when adding exchange nodes", "Jiexi Lin", "jessesleeping", "12/19/19, 04:06:26 AM", "---\r\nPreviously we only used merged partition preference when adding exchange for union. This commit allow us to merge partitioning preference when adding exchange for aggregation, window function and index join.\r\n\r\nBy merging partition preference we have chance to reduce data repartitions but may also result in increased skewness. We add a session property to control this behavior. \r\n\r\n---\r\n## Current `AddExchange` behavior\r\nWhen planning `Aggregation`, `MarkDistinct`, `RowNumber` and `TopNRowNumber` nodes, the current logic do the following:\r\n1. Calculate a `PreferredProperties` which merge the partition preference of the current node and its parent. (e.g. parent requires partitioning on `(col_A)`, current node requires partitioning on `(col_A, col_B)` --> merged result is partitioning on `(col_A)`). \r\n2. Recursively call rewriter on child node, with the merged `PreferredProperties` (partition on `(col_A)`).\r\n3. Check if the returned partitioning property from child plan. If it's not what the current node requires (`(col_A, col_B)`), add a remote exchange on the current requirement (`(col_A, col_B)`). \r\n\r\nThis can result in adding extra remote exchange for certain query shape. For example:\r\n\r\n> Take the following aggregation query as an example:\r\n> \r\n> ```\r\n> SELECT COUNT(orderdate), custkey \r\n> FROM (\r\n>     SELECT orderdate, custkey \r\n>     FROM orders \r\n>     GROUP BY orderdate, custkey\r\n> )\r\n> GROUP BY custkey\r\n> ```\r\n> \r\n> The previous plan has 2 repartition exchanges, partitioning on group by columns of the inner query and the outer query respectively:\r\n> \r\n> ```\r\n> OutputNode\r\n>     Aggregation (final, group by custkey)\r\n>         Exchange (repartition on custkey)        // Second exchange on custkey\r\n>             Aggregation (partial, group by custkey)\r\n>                 Aggregation (final, group by custkey, orderdate)               \r\n>                    Exchange (repartition on custkey, orderdate)        // First exchange on custkey and orderdate\r\n>                         Aggregation (partial, group by custkey, orderdate)\r\n>                             TableScan (orders)\r\n> ```\r\n\r\n## What this PR changes\r\nThe major confusion is between (1) passing the merged preference when planning child and (2) add exchange to the child plan based on unmerged preference. This PR added a session property to control this behavior **only for Aggregation nodes**. \r\n- With `aggregation_partitioning_merging_strategy` set to `LEGACY`, we preserve the previous behavior where we (1) pass the merged preference when planning child and (2) add exchange based on local preference.\r\n- With `aggregation_partitioning_merging_strategy` set to `TOP_DOWN`, we (1) pass the merged preference when planning child and (2) add exchange based on the merged preference.\r\n- With `aggregation_partitioning_merging_strategy` set to `BOTTOM_UP`, we (1) don't merge preference and (2) add exchange based on local preference. \r\n\r\nFor example, with `partition_merging_strategy` set to `TOP_DOWN`, the plan of the above example only has 1 remote exchange:\r\n\r\n> ```\r\n> OutputNode\r\n>     Aggregation (single, group by custkey)\r\n>         Aggregation (final, group by custkey, orderdate)               \r\n>             Exchange (repartition on custkey)        // Here is the merged exchange\r\n>                 Aggregation (partial, group by custkey, orderdate)\r\n>                     TableScan (orders)\r\n> ```\r\n\r\nNote that less remote exchange is not necessary always the optimal, it depends on the cardinality of partition columns. \r\n\r\n## Open discussions\r\n- ~~With the current approach, we don't have an option to return to our previous behavior. Maybe we should add 2 flag, one is for using merged partition preference when planning child, the other one is for using merged partition preference when adding exchange for current node.~~ \r\n  - update: Added an enum property to control the partition preference merging strategy. There are currently 3 options: `LEGACY`, `TOP_DOWN` and `BOTTOM_UP`. \r\n\r\n- ~~Open discussion before about controlling merging other properties and also top-down vs bottom-up partition preference (https://github.com/prestodb/presto/pull/11262#issuecomment-413796437).~~\r\n\r\n- `IndexJoin` and `Union` is handled differently in the current logic. They handle parent partition preference on their own.\r\n\r\n## Todo\r\n- Add detail commit message before merging. \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add new session property \"aggregation_partitioning_merging_strategy\" to control merging partitioning preference when adding repartition remote exchange around aggregation node. Default option is `LEGACY` and can be overwritten to `TOP_DOWN` or `BOTTOM_UP`.\r\n```", "NaN"], ["11264", "Determine grouped execution on an individual scan node basis ", "Haozhun Jin", "haozhun", "08/19/18, 07:14:13 AM", "Depends on #11261. Please skip the first commit in this PR during review.\r\n\r\nPreviously, grouped execution was determined on a per-fragment basis. This lead to two issues:\r\n\r\n* LocalExecutionPlanner has been making additional determinations about planning that conceptually should happen in planner on the coordinator.\r\n* Take a query with a plan like the following (letter represents whether it be executed in a grouped manner. This plan requires different execution strategy on an individual scan node basis for both SourceScheduler and LocalExecutionPlanner. Therefore, the determination has to happen on the coordinator.\r\n\r\n```\r\n                    J\r\n                   / \\\r\n                  J   TS\r\n                 / \\  (y)\r\n               TS   J\r\n              (y)  / \\\r\n                 TS   TS\r\n                 (y)  (n)", "NaN"], ["11266", "Remove StreamReader local buffers and performance improvement", "Ying", "yingsu00", "09/18/18, 08:59:38 AM", "We used to use local buffers in the StreamReader's to temporarily hold\r\ndata temporarily read from input streams, e.g. the isNullVector and\r\nthe inDictionaryVector. The memory used by these local buffers is\r\nnot neglegible because there could be many StreamReader objects on a\r\nworker node. These buffers could be eliminated by the following\r\nchanges:\r\n\r\n1) When the underlying input streams are null, we don't need to keep\r\nthe buffers;\r\n2) Reuse each value in these buffers when processing the data across\r\nmultiple streams.\r\n\r\nIt helps to improve the performance because it eliminates multiple\r\nrepeating data filtering (branching) on multiple input streams. It\r\nalso simplifies the code by removing the subBatchSize logic.\r\n\r\nBelow is the JMH microbenchmark results on reading 10M rows of\r\ndifferent types, with and without nulls. The unit is s/op. Each test\r\nwas ran on 3 forks, and 20 warm up runs and 20 real runs. It shows\r\nperformance improvements for most StreamReader's. The few regressed\r\nones were mostly run to run variations and not real regressions.\r\n\r\n                               | Before| var | After | var |Improvement\r\n    readBooleanNoNull          | 0.029 |0.001| 0.028 |0.001| 3%\r\n    readBooleanWithNull        | 0.125 |0.001| 0.102 |0.001|18%\r\n    readByteNoNull             | 0.035 |0.001| 0.033 |0.001| 6%\r\n    readByteWithNull           | 0.108 |0.002| 0.115 |0.001|-6%\r\n    readDecimalNoNull          | 0.505 |0.008| 0.510 |0.019|-1%\r\n    readDecimalWithNull        | 0.400 |0.008| 0.362 |0.019|10%\r\n    readDoubleNoNull           | 0.253 |0.004| 0.259 |0.004|-2%\r\n    readDoubleWithNull         | 0.215 |0.003| 0.204 |0.008| 5%\r\n    readFloatNoNull            | 0.246 |0.004| 0.239 |0.003| 3%\r\n    readFloatWithNull          | 0.211 |0.003| 0.203 |0.002| 4%\r\n    readLongDirectNoNull       | 0.071 |0.002| 0.071 |0.001| 0%\r\n    readLongDirectWithNull     | 0.138 |0.003| 0.120 |0.002|13%\r\n    readSliceDictionaryNoNull  | 0.046 |0.001| 0.034 |0.001|26%\r\n    readSliceDictionaryWithNull| 0.146 |0.002| 0.108 |0.002|26%\r\n    readTimestampNoNull        | 0.242 |0.007| 0.200 |0.003|17%\r\n    readTimestampWithNull      | 0.289 |0.006| 0.185 |0.003|36%\r", "NaN"], ["11268", "Propagate stats and cost in distributed explain plans", "Rebecca Schlussel", "rschlussel", "08/27/18, 02:51:20 PM", "fixes #10963 ", "NaN"], ["11269", "Move BenchmarkSpatialJoin to presto-geospatial", "Maria Basmanova", "mbasmanova", "08/16/18, 08:21:41 PM", "Remove presto-benchmark -> presto-geospatial dependency by updating\r\nBenchmarkSpatialJoin to use LocalQueryRunner and moving it to presto-\r\ngeospatial. This will allow distributed spatial join tests to use Hive\r\nconnector.\r\n\r\nSee #11072", "NaN"], ["11273", "Fix EmptySplit handling during bucket-by-bucket execution", "Haozhun Jin", "haozhun", "08/16/18, 05:56:44 PM", "NaN", "NaN"], ["11274", "Optimize multimap_agg with flattened group state", "Timothy Meehan", "tdcmeehan", "09/14/18, 09:19:09 PM", "Current multimap_agg implementation uses MultiKeyValuePairsState to store the aggregated key to list values.  This implementation may involve heavy object allocation and GC overhead, as it uses two BlockBuilders to store the key values per group.  This commit changes the default behavior to use a modified ArrayAggregationState, which flattens the groups and interleaves them as a linked list.  The result should be a reduction in object allocations and therefore reduced GC overhead.\r\n\r\nThe changes are comprised of three parts:\r\n\r\n1) Modification to ArrayAggregationState to be used as a unary and binary state, meaning it could support two types in the array.  This is reflected as a new type, which has iteration semantics that iterate over both values, and add semantics which take in both values.  That way it's obvious from the type itself if we are using ArrayAggregationState with one value or two.\r\n2) Extracting out the logic to serialize key value pairs as a multimap into its own utility, so it can be shared by both the legacy code and the new code.\r\n3) Slight code changes to gate this new feature via a configuration, and for the aggregation function itself to read this value and to switch between the old and the new behavior.\r\n\r\nCC: @wenleix ", "NaN"], ["11275", "Add convex_hull_agg function", "Yongyang Yu", "yuyongyang800", "08/16/18, 12:29:52 AM", "convex_hull_agg aggregate function computes convex hull of all input geometries.", "NaN"], ["11281", "Remove running and queue time limit from resource groups and add file based session property manager", "Raghav Sethi", "raghavsethi", "09/08/18, 03:15:24 AM", "Among other things, fixes https://github.com/prestodb/presto/issues/11277", "NaN"], ["11282", "Use catalog field instead of hardcoded catalog", "Shixuan Fan", "shixuan-fan", "08/15/18, 09:12:51 PM", "NaN", "NaN"], ["11284", "Document geometry_union_agg function", "Timothy Meehan", "tdcmeehan", "08/16/18, 03:27:48 PM", "This line of documentation was missing in the original pull request (#11171).\r\nThis commit just adds the appropriate external documentation.\r\n\r\nCC: @mbasmanova ", "NaN"], ["11286", "Remove CoefficientBasedStatsCalculator", "Grzegorz Kokosi\u0144ski", "kokosing", "08/18/18, 07:47:50 PM", "Remove CoefficientBasedStatsCalculator", "NaN"], ["11287", "Ensure global stream partitioning contains node partitioning columns", "Karol Sobczak", "sopel39", "08/30/18, 11:46:14 AM", "Streams global partitioning shouldn't be contradictionary to node\r\npartitioning. Otherwise invalid AddExchanges decisions could be made.", "NaN"], ["11289", "Support Avro schema evolution in Hive connector", "Anu Sudarsan", "anusudarsan", "08/31/18, 09:29:15 AM", "NaN", "NaN"], ["11290", "Respect X-Forwarded-Proto header in / endpoint", "Gaurang Shah", "Gaurang033", "08/20/18, 07:27:53 AM", "Fix for bug #11168. ", "NaN"], ["11291", "Update docs with new memory-related configs", "Nezih Yigitbasi", "nezihyigitbasi", "08/16/18, 10:52:27 PM", "supersedes https://github.com/prestodb/presto/pull/11021", "NaN"], ["11293", "Add WorkProcessor#flatten method", "Karol Sobczak", "sopel39", "08/17/18, 03:27:27 PM", "    This method enables flattening of WorkProcessors\r\n    created directly by WorkProcessor#create\r\n    It is also less complex then previous flatTransform\r\n    implementation", "NaN"], ["11295", "Don't include column aliases when checking access", "Rebecca Schlussel", "rschlussel", "08/22/18, 08:55:46 PM", "NaN", "NaN"], ["11297", "Fix incorrect predicate pushdown through empty grouping set", "Martin Traverso", "martint", "08/17/18, 09:19:47 PM", "PredicatePushdown is incorrectly pushing down predicates the refer only\r\nto grouping keys when there's a empty grouping set present in the aggregation.\r\n\r\nWhile it's safe to push filters below an aggregation with no empty grouping sets\r\n(because they will ultimately be filtered out anyway), the empty grouping set has\r\n to produce a row. So, the filter has to be retained above the aggregation in that\r\n case. This is analogous to pushing down a filter through a global aggregation.\r\n\r\nFixes #11296", "NaN"], ["11300", "Update to ASM 6.2.1 and bytecode 1.1", "David Phillips", "electrum", "08/20/18, 04:46:51 AM", "NaN", "NaN"], ["11304", "Remove null vector from ArrayBlock, MapBlock, and RowBlock when all values are not null", "Ying", "yingsu00", "09/05/18, 09:41:11 PM", "When all values are not null, the presentStream is null. In such cases we don't need the nullVector.", "NaN"], ["11305", "Simplify test SHOW TABLES assertions", "Grzegorz Kokosi\u0144ski", "kokosing", "08/20/18, 10:50:16 AM", "Simplify test SHOW TABLES assertions", "NaN"], ["11309", "Improve substraction and NOT statistics", "Karol Sobczak", "sopel39", "08/21/18, 09:12:56 AM", "NaN", "NaN"], ["11314", "Add workaround for Mac JDK IPv6 bug for CLI and JDBC", "David Phillips", "electrum", "08/20/18, 05:26:57 PM", "https://medium.com/@quelgar/java-sockets-broken-for-ipv6-on-mac-5aae72f06b21", "NaN"], ["11316", "Update to antlr 4.7.1", "Martin Traverso", "martint", "08/22/18, 06:28:52 PM", "NaN", "NaN"], ["11320", "Fix data loss when writing sorted Hive tables", "David Phillips", "electrum", "08/21/18, 06:00:31 AM", "NaN", "NaN"], ["11321", "Fix truncated print for ProjectionNode in PlanPrinter", "Haozhun Jin", "haozhun", "08/21/18, 04:25:11 AM", "Regression in beee9e848f5dddea35dde834e976766d1b917503", "NaN"], ["11322", "Avoid using currentConstraint as predicate for getLayouts call", "Haozhun Jin", "haozhun", "09/06/18, 10:31:25 PM", "Fixes #11134\r\n\r\nFor the query mentioned in #11134: a query with 16 scans of a table with 10000 partitions (without any partition filter). The query's planning time reduced by 4X, from 115 seconds to 25 seconds. The query's initial task update request size reduced by 600X, from 4.4MB (half of what was claimed in #11134 due to #11133) to 7KB.", "NaN"], ["11325", "Add WorkProcessor#transformProcessor and dispose unused resources", "Karol Sobczak", "sopel39", "08/21/18, 03:17:13 PM", "NaN", "NaN"], ["11327", "Fix hang in setSchedulingPolicy", "Elon Azoulay", "elonazoulay", "09/22/18, 07:04:34 PM", "When queries are queued and eligible subgroups is not empty\r\nsetSchedulingPolicy will hang indefinitely.\r\nThis can be triggered from the DbResourceConfigurationManager.\r\n\r\nFixes #11324 ", "NaN"], ["11328", "Ensure access control classes implement all methods v2", "Piotr Findeisen", "findepi", "08/22/18, 07:27:12 AM", "Ensure that access control implementations that are intended to be\r\ncomplete implement all methods from the interface they override\r\n\r\nsupersedes https://github.com/prestodb/presto/pull/10649\r\ncc @rschlussel ", "NaN"], ["11333", "Avoid flushing small stripes due to DICTIONARY_FULL", "Wenlei Xie", "wenleix", "08/22/18, 09:27:07 PM", "    Previously the ORC writer will not convert column to direct encoding\r\n    if the compression ratio is exceeding a threshold (3.0), even the\r\n    stripe size is less than the minimal stripe size. This can result\r\n    in flushing small stripes in some cases.", "NaN"], ["11334", "Replace most uses of QueryInfo with BasicQueryInfo", "Dain Sundstrom", "dain", "09/18/18, 05:38:32 PM", "NaN", "NaN"], ["11340", "Make HiveClientConfig.resourceConfigFiles non-nullable", "Piotr Findeisen", "findepi", "08/23/18, 10:27:19 AM", "NaN", "NaN"], ["11342", "Allow alternate implementations of HiveMetadata", "David Phillips", "electrum", "08/23/18, 09:18:37 PM", "Extracting an interface allows implementations that use delegation\nrather than subclasses.", "NaN"], ["11345", "Fix handling of thread interruption in JDBC driver", "David Phillips", "electrum", "08/24/18, 06:13:43 PM", "When using sockets created using SocketChannel, OkHttp handles thread\ninterruption differently and does not restore the interrupt status.", "NaN"], ["11346", "Fix queries against information_schema with uppercase names", "Grzegorz Kokosi\u0144ski", "kokosing", "08/31/18, 10:13:03 AM", "Fix queries against information_schema with uppercase names\r", "NaN"], ["11347", "Rename connectorId to catalogName in SPI", "Grzegorz Kokosi\u0144ski", "kokosing", "08/30/18, 07:09:35 AM", "Rename connectorId to catalogName in SPI", "NaN"], ["11351", "Fix Hive smoke test when running in parallel", "Wenlei Xie", "wenleix", "08/24/18, 10:34:38 PM", "Use a different table name for testCreateEmptyBucketedPartition\r\nand testInsertPartitionedBucketedTable to avoid test failures\r\nwhen running in parallel.", "NaN"], ["11353", "Separate EXCEEDED_MEMORY_LIMIT error into local and global", "Raghav Sethi", "raghavsethi", "08/24/18, 11:20:55 PM", "NaN", "NaN"], ["11357", "Improve JMX stats and add HTTP endpoint for runaway splits diagnostics", "Varun Gajjala", "varungajjala", "09/07/18, 10:40:53 PM", "#11317 \r", "NaN"], ["11358", "Fix more test failures due to ExceededMemoryLimitException message", "Wenlei Xie", "wenleix", "08/27/18, 05:20:12 AM", "A few tests failures are not fixed by\r\n47355c83b3388ccf3d19b5a9f212328afe60ddc8", "NaN"], ["11359", "Use Optional instead of nullable Principal in access control SPI", "Piotr Findeisen", "findepi", "08/30/18, 09:03:11 AM", "NaN", "NaN"], ["11360", "Replace usage of deprecated TreeTraverser", "Piotr Findeisen", "findepi", "08/27/18, 09:08:17 AM", "NaN", "NaN"], ["11361", "Remove usages of deprecated parseTimestampInLocalTime", "Piotr Findeisen", "findepi", "08/30/18, 09:03:27 AM", "NaN", "NaN"], ["11362", "Pass transactionHandle to checkCanSetCatalogSessionProperty", "Grzegorz Kokosi\u0144ski", "kokosing", "08/30/18, 06:24:50 AM", "Pass transactionHandle to checkCanSetCatalogSessionProperty", "NaN"], ["11366", "Support compiling accumulator with multiple states", "Wenlei Xie", "wenleix", "10/03/18, 05:53:10 AM", "NaN", "NaN"], ["11370", "Limit the max line size in LineRecordReader", "Ying", "yingsu00", "09/14/18, 11:41:17 PM", "This is the fix for https://github.com/prestodb/presto/issues/11367\r\n\r\nWhen reading large text files, in fairly rare cases Presto hive is\r\nrequesting allocation of byte[] of size 2,147,483,643 to 2,147,483,647.\r\nThis exceeds the VM limit which is Integer.MAX_VALUE-5. However apache\r\nhadoop library uses Integer.MAX_VALUE as the limit and allows such\r\nallocation. This results in the OutOfMemoryError but it's not caught\r\nby Presto Hive. As a result of our JVM configuration, this results in\r\nJVM restarts and kills all queries running on the cluster.\r\n\r\n    This commit is to\r\n    1) fix the above bug in Hadoop and allow loud failure when the line\r\n    is over the max limit;\r\n    2) make Presto depend on the snapshot version of the Hadoop library;\r\n    3) Set the default limit of text file line size 100MB in Presto.\r\n\r\nThe hadoop snapshot library PR is at:\r\nhttps://github.com/prestodb/presto-hadoop-apache2/pull/30", "NaN"], ["11372", "Fix schema checks in file-based access control", "Piotr Findeisen", "findepi", "09/04/18, 08:37:13 AM", "Update checks added in https://github.com/prestodb/presto/pull/11328", "NaN"], ["11374", "Avoid Stream API in RowBlockBuilder constructor", "Wenlei Xie", "wenleix", "09/05/18, 06:54:52 AM", "    Row/JSON to row cast will constructor a new RowBlockBuilder\r\n    for each invocation.\r\n\r", "NaN"], ["11375", "Add stage ID to operator stats", "Raghav Sethi", "raghavsethi", "08/28/18, 10:46:29 PM", "NaN", "NaN"], ["11376", "Add ANALYZE statement to collect column statistics on demand", "Jiexi Lin", "jessesleeping", "02/01/19, 08:54:54 PM", "Added the following SQL statement:\r\n`ANALYZE qualifiedName (WITH properties)? `\r\n\r\nUser can trigger column statistic collection on a table by calling this statement. Connector decides what statistic to collect and how to store the result. Connector can also support certain `WITH` properties to customized the statistic collection. If the `WITH` property is not specified, the default behavior is to collect default statistics for the whole table. \r\n\r\nThe third commit implements the SPI in hive connector. It supports an analyze property named `partitions` which is expected to be `ARRAY[ARRAY[VARCHAR]]`. The value of this property is a list of partitions, where each partition is represented by a list of partition column value in varchar. ", "NaN"], ["11384", "Add missing column to verifier SQL documentation", "David Phillips", "electrum", "08/29/18, 05:49:04 PM", "NaN", "NaN"], ["11386", "Handle aliasing columns for table containing hidden columns", "Martin Traverso", "martint", "08/30/18, 12:08:37 AM", "Hidden columns were not being ignored when planning an aliased\r\nrelation. This caused a planning failure due to a mismatch between\r\nthe actual and expected number of columns.\r\n\r\nFixes #11385", "NaN"], ["11387", "Add 0.209 release notes", "David Phillips", "electrum", "08/29/18, 10:34:24 PM", "Fixes #11227", "NaN"], ["11388", "Separate out SplitMonitor from QueryMonitor", "David Phillips", "electrum", "10/01/18, 10:39:22 PM", "NaN", "NaN"], ["11389", "Avoid writing huge stripes due to direct encode conversion", "Wenlei Xie", "wenleix", "09/03/18, 01:33:11 AM", "We always convert column to direct encoding when stripe is small after\r\n05b66c041970fff317f88b15bbc991bd8d1ead35. However, in cases that\r\ncolumns has extremely well compression, it can end up with writing\r\nhuge stripes (larger than 2G in a corner case that each value is the\r\nsame array of varchar).\r\n\r\nThis commit fixed this by only convert to direct encode when the new\r\nsize is under a given limit.", "NaN"], ["11392", "Add intermediate exceptions as suppressed in RetryDriver", "Nezih Yigitbasi", "nezihyigitbasi", "09/06/18, 11:11:58 AM", "Addresses #11242.", "NaN"], ["11394", "Remove extra HdfsConfiguration#getConfiguration method", "Andrii Rosa", "arhimondr", "08/30/18, 08:53:51 PM", "Create configuration in KerberosHadoopAuthentication using HdfsConfigurationUpdater", "NaN"], ["11396", "Remove JVM vendor check", "David Phillips", "electrum", "09/10/18, 08:15:31 PM", "This allows running on Zulu or other builds of OpenJDK\nthat pass all of the system requirements.", "NaN"], ["11397", "Use HadoopKerberosName#setConfiguration to setup auth_to_local rules", "Karol Sobczak", "sopel39", "08/31/18, 07:39:55 PM", "NaN", "NaN"], ["11398", "Add test for FileSystem caching", "Mark Wagner", "wagnermarkd", "08/31/18, 07:53:39 PM", "This change only adds a test. We got burned a while back by some cache issues when running impersonation that led to full GCs. Everything was resolved after porting the changes that went into presto-hadoop-apache2 and I wrote this to ensure there were no regressions. I chose to put it in presto rather than presto-hadoop-apache2 to validate the behavior with the HdfsEnvironment utility.", "NaN"], ["11399", "Remove unused builder from PrincipalPrivileges", "Mark Wagner", "wagnermarkd", "08/31/18, 07:52:47 PM", "NaN", "NaN"], ["11400", "Return a constructed exception from utility method instead of throwing it", "Mark Wagner", "wagnermarkd", "08/31/18, 01:39:36 PM", "Tiny detail: processException is called from other catch blocks. Returning instead of throwing makes the stack trace reference the catch clause.", "NaN"], ["11401", "Remove user cpu time tracking", "Andrii Rosa", "arhimondr", "08/31/18, 06:50:50 PM", "Tracking user cpu time is expensive, as in Linux it is implemented in a way\r\nof doing a full text parsing of the /proc/self/task/<process id>/stat.\r\n\r\nThe overall cpu time is implemented by reading lightweight pthreads cpu counter.\r\n\r\nTraking user cpu time is 40x - 400x slower depending on platform and concurrency.\r\nRemoving tracking of the user time allows us to instrument operators, and have the\r\ninstrumentation by default at the cost of removing user cpu tracking. That seems to\r\nbe way more useful. Usefulness of having the user cpu time is very vague and arguable.", "NaN"], ["11402", "Rename BASIC_SQL test group to HIVE_PARTITIONING", "Grzegorz Kokosi\u0144ski", "kokosing", "08/31/18, 10:11:44 AM", "NaN", "NaN"], ["11404", "Format pom.xml files", "Grzegorz Kokosi\u0144ski", "kokosing", "08/31/18, 06:30:59 PM", "Format pom.xml files\n\nCommand used for formatting:\nfind -name pom.xml -exec vim \\{} -c \"execute 'normal! gg=G' | wq\" \\;\n\npresto-presto-tests/pom.xml was fixed manually", "NaN"], ["11405", "Refactor avro tests", "Andrii Rosa", "arhimondr", "08/31/18, 06:43:16 PM", "- Use different table name for each test to avoid interference\r\n- Use temporary files instead of temporary directories\r\n- Allow subclasses to select only subset of DDL tested.\r\n- Run cleanup actions in finally block", "NaN"], ["11406", "Add ST_GeomFromWKB and ST_AsBinary functions", "Marcos Pertierra", "mpertierra", "09/07/18, 11:05:51 PM", "Test Plan: Run `mvn -pl presto-geospatial -Dtest=TestGeoFunctions test`", "NaN"], ["11408", "Per operator cpu tracking", "Andrii Rosa", "arhimondr", "09/06/18, 12:57:20 PM", "This commit enables cpu time traking for operators by default.\r\n\r\nUsing the OperatorTimer class allows to decrease the number of the \"getCpuTime\".\r\n\r\nAssuming that operations on the operators come one after another, we can reuse\r\nthe \"endCpuTime\" for the previous operation as \"startCpuTime\" for the next operations.", "NaN"], ["11411", "Add reslution of \"hadoop-master\" NetUtils#addStaticResolution", "Karol Sobczak", "sopel39", "09/04/18, 09:25:03 AM", "Even though Hadoop is accessed via Proxy in presto-hive-hadoop2\r\ntests, Hadoop code still tries to resolve hadoop-master IP\r\nwhich required complex tests setup with dnsmasq and custom\r\nJava resolution provider.\r\nThis commit statically binds \"hadoop-master\" to localhost\r\nso that custom DNS provider is not requred.", "NaN"], ["11412", "Minor improvements to ORC writer dictionary optimizer", "Wenlei Xie", "wenleix", "09/04/18, 05:06:10 PM", "MaxRowFlush writes small stripes when `MaxStripeRowCount=10M`, but such case becomes negligible when bumping  `MaxStripeRowCount` to `50M` (see the following figure). Unfortunately some corner-case bug will happen when bumping this config. This PR fix such bug to unblock bumping the config.\r\n\r\n\r\n<img width=\"1285\" alt=\"screen shot 2018-09-03 at 11 44 37 am\" src=\"https://user-images.githubusercontent.com/799346/44999045-33cb6b00-af6f-11e8-9a50-15facdfe48b6.png\">\r\n\r\n\r", "NaN"], ["11424", "Add 0.210 release notes", "Martin Traverso", "martint", "09/05/18, 08:28:14 PM", "NaN", "NaN"], ["11425", "Add disjunctive predicate for filtered aggregates", "Atri Sharma", "atris", "09/20/18, 09:04:54 AM", "Filtered aggregates do not make an explicit predicate of the filter clause\r\nthat is present. This limits the ability of the optimizer to potentially\r\npush down the predicate to table scan node. This commit adds a filter node\r\nwith a disjunctive predicate formed from all the filter clauses present in\r\nthe aggregation node's aggregates", "NaN"], ["11426", "Improve readability of Hive columns in explain plan", "David Phillips", "electrum", "09/10/18, 10:18:47 PM", "NaN", "NaN"], ["11427", "Remove null vector from ArrayBlock, MapBlock, and RowBlock when all v\u2026", "Ying", "yingsu00", "09/10/18, 10:02:55 PM", "Remove null vector from ArrayBlock, MapBlock, and RowBlock when all values are not null\r\n\r\nThis reverts commit d7256c59dede509e84ae127ecb20810f09de7014.\r\n\r\n@dain Sorry for the trouble, but could you please re-review it? I merged it without rerunning all tests and there was a test failure on a new test. Now I have corrected it. Beside this fix there was no new changes. You have approved this PR before, could you please do it again? Many thanks!", "NaN"], ["11431", "Ensure unoptimized ORC/Parquet implementations are tested", "Piotr Findeisen", "findepi", "09/14/18, 07:13:48 AM", "For example, Parquet optimized reader is used by default, so the test\r\ntested it twice, instead of testing unoptimized reader.", "NaN"], ["11433", "Track statistics collection cpu utilization", "Andrii Rosa", "arhimondr", "09/07/18, 03:34:44 PM", "NaN", "NaN"], ["11434", "Track table writer validation CPU time", "Wenlei Xie", "wenleix", "09/11/18, 02:52:19 AM", "NaN", "NaN"], ["11435", "Minor cleanup of resource group plugin", "David Phillips", "electrum", "09/10/18, 08:14:56 PM", "NaN", "NaN"], ["11437", "Add config for Raptor table organization discovery interval", "Jiexi Lin", "jessesleeping", "09/06/18, 11:53:30 PM", "Add config option to set how long we wait between 2 organization discovery run. The previous hard coded interval is 5 min plus a random 5 min jitter. We increase it to 6 hour by default so that it matches better for the default 7 day per table organization interval. Also, this spreads out organization job in a longer period so that we don't overload the backup storage. ", "NaN"], ["11439", "Consider 2 random nodes when reassigning buckets in Raptor", "Jiexi Lin", "jessesleeping", "09/07/18, 02:46:07 AM", "This is to avoid unbalance reassignments caused by pure random choices. \r\n\r\nReference:\r\nThesis paper: https://www.eecs.harvard.edu/~michaelm/postscripts/handbook2001.pdf\r\nBlog: https://brooker.co.za/blog/2012/01/17/two-random.html", "NaN"], ["11440", "Add enable_stats_calculator session property", "Andrii Rosa", "arhimondr", "09/07/18, 05:02:50 PM", "That property allows to completely disable stats calculator", "NaN"], ["11441", "Disable statistic computation for distributed plan in QueryMonitor", "Andrii Rosa", "arhimondr", "09/07/18, 06:01:47 PM", "At the moment when queryCompletedEvent is called the transaction is gone, and it is\r\nnot possible to recompute the statistics.", "NaN"], ["11446", "Use MAP_FROM_ENTRIES in JSON map example", "Daniel Lo Nigro", "Daniel15", "11/28/18, 07:10:34 PM", "It feels cleaner to use `MAP_FROM_ENTRIES` in this example, instead of having two separate arrays (one for keys and one for values)\r\n\r\nAlso removed the duplicate example.", "NaN"], ["11447", "Upgrade to Airlift 0.173", "Nezih Yigitbasi", "nezihyigitbasi", "09/10/18, 06:21:47 AM", "NaN", "NaN"], ["11448", "Inline TaskScheduler in ScaledWriterScheduler", "Wenlei Xie", "wenleix", "09/17/18, 10:02:00 PM", "NaN", "NaN"], ["11453", "Add geometry_union scalar function to efficiently union an array of geometries", "Timothy Meehan", "tdcmeehan", "10/11/18, 03:56:03 PM", "A benchmark has been added to measure geometry aggregation performance.\r\nHere are some benchmark numbers which demonstrate the performance over\r\n50 states:\r\n\r\n```\r\nBenchmark                                         Mode  Cnt    Score    Error  Units\r\nBenchmarkGeometryAggregation.benchmarkArrayUnion  avgt   30   24.601 \u00b1  0.659  ms/op\r\nBenchmarkGeometryAggregation.benchmarkConvexHull  avgt   30   23.169 \u00b1  0.673  ms/op\r\nBenchmarkGeometryAggregation.benchmarkUnion       avgt   30  647.646 \u00b1 14.411  ms/op", "NaN"], ["11457", "Add release notes for 0.211 ", "Haozhun Jin", "haozhun", "09/13/18, 07:35:34 PM", "NaN", "NaN"], ["11459", "Update to Drift 1.13", "David Phillips", "electrum", "09/13/18, 12:53:10 AM", "NaN", "NaN"], ["11460", "Ensure ORC stream is closed before reset", "Wenlei Xie", "wenleix", "09/12/18, 02:23:46 AM", "NaN", "NaN"], ["11461", "Add ST_LineString function", "Andrew Bass", "arbass22", "09/14/18, 02:26:51 PM", "Added a geospatial function that coverts an array of points into a LineString as well as unit tests and documentation.", "NaN"], ["11462", "Remove legacy support for ROUND(.., BIGINT)", "Piotr Findeisen", "findepi", "09/14/18, 07:02:53 AM", "It became legacy in 0.199, so it's about time to remove it completely.", "NaN"], ["11463", "Refactor HiveStatisticsProvider and statistics SPI", "Andrii Rosa", "arhimondr", "09/19/18, 11:29:49 PM", "NaN", "NaN"], ["11464", "Decrease amount of logs produced by product tests", "Andrii Rosa", "arhimondr", "09/12/18, 06:22:26 PM", "Build is failing with \"The log length has exceeded the limit of 4 MB\".\r\nMajority of the logs are listings of jars loaded by different plugins.\r\nAlthough it might be useful information for debugging, keeping it enabled\r\nby default doesn't make much sense.", "NaN"], ["11465", "Fix order when combining conjuncts in PickTableLayout", "Haozhun Jin", "haozhun", "09/12/18, 11:20:30 PM", "AddExchange had a more correct-ish order since its initial implementation.\r\nPickTableLayout had less desirable order since its initial implementation,\r\nwhich is strictly worse than the one used by AddExchange.\r\nIt didn't really matter whatever order PickTableLayout chose because\r\nAddExchange always runs last, and it will override the order (unless\r\nLocalQueryRunner is used).\r\n\r\nCommit 1c5499b075389463f85666bbf331bbba4b091791 broke this when it tries to\r\ndeduplicate the code of the two classes. The order in PickTableLayout was used.\r\nThis leads to regression where previously successfully query would\r\nfail due to logic errors such as out-of-bound access, div-by-zero, etc.", "NaN"], ["11469", "Fix Maven benchmark profile in presto-main/pom.xml", "Wenlei Xie", "wenleix", "09/14/18, 11:07:46 PM", "Accidentally changed in commit a83657cd24e3002d68cf3d2d29a7f40c9705c980.", "NaN"], ["11471", "Enable test for subquery prunning from predicate", "Grzegorz Kokosi\u0144ski", "kokosing", "09/14/18, 05:31:07 AM", "Enable test for subquery prunning from predicate", "NaN"], ["11473", "Make hive metastore test database name configurable", "Karol Sobczak", "sopel39", "09/14/18, 08:04:53 AM", "This allows for running multiple AWS Glue tests in parallel", "NaN"], ["11474", "Fix example in session property managers docs", "Piotr Findeisen", "findepi", "09/13/18, 03:26:46 PM", "NaN", "NaN"], ["11476", "Enhance Thrift Connector documentation", "Maria Basmanova", "mbasmanova", "09/17/18, 06:38:36 PM", "NaN", "NaN"], ["11478", "Remove old shard assignment when reassigning shard in Raptor", "Jiexi Lin", "jessesleeping", "09/24/18, 10:56:56 PM", "Before this commit we never remove old assignments when assigning missing shards. It can result in multiple assignments of the same shard on different nodes, causing waste in storage. \r\n\r\nThis commit guarantees at most one node is assigned for a shard.\r\n\r\n**Behavior changes in a rare corner case**\r\nThis commit also changes the behavior when 2 queries both discover the missing shard and try to reassignment it at the same time. Before this change, we can have one of the 2 results:\r\n(1) One of the query fails due to transaction conflict on metadata modification.\r\n(2) Both queries succeed in assigning shards, results in multiple assignments of the same shard.\r\n\r\nAfter this change, we can have:\r\n(1) One of the query fails due to transaction conflict on metadata modification.\r\n(2) Both queries succeed in reassigning shard, but one of them gets a stale split that contains the wrong node address, resulting in query failure due to shard not found on the node. ", "NaN"], ["11479", "Allow custom JDBC connectors to bind ConnectorAccessControl", "David Phillips", "electrum", "09/24/18, 06:55:54 PM", "This can be used as follows:\r\n\r\n```java\r\npublic class CustomMySqlPlugin\r\n        extends JdbcPlugin\r\n{\r\n    public CustomMySqlPlugin()\r\n    {\r\n        super(\"custom-mysql\", binder -> {\r\n            binder.install(new MySqlClientModule());\r\n            newOptionalBinder(binder, ConnectorAccessControl.class)\r\n                    .setBinding().to(CustomAccessControl.class).in(Scopes.SINGLETON);\r\n        });\r\n    }\r\n}", "NaN"], ["11481", "Simplify query assertion error message", "Grzegorz Kokosi\u0144ski", "kokosing", "09/17/18, 12:18:58 PM", "Simplify query assertion error message", "NaN"], ["11489", "Fix geospatial docs. Fix ST_EnvelopeAsPts and use array(...) syntax.", "Jacob Wasserman", "jwass", "09/18/18, 02:07:50 AM", "Returns array<Geometry>.", "NaN"], ["11496", "Add docs for SHOW STATS", "Andrew Bass", "arbass22", "10/02/18, 08:04:55 PM", "Fixes #11486", "NaN"], ["11497", "Remove transaction information from explain plan", null, "natewoodfb", "09/19/18, 07:34:26 PM", "Remove transaction handle from TableCommit part of the `explain <query>`\r\nplan as it can have random elements such as UUID.\r\n\r\nWhen comparing explain plans, they may change between calls\r\neven if the query or environment has not changed. This fixes one\r\nof the known causes since a new UUID is generated on each query.\r\n\r\nFollowing is an example of an `explain CREATE TABLE` query where the \r\nUUID of the transaction is printed out at the beginning:\r\n\r\n```\r\n- Output[rows] => [rows:bigint]\r\n    - TableCommit[hive:475171d7-69f3-49b5-b983-076c63f3da1a:tpch.copy_orders] => [rows:bigint]\r\n```\r\n\r\nThis fix makes it so that the InsertTableHandle::toString and\r\nOutputTableHandle::toString do not return transaction handle data,\r\nresulting in the following:\r\n\r\n```\r\n- Output[rows] => [rows:bigint]\r\n    - TableCommit[hive:tpch.copy_orders] => [rows:bigint]\r\n```\r\n\r\nSee: #11444, #11485 ", "NaN"], ["11502", "Update update-hadoop-apache2 to 2.7.4-4", "Andrii Rosa", "arhimondr", "09/17/18, 09:47:58 PM", "NaN", "NaN"], ["11503", "Recover documentation for node-scheduler.network-topology", "Thoralf Gutierrez", "thoralf-gutierrez", "10/09/18, 04:56:17 AM", "It looks like it was inadvertently deleted with the following commit: https://github.com/prestodb/presto/commit/06e9f7a817212e7919529ec7c3e31d47744b3d65 \r\n\r\nI hope it's still valid though?", "NaN"], ["11504", "Enable GeometryCollection type in convex_hull_agg and ST_ConvexHull functions", "Yongyang Yu", "yuyongyang800", "09/19/18, 01:01:53 AM", "Fixes #11488", "NaN"], ["11505", "Separate JWT related config from ProxyConfig", "Shixuan Fan", "shixuan-fan", "09/18/18, 10:57:48 PM", "NaN", "NaN"], ["11506", "Categorize the max text line length exceeded error as HIVE_BAD_DATA", "Ying", "yingsu00", "10/01/18, 10:33:34 PM", "NaN", "NaN"], ["11508", "Remove obsolete comment", "Grzegorz Kokosi\u0144ski", "kokosing", "09/18/18, 07:50:44 AM", "Remove obsolete comment", "NaN"], ["11511", "Use precomputed stats/cost estimates in distributed explain plans", "Rebecca Schlussel", "rschlussel", "10/03/18, 09:08:10 PM", "NaN", "NaN"], ["11514", "correct read time stats in parquet pagesource", "qqibrow", "qqibrow", "12/16/18, 02:56:44 AM", "current `readTimeNanos ` in `ParquetPageSource` doesn't reflect the correct stats. This patch aims to fix that. ", "NaN"], ["11517", "Add documentation for binary substr methods", "Mark Wagner", "wagnermarkd", "09/24/18, 06:56:56 PM", "NaN", "NaN"], ["11518", "Clean up SqlQueryManager", "Dain Sundstrom", "dain", "10/17/18, 03:54:31 AM", "* Fix warnings\n* Extract out complex book keeping like query timeout tracking\n* Simplify session creation code\n* Prepare manager for split into dispatcher and coordinator", "NaN"], ["11519", "Update javadoc for Type.getJavaType", "Piotr Findeisen", "findepi", "09/20/18, 08:12:46 AM", "NaN", "NaN"], ["11521", "Do not create new plan nodes in predicate pushdown when plan did not change", "Karol Sobczak", "sopel39", "10/27/18, 06:58:22 AM", "NaN", "NaN"], ["11526", "Revert \"Improve Stripe Size Estimation for SliceDictionaryColumnWriter\"", "Wenlei Xie", "wenleix", "09/21/18, 03:42:42 AM", "This reverts commit 9c0fe41.\r\n\r\nThe reverted commit avoids overestimating dictionary index stream by\r\nwriting temporary dictionary index to a data stream and use the\r\nbuffered size of this stream as the estimation.\r\n\r\nHowever, ORC spec requires dictionary to be sorted. Thus the dictionary\r\nindices have to be re-indexed when flushing. In most case the estimated\r\nsize is close to the actual size. However under some rare corner cases,\r\nthe estimated size can be up to 6x smaller than the actual size, which\r\ncauses flushing huge stripes and becomes a reliability threat.\r\n\r\nHere is an observed example: an column with ARRAY(VARCHAR), where\r\neach cell contains the same array with about 30,000 elements.\r\nThe array is unsorted. Thus the temporary dictionary index\r\nis [1, 2, 3, ...], while after re-indexing, the dictionary index\r\nis a random shuffle.  The dictionary index streams exhibit\r\nsignificantly different compression ratios under ZLIB.\r\n\r\nThe conclusion is with the re-indexing as the last step, it is\r\nextremely difficult, if not impossible, to accurately estimate\r\nthe index stream size after compression.\r\n\r\nWe decide to fall back to the previous mechanism which tends to\r\nover-estimate the stripe size, for the following reasons:\r\n\r\n 1. While the previous heuristics flush small compressed stripes,\r\nthe uncompressed stripe size is as expected thus each stripe still\r\nhas enough data for computation engine to work on.\r\n\r\n 2. Writing arbitrarily huge stripe is worse to Presto engine from\r\nreliability perspective.\r\n\r\nNote DWRF doesn't require dictionary to be sorted, in the future we can\r\nadd an option to write unsorted dictionary when writing DWRF files.", "NaN"], ["11527", "Update to Drift 1.14", "David Phillips", "electrum", "09/19/18, 08:51:50 PM", "This fixes failed connections not being removed from the pool.", "NaN"], ["11528", "Fix value of currentConstraint when resolving TableLayout", "Haozhun Jin", "haozhun", "09/24/18, 10:28:18 PM", "This change should have been part of d54521f24ebf49754da2833f748d8fb2d4149cc7\r\nbut was not placed there due to oversight.\r\n\r\nAs clarified in commit d54521f24ebf49754da2833f748d8fb2d4149cc7 and explained\r\nin the commit message of the same commit, currentConstraint is supposed to be\r\na TupleDomain that represents a predicate that every row produced by\r\nthis TableScan node is guaranteed to satisfy.\r\n\r\nTo recap, a table scan is not guaranteed to only produce rows that satisfies\r\nthe filter on top of itself. As a result, it is incorrect to have\r\nthe intersection.\r\n\r\nThis commit affects EXPLAIN IO. However, although EXPLAIN IO was introduced\r\nbefore this commit, only showing successfully pushed down predicates in\r\nEXPLAIN IO has always been the intended semantics.", "NaN"], ["11534", "Inline documentation for Avro schema evolution", "David Phillips", "electrum", "09/24/18, 06:21:16 PM", "Having the documentation inline makes it easier to read since it is all in\r\none place, allows us to customize it for each connector, fits better with\r\nthe notion that each connector separate (even if the code is shared),\r\nand de-clutters the already long list of connectors.", "NaN"], ["11536", "Use latest Jdbi 3, H2, and testing-mysql-server", "David Phillips", "electrum", "09/24/18, 10:23:24 PM", "These changes are extracted from the Raptor v2 change so that they can be tested and merged independently.", "NaN"], ["11538", "Do not update statics for Hive views", "Grzegorz Kokosi\u0144ski", "kokosing", "09/21/18, 06:15:52 AM", "Do not update statics for Hive views", "NaN"], ["11542", "Fix ST_GeomFromBinary", "Maria Basmanova", "mbasmanova", "09/21/18, 03:20:12 PM", "Fixes #11541", "NaN"], ["11545", "Add support for IN and BETWEEN to SHOW STATS", "Maria Basmanova", "mbasmanova", "10/04/18, 04:11:09 PM", "Fixes #11544\r\n\r\n@findepi @kokosing I couldn't find a place to add a test. Any suggestions?", "NaN"], ["11549", "[Hotfix]nullsCount in columnStatistic should marked as not present in\u2026", "Vico.Wu", "VicoWu", "10/09/18, 09:22:03 AM", "My presto is running on my Cloudera 5.7.6 hive meastore.\r\nWhen upgrading to 0.210, Many queries previously run successfully failed with following error:\r\n```\r\njava.lang.IllegalArgumentException: Nulls fraction should be within [0, 1] or NaN, got: -0.16666666666666666\r\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:210)\r\n\tat com.facebook.presto.cost.SymbolStatsEstimate.<init>(SymbolStatsEstimate.java:54)\r\n\tat com.facebook.presto.cost.SymbolStatsEstimate$Builder.build(SymbolStatsEstimate.java:225)\r\n\tat com.facebook.presto.cost.TableScanStatsRule.toSymbolStatistics(TableScanStatsRule.java:92)\r\n\tat com.facebook.presto.cost.TableScanStatsRule.lambda$doCalculate$0(TableScanStatsRule.java:73)\r\n\tat java.util.Optional.map(Optional.java:215)\r\n\tat com.facebook.presto.cost.TableScanStatsRule.doCalculate(TableScanStatsRule.java:73)\r\n\tat com.facebook.presto.cost.TableScanStatsRule.doCalculate(TableScanStatsRule.java:41)\r\n\tat com.facebook.presto.cost.SimpleStatsRule.calculate(SimpleStatsRule.java:39)\r\n\tat com.facebook.presto.cost.ComposableStatsCalculator.calculateStats(ComposableStatsCalculator.java:80)\r\n\tat com.facebook.presto.cost.ComposableStatsCalculator.calculateStats(ComposableStatsCalculator.java:70)\r\n\tat com.facebook.presto.cost.CachingStatsProvider.getStats(CachingStatsProvider.java:70)\r\n\tat com.facebook.presto.cost.CostCalculatorUsingExchanges$CostEstimator.getStats(CostCalculatorUsingExchanges.java:243)\r\n\tat com.facebook.presto.cost.CostCalculatorUsingExchanges$CostEstimator.visitTableScan(CostCalculatorUsingExchanges.java:141)\r\n\tat com.facebook.presto.cost.CostCalculatorUsingExchanges$CostEstimator.visitTableScan(CostCalculatorUsingExchanges.java:99)\r\n\tat com.facebook.presto.sql.planner.plan.TableScanNode.accept(TableScanNode.java:128)\r\n\tat com.facebook.presto.cost.CostCalculatorUsingExchanges.calculateCost(CostCalculatorUsingExchanges.java:96)\r\n\tat com.facebook.presto.cost.CostCalculatorWithEstimatedExchanges.calculateCost(CostCalculatorWithEstimatedExchanges.java:73)\r\n\tat com.facebook.presto.cost.CachingCostProvider.calculateCumulativeCost(CachingCostProvider.java:103)\r\n\tat com.facebook.presto.cost.CachingCostProvider.getGroupCost(CachingCostProvider.java:95)\r\n\tat com.facebook.presto.cost.CachingCostProvider.getCumulativeCost(CachingCostProvider.java:72)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.ReorderJoins$JoinEnumerator.createJoinEnumerationResult(ReorderJoins.java:391)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.ReorderJoins$JoinEnumerator.getJoinSource(ReorderJoins.java:342)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.ReorderJoins$JoinEnumerator.createJoin(ReorderJoins.java:258)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.ReorderJoins$JoinEnumerator.createJoinAccordingToPartitioning(ReorderJoins.java:229)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.ReorderJoins$JoinEnumerator.chooseJoinOrder(ReorderJoins.java:173)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.ReorderJoins$JoinEnumerator.access$000(ReorderJoins.java:135)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.apply(ReorderJoins.java:127)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.apply(ReorderJoins.java:88)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.transform(IterativeOptimizer.java:165)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreNode(IterativeOptimizer.java:138)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreGroup(IterativeOptimizer.java:103)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreChildren(IterativeOptimizer.java:185)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreGroup(IterativeOptimizer.java:105)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreChildren(IterativeOptimizer.java:185)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreGroup(IterativeOptimizer.java:105)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.optimize(IterativeOptimizer.java:94)\r\n\tat com.facebook.presto.sql.planner.LogicalPlanner.plan(LogicalPlanner.java:150)\r\n\tat com.facebook.presto.sql.planner.LogicalPlanner.plan(LogicalPlanner.java:139)\r\n\tat com.facebook.presto.execution.SqlQueryExecution.doAnalyzeQuery(SqlQueryExecution.java:360)\r\n\tat com.facebook.presto.execution.SqlQueryExecution.analyzeQuery(SqlQueryExecution.java:345)\r\n\tat com.facebook.presto.execution.SqlQueryExecution.start(SqlQueryExecution.java:289)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\nThis is because hive metastore use -1 to mark the column nullCounts as unavailable. \r\n\r\nAfter I upgrading to master branch, the error still exists and the error become this:\r\n```\r\ncom.facebook.presto.spi.PrestoException: Corrupted partition statistics (Table: default.plus_higher_category Partition: [<UNPARTITIONED>] Column: plus_higher_category_name): nullsCount must be greater than or equal to zero: -1\r\n\tat com.facebook.presto.hive.statistics.MetastoreHiveStatisticsProvider.checkStatistics(MetastoreHiveStatisticsProvider.java:350)\r\n\tat com.facebook.presto.hive.statistics.MetastoreHiveStatisticsProvider.lambda$validateColumnStatistics$14(MetastoreHiveStatisticsProvider.java:222)\r\n\tat java.util.OptionalLong.ifPresent(OptionalLong.java:142)\r\n\tat com.facebook.presto.hive.statistics.MetastoreHiveStatisticsProvider.validateColumnStatistics(MetastoreHiveStatisticsProvider.java:221)\r\n\tat com.facebook.presto.hive.statistics.MetastoreHiveStatisticsProvider.lambda$null$10(MetastoreHiveStatisticsProvider.java:211)\r\n\tat com.google.common.collect.RegularImmutableMap.forEach(RegularImmutableMap.java:187)\r\n\tat com.facebook.presto.hive.statistics.MetastoreHiveStatisticsProvider.lambda$validatePartitionStatistics$11(MetastoreHiveStatisticsProvider.java:211)\r\n\tat com.google.common.collect.SingletonImmutableBiMap.forEach(SingletonImmutableBiMap.java:65)\r\n\tat com.facebook.presto.hive.statistics.MetastoreHiveStatisticsProvider.validatePartitionStatistics(MetastoreHiveStatisticsProvider.java:204)\r\n\tat com.facebook.presto.hive.statistics.MetastoreHiveStatisticsProvider.getTableStatistics(MetastoreHiveStatisticsProvider.java:142)\r\n\tat com.facebook.presto.hive.HiveMetadata.getTableStatistics(HiveMetadata.java:536)\r\n\tat com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorMetadata.getTableStatistics(ClassLoaderSafeConnectorMetadata.java:201)\r\n\tat com.facebook.presto.metadata.MetadataManager.getTableStatistics(MetadataManager.java:401)\r\n\tat com.facebook.presto.cost.TableScanStatsRule.doCalculate(TableScanStatsRule.java:61)\r\n\tat com.facebook.presto.cost.TableScanStatsRule.doCalculate(TableScanStatsRule.java:36)\r\n\tat com.facebook.presto.cost.SimpleStatsRule.calculate(SimpleStatsRule.java:39)\r\n\tat com.facebook.presto.cost.ComposableStatsCalculator.calculateStats(ComposableStatsCalculator.java:80)\r\n\tat com.facebook.presto.cost.ComposableStatsCalculator.calculateStats(ComposableStatsCalculator.java:70)\r\n\tat com.facebook.presto.cost.CachingStatsProvider.getGroupStats(CachingStatsProvider.java:91)\r\n\tat com.facebook.presto.cost.CachingStatsProvider.getStats(CachingStatsProvider.java:68)\r\n\tat com.facebook.presto.cost.CostCalculatorWithEstimatedExchanges$ExchangeCostEstimator.getStats(CostCalculatorWithEstimatedExchanges.java:198)\r\n\tat com.facebook.presto.cost.CostCalculatorWithEstimatedExchanges$ExchangeCostEstimator.calculateJoinCost(CostCalculatorWithEstimatedExchanges.java:153)\r\n\tat com.facebook.presto.cost.CostCalculatorWithEstimatedExchanges$ExchangeCostEstimator.visitJoin(CostCalculatorWithEstimatedExchanges.java:133)\r\n\tat com.facebook.presto.cost.CostCalculatorWithEstimatedExchanges$ExchangeCostEstimator.visitJoin(CostCalculatorWithEstimatedExchanges.java:76)\r\n\tat com.facebook.presto.sql.planner.plan.JoinNode.accept(JoinNode.java:289)\r\n\tat com.facebook.presto.cost.CostCalculatorWithEstimatedExchanges.calculateCost(CostCalculatorWithEstimatedExchanges.java:72)\r\n\tat com.facebook.presto.cost.CachingCostProvider.calculateCumulativeCost(CachingCostProvider.java:103)\r\n\tat com.facebook.presto.cost.CachingCostProvider.getCumulativeCost(CachingCostProvider.java:80)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.DetermineJoinDistributionType.getJoinNodeWithCost(DetermineJoinDistributionType.java:133)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.DetermineJoinDistributionType.getCostBasedJoin(DetermineJoinDistributionType.java:77)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.DetermineJoinDistributionType.apply(DetermineJoinDistributionType.java:66)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.DetermineJoinDistributionType.apply(DetermineJoinDistributionType.java:43)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.transform(IterativeOptimizer.java:165)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreNode(IterativeOptimizer.java:138)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreGroup(IterativeOptimizer.java:103)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreChildren(IterativeOptimizer.java:185)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreGroup(IterativeOptimizer.java:105)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreChildren(IterativeOptimizer.java:185)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.exploreGroup(IterativeOptimizer.java:105)\r\n\tat com.facebook.presto.sql.planner.iterative.IterativeOptimizer.optimize(IterativeOptimizer.java:94)\r\n\tat com.facebook.presto.sql.planner.LogicalPlanner.plan(LogicalPlanner.java:150)\r\n\tat com.facebook.presto.sql.planner.LogicalPlanner.plan(LogicalPlanner.java:139)\r\n\tat com.facebook.presto.execution.SqlQueryExecution.doAnalyzeQuery(SqlQueryExecution.java:362)\r\n\tat com.facebook.presto.execution.SqlQueryExecution.analyzeQuery(SqlQueryExecution.java:347)\r\n\tat com.facebook.presto.execution.SqlQueryExecution.start(SqlQueryExecution.java:291)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\nThe safe solution is to mark the nullsCount as unavailable when hive return -1 for it, instead of just failing user's query!\r\n**Anytime we should not fail users' query when statistic information is not present or looks illegal! Illegal statistics information should be marked as absent **  ", "NaN"], ["11550", "Make it easier to debug product tests", "Piotr Findeisen", "findepi", "09/21/18, 08:19:00 PM", "NaN", "NaN"], ["11552", "Infer output predicates for inner joins in equality inference", "Karol Sobczak", "sopel39", "10/01/18, 09:27:28 PM", "There are more possible improvements in join equality inference:\r\n* for outer joins outer side we could infer via inner side symbols (with `OR inner side symbols null` condition)\r\n* for outer joins inner side we could infer via outer side symbols with (with `OR inner side symbols null` condition)\r\n* for outer joins filter conditions we could produce predicate (with `OR inner side symbols null` condition)\r\n\r\nAdditionally equality inference `rewrite` mechianism could be improved so that expressions like:\r\n```\r\np1 OR (p2 AND p3 AND p4)\r\n```\r\nare rewritten (expanded) into:\r\n```\r\np1 OR (p2 AND p4)\r\n```\r\nwhen `p3` cannot be rewritten. Currently `rewrite` would just return `null`", "NaN"], ["11555", "Remove duplicate definitions of onHive() method", "Piotr Findeisen", "findepi", "09/24/18, 09:12:28 AM", "NaN", "NaN"], ["11557", "Update to docker images version 10", "Piotr Findeisen", "findepi", "09/24/18, 10:27:40 PM", "NaN", "NaN"], ["11559", "Do not set distinct values count to 0 when nulls fraction is not 1.0", "Karol Sobczak", "sopel39", "10/01/18, 09:27:11 PM", "In Hive we got table:\r\n```\r\nhive> select distinct ca_country from customer_address;\r\nNULL\r\nUnited States\r\n```\r\n\r\nbut:\r\nhive> describe formatted customer_address.ca_country;\r\nOK\r\n```\r\nname       type         ndv\r\nca_country varchar(20)  1\r\n```", "NaN"], ["11560", "Do not create Pages for channels in each JoinFilterFunctionFactory#cr\u2026", "Karol Sobczak", "sopel39", "09/24/18, 10:13:08 PM", "\u2026eate\r\n\r\nWhen broadcast join is used then JoinFilterFunction\r\nis created for every instance of LookupJoinOperator\r\n(therefore for every split) via JoinHash and\r\nJoinHashSupplier.\r\nJoinFilterFunction factory was creating new Pages\r\nfor the same channels. This caused GC pressure\r\nand leak on unaccounted query memory\r\n(created Pages reference same blocks but contain\r\noverhead structures).\r\nNow Pages are created beforehand from channels.", "NaN"], ["11561", "Adding support for NOT NULL in DDL", "Timothy Meehan", "tdcmeehan", "03/06/19, 06:11:47 PM", "Split out from #11490\r\n\r\n* Introduce support for `NOT NULL` in DDL.  Connectors can opt-in to this by turning on `ConnectorMetadata#supportsNotNullColumns`.\r\n* Add support for `CREATE TABLE`, `ADD COLUMN`, `DROP COLUMN`, `RENAME COLUMN`, `RENAME TABLE` in JDBC connectors\r\n* Enable `NOT NULL` for JDBC connectors\r\n\r", "NaN"], ["11562", "Fix flaky TestShardoDao not to rely on order", "David Phillips", "electrum", "09/24/18, 10:22:33 PM", "NaN", "NaN"], ["11564", "Update Aircompressor to 0.12", "Dain Sundstrom", "dain", "10/01/18, 09:05:53 PM", "NaN", "NaN"], ["11565", "Add support for writing ORC files with ZSTD compression", "David Phillips", "electrum", "10/01/18, 10:57:03 PM", "This will allow Raptor v2 to use ZSTD.", "NaN"], ["11568", "Make JoinUtils public", "Karol Sobczak", "sopel39", "09/25/18, 10:15:52 AM", "JoinHashSupplier can be isolated via\r\nJoinCompiler#internalCompileLookupSourceFactory.\r\nIn such case it cannot access package private\r\nJoinUtils. We get errors like:\r\n\r\ncom.facebook.presto.sql.gen.JoinCompiler$LookupSourceSupplierFactory.createLookupSourceSupplier(JoinCompiler.java:926)\r\n\t... 32 more\r\nCaused by: java.lang.IllegalAccessError: com/facebook/presto/operator/JoinUtils\r\n\tat com.facebook.presto.operator.JoinHashSupplier.<init>(JoinHashSupplier.java:72)\r\n\r\nTherefore JoinUtils needs to be public", "NaN"], ["11571", " Reveal storage format used when test fails", "Piotr Findeisen", "findepi", "10/02/18, 09:15:04 AM", "NaN", "NaN"], ["11573", "Return zero statistics when no partitions were selected", "Andrii Rosa", "arhimondr", "10/01/18, 07:01:24 PM", "NaN", "NaN"], ["11574", "Add StorageFormat#toString method", "Andrii Rosa", "arhimondr", "10/01/18, 07:00:09 PM", "NaN", "NaN"], ["11577", "Fix JDBC connectors to clean up temp table after exception", "Timothy Meehan", "tdcmeehan", "10/01/18, 11:40:33 PM", "JDBC based connectors use the following strategy to insert into the\r\ndestination table:\r\n\r\n* Create a temp table which has the same columns as the destination\r\ntable\r\n* Direct JdbcPageSink to write to this temp table\r\n* Select into the destination table from the temp table\r\n\r\nIf an exception occurs during the write, then no attempt is made to\r\nclean up this temp table.  This commit adds code to attempt this clean\r\nup during the phases where we could receive errors from the destination\r\ndatabase and which may result in orphaned tables.", "NaN"], ["11579", "Add TempFileSinkFactory abstraction", "Wenlei Xie", "wenleix", "09/26/18, 02:42:06 AM", "This allows to use customized ORC data sink for temporary files when\r\nwriting Hive sorted bucketed partitions.", "NaN"], ["11581", "Add SphericalGeography type and ST_Distance support for points on a sphere", "Xiaoming Gao", "gaoxm", "01/28/19, 04:39:04 PM", "This PR covers phase 1 and a small step in phase 2 towards [Issue #11580](https://github.com/prestodb/presto/issues/11580)\r\n1. Add a new SQL type `SphericalGeography` to represent geometry objects in lat/lon space.\r\n1. Add `to_geometry` and `to_spherical_geography` function to convert between `SphericalGeography` and `Geometry` values. The conversion is mainly about validating the range of lat/lon values.\r\n1. Add `ST_Distance` support for SphericalGeography objects. Right now it only supports points and returns the great circle distance between them. Support for other shapes will be added in later PRs.\r\n1. Spatial functions operating on SphericalGeography objects are excluded from spatial joins for now.", "NaN"], ["11582", "Support avro.schema.url being HDFS location", "Piotr Findeisen", "findepi", "10/09/18, 03:09:18 PM", "Fixes #11548", "NaN"], ["11583", "Fix/improve some JDBC tests", "Wojciech Biela", "ilfrin", "11/22/18, 04:16:41 PM", "NaN", "NaN"], ["11584", "Remove unreachable test code", "Piotr Findeisen", "findepi", "10/02/18, 09:14:44 AM", "NaN", "NaN"], ["11585", "Move statistics aggregations on write to system memory pool", "Andrii Rosa", "arhimondr", "09/26/18, 10:09:54 PM", "With the change that removes the memory limit for partial aggregations the\r\nstatistics aggregations were updated to use user memory instead of system memory.\r\n\r\nStatistics aggregations is an implicit process that the query doesn't\r\nhave much control over. It is very similar to implicit buffer allocations required\r\nwhen writing ORC files. Although the amount of memory used depends on the user input\r\n(number of columns in the table) the user may not even be aware that stats will be\r\ncollected. Therefore, it sounds like it is more suitable to use system memory\r\nfor stats collecting aggregations.\r\n\r\nThis change introduces a \"useSystemMemory\" parameter that sets the type of\r\nmemory that aggregation operators should use. The existing rule of using\r\nsystem memory for partial aggregations is also removed. The type of memory to use is\r\nnow explicitly set in LocalExecutionPlanner based on the type of aggregation --\r\nsystem memory is used for partial aggregations and for column statistics aggregations\r\nused during writes.", "NaN"], ["11586", "Fix behavior and add validation to ORC writer validation percentage", "Haozhun Jin", "haozhun", "10/01/18, 09:27:26 PM", "It is very unintuitive that setting the config property or session property\r\nto 0.0 results in 100% validation. This commit fixes that.\r\nThis commit also adds validation to the properties to require that they be\r\nbetween 0 and 100 inclusive.", "NaN"], ["11588", "Add release notes for 0.212", "Raghav Sethi", "raghavsethi", "09/27/18, 07:07:11 PM", "Closes https://github.com/prestodb/presto/issues/11477", "NaN"], ["11589", "Add JMH Benchmark for window operator", "Atri Sharma", "atris", "10/09/18, 09:15:36 PM", "NaN", "NaN"], ["11594", "Fix planning failure for DistinctLimit with hash generation optimizer", "Martin Traverso", "martint", "09/27/18, 08:22:01 PM", "A recent change (032575a1f2d47de2cc034716de645ce3c1b14281) uncovered a latent bug in\r\nthe precomputed hash optimizer for DistinctLimit. The code was declaring that\r\nthe subplan rooted at DistinctLimit produced the same set of hashes as its source,\r\nwhich is incorrect given that DistinctLimit propagates only the distinct columns\r\nand a *single* precomputed hash field.\r\n\r\nFixes #11593", "NaN"], ["11595", "Do not return wrong stats/cost for partial aggregations", "Piotr Findeisen", "findepi", "10/02/18, 09:12:47 AM", "NaN", "NaN"], ["11602", "Make ST_Distance return null on empty input geometries", "Xiaoming Gao", "gaoxm", "10/03/18, 06:42:20 PM", "According to SQL/MM Part 3 spec, ST_Distance should return null if either input is an\r\nempty geometry: http://jtc1sc32.org/doc/N1101-1150/32N1107-WD13249-3--spatial.pdf\r\n\r\nThe current implementation of ST_Distance in Presto ultimately goes here and returns NaN\r\n(the version used in Presto returns 0.0D / 0.0) when one or both arguments are empty:\r\nhttps://github.com/Esri/geometry-api-java/blob/ecaedf4dd5acf00607023bcf9604380c0731e376/src/main/java/com/esri/core/geometry/OperatorDistanceLocal.java#L44-L45\r\n\r\nFixing this to return null in case either input is empty.\r\n\r\nResolves: Issue #11600", "NaN"], ["11603", "Add experimental config for disabling the reserved pool", "Nezih Yigitbasi", "nezihyigitbasi", "10/09/18, 09:14:59 PM", "This PR adds a new experimental config option for disabling the reserved\r\npool. The reason we have a reserved pool is to prevent deadlocks when\r\nthe general pool gets full. However, it's a source of inefficiency as a\r\ngood chunk of the heap just stays unused most of the time. Now that the\r\ncluster oom killer works reasonably well (after making it resilient to\r\nmemory accounting leaks/bugs) we can run some experiments by disabling\r\nthe reserved pool -- when the general pool is full on some worker the\r\noom killer should just resolve the situation by killing a query. It's\r\npossible that the killed query is retried by the client and cause oom\r\nagain, but the existing implementation is also already susceptible to\r\nthat issue, so this change doesn't make the situation any worse.", "NaN"], ["11610", "Improve readability of Thrift handles in explain plan", "David Phillips", "electrum", "10/01/18, 10:38:58 PM", "Fixes #11456", "NaN"], ["11616", "Remove path from ConnectorSession", "David Phillips", "electrum", "11/06/18, 10:41:20 PM", "It only exists for CURRENT_PATH and should not be exposed to plugins.", "NaN"], ["11617", "Fix typo in geospatial functions docs", "contra", "contra", "10/03/18, 02:53:42 PM", "`don not` -> `do not`", "NaN"], ["11619", "Add targetResultSize query parameter to statement endpoint", "Dain Sundstrom", "dain", "10/01/18, 11:13:14 PM", "Allow client to set target result size for response data.  The default remains 1 MB,\nbut the client can increase this to 128 MB.", "NaN"], ["11620", "Improve geometry_to_bing_tiles error message ", "Vic Zhang", "viczhang861", "10/05/18, 06:20:03 PM", "Fixes #11599 \r\n1. Improve error message for geo-function geometry_to_bing_tiles with zoom level,  tile number and rectangle geometry in wkt format \r\n2. Improve error message for converting circle to bing tiles\r\nwith zoom level,  tile number and radius in error message \r\ncc: @mbasmanova ", "NaN"], ["11624", "Fix typo in show_schemas.sql", "Karol Sobczak", "sopel39", "10/02/18, 07:10:10 PM", "NaN", "NaN"], ["11625", "Avoid making wild guesses in FILTER and JOIN estimates", "Andrii Rosa", "arhimondr", "10/17/18, 04:12:25 PM", "Apply 0.9 coefficient for a conjunct that cannot be estimated only if at least a single conjunct was estimated.", "NaN"], ["11627", "Fix invalid time unit conversion in BasicStageStats creation", "Dain Sundstrom", "dain", "10/02/18, 07:34:57 PM", "Use unit when summing duration and when creating result.", "NaN"], ["11630", "Fix concurrent lifespans for grouped execution", "Wenlei Xie", "wenleix", "10/10/18, 12:37:56 AM", "Previously when grouped execution is enabled, the concurrent lifespan\r\nwill be number of scheduled nodes, no matter what\r\n`concurrent_lifespans_per_task` is set to.", "NaN"], ["11634", "Fix flakiness in testTableWithNoSupportedColumns", "Piotr Findeisen", "findepi", "10/03/18, 08:45:09 PM", "The test failed recently with:\r\n```\r\nFor query:\r\n SHOW TABLES\r\nnot equal\r\nActual rows (up to 100 of 1 extra rows shown, 5 rows in total):\r\n    [tmp_presto_9463216b05bc4223a243554a6933634b]\r\nExpected rows (up to 100 of 0 missing rows shown, 4 rows in total):\r\n\r\n\tat com.facebook.presto.plugin.postgresql.TestPostgreSqlIntegrationSmokeTest.testTableWithNoSupportedColumns(TestPostgreSqlIntegrationSmokeTest.java:141)\r\n```", "NaN"], ["11635", "Verify that the SQL value can be decoded by the property", "Thoralf Gutierrez", "thoralf-gutierrez", "10/04/18, 04:44:33 AM", "Fixes https://github.com/prestodb/presto/issues/9433 and fixes https://github.com/prestodb/presto/issues/6741\r\n\r\nThe `// verify the SQL value can be decoded by the property` comment seemed to have been orphaned [here](https://github.com/prestodb/presto/pull/6136/files#diff-860d43d77c1c746c4603e32873b55f8cL79), and this commit adds a check back with some tests to make sure it doesn't happen again.\r\n\r\nLet me know if a higher level function should be used that would also include decoding\r", "NaN"], ["11636", "Reimplement JoinBridgeManager to fix bugs and simplify logic", "Haozhun Jin", "haozhun", "10/10/18, 07:55:44 PM", "NaN", "NaN"], ["11641", "Tighten show stats", "Vic Zhang", "viczhang861", "12/17/18, 06:26:45 PM", "Fixes #11551\r\n\r\n- Deprecate on keyword in SHOW STATS ON table\r\n\r\n - No longer support SELECT columns clause, only SELECT * is allowed, e.g., SHOW STATS FOR (SELECT * FROM t)\r\n\r\n - Columns in WHERE clause must be enforced columns, e.g., Hive partition columns", "NaN"], ["11643", "Preserve original exception for metastore network errors", "David Phillips", "electrum", "10/05/18, 12:11:41 AM", "NaN", "NaN"], ["11644", "Add session property to overwrite configured min/max drivers per task", "Jiexi Lin", "jessesleeping", "10/10/18, 08:56:40 PM", "[Edited]\r\nBy default max number of driver per task will be capped by the configured value in `TaskManagerConfig`. Valid value of this session property is either `NULL` or a positive integer. If the session property is set and it's not `NULL`:\r\n\r\n(1) If the session property value is less than `task.min-drivers-per-task`, it overwrites the min drivers count.\r\n(2) If the session property value is greater than `task.max-drivers-per-task`, the session property will be ignored. The actual max driver count per task is still capped by the system configured value.\r\n\r\nIt basically enforces a cap on the driver count per task based on user specified value. ", "NaN"], ["11649", "Fix query hang when a group has no splits in grouped execution", "Haozhun Jin", "haozhun", "10/12/18, 10:04:10 PM", "NaN", "NaN"], ["11650", "Count splits for unstarted source nodes as queuedDrivers", "Haozhun Jin", "haozhun", "10/09/18, 11:55:09 PM", "Fixes bug 2 in https://github.com/prestodb/presto/issues/11253", "NaN"], ["11660", "Replace constants with static factory methods", "Andrii Rosa", "arhimondr", "10/10/18, 02:56:07 PM", "NaN", "NaN"], ["11665", "Add test for CBO decisions on TPC-H, TPC-DS queries", "Grzegorz Kokosi\u0144ski", "kokosing", "10/16/18, 08:20:12 AM", "Add test for CBO decisions on TPC-H, TPC-DS queries", "NaN"], ["11667", "Restrict size of automatically broadcasted tables", "Karol Sobczak", "sopel39", "10/11/18, 03:46:09 PM", "Add join-max-broadcast-table-size property\r\n\r\nProperty defines maximum size of a table\r\nthat can be broadcast for JOIN (0 disables the limit).\r\n\r\nSupersedes https://github.com/prestodb/presto/pull/11570", "NaN"], ["11670", "Declare missing dependency on sudo in RPM", "Piotr Findeisen", "findepi", "10/10/18, 07:32:55 AM", "NaN", "NaN"], ["11671", "Provide log.properties in RPM", "Piotr Findeisen", "findepi", "10/10/18, 07:34:43 AM", "For convenience of administrators, provide `log.properties` as part of\r\nan RPM install. That way, people do not have to think where the file\r\nshould be put. Later, we can provide useful logging configuration:\r\n- suppress overly verbose logging from Hive, see\r\n  https://github.com/prestodb/presto-hive-apache/pull/33\r\n- logging configuration for troubleshooting problems (\"uncomment ...\r\n  when diagnosing ...\"), see e.g.\r\n  https://github.com/prestodb/presto/wiki/Security-Troubleshooting-Guide#before-you-begin", "NaN"], ["11672", "Remove JVM vendor check from RPM", "Piotr Findeisen", "findepi", "10/15/18, 09:14:25 AM", "988f0f2e7bd0a33b2d0d2585cb70eab078fb226c removed JVM vendor check from\r\nPresto server.", "NaN"], ["11673", "Report exceptions from SerDe better, especially for Avro", "Piotr Findeisen", "findepi", "10/10/18, 07:11:42 AM", "NaN", "NaN"], ["11676", "Create quarantine file only once on recovery corruption", "Jiexi Lin", "jessesleeping", "11/06/18, 10:18:52 PM", "Remove the timestamp suffix of the quarantine file so that we only\r\ncopy the file on first discovery of the corrupt file.", "NaN"], ["11677", "Add support for writing DWRF files with ZSTD compression", "David Phillips", "electrum", "10/09/18, 09:20:24 PM", "This fixes `TestFullOrcReader`, which broke after adding ZSTD for ORC.", "NaN"], ["11678", "Remove old zstd decoder implementation", "Martin Traverso", "martint", "11/28/18, 07:08:43 PM", "It is now part of https://github.com/airlift/aircompressor", "NaN"], ["11680", "Implement qdigest type and associated functions", "Timothy Meehan", "tdcmeehan", "11/14/18, 11:07:58 PM", "Adds the following functions for creating and manipulating qdigests:\r\n- `qdigest_agg`: aggregation function to create qdigest from real, bigint or double\r\n  values\r\n- `merge`: aggregation function to merge multiple qdigests\r\n- `value_at_quantile`: scalar function that retrieves a given quantile from a\r\n  qdigest\r\n- `values_at_quantiles`: scalar function that retrieves a list of quantiles from a\r\n  qdigest\r\n\r\nWeights and accuracy specifications can be set to `qdigest_agg` in the same way they can be set on `approx_percentile`.  `qdigest`s support casting to and from `varbinary`.\r\n\r\nFixes #8695 \r\n\r\nThe serialization format still needs to be agreed on and documented, so this should not be merged until that is done.  airlift/airlift#678", "NaN"], ["11681", "Fix compiler error in BenchmarkWindowOperator due to merge conflict", "Haozhun Jin", "haozhun", "10/10/18, 12:11:27 AM", "NaN", "NaN"], ["11686", "Remove Optional from FilterStatsCalculator and ComparisonStatsCalculator method signatures", "Andrii Rosa", "arhimondr", "10/11/18, 08:52:54 PM", "Having Optional in these interfaces makes code messy. `Optional.empty()` indicates an absence of the estimate. However it doesn't mean that if the result is not empty, the estimate is not unknown. That makes code harder to reason about.", "NaN"], ["11688", "Remove TypeLiteral from ExchangeClientSupplier binding", "Elon Azoulay", "elonazoulay", "10/10/18, 08:33:15 PM", "No need to use TypeLiteral Since ExchangeClientSupplier is not generic", "NaN"], ["11690", "Increase partitionTargetPath visibility", "Andrii Rosa", "arhimondr", "10/11/18, 04:46:45 PM", "NaN", "NaN"], ["11691", "Verify Connection returned from driver", "Piotr Findeisen", "findepi", "10/11/18, 06:57:48 AM", "A `Driver` can return `null` connection when e.g. it does not recognize\r\nthe connection URL.", "NaN"], ["11692", "Display real cpu time in explain analyze", "Andrii Rosa", "arhimondr", "10/16/18, 06:06:58 PM", "```\r\npresto:default> explain analyze select linestatus, count(orderkey) from lineitem group by 1;\r\n                                                                                Query Plan                                                                                 \r\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n Fragment 1 [HASH]                                                                                                                                                         \r\n     CPU: 31.86ms, Scheduled: 140.80ms, Input: 10 rows (240B); per task: avg.: 10.00 std.dev.: 0.00, Output: 2 rows (30B)                                                  \r\n     Output layout: [linestatus, count]                                                                                                                                    \r\n     Output partitioning: SINGLE []                                                                                                                                        \r\n     Grouped Execution: false                                                                                                                                              \r\n     - Project[] => [linestatus:varchar(1), count:bigint]                                                                                                                  \r\n             CPU: 2.00ms (0.10%), Scheduled: 5.00ms (0.04%), Input: 2 rows (48B), Output: 2 rows (30B), Filtered: 0.00%                                                    \r\n             Input avg.: 0.13 rows, Input std.dev.: 264.58%                                                                                                                \r\n         - Aggregate(FINAL)[linestatus][$hashvalue] => [linestatus:varchar(1), $hashvalue:bigint, count:bigint]                                                            \r\n                 CPU: 4.00ms (0.20%), Scheduled: 7.00ms (0.06%), Output: 2 rows (48B)                                                                                      \r\n                 Input avg.: 0.63 rows, Input std.dev.: 264.58%                                                                                                            \r\n                 count := \"count\"(\"count_9\")                                                                                                                               \r\n             - LocalExchange[HASH][$hashvalue] (\"linestatus\") => linestatus:varchar(1), count_9:bigint, $hashvalue:bigint                                                  \r\n                     CPU: 5.00ms (0.24%), Scheduled: 23.00ms (0.18%), Output: 10 rows (240B)                                                                               \r\n                     Input avg.: 0.63 rows, Input std.dev.: 186.55%                                                                                                        \r\n                 - RemoteSource[2] => [linestatus:varchar(1), count_9:bigint, $hashvalue_10:bigint]                                                                        \r\n                         CPU: 1.00ms (0.05%), Scheduled: 5.00ms (0.04%), Output: 10 rows (240B)                                                                            \r\n                         Input avg.: 0.63 rows, Input std.dev.: 186.55%                                                                                                    \r\n                                                                                                                                                                           \r\n Fragment 2 [SOURCE]                                                                                                                                                       \r\n     CPU: 2.03s, Scheduled: 7.08s, Input: 6001215 rows (137.36MB); per task: avg.: 6001215.00 std.dev.: 0.00, Output: 10 rows (240B)                                       \r\n     Output layout: [linestatus, count_9, $hashvalue_11]                                                                                                                   \r\n     Output partitioning: HASH [linestatus][$hashvalue_11]                                                                                                                 \r\n     Grouped Execution: false                                                                                                                                              \r\n     - Aggregate(PARTIAL)[linestatus][$hashvalue_11] => [linestatus:varchar(1), $hashvalue_11:bigint, count_9:bigint]                                                      \r\n             CPU: 392.00ms (19.18%), Scheduled: 436.00ms (3.47%), Output: 10 rows (240B)                                                                                   \r\n             Input avg.: 1200243.00 rows, Input std.dev.: 11.99%                                                                                                           \r\n             count_9 := \"count\"(\"orderkey\")                                                                                                                                \r\n         - ScanProject[table = hive:default:lineitem, grouped = false] => [orderkey:bigint, linestatus:varchar(1), $hashvalue_11:bigint]                                   \r\n                 Cost: {rows: 6001215 (85.85MB), cpu: 90018225.00, memory: 0.00, network: 0.00}/{rows: 6001215 (137.36MB), cpu: 234047385.00, memory: 0.00, network: 0.00} \r\n                 CPU: 1.64s (80.23%), Scheduled: 12.10s (96.22%), Input: 6001215 rows (215.05MB), Output: 6001215 rows (137.36MB), Filtered: 0.00%                         \r\n                 Input avg.: 1200243.00 rows, Input std.dev.: 11.99%                                                                                                       \r\n                 $hashvalue_11 := \"combine_hash\"(bigint '0', COALESCE(\"$operator$hash_code\"(\"linestatus\"), 0))                                                             \r\n                 LAYOUT: default.lineitem                                                                                                                                  \r\n                 orderkey := orderkey:bigint:0:REGULAR                                                                                                                     \r\n                 linestatus := linestatus:varchar(1):9:REGULAR                                                                                                             \r\n                                                                                                                                                                           \r\n                                                                                                                                                                           \r\n(1 row)\r\n```", "NaN"], ["11693", "Support dynamic node assignment for grouped execution", "Wenlei Xie", "wenleix", "12/02/18, 03:53:11 PM", "- [x] Make connector bucket to node map optional \r\n- [x] Introduce BucketNodeMap\r\n- [x] Support dynamic node assignment for grouped execution\r\n- [x] Cleanup and Rebase on #11649 \r\n\r\n---------------\r\n\r\nMinor fixes requested by the reviews:\r\n- [x] Make `FixedLifespanScheduler` a top level class\r\n- [x] Fix `nodeScheduler.createNodeSelector(null)`\r\n- [x] Move `newDriverGroupReady.set(null)` out of critical section. \r\n\r\n-----------------------\r\n\r\nRequired by https://github.com/prestodb/presto/issues/12124 to enable recoverable grouped execution.", "NaN"], ["11697", "Fix typo in docs", "Piotr Findeisen", "findepi", "10/12/18, 09:30:37 AM", "NaN", "NaN"], ["11698", "Fix exception message incorrectly referring to partition", "Piotr Findeisen", "findepi", "10/12/18, 12:23:55 PM", "NaN", "NaN"], ["11700", "Embed Presto version in stacktraces", "Piotr Findeisen", "findepi", "10/15/18, 07:34:06 PM", "Sometimes, users will just report a stacktrace, without telling which\r\nPresto version it is for. Embed version information in exceptions\r\nproduced by Presto, so that the version is immediately know.\r\n\r\nExample output\r\n```\r\n$ ./presto-product-tests/conf/docker/singlenode/compose.sh run application-runner /docker/volumes/conf/docker/files/presto-cli.sh --debug --catalog hive --schema default --user hive\r\n+ java -jar /docker/volumes/presto-cli/presto-cli-executable.jar --server presto-master:8080 --debug --catalog hive --schema default --user hive\r\npresto:default> select * from x;\r\nQuery 20181012_121120_00002_k4ata failed: line 1:15: Table hive.default.x does not exist\r\ncom.facebook.presto.sql.analyzer.SemanticException: line 1:15: Table hive.default.x does not exist\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitTable(StatementAnalyzer.java:848)\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitTable(StatementAnalyzer.java:258)\r\n\tat com.facebook.presto.sql.tree.Table.accept(Table.java:53)\r\n\tat com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:270)\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.analyzeFrom(StatementAnalyzer.java:1772)\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuerySpecification(StatementAnalyzer.java:954)\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuerySpecification(StatementAnalyzer.java:258)\r\n\tat com.facebook.presto.sql.tree.QuerySpecification.accept(QuerySpecification.java:127)\r\n\tat com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:270)\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:280)\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuery(StatementAnalyzer.java:676)\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuery(StatementAnalyzer.java:258)\r\n\tat com.facebook.presto.sql.tree.Query.accept(Query.java:94)\r\n\tat com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:270)\r\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer.analyze(StatementAnalyzer.java:244)\r\n\tat com.facebook.presto.sql.analyzer.Analyzer.analyze(Analyzer.java:72)\r\n\tat com.facebook.presto.sql.analyzer.Analyzer.analyze(Analyzer.java:64)\r\n\tat com.facebook.presto.execution.SqlQueryExecution.<init>(SqlQueryExecution.java:184)\r\n\tat com.facebook.presto.execution.SqlQueryExecution$SqlQueryExecutionFactory.createQueryExecution(SqlQueryExecution.java:721)\r\n\tat com.facebook.presto.execution.SqlQueryManager.createQueryInternal(SqlQueryManager.java:453)\r\n\tat com.facebook.presto.execution.SqlQueryManager.lambda$createQuery$3(SqlQueryManager.java:386)\r\n\tat com.facebook.presto.$gen.Presto_0_212_94_g1fabb82____20181012_121051_1.run(Unknown Source)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```", "NaN"], ["11701", "Always compact pages in memory connector", "Karol Sobczak", "sopel39", "10/12/18, 01:52:19 PM", "Memory connector might receive pages directly\r\nfrom exchange. Such pages are composed from\r\nblocks that refer to the same byte array.\r\nIt causes overaccounting of Page memory", "NaN"], ["11705", "Redirects from root include host name", "alysha", "actgardner", "10/17/18, 08:45:24 PM", "In the common case that a user visits `/`, we want to redirect them to `/ui/`. Before we added the protocol explicitly (here: https://github.com/prestodb/presto/commit/656c2fcd3eddb27dde890ce28eb371333bd0ce25), this would be achieved with a relative redirect:\r\n\r\n```\r\n17754 $ curl -v localhost:8080\r\n* Rebuilt URL to: localhost:8080/\r\n*   Trying ::1...\r\n* TCP_NODELAY set\r\n* Connected to localhost (::1) port 8080 (#0)\r\n> GET / HTTP/1.1\r\n> Host: localhost:8080\r\n> User-Agent: curl/7.61.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 301 Moved Permanently\r\n< Date: Fri, 12 Oct 2018 20:48:14 GMT\r\n< Location: /ui/\r\n< Content-Length: 0\r\n<\r\n```\r\n\r\nSince adding the schema we now get a mangled URI (`http:/ui/`), which Chrome seems to tolerate on localhost due to some special-casing, but it breaks in most cases:\r\n```\r\n17760 $ curl -v http://localhost:8080/\r\n*   Trying ::1...\r\n* TCP_NODELAY set\r\n* Connected to localhost (::1) port 8080 (#0)\r\n> GET / HTTP/1.1\r\n> Host: localhost:8080\r\n> User-Agent: curl/7.61.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 301 Moved Permanently\r\n< Date: Fri, 12 Oct 2018 20:54:10 GMT\r\n< Location: http:/ui/\r\n< Content-Length: 0\r\n<\r\n```\r\n\r\nThis PR fixes the URI by using the host and forwarded protocol to create an absolute redirect:\r\n```\r\n17754 $ curl -v localhost:8080\r\n* Rebuilt URL to: localhost:8080/\r\n*   Trying ::1...\r\n* TCP_NODELAY set\r\n* Connected to localhost (::1) port 8080 (#0)\r\n> GET / HTTP/1.1\r\n> Host: localhost:8080\r\n> User-Agent: curl/7.61.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 301 Moved Permanently\r\n< Date: Fri, 12 Oct 2018 20:48:14 GMT\r\n< Location: http://localhost:8080/ui/\r\n< Content-Length: 0\r\n<\r\n```\r\n\r\nIt's not clear what the motivation for adding the scheme was in the first place - relative redirects should always work, regardless of whether they're proxied. I would be open to just reverting 656c2fcd3eddb27dde890ce28eb371333bd0ce25, I don't understand the motivation behind this change?", "NaN"], ["11707", "Document Hive connector procedure call to create empty partition", "Wenlei Xie", "wenleix", "11/22/18, 04:58:49 AM", "NaN", "NaN"], ["11709", "Remove redundant byte[] allocation in VariableWidthBlock", "cem cayiroglu", "cemcayiroglu", "10/15/18, 10:48:15 PM", "Slice.getBytes is allocating a redundant byte[] which can be avoided\r\nby using SliceOutput.writeBytes in VariableWidthBlock.copyPositions.\r\n\r\nFrom production:\r\n\r\n```\r\n --- 3058697816 bytes (2.02%), 446 samples\r\n[ 0] byte[]\r\n[ 1] io.airlift.slice.Slice.getBytes\r\n[ 2] com.facebook.presto.spi.block.VariableWidthBlock.copyPositions\r\n[ 3] com.facebook.presto.operator.project.InputPageProjection.project\r\n ```\r\nThe fix has been tested with tpch sf100 q10 and got rid of the below 3.6 GB byte[] allocation. \r\n\r\n```\r\n--- 3652861448 bytes (0.26%), 958 samples\r\n  [ 0] byte[]\r\n  [ 1] io.airlift.slice.Slice.getBytes\r\n  [ 2] com.facebook.presto.spi.block.VariableWidthBlock.copyPositions\r\n  [ 3] com.facebook.presto.operator.exchange.PartitioningExchanger.accept\r\n```\r\n\r\n\r", "NaN"], ["11717", " Limit the size of page chunk for OrcWriter", "Wenlei Xie", "wenleix", "10/28/18, 03:45:29 AM", "Presto can produce huge pages (e.g. dictionary blocks produced in\r\nJoinOperator). Writing a large page at one time for ORC Writer will\r\ncause excessively huge stripe.\r\n\r\n\r\nBased on the `Block.getLogicalSize` method added in https://github.com/prestodb/presto/pull/11628", "NaN"], ["11718", "Simpify domain before parsing for EXPLAIN (TYPE IO)", "Shixuan Fan", "shixuan-fan", "11/26/18, 07:21:01 PM", "NaN", "NaN"], ["11719", "Update to Airlift 0.174", "Shixuan Fan", "shixuan-fan", "10/16/18, 04:43:11 PM", "NaN", "NaN"], ["11722", "Change cost estimate formatting in PlanPrinter", "Andrii Rosa", "arhimondr", "10/18/18, 02:51:33 PM", "- Do not print estimates if stats are unknown for all the plan nodes for the operator\r\n- Print \"?\" for unknown stats if stats are known for some plan nodes for the operator\r\n\r\nFor example ScanFilterProject operation consist of 3 different plan nodes. ScanNode, FilterNode, ProjectNode.\r\n\r\nIf the cost estimate is lost, say, on Filter node, for the matter of readability it makes sense to print:\r\n\r\n`{rows: 123 (23kB), cpu: 234, memory: 345, network: 567}/{rows: ? (?), cpu: ?, memory: ?, network: ?}/{rows: ? (%s), cpu: ?, memory: ?, network: ?}`\r\n\r\nso it is clear that the estimate is lost on the filter operation.\r\n\r\nIt doesn't make much sense to always print \"?\" though, as it makes plan to look cumbersome without introducing any value.", "NaN"], ["11724", "Remove unnecessary byte[] allocation in block builders", "cem cayiroglu", "cemcayiroglu", "10/17/18, 03:44:21 AM", "This is a follow up PR of #11709. Slice.getBytes is allocating a redundant \r\nbyte[] which can be avoided by using SliceOutput.writeBytes in \r\nFixedWidthBlockBuilder.copyPositions and VariableWidthBlockBuilder.copyPositions. \r\n\r", "NaN"], ["11726", "Cleanup code formatting", "David Phillips", "electrum", "10/31/18, 12:01:46 AM", "NaN", "NaN"], ["11727", "Add UDF Ngrams", "Ang Chen", "chanllen", "10/29/18, 05:46:29 AM", "Create a UDF for ngrams\r\n\r\n```\r\n Returns ``n``-grams for the ``array``::\r\n        SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 2); --[['foo', 'bar'], ['bar', 'baz'], ['baz', 'foo']]\r\n        SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 3); -- [['foo', 'bar', 'baz'], ['bar', 'baz', 'foo']]\r\n        SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 4); -- [['foo', 'bar', 'baz', 'foo']]\r\n        SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 5); -- [['foo', 'bar', 'baz', 'foo']]\r\n        SELECT ngrams(ARRAY[1, 2, 3, 4], 2); --[[1, 2], [2, 3], [3, 4]]\r\n```", "NaN"], ["11730", "Optimize ST_IsEmpty function", "Maria Basmanova", "mbasmanova", "10/17/18, 06:12:12 PM", "It is sufficient to deserialize the bounding box to decide whether the geometry is empty.", "NaN"], ["11731", "Update to Airbase 86", "David Phillips", "electrum", "10/30/18, 05:01:57 PM", "NaN", "NaN"], ["11732", "Harden ST_IsEmpty function", "Maria Basmanova", "mbasmanova", "10/18/18, 02:11:28 AM", "Make sure ST_IsEmpty returns `true` for empty envelopes.", "NaN"], ["11733", "Add ST_MultiPoint function", "Maria Basmanova", "mbasmanova", "10/18/18, 06:36:25 PM", "ST_MultiPoint function constructs a MultiPoint geometry from an array of points.", "NaN"], ["11734", "Simplify query management", "Dain Sundstrom", "dain", "11/04/18, 11:33:44 PM", "* Use future for pre allocated memory.\r\n* Use future for min worker nodes.\r\n* Add min worker nodes to waiting for resources stage.\r\n* Simplify code in QueryStateMachine, QueryMonitor, NodeManager.\r\n* Consolidate to one way to get QueryId and ResourceGroupId.\r\n* Remove unused code, fix warnings, and such", "NaN"], ["11738", "Remove duplicate dependencies", "David Phillips", "electrum", "10/18/18, 08:07:43 PM", "NaN", "NaN"], ["11740", "Fix typo in TransactionMetadata method name", "Piotr Findeisen", "findepi", "10/27/18, 07:20:57 AM", "NaN", "NaN"], ["11743", "Improve scalar stats calculations", "Karol Sobczak", "sopel39", "10/27/18, 06:57:50 AM", "* Improve calculations involving tinyint, smallint conditions\r\n* Improve calculations involving magic function call (e.g: varbinary, maps? arrays?)", "NaN"], ["11746", "Support multiple conventions in PolymorphicScalarFunction", "cem cayiroglu", "cemcayiroglu", "11/22/18, 08:01:35 PM", "PolymorphicScalarFunction and PolymorphicScalarFunctionBuilder have \r\nchanged to support adding multiple convention choices.\r\nTestPolymorphicScalarFunction.testSelectsMultipleChoice is \r\nan example of the usage.\r\n\r\nThis fixes #11723", "NaN"], ["11748", "Make QueryResults backwards compatible with warnings", "Elon Azoulay", "elonazoulay", "10/19/18, 10:40:14 PM", "NaN", "NaN"], ["11752", "Remove usage of javax.annotation.Nonnull", "Grzegorz Kokosi\u0144ski", "kokosing", "10/27/18, 09:58:19 PM", "Remove usage of javax.annotation.Nonnull\n\nBy default fields and parameters are by default non nullable.\nThere is no need to annotate that.", "NaN"], ["11753", "Implement recording metastore", "Karol Sobczak", "sopel39", "10/30/18, 07:53:07 AM", "    RecordingHiveMetastore will capture results of\r\n    metastore calls for a predefined period of time.\r\n    It can then save such collected results as a JSON\r\n    file which can be loaded later on for debugging\r\n    purpsoses.\r\n\r\n    This change will simplify remote debugging of\r\n    planner and CBO issues. Therefore community\r\n    won't have to reproduce planner issues using\r\n    TPCH connector which is sometimes impossible\r\n    (e.g: CBO issues).\r\n\r\nFYI: @electrum ", "NaN"], ["11756", "Fix inverted logic for Presto version in stack traces", "David Phillips", "electrum", "10/23/18, 01:06:26 AM", "NaN", "NaN"], ["11757", "Improve performance for DictionaryBlock.getSizeInBytes", "Rongrong Zhong", "rongrong", "11/27/18, 11:04:38 PM", "When `getSizeInBytes` of the `DictionaryBlock` is called for the first time, it would try to calculate the size by calling `getRegionSizeInBytes` for each element with `length` 1. The implementation of `getRegionSizeInBytes` of `DictionaryBlock` would allocate a boolean array of the size of the block. Thus in the case where the underlying block also contains a `DictionaryBlock`, the boolean array would be allocated for each element in the `DictionaryBlock`. This gets really expensive when the block is large. As the following benchmark numbers demonstrate:\r\n```\r\n    Before:\r\n    Benchmark                                              (selectedPositions)  Mode  Cnt       Score       Error  Units\r\n    BenchmarkDictionaryBlockGetSizeInBytes.getSizeInBytes                  100  avgt   20    5173.612 \u00b1  6164.744  us/op\r\n    BenchmarkDictionaryBlockGetSizeInBytes.getSizeInBytes                 1000  avgt   20   32607.266 \u00b1 37310.523  us/op\r\n    BenchmarkDictionaryBlockGetSizeInBytes.getSizeInBytes                10000  avgt   20  113352.382 \u00b1 13933.478  us/op\r\n    BenchmarkDictionaryBlockGetSizeInBytes.getSizeInBytes               100000  avgt   20  771976.351 \u00b1 54696.701  us/op\r\n\r\n    After:\r\n    Benchmark                                              (selectedPositions)  Mode  Cnt     Score     Error  Units\r\n    BenchmarkDictionaryBlockGetSizeInBytes.getSizeInBytes                  100  avgt   20   434.331 \u00b1  49.481  us/op\r\n    BenchmarkDictionaryBlockGetSizeInBytes.getSizeInBytes                 1000  avgt   20   612.484 \u00b1  94.415  us/op\r\n    BenchmarkDictionaryBlockGetSizeInBytes.getSizeInBytes                10000  avgt   20   908.409 \u00b1  82.704  us/op\r\n    BenchmarkDictionaryBlockGetSizeInBytes.getSizeInBytes               100000  avgt   20  3043.432 \u00b1 289.973  us/op\r\n```", "NaN"], ["11761", "Update documentation for guarantees provided by startMemoryRevoke()", "Piotr Findeisen", "findepi", "10/27/18, 07:20:40 AM", "The guarantees were changed in 7d1157312873e83d9fa662855416bc21427cdf42\r\nand cf50577d755c943f70dc9de01cfd6c2545c8fb3d.", "NaN"], ["11764", "Building pages in MergeSortedPages should progress computations", "Karol Sobczak", "sopel39", "10/27/18, 06:58:53 AM", "When yield signal is on and computations yielded previously\r\nthen during next process() computations should progress.", "NaN"], ["11765", "Disable stats and cost calculators by default", "Andrii Rosa", "arhimondr", "10/23/18, 06:53:09 PM", "Stats calculator fails on assertions for complex queries, thus it is not production ready yet.\r\n\r\nThe https://github.com/prestodb/presto/pull/11511 changes the place where the stats are computed to be\r\ndisplayed in the final plan. Before this patch, the stats were computed in QueryMonitor, when printing a final plan.\r\nBefore if the stats couldn't be computed for whatever reason, only the text plan generation would fail.\r\n\r\nCurrently, when the stats calculator is invoked for every query during the initial planning, queries may\r\nbe failing, even when the CBO is not used.\r\n\r\nThis change disables stats calculator by default. It can be enabled back with a session property on per query basis.", "NaN"], ["11769", "Make FileBased*AccessControls to be refreshable", "Grzegorz Kokosi\u0144ski", "kokosing", "10/28/18, 08:33:09 PM", "Make FileBased*AccessControls to be refreshable", "NaN"], ["11772", "Add release notes for 0.213", "David Phillips", "electrum", "10/26/18, 09:59:28 PM", "NaN", "NaN"], ["11773", "Add feature toggle for TopNRowNumber optimization", "Karol Sobczak", "sopel39", "10/27/18, 06:57:20 AM", "    TopNRowNumberOperator can cause increased memory footprint\r\n    (e.g: 2x-3x) vs plain WindowOperator if partitions are small\r\n    (e.g: few rows). This can cause queries to fail because of\r\n    out of memory.", "NaN"], ["11775", "Update documentation for the kill_query procedure", "David Phillips", "electrum", "10/24/18, 06:22:13 PM", "<img src=\"https://user-images.githubusercontent.com/9230/47450834-1189e880-d77b-11e8-83d3-f85c542f756a.png\">\r", "NaN"], ["11776", "Use canonical form for parametric types in documentation", "Martin Traverso", "martint", "10/24/18, 06:24:30 PM", "The <> syntax is deprecated. References to parametric types should use the ()\r\nsyntax instead:\r\n\r\n    array(bigint)\r\n\r\ninstead of\r\n\r\n    array<bigint>", "NaN"], ["11778", "Streamline WorkProcessor state API", "Piotr Findeisen", "findepi", "10/31/18, 11:47:36 AM", "NaN", "NaN"], ["11779", "Remove legacy TransformCorrelatedSingleRowSubqueryToProject optimizer", "Piotr Findeisen", "findepi", "10/27/18, 07:20:05 AM", "The corresponding rule has been added in 0.208.\r\n\r\n(Extracted from #11736)", "NaN"], ["11781", "Update to Aircompressor 0.13", "David Phillips", "electrum", "10/25/18, 06:26:18 PM", "This fixes a potential out-of-bounds read for ZSTD on corrupted input.", "NaN"], ["11782", "Add distribution type to semi join query plan", null, "gozingdown", "10/27/18, 09:17:04 PM", "NaN", "NaN"], ["11784", "Compare Avro SerDe class name case sensitively", "Grzegorz Kokosi\u0144ski", "kokosing", "10/27/18, 06:34:12 PM", "Compare Avro SerDe class name case sensitively", "NaN"], ["11787", "Add removed comment", "Karol Sobczak", "sopel39", "10/27/18, 06:56:11 AM", "NaN", "NaN"], ["11788", "Rename NestedLoopJoinPagesBridge to NestedLoopJoinBridge", "Haozhun Jin", "haozhun", "11/02/18, 12:32:23 AM", "This is a followup item from a previous PR.", "NaN"], ["11791", "Lazily build hashtable for MapBlock", "Ying", "yingsu00", "11/08/18, 07:00:19 AM", "Fix for #11808\r\nPresto builds hashtable for MapBlocks eagerly when constructing the\r\nMapBlock even it's not needed in the query. Building a hashtable could\r\ntake up to 30% CPU of the scan cost on a map column. This commit defers\r\nthe hashtable build to the time it's needed in SeekKey(). Note that we\r\nonly do this to the MapBlock, not the MapBlockBuilder to avoid complex\r\nsynchronization problems. The MapBlockBuilder will always build the\r\nhashtable. As the result MergingPageOutput and PartitionOutputOperator\r\nwill still rebuild the hashtables when needed. The measurements shows\r\nthere will be less than 10% pages for MergingPageOutput to build the\r\nhashtables. We will have a seperate PR to improve PartitionOutput\r\nand avoid rebuilding the pages so as to avoid hashtable rebuilding.\r\n\r\nSimple select checsum queries show over 40% CPU gain:\r\n```\r\nTest                          | After  | Before | Improvement\r\nselect 2 map columns checksum | 11.69d | 20.06d | 42%\r\nSelect 1 map column checksum  |  9.67d | 17.73d | 45%\r\n```", "NaN"], ["11792", "Make exception handling in SqlQueryManager more resilient", "Anu Sudarsan", "anusudarsan", "10/27/18, 08:39:34 PM", "Today, there are many places where `RuntimeException` can be thrown and not wrapped in\r\nproper `PrestoException`. One such example is `ConnectorMetadata#getTableHandle`.\r\nWhen a connector throws `RuntimeException`, no valid response is send back to the client\r\nand CLI displays \"Query is gone(server restarted?)\".\r\n\r\nThis change fixes exception propagation to the client. Fixing all the places where `RuntimeException`\r\nis not wrapped into `PrestoException` is a next step, but is beyond scope of this PR (and of any single\r\nPR for that matter).", "NaN"], ["11793", "Handle SQL execution timeouts for verifier rewrite", "David Phillips", "electrum", "10/28/18, 07:33:52 PM", "NaN", "NaN"], ["11794", "Stop using grouped execution for certain build-outer joins", "Haozhun Jin", "haozhun", "10/26/18, 06:22:22 PM", "This fixes execution failure due to sanity check violation\r\nfor fragments with the following shape.\r\n\r\n          LJoin\r\n          /   \\\r\n      RJoin   Scan\r\n      /   \\\r\n    Scan Remote", "NaN"], ["11796", "Add warnings to the webapp", "Elon Azoulay", "elonazoulay", "11/12/18, 11:37:50 PM", "NaN", "NaN"], ["11797", "Simplify WindowOperator.findEndPosition", "Piotr Findeisen", "findepi", "10/29/18, 10:21:01 PM", "NaN", "NaN"], ["11798", "Add @Nullable annotation and requireNonNull", "Karol Sobczak", "sopel39", "10/29/18, 09:41:43 AM", "NaN", "NaN"], ["11799", "[bugfix]return next if query is not done and there is more data to send", "Vico.Wu", "VicoWu", "12/15/18, 08:17:15 PM", "Hi @dain  @electrum \r\nI created an issue 15 days ago, saying that my presto jdbc client hangs for several minutes after the query has exactly failed in this [issue-11711](https://github.com/prestodb/presto/issues/11711 ) (For presto server 0.212).\r\nSorry for late reply, because during the past 15 days, I spend much time to find out the root cause of this unexpected phenomenon. The reason that I believe it is not caused by JDBC client is that I see that the final log:\r\n\r\n `2018-10-27T04:47:49.380-0700\tINFO\tquery-execution-27\tcom.facebook.presto.event.query.QueryMonitor\tTIMELINE: Query 20181027_112837_00001_8zsss :: Transaction:[a9aafaa7-b874-4aa0-acae-d96cdace2a48] :: elapsed 546881ms :: planning 7583ms :: scheduling 7997ms :: running 532335ms :: finishing 0ms :: begin 2018-10-27T04:28:39.084-07:00 :: end 2018-10-27T04:37:45.965-07:00` \r\n\r\n is printed after nearly 10 minutes when the query already failed, so , it seems that even the presto server didn't consider the query has failed but in fact it has already failed for several minutes;\r\n\r\nIt is hard to reproduce this weird phenomenon. I added many debug logs and after reproducing it for several times and add log again and again(In fact, the key is that we should make the presto worker crash more frequently),  I got the root cause.\r\n\r\nOverall, the story is: \r\n\r\n- **2018-10-27 04:28:39** I submitted my query to Presto server\r\n\r\n- **2018-10-27 04:37:45** the query failed according to my debug log added in `SqlStageExecution` and `SqlQueryExecution`. Also , from the listened I added, it also proved that the query already failed at this time. But the JDBC client didn't get any failed information , still waiting at `PrestoResultSet.next()`\r\n\r\n- Even after the query already failed, the logs below continues producing:\r\n  ```\r\n  2018-10-27T04:47:39.131-0700\tWARN\tTaskInfoFetcher-20181027_112837_00001_8zsss.2.22-894\tcom.facebook.presto.server.remotetask.RequestErrorTracker\tError getting info for task 20181027_112837_00001_8zsss.2.22: Server refused connection: http://10.88.193.19:8087/v1/task/20181027_112837_00001_8zsss.2.22?summarize: http://10.88.193.19:8087/v1/task/20181027_112837_00001_8zsss.2.22\r\n  \r\n  ```\r\n   It is because some worker node crashed, and the `TaskInfoFetche`r is stilled trying to get the task \r\n   information, but failed and retried.\r\n\r\n- So, in this state, although the state of the query marked as `FAILED`, but one of the stage and one task in this stage has not finished yet. This make the function `isFinalQueryInfo()` return false:\r\n    ```@JsonProperty\r\n    public boolean isFinalQueryInfo()\r\n    {\r\n        return state.isDone() && getAllStages(outputStage).stream().allMatch(StageInfo::isFinalStageInfo);\r\n    }\r\n    ```\r\n\r\n- Then, the api in `Query.java` has following logic:\r\n   ```\r\n   closeExchangeClientIfNecessary(queryInfo); //Yes, since the state of query is failed, so the exchangeClient will be closed\r\n   \r\n        // for queries with no output, return a fake result for clients that require it\r\n        if ((queryInfo.getState() == QueryState.FINISHED) && !queryInfo.getOutputStage().isPresent()) {\r\n            columns = ImmutableList.of(new Column(\"result\", BooleanType.BOOLEAN));\r\n            data = ImmutableSet.of(ImmutableList.of(true));\r\n        }\r\n   \r\n        // only return a next if the query is not done or there is more data to send (due to buffering)\r\n        URI nextResultsUri = null;\r\n        if (!queryInfo.isFinalQueryInfo() || !exchangeClient.isClosed()) {\r\n            nextResultsUri = createNextResultsUri(scheme, uriInfo);\r\n        }\r\n   ```\r\n    although `closeExchangeClientIfNecessary()` has already closed the exchange client, but in fact the `queryInfo.isFinalQueryInfo()` return false, so the predicate `if (!queryInfo.isFinalQueryInfo() || !exchangeClient.isClosed()) ` return true, thus creating a `nextResultsUri` . Since the  `nextResultsUri` is provided, my jdbc client believe there are more result, but in fact, the query has failed several minutes ago and in fact, the `errorCode` and `errorMsg` has already be provided in `QueryResult` several minutes ago:\r\n\r\n   ```\r\n           QueryResults queryResults = new QueryResults(\r\n                queryId.toString(),\r\n                queryHtmlUri,\r\n                findCancelableLeafStage(queryInfo),\r\n                nextResultsUri,\r\n                columns,\r\n                data,\r\n                toStatementStats(queryInfo),\r\n                toQueryError(queryInfo), //the query error has been provided when the query failed\r\n                queryInfo.getUpdateType(),\r\n                updateCount);\r\n   ```\r\n\r\n- **2018-10-27T04:47:49**  after retrying for several minutes, the `TaskInfoFetcher` finally give up retrying to fetch the task status and failed the task. So, the `closeExchangeClientIfNecessary()` return true now and the `nextResultsUri` is null. Then the jdbc client believe the query failed.\r\n   we could see the final log :\r\n\r\n    ```\r\n   2018-10-27T04:47:49.380-0700\tINFO\tquery-execution-27\tcom.facebook.presto.event.query.QueryMonitor\tTIMELINE: Query 20181027_112837_00001_8zsss :: Transaction:[a9aafaa7-b874-4aa0-acae-d96cdace2a48] :: elapsed 546881ms :: planning 7583ms :: scheduling 7997ms :: running 532335ms :: finishing 0ms :: begin 2018-10-27T04:28:39.084-07:00 :: end 2018-10-27T04:37:45.965-07:00\r\n    ```\r\n\r\nSo, the fix is , the predicate should be:\r\n```\r\n        if (!queryInfo.isFinalQueryInfo() && !exchangeClient.isClosed()) {\r\n            nextResultsUri = createNextResultsUri(scheme, uriInfo);\r\n        }\r\n```\r\nit means,  only return a non-empty `nextResultsUri` if  and only if the query is not done **AND** there is more data to send (due to buffering).  Namely, we sould make `nextResultsUri` null as long as the query is done, **OR**, there is no more data to send(exchangeClient is closed).", "NaN"], ["11801", "Remove insert_const_special_char product test", "Grzegorz Kokosi\u0144ski", "kokosing", "10/29/18, 08:27:35 AM", "Remove insert_const_special_char product test\n\n\\n has no special meaning in SQL", "NaN"], ["11803", "Remove hive_connector test group", "Piotr Findeisen", "findepi", "10/30/18, 08:50:04 AM", "It's not used for composing test runs.\r\nIt's not applied uniformly (some Hive tests lack it), so it cannot be\r\nused for composing test runs reliably.", "NaN"], ["11810", "Categorize error for broken array_sort comparator", "David Phillips", "electrum", "10/30/18, 12:36:38 AM", "NaN", "NaN"], ["11812", "Remove unused constructor for PrestoException", "David Phillips", "electrum", "11/06/18, 09:33:30 PM", "NaN", "NaN"], ["11813", "Fix typo in json property in ThreadResource", "Dongmin Yu", "miniway", "10/30/18, 08:07:16 AM", "I found that `JsonProperty`s are not compatible between ser-de at ThreadResource classes. ", "NaN"], ["11814", "Assure output page is loaded in PageSourceOperator", "Karol Sobczak", "sopel39", "10/30/18, 07:40:01 PM", "NaN", "NaN"], ["11815", "Rename misleading method name", "Karol Sobczak", "sopel39", "10/30/18, 10:03:59 PM", "NaN", "NaN"], ["11817", "Add space after hyphens for ngrams example", "Nezih Yigitbasi", "nezihyigitbasi", "10/31/18, 06:08:35 PM", "NaN", "NaN"], ["11819", "Update tempto to 1.50", "Grzegorz Kokosi\u0144ski", "kokosing", "10/31/18, 01:48:51 PM", "Update tempto to 1.50", "NaN"], ["11820", "Remove redundant isSingleNode check", "Karol Sobczak", "sopel39", "10/31/18, 01:05:46 PM", "NaN", "NaN"], ["11823", "Minor cleanup in SPI", "Nezih Yigitbasi", "nezihyigitbasi", "10/31/18, 11:02:24 PM", "NaN", "NaN"], ["11824", "Minor fixes to Web UI", "Raghav Sethi", "raghavsethi", "11/02/18, 04:51:33 PM", "NaN", "NaN"], ["11826", "Remove unused field in PartitionedOutputOperator", "Nezih Yigitbasi", "nezihyigitbasi", "11/01/18, 02:09:00 PM", "NaN", "NaN"], ["11829", "Support millisecond extraction from time/timestamp values", null, "kasiafi", "12/16/18, 02:51:13 AM", "Fixes #9524", "NaN"], ["11832", "Replace floorMod with faster reduction function", "Ying", "yingsu00", "11/08/18, 07:05:27 AM", "Building hashtable in map column uses 30% of CPU out of the scan cost\r\non that column, and Math.floorMod() uses 60% CPU out of building\r\nhashtable. We tested 5 custom range reduction functions and chose\r\ncomputePositionWithBitShifting(), which is one of the top 3 implementations\r\n in performance and does not require the hash table size to be power of 2. \r\nAfter the change buildHashTable improved 31% and the total CPU for the \r\ntest select map query improved 14%.\r\n\r\nThe following table shows the CPU percentage for the Mod,\r\ngetHashPosition and buildHashTable and the total CPU hours for the\r\nselect checksum query on a map column. The numbers are the average\r\nof 3 runs on a verifier cluster.\r\n\r\nThe following table shows the CPU percentage for computePosition,\r\ngetHashPosition and buildHashTable and the total CPU hours for a\r\nselect checksum query on a map column.\r\n\r\n```\r\n \t\t\tcomputePosition% getHashPosition% buildHashTable% Total CPU Hours\r\ncomputePositionWithMod\t\t16.62\t\t21.89\t\t28.04\t          14.37\r\ncomputePositionWithMask\t\t0.51\t\t9.53\t\t14.44\t          12.24\r\ncomputePositionWithBitShifting\t4.61\t\t11.66\t\t18.97\t          12.32\r\ncomputePositionWithFloorMod\t16.22\t\t22.03\t\t27.33\t          14.21\r\n```\r\n\r\nThe JMH benchmark results on JDK 10:\r\n```\r\nBenchmark                                                Mode  Cnt   Score   Error  Units\r\nBenchmarkComputePosition.computePositionWithBitShifting  avgt   30   2.954 \u00b1 0.122  ns/op\r\nBenchmarkComputePosition.computePositionWithDivision     avgt   30   2.852 \u00b1 0.145  ns/op\r\nBenchmarkComputePosition.computePositionWithFloorMod     avgt   30  11.212 \u00b1 0.215  ns/op\r\nBenchmarkComputePosition.computePositionWithMask         avgt   30   2.455 \u00b1 0.054  ns/op\r\nBenchmarkComputePosition.computePositionWithMod          avgt   30   4.490 \u00b1 0.113  ns/op\r\n```\r\n\r\nThe benchmark shows similar performance on JDK 8 and 11 for all methods.\r\n\r\ncomputePositionWithBitShifting() and computePositionWithDivision() can reduce the 64 bit hashcode to range [0, 2^32) then to [0, hashTableSize) uniformly if the hashcode is uniformly distributed.\r\n\r\nThe plotted the distribution of these functions and floorMod() show similarly good uniform distributions.\r\n<img width=\"1107\" alt=\"screen shot 2018-11-01 at 4 54 56 pm\" src=\"https://user-images.githubusercontent.com/33299678/47893351-cf674380-de19-11e8-99f8-6af2458f0f04.png\">\r\n\r\n<img width=\"1198\" alt=\"screen shot 2018-11-01 at 4 54 23 pm\" src=\"https://user-images.githubusercontent.com/33299678/47893363-da21d880-de19-11e8-8e30-dc574dd8ec80.png\">\r\n\r", "NaN"], ["11834", "Refactor SourcePartitionedScheduler as SourceScheduler", "Wenlei Xie", "wenleix", "11/22/18, 08:15:34 AM", "NaN", "NaN"], ["11836", "Count max code point once per batch for slice readers", "James Sun", "highker", "11/02/18, 05:27:16 PM", "SliceStreamReader::computeTruncatedLength computes truncated length for\r\na slice. Type length is calculated per function call, even it is\r\nredundant for slice reader reading every row given the type is fixed.\r\nThis patch moves the type length calculation outside the batch loop and\r\nonly computes the truncated length within the loop.\r\n\r\nResolves #11694 ", "NaN"], ["11838", "Explicitly measure raw input and generated input in operators", "Karol Sobczak", "sopel39", "11/22/18, 07:46:26 AM", "Raw input = physical data read by operator\r\nGenerated input = Size of input data (loaded in memory, decompressed) processed by operator\r\n\r\nProblem:\r\n* Stages report their input data as `outputDataSize/outputPositions` of first operator.\r\n* In `EXPLAIN` input data in table scan operators is in fact \"raw data\". This is misleading. Raw data and generated input data should be reported separately\r\n\r\nFuture: This will enable to report network traffic per query and also physical data read by scan operators.\r\n\r\nFYI: @dain @martint \r\n ", "NaN"], ["11839", "Upgrade Postgres JDBC driver", "Piotr Findeisen", "findepi", "11/22/18, 09:32:43 AM", "For example, there were some fixes around SSL and cert validation recently.", "NaN"], ["11843", "Fix leak of failed/aborted queries in coordinator", "Dain Sundstrom", "dain", "11/06/18, 10:06:22 PM", "This only addresses some of the issues in #11844\r", "NaN"], ["11851", "Add FunctionManager", "Rongrong Zhong", "rongrong", "02/23/19, 01:06:31 AM", "Add FunctionManager and FunctionNamespace, and use FunctionManager instead of FunctionRegistry through out the codebase.", "NaN"], ["11853", "Emit Kudu table property number_of_replicas in SHOW CREATE TABLE", "Martin Weindel", "MartinWeindel", "12/20/18, 09:14:37 PM", "Table properties `num_replicas` and `table_id` are included for `SHOW CREATE TABLE`. Kudu client has been updated to 1.8.0.", "NaN"], ["11854", "Use named constant in stats aggregation output channel", "Wenlei Xie", "wenleix", "11/23/18, 05:09:15 AM", "NaN", "NaN"], ["11857", " Fix race condition in remote task final info visibility", "Dain Sundstrom", "dain", "11/07/18, 12:20:12 AM", "This builds on #11843.  This adds \"Fix race condition in remote task final info visibility` and reverts `Fix memory leak for failed queries`", "NaN"], ["11860", "Fix typo in the regexp_like documentation", "Thoralf Gutierrez", "thoralf-gutierrez", "11/06/18, 08:05:09 AM", "Noticed this small typo.", "NaN"], ["11861", "Ignore cost model failures", "Andrii Rosa", "arhimondr", "11/08/18, 04:23:19 PM", "Currently stats and costs are pre-computed for every query.\r\nThere are still some bugs in stats calculator that are being\r\ntriggered in very rare cases that are incredibly hard to catch.\r\nGiven that the stats calculator has to be run for every query, it\r\nis better to be fail safe. As a failure in stats calculation must\r\nnot result in failing query.", "NaN"], ["11862", "Cleanup execution changes", "Dain Sundstrom", "dain", "11/07/18, 12:21:31 AM", "Cleanup for #11843, but suggested in #11857", "NaN"], ["11864", "Add unprocessed catalog properties to query events", "Dain Sundstrom", "dain", "11/07/18, 10:54:52 PM", "If a query fails before execution is created, catalog events may not be processed and are not added to the query events.\r\n\r\nAddresses issues in #11831", "NaN"], ["11870", "[Please Ignore, Test Only] Travis Test before merge DWRF flat map reader", "Wenlei Xie", "wenleix", "11/07/18, 10:28:49 PM", "Original PR: https://github.com/prestodb/presto/pull/11319", "NaN"], ["11872", "Adding a different error code for preempted queries", "cem cayiroglu", "cemcayiroglu", "11/09/18, 10:31:34 PM", "A new error code ADMINISTRATIVELY_PREEMPTED  and a new button\r\nare added to let admins to preempt a query. Preempt kills the query\r\nwith a error code ADMINISTRATIVELY_PREEMPTED.", "NaN"], ["11873", "Avoid query failures during partition pruning", "David Phillips", "electrum", "01/03/19, 06:45:12 AM", "NaN", "NaN"], ["11874", "Cleanup and simplify timings in QueryStateMachine", "Dain Sundstrom", "dain", "12/18/18, 06:36:57 AM", "NaN", "NaN"], ["11878", "Use ForwardingConnectorAccessControl for PartitionsAwareAccessControl", "Grzegorz Kokosi\u0144ski", "kokosing", "11/09/18, 10:29:22 AM", "Use ForwardingConnectorAccessControl for PartitionsAwareAccessControl", "NaN"], ["11880", "Update AWS SDK to version 1.11.445", "Sameer Choudhary", "same3r", "11/08/18, 09:20:41 PM", "Current AWS SDK version 1.11.293 does not support S3Select API, which is pre-requisite for https://github.com/prestodb/presto/pull/11033. Thus, updating AWS SDK to latest version, 1.11.445.", "NaN"], ["11882", "Use Optional for nullable field in HiveConnectorFactory", "Grzegorz Kokosi\u0144ski", "kokosing", "11/09/18, 02:25:41 PM", "Use Optional for nullable field in HiveConnectorFactory", "NaN"], ["11884", "Do not fail glue tests on codestyle errors", "Grzegorz Kokosi\u0144ski", "kokosing", "11/09/18, 10:51:56 AM", "Do not fail glue tests on codestyle errors", "NaN"], ["11888", "Additional UI improvements", "Raghav Sethi", "raghavsethi", "11/22/18, 12:44:40 AM", "Example run that has a flow failure: https://api.travis-ci.org/v3/job/455267731/log.txt", "NaN"], ["11889", "Fail query that uses non-leaf resource group", "Elon Azoulay", "elonazoulay", "11/23/18, 06:30:32 AM", "Fix bug where a query is queued indefinitely if it is submitted into a non-leaf resource group.\r\n\r\nThis can be triggered by deleting a resource groups subgroups and updating a selector in the database used by the DbConfigurationManager.", "NaN"], ["11890", "Update joda to 2.10", "Haozhun Jin", "haozhun", "11/13/18, 07:40:10 PM", "The tzdata in joda 2.10 matches that of Java 10.0.2 and 8u181", "NaN"], ["11891", "Use JVM time zone rules in Joda ", "Haozhun Jin", "haozhun", "12/17/18, 10:59:49 PM", "Fixes #8233, fixes #11855 depends on #11890 ", "NaN"], ["11892", "Annotate redis.password with @ConfigSecuritySensitive", "Grzegorz Kokosi\u0144ski", "kokosing", "11/22/18, 05:44:31 AM", "Annotate redis.password with @ConfigSecuritySensitive", "NaN"], ["11894", "Stats calculator fixes [Part 1]", "Andrii Rosa", "arhimondr", "11/22/18, 05:04:47 PM", "NaN", "NaN"], ["11895", "Stats calculator fixes [Part 2]", "Andrii Rosa", "arhimondr", "11/23/18, 06:00:31 PM", "NaN", "NaN"], ["11898", "Update to Airbase 88", "David Phillips", "electrum", "11/14/18, 08:28:27 PM", "NaN", "NaN"], ["11901", "Update the batch resizing heuristic in PageProcessor", "Nezih Yigitbasi", "nezihyigitbasi", "11/27/18, 11:23:10 PM", "The yield check in the generated projection tight loop may have negative\r\nimpact on the performance. The yield check was introduced for expensive\r\nprojections so that the engine can yield those. However, that check is\r\nnot really necessary for cheap projections and having a branch in the\r\ntight for loop seems to hurt the performance.\r\n\r\nThis change removes the yield check from the generated projection loop,\r\nand to handle the expensive expressions properly it adds the cost of the\r\nevaluated expression as another variable to the batch resizing heuristic\r\nin PageProcessor. With that for expensive expressions Presto now uses\r\nsmall batch sizes, and uses larger batch sizes for cheaper expressions.\r\n\r\nRemoving the yield check from the projection loop improves the performance\r\nas much as ~5% with microbenchmarks and various production queries.\r\n\r\n```\r\nBenchmark                                   (expressionProfilerEnabled)  Mode  Cnt   Score   Error  Units\r\nExpressionProfilerBenchmark.pageProjection                        false  avgt   15  37.672 \u00b1 4.401  us/op\r\nExpressionProfilerBenchmark.pageProjection                         true  avgt   15  35.336 \u00b1 4.724  us/op\r\n```\r\n\r\nAlternative to #11816.", "NaN"], ["11904", "Optionally use default filter factor to estimate filter node", "Rebecca Schlussel", "rschlussel", "12/04/18, 11:47:09 PM", "Using the default filter factor to estimate joins can be very off base,\r\nbut using it for filter nodes means we will still behave well for small\r\ntables joined to large ones, but not accidentally broadcast huge tables.\r\nTherefore, we suspect this heuristic is lower risk.  Nevertheless, we\r\ngate it with the session property default_filter_factor_enabled.  ", "NaN"], ["11906", "Add RowNumber stats and cost rules", "Andrii Rosa", "arhimondr", "11/28/18, 02:39:02 PM", "NaN", "NaN"], ["11912", "Drop snappy test table in the same way as it was created", "Grzegorz Kokosi\u0144ski", "kokosing", "11/22/18, 05:44:04 AM", "Drop snappy test table in the same way as it was created\n\nThat way this test is less vulnerable on test environment in which it\nis executed.", "NaN"], ["11915", "Add support for lambdas in ExpressionEquivalence", "Karol Sobczak", "sopel39", "11/15/18, 09:28:40 AM", "Fixes https://github.com/prestodb/presto/issues/11900", "NaN"], ["11920", "Refactor setting distribution for plan fragment", "Wenlei Xie", "wenleix", "01/25/19, 06:47:08 PM", "Both FragmentProperties#addSourceDistribution and\r\nFragmentProperties#setDistribution set fragment distribution. Previously\r\nthe logic is replicated in both methods.\r\n\r\nThis commit make FragmentProperties#addSourceDistribution call\r\nsetDistribution.", "NaN"], ["11921", "Fix distributed spatial join over union", "Maria Basmanova", "mbasmanova", "11/15/18, 04:47:28 PM", "Due to a typo, AddExchanges#visitSpatialJoin didn't preserve re-write of a union\r\ninto remote exchange.\r\n\r\nFixes #11919", "NaN"], ["11924", "Rename nodePartitioning variables to tablePartitioning", "Mark Wagner", "wagnermarkd", "11/28/18, 07:05:11 PM", "Clean-up after https://github.com/prestodb/presto/commit/8385a700c701a234315a52a15f3c94ef4f402eea, which renamed NodePartitioning to TablePartitioning.", "NaN"], ["11926", "Reveal partial TopN in plan", "Piotr Findeisen", "findepi", "11/22/18, 08:29:07 AM", "This is similar to what we do with partial Limit.", "NaN"], ["11929", "Allow StringStatistics with both min and max set to null", "Andrii Rosa", "arhimondr", "11/15/18, 05:58:01 PM", "The ColumnStatistics#mergeColumnStatistics method is now used also for\r\nmerging stats from the ORC files written by other systems. Thus we\r\ncannot enforce that there will be no StringStatistics with both min\r\nand max set to null.", "NaN"], ["11932", "Add input validation checks to ST_LineString", "Pavan Chitumalla", "pavanch007", "11/26/18, 03:52:12 PM", "Throw an exception if input array contains null or empty points or if any consecutive \r\npoints in the array are the same.", "NaN"], ["11935", "Do not call mergeColumnStatistics when not needed", "Andrii Rosa", "arhimondr", "11/15/18, 08:03:45 PM", "NaN", "NaN"], ["11937", "Give Hive \"split buffering limit\" error a specific error code", "Haozhun Jin", "haozhun", "11/15/18, 07:58:56 PM", "This error, in general, only happens with grouped execution.\r\n\r\nIt could, in theory, also happen when a table has a huge number of\r\nbuckets, and each file has a huge number of HDFS blocks. In such case,\r\nit's up to the cluster admin to decide whether to allow a higher\r\nthreshold at the risk of coordinator OOM.", "NaN"], ["11938", "Add release notes for 0.214", "Greg", "ggreg", "11/20/18, 11:19:56 PM", "NaN", "NaN"], ["11944", "Use spatial joins for st_within", "Anoop Johnson", "anoopj", "11/26/18, 03:51:38 PM", "This is the pull request for fixing https://github.com/prestodb/presto/issues/11835\r\n\r\nI tested this on a cluster and saw speedups where the query time dropped from about 30 minutes to 3 seconds.\r\n\r\ncc @mbasmanova ", "NaN"], ["11950", "Cleanup Tpch/Tpcds CBO planner tests and benchto modules", "Karol Sobczak", "sopel39", "11/22/18, 09:45:49 AM", "NaN", "NaN"], ["11951", "Added classloader safe configuration of event listener", null, "gray-eb", "02/22/19, 07:24:53 PM", "I had an Event Listener plugin that needed to utilize Jersey REST client libraries, but it constantly clashed with the server's libraries. Even after adding a wrapper within the plugin, the issue persisted. It led me to this solution.\r\n\r\nThis change is analogous to the current state of ConnectorManager.java\r\n\r\nCreating a ClassLoaderSafeEventListener similar to the ClassLoaderSafeConnectorMetadata allows me to use the libraries without any conflicts.", "NaN"], ["11957", "Add exchange for union with TableScan and Values", "Rebecca Schlussel", "rschlussel", "02/04/19, 09:05:46 PM", "Fixes #10612 ", "NaN"], ["11958", "Fix cumulative memory UI", "Wojciech Biela", "ilfrin", "11/22/18, 11:03:07 AM", "This came up in a couple user conversations so I thought we should clean it up.\r\n\r\nFor clarity I could also rename the Java variable to cumulativeUserMemoryByteSeconds. The motivation for this would be that I needed to get to the bottom of how stat is calculated to see why it's being divided by 1000. But maybe that's too much.", "NaN"], ["11960", "Annotate Live Plan with additional node info", "Raghav Sethi", "raghavsethi", "02/05/19, 12:05:13 AM", "Variant of #11927 which allows the output to be even more structured.\r\n\r\nThis change paves the way for PlanPrinter to generate more structured\r\noutput than a text blob. This structured output can then be used to\r\nenrich a raw JSON plan.\r\n\r\nTest plan:\r\n- Run AbstractTestQueries with patch that dumps plan for every query, compare two dozen or so query plans with master.\r\n- Run AbstractTestQueries with patch that does not catch exceptions thrown when constructing text plans, and verify that tests all pass.", "NaN"], ["11966", "Don't record raw input for operators that don't really consume raw input", "Karol Sobczak", "sopel39", "11/22/18, 10:08:06 PM", "This is a small follow up from: https://github.com/prestodb/presto/pull/11838\r\n\r\n`Values, LocalExchange, LocalMerge` don't really consume raw input so it doesn't make sense to account their input as such", "NaN"], ["11967", "Print only table name in AbstractCostBasedPlanTest", "Karol Sobczak", "sopel39", "11/22/18, 02:33:25 PM", "This makes query plans more abstract which allows\r\nto test them againt non TPCH/TPCDS metadata sources\r\n(e.g: Hive)", "NaN"], ["11970", "Add support for query pushdown to S3 using S3 Select", "Sameer Choudhary", "same3r", "12/29/18, 06:57:11 AM", "This change will allow Presto users to improve the performance of their queries using S3SelectPushdown.\r\nIt pushes down projections and predicate evaluations to S3. As a result\r\nPresto doesn't need to download full S3 objects and only data required to answer the user's query\r\nis returned to Presto, thereby improving performance.\r\n\r\nS3SelectPushdown Technical Document: [S3SelectPushdown.pdf](https://github.com/prestodb/presto/files/2406949/S3SelectPushdown.pdf)\r\n\r\nThis PR is a continuation of https://github.com/prestodb/presto/pull/11033.", "NaN"], ["11972", "Fix stats calculator [part 3]", "Andrii Rosa", "arhimondr", "11/30/18, 04:03:59 PM", "NaN", "NaN"], ["11974", "Improve StateMachine addStateChangeListener use and semantics", "Dain Sundstrom", "dain", "12/18/18, 03:29:33 AM", "* Fix `this` leaks when adding state change listeners in constructors\r\n* Immediately fire current state when a state change listener is added", "NaN"], ["11980", "Fix 'Capture Snapshot' button for showing threads", "Piotr Findeisen", "findepi", "11/27/18, 06:41:38 PM", "Before the change, the code was attempting `result[...].push(...)`\r\nwithout first adding lists as `result` map values.\r\n\r\nFollowing was being logged:\r\n```\r\nWorkerThreadList.jsx:472 Uncaught TypeError: Cannot read property 'push' of undefined\r\n    at Function.processThreads (WorkerThreadList.jsx:472)\r\n    at WorkerThreadList.eval (WorkerThreadList.jsx:75)\r\n    at i (jquery-2.2.3.min.js:2)\r\n    at Object.fireWith [as resolveWith] (jquery-2.2.3.min.js:2)\r\n    at z (jquery-2.2.3.min.js:4)\r\n    at XMLHttpRequest.<anonymous> (jquery-2.2.3.min.js:4)\r\n```", "NaN"], ["11982", "Stats based join distribution type", "Andrii Rosa", "arhimondr", "12/04/18, 11:23:15 PM", "Resolves: https://github.com/prestodb/presto/issues/11745", "NaN"], ["11983", "Add block/position convention to all distinct from operators", "Haozhun Jin", "haozhun", "12/17/18, 01:01:20 AM", "This supersedes #11263.\r\n\r\n#11746 removed the blocker preventing its merge.", "NaN"], ["11984", "Change ARRAY_INTERSECT to use TypedSet", "Ying", "yingsu00", "12/31/18, 08:50:20 AM", "Resolves:\r\nhttps://github.com/prestodb/presto/issues/11493\r\n\r\nThis commit improves the performance comparing to the original sort-merge\r\nsolution. The gain is from 2.3x to 11x for different data types.\r\n\r\nbefore\r\n\r\n```\r\nBenchmark                                        (name)   (type)  Mode  Cnt    Score   Error  Units\r\nBenchmarkArrayIntersect.arrayIntersect  array_intersect   BIGINT  avgt   20  193.023 \u00b1 2.840  ns/op\r\nBenchmarkArrayIntersect.arrayIntersect  array_intersect  VARCHAR  avgt   20  336.516 \u00b1 5.301  ns/op\r\nBenchmarkArrayIntersect.arrayIntersect  array_intersect   DOUBLE  avgt   20  219.812 \u00b1 6.615  ns/op\r\nBenchmarkArrayIntersect.arrayIntersect  array_intersect  BOOLEAN  avgt   20   23.913 \u00b1 0.553  ns/op\r\n```\r\n\r\nAfter\r\n\r\n```\r\nBenchmark                                (type)  Mode  Cnt    Score   Error  Units\r\nBenchmarkArrayIntersect.arrayIntersect   BIGINT  avgt   20   30.175 \u00b1 5.077  ns/op\r\nBenchmarkArrayIntersect.arrayIntersect  VARCHAR  avgt   20  146.365 \u00b1 5.405  ns/op\r\nBenchmarkArrayIntersect.arrayIntersect   DOUBLE  avgt   20   63.752 \u00b1 5.329  ns/op\r\nBenchmarkArrayIntersect.arrayIntersect  BOOLEAN  avgt   20    2.133 \u00b1 0.243  ns/op\r\n```", "NaN"], ["11985", "Fix variable name typo in ArrayNgramsFunction", "Ying", "yingsu00", "11/27/18, 03:58:13 AM", "NaN", "NaN"], ["11990", "Update to Airlift 0.177", "Nezih Yigitbasi", "nezihyigitbasi", "11/28/18, 06:44:50 PM", "NaN", "NaN"], ["11995", "Fix bug in Lazy build map hashtables", "Ying", "yingsu00", "12/05/18, 12:29:16 AM", "In 62dc3a5765d2fd5395b728ad93c10631c1ef0271, we lazily build hashtable\r\nfor MapBlock when seek is performed.  When the block is a view over\r\na large block (e.g. created via getRegion), the hash table is built for the\r\nunderlying raw block.\r\n\r\nHowever the hashtable size is incorrectly calculated\r\nbased on the view block (instead of the underlying raw block).\r\nThis commit fixes this bug.", "NaN"], ["11996", "Rename TestHiveClientS3 to TestHiveFileSystemS3", "Grzegorz Kokosi\u0144ski", "kokosing", "11/29/18, 11:58:31 AM", "Rename TestHiveClientS3 to TestHiveFileSystemS3\n\nNotice that TestHiveClientS3 did not extend AbstractTestHiveClient, but\nAbstractTestHiveFileSystem.", "NaN"], ["11998", "Accelerate geometries for Spatial Join", "James Gill", "jagill", "12/05/18, 12:43:05 PM", "Build gemetries are often used repeated for spatial relation checks in a\r\nspatial join.  Accelerating a geometry is a one-time cost which reduces\r\nthe complexity of these checks from `O(n)` to `O(lg n)` or even `O(1)`,\r\nwhere n is the number of vertices.  When a geometry is accessed for the\r\njoin, accelerate it (an idempotent operation).\r\n\r\nIn a real world example of joining ~400k polygons with ~1B points, wall time\r\nwent from 1.5h to 4m after acceleration.\r\n\r\nFixes #11953", "NaN"], ["11999", "Remove deprecated add/alter partition methods", "Andrii Rosa", "arhimondr", "11/29/18, 11:03:55 PM", "NaN", "NaN"], ["12004", "Minor ConnectorBucketNodeMap improvements", "David Phillips", "electrum", "12/18/18, 02:27:41 AM", "NaN", "NaN"], ["12005", "Move InvocationConvention to function package", "David Phillips", "electrum", "12/03/18, 09:22:41 AM", "NaN", "NaN"], ["12011", "Add embeddable live plan HTML view", "Raghav Sethi", "raghavsethi", "12/03/18, 09:10:40 PM", "NaN", "NaN"], ["12013", "Propagate partition properties for full outer join", "Rongrong Zhong", "rongrong", "01/28/19, 08:34:41 PM", "For full outer join, if both sides of the equi-join contains all partitioning columns,\r\nthe output of the join would be partitioned on coalesce of these columns.", "NaN"], ["12015", "Fix integer overflow when turning dictionary into direct", "Wenlei Xie", "wenleix", "01/08/19, 06:41:15 PM", "Turning dictionary into direct in ORC writer can end up with writing\r\nhuge data.\r\neb6028d8a8e3758cfc5f66cecf237eb1fa750643 tries to fix this by\r\nchecking the written size after converting each row group. However,\r\nin certain cases a single row group already exceed 2G after converting\r\ninto direct. For example, a varchar column repeated with a single\r\nvalue with length around 500k.\r\n\r\nThis commit fixed this by checking the written size after each batch of\r\nwrite.\r\n\r\nFixes https://github.com/prestodb/presto/issues/11930", "NaN"], ["12021", "Use list instead of map for bucket-to-node", "David Phillips", "electrum", "12/18/18, 07:59:45 PM", "A list is better because the mapping is contiguous.", "NaN"], ["12023", "Remove PartitionedLookupSourceFactory specific methods from interface", "Wenlei Xie", "wenleix", "12/21/18, 11:05:37 PM", "The following methods are specific to PartitionedLookupSourceFactory:\r\n- lendPartitionLookupSource()\r\n- setPartitionSpilledLookupSourceHandle()\r\n\r\nThey are introduced to interface LookupSourceFactory in\r\n9c60025bbbedf8c1c32295a013630890d4d2bde3.\r\n\r\nThese methods are no longer required to stay in interface with\r\ntemplatized JoinBridgeManager.", "NaN"], ["12027", "Pick max distinct values in ExchangeStatsRule", "Andrii Rosa", "arhimondr", "12/17/18, 04:13:20 PM", "Also set distinct values to unknown in any input distinct values unknown", "NaN"], ["12028", "Add minor state check to PartitionedLookupSourceFactory", "Wenlei Xie", "wenleix", "12/15/18, 11:48:36 PM", "PartitionedLookupSourceFactory#createLookupSourceProvider() should be\r\ncalled before the factory is destroyed. Otherwise, the returned future\r\nwill never complete.", "NaN"], ["12031", "Fix Kudu example configuration docs", "Grzegorz Kokosi\u0144ski", "kokosing", "12/15/18, 09:37:22 AM", "Fix Kudu example configuration docs", "NaN"], ["12034", "Push predicates through windows when the predicate is on a partition symbol", "Mark Wagner", "wagnermarkd", "01/07/19, 09:19:58 AM", "Predicates can be pushed below windows if the predicate is constant for all rows\r\nwithin each partition of the windowing operation. The most tractable case where\r\nthis is true is if the predicate is a deterministic expression where all the\r\ninput symbols are also partition symbols.", "NaN"], ["12036", "Fail loudly with corrupted Parquet statistics", "Zhenxiao Luo", "zhenxiao", "12/19/18, 09:08:49 PM", "@nezihyigitbasi @dain @findepi @qqibrow ", "NaN"], ["12038", "Add empty check for types in ORC file", "Wenlei Xie", "wenleix", "12/16/18, 11:35:52 PM", "NaN", "NaN"], ["12041", "Fix assertion in tryUpdateUserMemory method", "Nezih Yigitbasi", "nezihyigitbasi", "12/10/18, 09:37:35 PM", "When DefaultQueryContext::tryUpdateUserMemory() is called with a delta\r\nof 0 and when the general pool is full, the updateUserMemory call will\r\nreturn a future that's not done. However, tryUpdateUserMemory was simply\r\nassuming that for that case the future must be done and asserting on\r\nthat, which causes query failures. This change fixes that.\r\n\r\nIt makes sense for updateUserMemory to return a future that's not done\r\nwhen we reserve 0 bytes and the pool is full, so that the caller knows\r\nthat the pool is full and waits on that future to make the next\r\nreservation. Therefore, for the delta == 0 case it's not correct to\r\nsimply assume that future returned from updateUserMemory is done.\r\n\r\nFixes #12040", "NaN"], ["12042", "Fix TestColumnarPageProcessor", "Nezih Yigitbasi", "nezihyigitbasi", "12/10/18, 08:54:02 PM", "When TestColumnarPageProcessor test methods run concurrently it's\r\npossible to see test failures due to the exception below as multiple\r\nthreads will be sharing the PageProcessor instance (hence the\r\nExpressionProfiler instance). This change fixes that by creating a new\r\nPageProcessor for each test method.\r\n\r\ncom.google.common.base.VerifyException: start() is not called\r\n\tat com.google.common.base.Verify.verify(Verify.java:123)\r\n\tat com.facebook.presto.sql.gen.ExpressionProfiler.stop(ExpressionProfiler.java:59)", "NaN"], ["12045", "Make sure TestColumnarPageProcessor runs on a single thread", "Nezih Yigitbasi", "nezihyigitbasi", "12/11/18, 05:10:55 AM", "Otherwise, the processor instance will still be shared by multiple\r\nthreads.\r\n\r\nFollow up for https://github.com/prestodb/presto/pull/12042 per @findepi's comment.", "NaN"], ["12050", "Use cost estimates to choose semi-join distribution type", "Rebecca Schlussel", "rschlussel", "01/25/19, 08:01:02 PM", "Fixes #11789", "NaN"], ["12052", "only take last defined value for log locations in node.properties", "Will Morrison", "willmostly", "12/15/18, 06:39:14 AM", "NaN", "NaN"], ["12053", "Add suppressed exceptions to tear down exceptions in verifier", "Nezih Yigitbasi", "nezihyigitbasi", "12/16/18, 05:46:46 AM", "Otherwise, when we have failures in setup or query run, tear down\r\nfailures will hide them. This change makes sure that we retain that\r\ninformation in those cases.", "NaN"], ["12056", "Fix Live Plan UI for index joins", "Ray", "neuzrui", "12/16/18, 02:49:35 AM", "NaN", "NaN"], ["12057", "Add Elasticsearch connector", "Zhenxiao Luo", "zhenxiao", "01/30/19, 04:46:15 PM", "continue with https://github.com/prestodb/presto/pull/11942", "NaN"], ["12061", "Add missing @Nullable annotations", "Piotr Findeisen", "findepi", "12/15/18, 06:40:16 AM", "`ArrayBlock#getValueIsNull()` is `@Nullable`, so\r\n`AbstractArrayBlock#getValueIsNull()` should also be annotated as such.\r\nSame for `AbstractMapBlock#getMapIsNull()` and\r\n`MapBlock#getMapIsNull()`.", "NaN"], ["12062", "Skip all-false nullness array in ArrayBlockBuilder", "Piotr Findeisen", "findepi", "12/17/18, 10:30:17 PM", "NaN", "NaN"], ["12064", "Fix SemiTransactionalHiveMetastore error message", "Tushar Agarwal", "tusharag171", "12/18/18, 03:48:05 PM", "Fixes #12063", "NaN"], ["12065", "Allow function implementation choices to have different dependencies ", "Haozhun Jin", "haozhun", "01/31/19, 11:06:32 PM", "NaN", "NaN"], ["12066", "add documentation for security.refresh-period", "Matt Fuller", "mattsfuller", "12/14/18, 08:55:08 AM", "NaN", "NaN"], ["12068", "Include nulls in dictionary block logical size", "David Phillips", "electrum", "12/17/18, 11:31:32 PM", "NaN", "NaN"], ["12070", "Add basic TupleDomain#toString", "Piotr Findeisen", "findepi", "12/15/18, 06:40:29 AM", "While proper `toString` requires `ConnectorSession`, a basic `toString`\r\nis still helpful when debugging.", "NaN"], ["12079", "Add release notes for 0.215", "Nezih Yigitbasi", "nezihyigitbasi", "12/15/18, 01:06:43 AM", "Per our offline discussion with @zaynoon, here is a follow up PR for https://github.com/prestodb/presto/pull/12076 with comments addressed.\r\n\r\nFixes https://github.com/prestodb/presto/issues/11961", "NaN"], ["12084", "Support lambda in aggregation functions", "Wenlei Xie", "wenleix", "12/24/18, 07:09:38 PM", "The code is split into two parts to help review. The first part add lambda support in `Accumulator` class, while the lambda compilation part is done in second part.\r\n\r\n\r\n\r", "NaN"], ["12086", "Fix double planning in assertPlannerWarnings", "Piotr Findeisen", "findepi", "12/18/18, 01:31:12 PM", "NaN", "NaN"], ["12091", "Do not change explicitly selected join distribution type", "Andrii Rosa", "arhimondr", "12/18/18, 05:38:37 PM", "ReorderJoin rule should not change distribution type if distribution\r\ntype is explicitly set as BROADCAST or REPLICATED.\r\n\r\nThis also fixes a failure when BROADCAST join type is selected explicitly,\r\nbut the broadcasted table size exceeds the broadcast table limit. In\r\nsuch case ReorderJoin rule used to fail with the java.util.NoSuchElementException,\r\nbecause no join options were added to the possibleJoinNodes list.\r\nExample stack trace: https://gist.github.com/arhimondr/03770d751d3d19936c6c5885708659de", "NaN"], ["12092", "Remove unused PUBLIC token from parser", "David Phillips", "electrum", "12/18/18, 09:19:17 PM", "NaN", "NaN"], ["12093", "Fix a typo in the description of the bing_tile_at function", "Maria Basmanova", "mbasmanova", "12/19/18, 11:23:05 AM", "NaN", "NaN"], ["12097", "Remove unnecessary properties from mongo session", "Rebecca Schlussel", "rschlussel", "12/20/18, 09:08:32 PM", "Having the mongo query runner explicitly set the time zone instead of\r\nusing the default zone from testing session caused test failures once\r\nthe default was changed. Therefore, we don't want to be setting\r\nproperties unnecessarily.  This change fixes the failing test\r\nTestMongoDistributedQueries.testAtTimeZone()\r\n\r\nFixes #12089 ", "NaN"], ["12098", "Cleanup code in FileSingleStreamSpiller", "David Phillips", "electrum", "01/03/19, 06:35:43 AM", "Move single-use utility method and cleanup exception handling", "NaN"], ["12100", "Fix typo in variable name", "Rebecca Schlussel", "rschlussel", "12/19/18, 04:09:30 AM", "NaN", "NaN"], ["12103", "Fix ARRAY_DISTINCT wrong results on NULL and 0", "Ying", "yingsu00", "12/19/18, 11:52:39 PM", "Resolves #11977\r\nBreaking #12080 into separate PRs. This PR is to fix ARRAY_DISTINCT only.", "NaN"], ["12104", "Enable char predicates to be pushed down.", "Christine Banek", "cbanek", "01/07/19, 10:07:58 AM", "When running queries with presto, if the underlying\r\ntable is of type varchar, the predicate gets pushed.\r\n\r\nIf it's of type char, it will NOT get pushed, which\r\ncauses presto to get the whole table, and then filter\r\nlocally, which is a big pain (and bad performance).\r\n\r\nUsing Presto PR #4842 as a guide, I've added support\r\nfor char types to also be passed down.  Unit tests\r\nalso updated for the type following the pattern.\r\nAlso integration tested against a mysql-like backend.", "NaN"], ["12105", "Simplify BucketSplitInfo creation expression", "David Phillips", "electrum", "12/27/18, 08:37:12 PM", "NaN", "NaN"], ["12110", "Leave trace in logs when query is moved to RESERVED_POOL", "Piotr Findeisen", "findepi", "12/20/18, 07:50:45 AM", "NaN", "NaN"], ["12112", "Fix bug in ARRAY_INTERSECT", "Ying", "yingsu00", "12/21/18, 08:49:03 PM", "Resolves https://github.com/prestodb/presto/issues/11978\r\n\r\nThe performance improvement and semantic change of TypedSet/HashTable to use isDistinctFrom will be done in separate PRs.", "NaN"], ["12113", "Simplify Docker instructions for product-tests", "David Phillips", "electrum", "01/03/19, 06:16:41 AM", "NaN", "NaN"], ["12115", "Fix Count(*) on empty relation returns NULL when optimize_mixed_distinct_aggregation is turned on.", "Qi Chen", "kaka11chen", "01/07/19, 09:21:28 AM", "Introduce coalesce(x, 0) on a projection node to fix Count(*) on empty relation returns NULL when optimize_mixed_distinct_aggregation is turned on.", "NaN"], ["12117", "Change representation of EnforceSingleRowNode in EXPLAIN", "Piotr Findeisen", "findepi", "12/20/18, 07:17:23 PM", "In my opinion, \r\n```\r\n...\r\n   - Scalar\r\n      - ....\r\n```\r\nis not self-explanatory. I think the following is:\r\n```\r\n...\r\n   - EnforceSingleRow\r\n      - ....\r\n```", "NaN"], ["12118", "PlanFragmenter small refactorings", "Andrii Rosa", "arhimondr", "12/20/18, 08:45:10 PM", "NaN", "NaN"], ["12120", "Add joda-to-java-time-bridge dependency in example connector", "Haozhun Jin", "haozhun", "12/21/18, 07:02:41 AM", "This commit fixes \"server start\" failure of presto-main with default trunk dev config. As a result, I believe it should be merged for now regardless of whether this is the right solution.\r\n\r\n~~However, I think we should reevaluate our choice to have each connector declare this run-time dependency now that we noticed that people don't generally realize they have a dependency on joda. For one, I didn't realize that presto-example-http has a dependency on joda transitively when I walked through the dependency list. I had to resort to tools to find out how after the trunk failure is brought to my attention. I propose adding joda-to-java-time-bridge to SPI_PACKAGES (in the context of PluginClassLoader).~~", "NaN"], ["12122", "Remove extra remote exchange in window function", "Shixuan Fan", "shixuan-fan", "01/31/19, 01:32:46 AM", "When the node is partitioned on a subset of window function partition\r\nkey, the required data is available locally, so we don't need another\r\nremote exchange before window function.\r\n\r\nAddresses #12109 ", "NaN"], ["12125", "Move tests out of big_query test group", "Grzegorz Kokosi\u0144ski", "kokosing", "01/04/19, 10:25:20 AM", "Move tests out of big_query test group\n\nThese tests used to not pass on travis due to low amount of (memory)\nresources. Now it looks they are fine.", "NaN"], ["12131", "Remove node_id column from system.runtime.queries", "Piotr Findeisen", "findepi", "01/04/19, 11:25:05 AM", "The column shows the coordinator ID. This is not useful. Also, in the\r\ntable that lists queries, it  can be mistaken as a node which executes\r\nthe query.\r\n\r\nWhen we support multiple coordinator, we can add the column back,\r\nperhaps under some more meaningful name.\r\n\r\nExample of apparent confusion: https://stackoverflow.com/q/53904617/65458", "NaN"], ["12137", "Allow HiveQueryRunner to persist data to a custom path", "Ying", "yingsu00", "02/09/19, 02:56:27 PM", "    Add an optional argument to HiveQueryRunner to provide a path to read\r\n    and write the data and metadata to. The path must point to a writable\r\n    directory. On first run, HiveQueryRunner will create tpch and\r\n    tpch_bucketed tables and store them in the specified directory. On\r\n    subsequent runs using the same path, HiveQueryRunner will re-use the\r\n    data created before.\r\n\r\n    If no argument is provided, HiveQueryRunner will create a temporary\r\n    directory to store data and delete it before exiting.", "NaN"], ["12140", "Update kafka docs", "Nezih Yigitbasi", "nezihyigitbasi", "01/07/19, 06:14:51 PM", "Fixes #12138.", "NaN"], ["12145", "Report attempted allocation when memory limit exceeded", "Piotr Findeisen", "findepi", "12/30/18, 05:01:39 PM", "cc @sopel39 @ilfrin ", "NaN"], ["12147", "Minor refactor in presto-tests", "Nezih Yigitbasi", "nezihyigitbasi", "01/02/19, 08:59:29 PM", "NaN", "NaN"], ["12149", "Fix Kudu build to not enable Javadoc by default", "David Phillips", "electrum", "12/30/18, 09:48:28 PM", "NaN", "NaN"], ["12150", "Update to Airlift 0.178", "David Phillips", "electrum", "12/31/18, 08:59:46 AM", "NaN", "NaN"], ["12152", "Remove obsolete test files", "Piotr Findeisen", "findepi", "01/01/19, 10:43:54 PM", "For S3 tests, `presto_test_external_fs` table is created with\r\n`run_hive_s3_tests.sh`.", "NaN"], ["12156", "Remove unused connectorId from TpchTableHandle", "Piotr Findeisen", "findepi", "01/01/19, 11:46:10 PM", "NaN", "NaN"], ["12158", "Fix time zone difference for GROUP BY over EXTRACT test", "David Phillips", "electrum", "01/04/19, 10:18:00 PM", "The Presto query uses the time zone from TestingSession, but H2 uses\nthe JVM time zone, which are different. This causes a query failure\nduring the new year period where the years are different. Changing to\nuse a date from the table fixes this and avoids the race condition\nwhere the two queries run on different sides of the cutover.", "NaN"], ["12166", "Implement PROVIDED_BLOCKBUILDER return place convention for scalar function", "Wenlei Xie", "wenleix", "01/26/19, 11:05:24 PM", "Currently the only return place convention for scalar function is STACK, and the callee will append the results value on stack into the result BlockBuilder (for out-most function call) or use it to invoke other functions (for inner/nested function call like f(g(x))).\r\n\r\nFor functions returns Slice and Block, this return place convention usually result in copying the data twice -- once generate the data, once copy the data into the output BlockBuilder.\r\n\r\nThis commit implements PROVIDED_BLOCK return place convention to allow scalar function implementation choice to directly write to the desired place. Similar to the BLOCK_POSITION null convention, a implementation choice used default STACK return place convention must be implemented. Besides STACK return place convention, the developer of the scalar function can choose to provide an additional implementation choice with PROVIDED_BLOCK return place convention.\r\n\r\nIn the future, an invocation adapter should be able to automatically adapt between different calling conventions (null convention and return place convention) when feasible.\r\n\r\n----------------\r\n\r\nSee more details in https://github.com/prestodb/presto/pull/9638\r", "NaN"], ["12168", "Add peak task user memory usage to query stats", "Nezih Yigitbasi", "nezihyigitbasi", "02/05/19, 12:23:23 AM", "This information is useful for debugging.", "NaN"], ["12170", "Report top memory consumers when local memory limit is exceeded", "Nezih Yigitbasi", "nezihyigitbasi", "01/26/19, 12:00:40 AM", "Sample exception message:\r\n\r\n```\r\ncom.facebook.presto.ExceededMemoryLimitException: Query exceeded per-node user memory limit of 1MB [Allocated: 754.36kB, Delta: 377.18kB, Top Consumers: {HashAggregationOperator=754.36kB, PartitionedOutputOperator=147.32kB, FilterAndProjectOperator=25.00kB}]\r\n```", "NaN"], ["12172", "Run sphinx-build in parallel", "Grzegorz Kokosi\u0144ski", "kokosing", "01/04/19, 10:26:32 AM", "Run sphinx-build in parallel\n\nIt does not influence testing automation, because maven is running\nin parallel already. However, it speeds up vastly manual\ngeneration of documentation with: `make clean html`", "NaN"], ["12177", "Add a comment about ColumnStatistics' \"unknown\" estimates semantics", "Roman Zeyde", "rzeyde-varada", "02/22/19, 08:36:27 PM", "Recently, we have implemented column statistics estimation for our SPI connector [1].\r\n\r\nOne (a bit surprising) issue was that we had to return a not-NaN estimate for `nullsFraction` [2] even if we had an estimate of the row count of the table (initially we assumed that Presto will assume 0 NULLs if it isn\u2019t specified, so we specified only the `dataSize` of each column statistic).\r\n\r\nSince \u201cunknown\u201d values are marked by NaN, the resulting costs became NaN as well [3], so the CBO was ignoring our queries (until we fixed that by returning a not-NaN values for each column statistic).\r\n\r\n[1] https://github.com/prestodb/presto/blob/57c70aee95fe6c00666f13ba5dc2f930fae87d84/presto-main/src/main/java/com/facebook/presto/metadata/Metadata.java#L102\r\n\r\n[2] https://github.com/prestodb/presto/blob/b6addd420c5bec97e675eb5f4c238301cf80c083/presto-spi/src/main/java/com/facebook/presto/spi/statistics/ColumnStatistics.java#L22\r\n\r\n[3] https://github.com/prestodb/presto/blob/f112441aa3f4e6c485f5333bd4f38901c0f96433/presto-main/src/main/java/com/facebook/presto/cost/TableScanStatsRule.java#L75", "NaN"], ["12179", "Fix issues with final stage info", "Dain Sundstrom", "dain", "01/09/19, 01:04:12 AM", "Two things that I think need to be fixed:\r\n\r\n1) we should \"capture\" the final stage info once it is created, so it can not be changed.\r\n2) we should not allow tasks to be added after the stage is in a done state.\r\n\r\nI think the first one is pretty straight forward, but I think the second one should be fixed first since it would be strange to have tasks in a stage that are not in the stage info.  The second one is more tricky to fix, because we can't do the easy thing and simply throw an exception because a stage could finish early due to a limit.  Instead, we need to adapt the callers to expect an `Optional<RemoteTask>`.  I'm working on these now.\r\n\r\nFixes #12095", "NaN"], ["12186", "Update properties docs for memory-related config options", "Nezih Yigitbasi", "nezihyigitbasi", "01/08/19, 06:50:13 PM", "This change adds the documentation for the distributed memory-related\r\nconfig options to the properties docs, and adds a new separate section\r\nfor the memory management related properties.\r\n\r\nFixes #12181.", "NaN"], ["12194", "Add release notes for 0.216", "Maria Basmanova", "mbasmanova", "01/24/19, 04:27:13 PM", "#12081", "NaN"], ["12195", "Document reduce_agg aggregate function", "Wenlei Xie", "wenleix", "01/22/19, 10:56:53 PM", "NaN", "NaN"], ["12196", "Revert lazy map", "Ying", "yingsu00", "01/09/19, 03:29:55 AM", "PR #11791 (commit 23de11f), which lazily builds the hashtables for maps, introduced a regression for the case where the MapBlock is created through AbstractMapBlock.getRegion(). The hashtables built on the MapBlock region were not updated in the original MapBlock, thus causing hashtables repeatedly being built on the same base MapBlock. \r\n\r\nThe fix https://github.com/prestodb/presto/pull/12189 doesn't get time to get into release 0.216. So we will revert the original commit 23de11f and make the original commits and this fix in the next release 0.217\r", "NaN"], ["12198", "Lazily build hashtable for map", "Ying", "yingsu00", "04/26/19, 07:58:14 AM", "This PR contains \r\n1) e9cecd5 The original lazily build hashtable(23de11f) that was merged in 0.214\r\n2) 1ef0753 Fix lazy map hashtable regression\r\n\r\n23de11f introduced a regression(Issue #12187) for the case where the \r\nMapBlock is created through AbstractMapBlock.getRegion(), in which the \r\nhashtables built on the MapBlock region was not updated in the original \r\nMapBlock, thus causing repeated hashtables build on the same base MapBlock. \r\n\r\n1ef0753 fixes the above issue. It encapsulates the int[] hashtables in \r\nAbstractMapBlock$HashTables object and make it a member of MapBlock/MapBlockBuilder, so that when the sliced MapBlock builds the hashtables, the base MapBlock would also gets\r\nupdated.\r\n\r\nThis PR is to review 1ef0753 mainly.", "NaN"], ["12199", "Make hive temporary staging directory to be configurable", "Grzegorz Kokosi\u0144ski", "kokosing", "01/25/19, 12:11:33 PM", "NaN", "NaN"], ["12203", "Simplify creation of succinct Duration instances", "Nezih Yigitbasi", "nezihyigitbasi", "01/26/19, 12:00:24 AM", "NaN", "NaN"], ["12204", "Fix creating empty non-bucketed Hive partition", "Wenlei Xie", "wenleix", "01/31/19, 04:46:05 AM", "Previously, creating empty non-bucketed Hive partition procedure call\r\ndoes not work when the WriteMode is STAGE_AND_MOVE_TO_TARGET_DIRECTORY,\r\nsince no file is created and the stage directory will not exist.\r\n\r\nFixes https://github.com/prestodb/presto/issues/12002", "NaN"], ["12209", "Add approx_distinct for boolean corresponding to issue #11918", "Ankit Dixit", "ankitdixit", "01/27/19, 04:03:16 AM", "Fixes https://github.com/prestodb/presto/issues/11918", "NaN"], ["12217", "Decide dynamic lifespan schedule at plan time", "Wenlei Xie", "wenleix", "01/28/19, 11:17:41 PM", "Previously this is decided at execution time, and there is no easy way\r\nto tell whether a query use dynamic lifespan schedule for grouped\r\nexecution.\r\n\r\nPart of https://github.com/prestodb/presto/issues/12124, and a follow-up to https://github.com/prestodb/presto/pull/11693", "NaN"], ["12220", "Fix schedule deadlock in presence of grouped execution and broadcast join", "Wenlei Xie", "wenleix", "01/17/19, 07:11:13 AM", "\r\nPreviously, all tasks are proactively created when schedule\r\nis blocked on SPLIT_QUEUES_FULL to prevent potential deadlock in the\r\npresence of broadcast join.\r\n\r\nSuch deadlock usually happens with blockedReason SPLIT_QUEUES_FULL.\r\nHowever, it could also happen with blockedReason NO_ACTIVE_DRIVER_GROUP,\r\nwhen the probe side of broadcast join enables grouped execution,\r\nand partitions on the probe side table are small.\r\nSo lifespan can finish split schedule without blocked by\r\nSPLIT_QUEUES_FULL.\r\n\r\nThis is a regression introduced by 55c10010bbe97ac5de8febf59c7ef58aa4d33f95. \r\nPrior to this commit, all tasks are created unconditionally. ", "NaN"], ["12224", "Extend JdbcSplit with additional predicate field", "Grzegorz Kokosi\u0144ski", "kokosing", "01/25/19, 12:14:00 PM", "Extend JdbcSplit with additional predicate field", "NaN"], ["12225", "Add support for getTableStatistics to base-jdbc connector", "Grzegorz Kokosi\u0144ski", "kokosing", "01/25/19, 12:15:22 PM", "Add support for getTableStatistics to base-jdbc connector", "NaN"], ["12226", "Handle S3 empty directories as directories", "Grzegorz Kokosi\u0144ski", "kokosing", "02/01/19, 09:55:03 AM", "Handle S3 empty directories as directories\n\nA prefix can be added to an S3 bucket without any associated objects\nbeneath it using the Create Folder option in the S3 web portal.\nIf you try to create a table with this S3 prefix you will get an\n\"External location must be a directory\" error.\n\nThis commit fixes this by:\n - trying to access s3 object metadata for key ending with /.\n - checking the content type of of s3 object metadata", "NaN"], ["12234", "Support grouped execution for window function", "Shixuan Fan", "shixuan-fan", "02/01/19, 06:10:04 PM", "Depends on #12122 ", "NaN"], ["12238", "Support boolean type in EXPLAIN IO", "Shixuan Fan", "shixuan-fan", "01/25/19, 08:10:16 PM", "NaN", "NaN"], ["12240", "Disable registering JdkBasedZoneInfoProvider", "Elon Azoulay", "elonazoulay", "01/22/19, 06:57:38 PM", "Resolves #12235 ", "NaN"], ["12241", "Update joda-to-java-time-bridge to version 3", "Elon Azoulay", "elonazoulay", "01/17/19, 10:14:50 AM", "Fixes https://github.com/prestodb/presto/issues/12235", "NaN"], ["12242", "Allow JDBC connector to define procedure", "Grzegorz Kokosi\u0144ski", "kokosing", "01/25/19, 12:17:00 PM", "Allow JDBC connector to define procedure", "NaN"], ["12247", "Upgrading to parquet 0.10.1 and handling TIMESTAMP type.", "Parth Brahmbhatt", "Parth-Brahmbhatt", "03/05/19, 07:15:56 PM", "This patch upgrades parquet to version 0.10.1. The test cases still write using the old version of parquet which is still pulled in through ```presto-hive-apache``` dependency. The old version of parquet lived in `parquet` namespace and the new version is in `org.apache.parquet` namespace so they both can coexist without any conflicts. I have left the unit tests so they are still writing using the old version and reading using the new version to show the backward compatibility. We can add more tests that writes using new version and reads using the new version if everyone agrees with the version upgrade. \r\n\r\nThe ```duplicate-finder-plugin``` snippets can be avoided if we remove the ```parquet.thrift``` and ```about.html``` resources from ```presto-hive-apache```.", "NaN"], ["12249", "Add too many stages warning", "Elon Azoulay", "elonazoulay", "02/04/19, 06:56:14 PM", "Add a soft limit to the stage count to emit a warning.", "NaN"], ["12252", "Add suppressed exceptions to exception that is thrown", "Grzegorz Kokosi\u0144ski", "kokosing", "01/25/19, 12:18:39 PM", "Add suppressed exceptions to exception that is thrown", "NaN"], ["12255", "Update references to website", "Nezih Yigitbasi", "nezihyigitbasi", "01/23/19, 12:39:14 AM", "NaN", "NaN"], ["12256", "Aria dev first cut", "Maria Basmanova", "mbasmanova", "01/23/19, 10:09:08 PM", "Trying to add a single initial commit with all of Orri's work to aria-dev branch.\r\n\r\nCC: @nezihyigitbasi @yingsu00 @elonazoulay ", "NaN"], ["12258", "Replace ByteArrayUtils.getDouble with ByteArrays.getDouble", "Maria Basmanova", "mbasmanova", "01/28/19, 05:07:11 PM", "This fixes java.lang.NoSuchMethodError: sun.misc.Unsafe.getDouble exception\r\nin DoubleStreamReader.scan.\r\n\r\njava.lang.NoSuchMethodError: sun.misc.Unsafe.getDouble(Ljava/lang/Object;I)D\r\n        at com.facebook.presto.spi.block.ByteArrayUtils.getDouble(ByteArrayUtils.java:57)\r\n        at com.facebook.presto.orc.reader.DoubleStreamReader.scan(DoubleStreamReader.java:264)\r\n        at com.facebook.presto.orc.ColumnGroupReader.advance(ColumnGroupReader.java:473)\r\n        at com.facebook.presto.orc.OrcRecordReader.getNextPage(OrcRecordReader.java:808)\r\n        at com.facebook.presto.hive.orc.OrcPageSource.getNextPage(OrcPageSource.java:149)\r\n        at com.facebook.presto.hive.HivePageSource.getNextPage(HivePageSource.java:226)\r\n        at com.facebook.presto.operator.ScanFilterAndProjectOperator.processPageSource(ScanFilterAndProjectOperator.java:418)\r\n        at com.facebook.presto.operator.ScanFilterAndProjectOperator.getOutput(ScanFilterAndProjectOperator.java:303)\r\n        at com.facebook.presto.operator.Driver.processInternal(Driver.java:389)\r\n        at com.facebook.presto.operator.Driver.lambda$processFor$8(Driver.java:293)\r\n        at com.facebook.presto.operator.Driver.tryWithLock(Driver.java:685)\r\n        at com.facebook.presto.operator.Driver.processFor(Driver.java:286)\r\n        at com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)\r\n        at com.facebook.presto.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:162)\r\n        at com.facebook.presto.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:489)\r\n        at com.facebook.presto.$gen.Presto_0_215_117_g026543d____20190124_193910_1.run(Unknown Source)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n        at java.base/java.lang.Thread.run(Thread.java:844)", "NaN"], ["12266", "Update Drift to 1.15", "James Sun", "highker", "01/28/19, 12:24:11 AM", "NaN", "NaN"], ["12269", "Fixes to enable Queen of the Night", "Maria Basmanova", "mbasmanova", "01/28/19, 10:44:57 PM", "Queen of the Night (QoN) query is the following:\r\n\r\n```\r\nselect count (*), sum (l.extendedprice * (1 - l.discount) - l.quantity * p.supplycost) \r\nfrom lineitem l, partsupp p \r\nwhere l.partkey = p.partkey and l.suppkey = p.suppkey and p.availqty < 1000;\r\n```\r\n\r\nCC: @tdcmeehan @oerling @nezihyigitbasi @yingsu00 @elonazoulay ", "NaN"], ["12270", "Fix memory leak when query fails with permission error", "Vic Zhang", "viczhang861", "02/25/19, 06:57:33 PM", "When permission error is thrown inside creation of QueryExecution object,\r\ntransaction metadata is not marked as inactive resulting in memory leak.\r\n\r\nExisting handling of error via method TransactionMetadata::fail is not\r\nworking when transaction id is empty in session object.\r\n\r\nA heapdump is taken to test that previously leaked objects become \r\nunreachable from GC roots after this fix.", "NaN"], ["12272", "Fix stylecheck errors", "Maria Basmanova", "mbasmanova", "01/29/19, 01:11:01 AM", "Fixed stylecheck errors and enabled style checks in the POM file.\r\n\r\nRemoved TestAriaHash.java. This file had a lot of checkstyle errors, lots of dead code and no way to run it. \r\n\r\nCC: @elonazoulay @oerling @yingsu00 @tdcmeehan @nezihyigitbasi ", "NaN"], ["12274", " Add option not to delete staging directories on failure", "Rebecca Schlussel", "rschlussel", "01/31/19, 07:55:34 PM", "Deleting staging directories can cause data loss when:\r\n1. a query reads from and writes to the same partition\r\n2. the metastore alter table call times out, but eventually completes\r\n3. the metastore calls to undo the alter table don't succeed\r\n\r\nIf the query does not read from and write to the same partition, it\r\nwon't cause data loss but there will still be an invalid entry in the\r\nmetastore.\r", "NaN"], ["12277", "Update Drift to 1.17", "Andrii Rosa", "arhimondr", "01/30/19, 04:15:47 PM", "This version includes multiple important bug fixes:\r\n\r\n- Fail only single request on too large frame\r\n  https://github.com/airlift/drift/pull/102\r\n- Add delay before retrying same address\r\n  https://github.com/airlift/drift/pull/101\r\n- Check if pooled connection is still open\r\n  https://github.com/airlift/drift/pull/95", "NaN"], ["12280", "Fix tests", "Maria Basmanova", "mbasmanova", "01/31/19, 01:44:21 AM", "CC: @nezihyigitbasi @tdcmeehan @yingsu00 @elonazoulay @oerling ", "NaN"], ["12281", "Fix Elasticsearch connector pom", "Nezih Yigitbasi", "nezihyigitbasi", "01/30/19, 10:11:05 PM", "NaN", "NaN"], ["12285", "Add lambda functions for handling duplicate keys in split_to_map", null, "mayankgarg1990", "02/22/19, 08:28:53 PM", "NaN", "NaN"], ["12287", "Fix Aria scan for partitioned tables", "Maria Basmanova", "mbasmanova", "02/01/19, 03:52:27 PM", "When reading from partitioned tables, `outputChannels` may include partition key columns, but there are no `StreamReader`s for these columns in the `ColumnGroupReader`. This causes NPE in `ColumnGroupReader.getBlocks()`. This PR fixes the NPE and allows \r\n `TestOrcPageSourceMemoryTracking#testScanFilterAndProjectOperator` to pass.\r\n\r\n`TestOrcPageSourceMemoryTracking#testTableScanOperator` is still failing, but with a different error:\r\n\r\n```\r\njava.lang.AssertionError: expected:<362883> to be between <460000> and <469999> inclusive\r\n\r\n\tat io.airlift.testing.Assertions.fail(Assertions.java:323)\r\n\tat io.airlift.testing.Assertions.assertBetweenInclusive(Assertions.java:252)\r\n\tat io.airlift.testing.Assertions.assertBetweenInclusive(Assertions.java:230)\r\n\tat com.facebook.presto.hive.TestOrcPageSourceMemoryTracking.testTableScanOperator(TestOrcPageSourceMemoryTracking.java:344)\r\n```\r\n\r\nCC: @oerling @nezihyigitbasi @elonazoulay @yingsu00 @tdcmeehan ", "NaN"], ["12292", "Log details around OOM decisions in ClusterMemoryManager", "Nezih Yigitbasi", "nezihyigitbasi", "02/02/19, 02:52:27 AM", "NaN", "NaN"], ["12293", "Fix a bad merge", "Maria Basmanova", "mbasmanova", "02/01/19, 11:26:33 PM", "NaN", "NaN"], ["12298", "Remove system pool entirely", "Nezih Yigitbasi", "nezihyigitbasi", "02/06/19, 12:51:32 AM", "Presto has been running fine without a system memory pool in our\r\nproduction clusters for quite some time. This change removes it entirely.", "NaN"], ["12299", "Fix JVM crash in TestHiveDistributedQueries", "Maria Basmanova", "mbasmanova", "02/04/19, 09:41:13 PM", "The following test was crashing JVM:\r\n\r\nmvn test -pl presto-hive -Dtest=TestHiveDistributedQueries\r\n\r\n```\r\nJ 23694 c1 com.facebook.presto.spi.block.ByteArrayUtils.getDouble([BI)D (14 bytes) @ 0x000000011366cb88 [0x000000011366cb40+0x0000000000000048]\r\nj  com.facebook.presto.orc.reader.DoubleStreamReader.scan()V+347\r\nj  com.facebook.presto.orc.ColumnGroupReader.advance()V+139\r\nj  com.facebook.presto.orc.OrcRecordReader.getNextPage()Lcom/facebook/presto/spi/Page;+170\r\nj  com.facebook.presto.hive.orc.OrcPageSource.getNextPage()Lcom/facebook/presto/spi/Page;+11\r\n``` \r\n\r\nI believe this is because ByteArrayUtils doesn't perform a range check because reading from `Unsafe`. The fix is to use methods from ByteArrays instead.", "NaN"], ["12301", "Fix starvation while waiting for minimum number of workers", "Raghav Sethi", "raghavsethi", "02/05/19, 02:01:39 AM", "Callers of waitForMinimumWorkers should not use a direct executor in listeners,\r\nas this can delay notifications to other listeners.\r\n\r\nRef: https://github.com/prestosql/presto/pull/155", "NaN"], ["12302", "Fix accounting bug in DoubleStreamReader", "Elon Azoulay", "elonazoulay", "02/05/19, 10:30:19 PM", "Fix accounting bug in DoubleStreamReader.\r\nExpose various overflow conditions in OrcRecordReader in a separate commit.", "NaN"], ["12303", "Use non-Aria scan if no columns need to be read", "Maria Basmanova", "mbasmanova", "02/05/19, 07:50:32 PM", "Fix `SELECT count(null) FROM orders` query from `AbstractTestQueries#testCountAll` by using non-Aria scan path when no columns need to be read. Added input validation to `PageSourceOptions`'s constructor.\r\n\r\nRan `mvn test -pl presto-hive -Dtest=TestHiveDistributedQueries`:\r\n\r\nTests run: 600, Failures: 89, Errors: 0, Skipped: 0, Time elapsed: 206.375 s", "NaN"], ["12304", "Add ST_Points spatial function", "Jay Zhuang", "jay-zhuang", "02/06/19, 09:54:18 PM", "Add ST_Points(geometry) function to return an array of points in a\r\nLineString geometry.", "NaN"], ["12306", "Fix missing child node for ProjectNode in PlanPrinter", "Jiexi Lin", "jessesleeping", "02/06/19, 06:08:46 PM", "Previous code will ignore any child node of the ProjectNode while printing text query plan in EXPLAIN.", "NaN"], ["12307", "Fix test failures in presto-orc", "Maria Basmanova", "mbasmanova", "02/06/19, 08:25:46 PM", "```\r\n> mvn test -pl presto-orc\r\n\r\nTests run: 189, Failures: 0, Errors: 0, Skipped: 0\r\n```", "NaN"], ["12308", "Fix condition for not deleting target directories", "Rebecca Schlussel", "rschlussel", "02/06/19, 07:59:06 PM", "It should only happen when write mode is direct_to_target_new_directory.", "NaN"], ["12310", "[WIP] Aria scan fixes", "Maria Basmanova", "mbasmanova", "02/12/19, 01:12:45 AM", "Numerous scan fixes from @oerling ", "NaN"], ["12312", "Add UnnestStatsRule to estimate stats for UnnestNode", "Rongrong Zhong", "rongrong", "02/26/19, 06:40:46 PM", "Hey @arhimondr, follow up with our conversation, this is the least hacky way I got so far. I feel this is almost reasonable as a trade off before we can get stats on complex types.", "NaN"], ["12314", "Fix reading new created Hive unpartitioned table within txn", "Wenlei Xie", "wenleix", "02/22/19, 07:38:19 PM", "Resolves https://github.com/prestodb/presto/issues/12311", "NaN"], ["12315", "Implement ST_Area for SphericalGeography", null, "ochalouhi", "03/04/19, 05:30:09 PM", "See https://www.movable-type.co.uk/scripts/latlong.html\r\nand http://osgeo-org.1560.x6.nabble.com/Area-of-a-spherical-polygon-td3841625.html\r\nand https://www.element84.com/blog/determining-if-a-spherical-polygon-contains-a-pole\r\nfor the underlying Maths", "NaN"], ["12316", "Change error code for empty table name", "Elon Azoulay", "elonazoulay", "02/15/19, 08:50:48 PM", "Changing the error code from GENERIC_INTERNAL to SYNTAX_ERROR\r\nfor queries where the table name is empty.", "NaN"], ["12317", "Improve error message when column mismatch in insert query", "Vic Zhang", "viczhang861", "03/08/19, 08:35:52 PM", "The first mismatched column position, column name, column type will be displayed.\r\n\r\n1.   Size mismatch\r\n(1) Insert query has more columns than expected \r\n   example:   \" Insert query has more expressions than target columns. Mismatch at column 5\"\r\n(2) Insert query has less columns than expected (the first missing column's name printed)\r\n   example:    \"Insert query has more target columns than expressions. Mismatch at column 4: 'd' \"\r\n\r\n2.  Type mismatch , independent of error message for size mismatch\r\n    Whenever a mismatched column appears in both target location and user expression,  target column's name and expected type will be printed.\r\n   example:   \"Mismatch at column 3: 'c' is of type bigint but expression is of type varchar\"\r\n\r\nResolves #4322 ", "NaN"], ["12318", "Minor method rename in GroupedExecutionTagger", "Wenlei Xie", "wenleix", "02/22/19, 07:35:22 PM", "Rename visitWindowFunctionNode to processWindowFunction to make it\r\nclear it is not part of the visitor interface.\r\n\r\nExtracted-From: https://github.com/prestosql/presto/commit/cd45f35ef429dbc8ee232b41047da7096656be7b\r\n\r\nOriginal PR: https://github.com/prestodb/presto/pull/12234\r", "NaN"], ["12321", "Fix session property dynamic_schedule_for_grouped_execution", "Wenlei Xie", "wenleix", "02/22/19, 07:35:04 PM", "Previously the parameter `default` and `hidden` are in wrong order.", "NaN"], ["12322", "Move classes out of root package", "Wenlei Xie", "wenleix", "02/23/19, 01:48:25 AM", "Extracted-From: https://github.com/prestosql/presto/pull/31", "NaN"], ["12326", "Extract common logic in HiveLocationService", "Shixuan Fan", "shixuan-fan", "02/22/19, 07:54:21 PM", "NaN", "NaN"], ["12330", " Add option to preload splits for grouped execution", "Shixuan Fan", "shixuan-fan", "02/22/19, 07:54:40 PM", "NaN", "NaN"], ["12331", "Disable bucket filter with compatible bucketing read", "Wenlei Xie", "wenleix", "02/13/19, 01:55:34 AM", "Unnecessary remote exchange between compatible bucketing\r\nin 57c70aee95fe6c00666f13ba5dc2f930fae87d84 (compatible bucketing\r\nrefers to when Hive tables have same bucketing key, and bucket numbers\r\nare different but compatible).\r\n\r\nTo remove unnecessary remote exchange, the concept of `readBucketNumber`,\r\nwhich might be different from `tableBucketNumber` was introduced. This\r\nnumber indicates the logical bucket number from engine's perspective.\r\n\r\nAfter the change `$bucket` column respects `readBucketNumber` instead of\r\n`tableBucketNumber`, this can cause correctness issues,  as described in\r\nhttps://github.com/prestodb/presto/issues/12329.\r\n\r\nThis commit disable bucket filter with compatible bucketing read to\r\navoid correctness problem. To fix the root cause, we should make `$bucket`\r\nrespect `tableBucketNumber`.\r\n\r\n\r\n\r\nShort term fix to https://github.com/prestodb/presto/issues/12329", "NaN"], ["12332", "Improve explain by predictably ordering predicates", "Gunjan Jha", "jha-gunjan", "02/22/19, 06:41:24 PM", "Fixes #11444 . Cherry pick of TupleDomain.java from https://github.com/prestosql/presto/pull/226/\r\n\r\nQuery PlanPrinter uses TupleDomain, which was giving unpredictable\r\norder of tuples due to the use of HashMap/HashSet.\r\nSwitched to LinkedHashMap/LinkedHashSet to get predictable ordering.\r", "NaN"], ["12333", "Add TestAriaHiveDistributedQueries", "Maria Basmanova", "mbasmanova", "02/12/19, 10:54:57 PM", "Pulled latest from oerling/test_struct_4 and transferred test queries from scripts/tests into TestAriaHiveDistributedQueries.\r", "NaN"], ["12334", "Update geospatial docs", "James Gill", "jagill", "03/01/19, 06:13:19 PM", "ST_Point signature is `double, double`, which is not clear whether that\r\nshould be `latitude, longitude` or `longitude, latitude`.  This message\r\nclarifies that.\r\n\r\ncc @mbasmanova ", "NaN"], ["12335", "Add more queries to TestAriaHiveDistributedQueries", "Maria Basmanova", "mbasmanova", "02/13/19, 02:37:54 PM", "Add all queries from scripts/tests in oerling/test_struct_4 plus Queen-of-the-Night to TestAriaHiveDistributedQueries. The test is passing.\r\n\r\nCC: @elonazoulay @tdcmeehan @yingsu00 @oerling ", "NaN"], ["12336", "Mark lifespan as completed after all output delivered", "Wenlei Xie", "wenleix", "03/05/19, 05:04:35 AM", "Currently, a lifepsan reported to be completed might still have\r\npending data in OutputBuffer. This makes it difficult to decide\r\nlifespans to be rerun in the scenario of failure recovery. Since even\r\n\"completed\" lifespan from a failed task might not finish flushing\r\ndata in OutputBuffer.", "NaN"], ["12337", "Refactor StreamReaders", "Maria Basmanova", "mbasmanova", "02/14/19, 02:20:30 AM", "TestAriaHiveDistributedQueries is passing.\r\n\r\n```\r\n> mvn test -pl presto-hive -Dtest=*Aria*\r\n...\r\n[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 62.184 s - in com.facebook.presto.hive.TestAriaHiveDistributedQueries\r\n```\r\n\r\nCC: @tdcmeehan @elonazoulay @yingsu00 @oerling ", "NaN"], ["12339", "First cut of Aria-based SliceDictionaryStreamReader", "Maria Basmanova", "mbasmanova", "02/15/19, 09:24:41 PM", "TestAriaHiveDistributedQueries is passing\r\n\r\nCC: @elonazoulay @oerling @yingsu00 @tdcmeehan ", "NaN"], ["12345", "Add release notes for 0.217", "Leiqing Cai", "caithagoras", "02/21/19, 05:40:39 AM", "https://github.com/prestodb/presto/issues/12257", "NaN"], ["12347", "Add SortNode stats and cost rules", "Vic Zhang", "viczhang861", "02/26/19, 09:57:26 PM", "No stats change in SortNode, just Copy stats from source node.\r\n\r\nPart of #12202 ", "NaN"], ["12348", "Fix exception created and dropped rather than thrown", "Hiroaki Yoshida", "hiroakiy", "02/22/19, 06:56:53 PM", "FindBugs-3.0.1 ([http://findbugs.sourceforge.net/](http://findbugs.sourceforge.net/)) reported an RV_EXCEPTION_NOT_THROWN warning on master:\r\n```\r\nRV_EXCEPTION_NOT_THROWN: new IllegalArgumentException(String) not thrown in com.facebook.presto.execution.SqlTaskManager.updateMemoryPoolAssignments(MemoryPoolAssignmentsRequest) At SqlTaskManager.java:[line 210]\r\n```\r\nThe description of this bug is as follows:\r\n> RV: Exception created and dropped rather than thrown (RV_EXCEPTION_NOT_THROWN)\r\n> This code creates an exception (or error) object, but doesn't do anything with it. For example, something like\r\n> ```\r\n> if (x < 0)\r\n>   new IllegalArgumentException(\"x must be nonnegative\");\r\n> ```\r\n> It was probably the intent of the programmer to throw the created exception:\r\n> ```\r\n> if (x < 0)\r\n>   throw new IllegalArgumentException(\"x must be nonnegative\u201d);\r\n> ```\r\n [http://findbugs.sourceforge.net/bugDescriptions.html#RV_EXCEPTION_NOT_THROWN](http://findbugs.sourceforge.net/bugDescriptions.html#RV_EXCEPTION_NOT_THROWN)", "NaN"], ["12349", "Fix check for oddness that won't work for negative numbers", "Hiroaki Yoshida", "hiroakiy", "02/22/19, 06:51:56 PM", "FindBugs-3.0.1 ([http://findbugs.sourceforge.net/](http://findbugs.sourceforge.net/)) reported an IM_BAD_CHECK_FOR_ODD warning on master:\r\n```\r\nIM_BAD_CHECK_FOR_ODD: Check for oddness that won't work for negative numbers in com.facebook.presto.cli.FormatUtils.formatProgressBar(int, int) At FormatUtils.java:[line 179]\r\n```\r\nThe description of this bug is as follows:\r\n> IM: Check for oddness that won't work for negative numbers (IM_BAD_CHECK_FOR_ODD)\r\n> The code uses `x % 2 == 1` to check to see if a value is odd, but this won't work for negative numbers (e.g., `(-5) % 2 == -1`). If this code is intending to check for oddness, consider using `x & 1 == 1`, or `x % 2 != 0`.\r\n [http://findbugs.sourceforge.net/bugDescriptions.html#IM_BAD_CHECK_FOR_ODD](http://findbugs.sourceforge.net/bugDescriptions.html#IM_BAD_CHECK_FOR_ODD)", "NaN"], ["12350", "Fix int value cast to float and then passed to Math.round", "Hiroaki Yoshida", "hiroakiy", "02/22/19, 06:50:54 PM", "FindBugs-3.0.1 ([http://findbugs.sourceforge.net/](http://findbugs.sourceforge.net/)) reported two ICAST_INT_CAST_TO_FLOAT_PASSED_TO_ROUND warnings on master:\r\n```\r\nICAST_INT_CAST_TO_FLOAT_PASSED_TO_ROUND: int value cast to float and then passed to Math.round in com.facebook.presto.raptor.storage.BackupStats.addCopyShardDataRate(DataSize, Duration) At BackupStats.java:[line 42]\r\nICAST_INT_CAST_TO_FLOAT_PASSED_TO_ROUND: int value cast to float and then passed to Math.round in com.facebook.presto.raptor.storage.ShardRecoveryStats.addShardRecoveryDataRate(DataSize, DataSize, Duration) At ShardRecoveryStats.java:[line 68]\r\n```\r\nThe description of this bug is as follows:\r\n> ICAST: int value cast to float and then passed to Math.round (ICAST_INT_CAST_TO_FLOAT_PASSED_TO_ROUND)\r\n> This code converts an int value to a float precision floating point number and then passing the result to the Math.round() function, which returns the int/long closest to the argument. This operation should always be a no-op, since the converting an integer to a float should give a number with no fractional part. It is likely that the operation that generated the value to be passed to Math.round was intended to be performed using floating point arithmetic.\r\n[http://findbugs.sourceforge.net/bugDescriptions.html#ICAST_INT_CAST_TO_FLOAT_PASSED_TO_ROUND](http://findbugs.sourceforge.net/bugDescriptions.html#ICAST_INT_CAST_TO_FLOAT_PASSED_TO_ROUND)", "NaN"], ["12351", "Revert \"Propagate partition properties for full outer join\"", "Rongrong Zhong", "rongrong", "02/19/19, 08:21:53 AM", "This reverts commit 801347e3c6409c649370a3b55951ef7a73c4f04c.", "NaN"], ["12355", "Propagate partition properties for full outer join", "Rongrong Zhong", "rongrong", "02/22/19, 07:22:12 PM", "For full outer join, if both sides of the equi-join contains all partitioning columns,\r\nthe output of the join would be partitioned on coalesce of these columns.\r\n\r\nFixed the issue with incorrectly optimize out remote exchange for aggregation. Added new test to verify aggregation is correct. Also changed existing test to be more robust.", "NaN"], ["12357", "Make test for empty table statistics overridable", "Rebecca Schlussel", "rschlussel", "02/26/19, 04:55:16 PM", "Make the test for empty table statistics overridable.  In our system\r\nzero statistics often indicates that statistics are corrupt, so we want\r\nto be able to handle it specially", "NaN"], ["12358", "Remove unused ConncetorAnalyzeMetadata", "Jiexi Lin", "jessesleeping", "02/22/19, 07:43:15 PM", "Remove an unused class merged with the ANALYZE statement PR.\r\n\r\nRef https://github.com/prestosql/presto/pull/112", "NaN"], ["12359", "Fix memory accounting for geometry aggregations", "Timothy Meehan", "tdcmeehan", "02/27/19, 05:30:16 PM", "OGCGeometry objects are stateful and may change in memory size due\r\nsimply to being queried.  This means memory usage must be updated before\r\nany OGCGeometry operation is performed and then after.\r\n\r\nFixes #12356\r\n\r\ncc: @mbasmanova ", "NaN"], ["12363", "Add support for smile encoding", "Nezih Yigitbasi", "nezihyigitbasi", "02/27/19, 06:30:09 AM", "This change adds support for smile encoding to Presto.  When the\r\ncorresponding PR in Airlift is merged (https://github.com/airlift/airlift/pull/706)\r\nAirlift should be used and these changes should be removed.", "NaN"], ["12366", "Add support for Jackson afterburner module", "cem cayiroglu", "cemcayiroglu", "02/25/19, 06:46:29 PM", "set json.module.after-burner.enabled=true to\r\nenable the feature.", "NaN"], ["12370", "Minor cleanup & a fix", "Nezih Yigitbasi", "nezihyigitbasi", "02/27/19, 07:38:38 AM", "This one is a minor fix: https://github.com/prestodb/presto/commit/f1b76e25c8ce9f43025d62a0a690b077f9d94b7b\r\n\r\nThe rest are cleanups & refactorings.", "NaN"], ["12371", "Add smile support to coordinator-to-worker protocol", "Nezih Yigitbasi", "nezihyigitbasi", "02/28/19, 10:27:11 PM", "Depends on https://github.com/prestodb/presto/pull/12363, so only 1614354 should be reviewed.", "NaN"], ["12372", "Add smoke test for AfterBurner support", "cem cayiroglu", "cemcayiroglu", "02/25/19, 07:45:59 PM", "NaN", "NaN"], ["12373", "Add connector specific partitioning support for remote exchanges", "Andrii Rosa", "arhimondr", "02/28/19, 06:17:09 PM", "NaN", "NaN"], ["12374", "Reduce logging in tests", "Andrii Rosa", "arhimondr", "02/22/19, 06:40:57 PM", "NaN", "NaN"], ["12377", "Add queryType to QueryCompletedEvent", null, "mayankgarg1990", "03/26/19, 06:52:16 PM", "This is needed for additional information for logging query completion.", "NaN"], ["12378", "Update Drift to 1.18", "Andrii Rosa", "arhimondr", "02/23/19, 01:37:13 AM", "NaN", "NaN"], ["12379", "Fix test failure on master", "Nezih Yigitbasi", "nezihyigitbasi", "02/24/19, 12:39:37 AM", "Fixes TestHiveS3Config.\r\n\r\n[Reviewer, please feel free to merge once approved.]", "NaN"], ["12381", "Move ClassLoaderSafeConnectorPageSink to correct package", "Wenlei Xie", "wenleix", "02/26/19, 04:50:58 AM", "Extracted-From: https://github.com/prestosql/presto/pull/300", "NaN"], ["12382", "Add documentation about with clause non-determinism", "Rebecca Schlussel", "rschlussel", "03/05/19, 02:42:09 PM", "The current behavior of the with clause, where the query gets executed\r\neach time the relation is used is contrary to the SQL standard.  Add\r\nwarning about it to the docs.", "NaN"], ["12383", "Roles management", "Andrii Rosa", "arhimondr", "02/27/19, 06:31:56 PM", "Cherry picked from\r\n\r\n- https://github.com/prestosql/presto/pull/90 (originally https://github.com/prestodb/presto/pull/11645)\r\n- https://github.com/prestosql/presto/pull/113\r\n- https://github.com/prestosql/presto/pull/291", "NaN"], ["12385", "Update concepts.rst - grammar fix", "Jon Haygood", "jaygood", "02/26/19, 06:33:27 PM", "remove the 'a'", "NaN"], ["12386", "Add hive session property to enable writing staging files", "Shixuan Fan", "shixuan-fan", "03/09/19, 12:00:40 AM", "This is part of the effort on recoverable grouped execution (#12124).", "NaN"], ["12391", "Implement SliceDictionaryStreamReader.compactValues", "Maria Basmanova", "mbasmanova", "02/27/19, 03:31:13 PM", "TestAriaHiveDistributedQueries is passing. ", "NaN"], ["12392", "Remove flaky test case for transaction manager", "Vic Zhang", "viczhang861", "02/27/19, 01:45:06 AM", "Query execution is async task, which may cause assertion test in caller thread to fail :  metadata empty check is run before query fails and cleaned.\r", "NaN"], ["12393", "Add output types back to EXPLAIN output", "Raghav Sethi", "raghavsethi", "03/04/19, 05:43:50 PM", "NaN", "NaN"], ["12394", "Revert \"Fix Hive table read within transaction\"", "Wenlei Xie", "wenleix", "02/27/19, 05:02:26 PM", "This reverts the following commits:\r\n * a452ccf59bdc09750abd63bf285c1b2e9347a578\r\n * 4666559c319d6b1118568b03083d9e12479018d4\r\n * 90f8e3acfb6591afcef6718c65ce6b350defa012\r\n\r\nIn order to fix reading new created Hive unpartitioned table within\r\ntransaction, 90f8e3acfb6591afcef6718c65ce6b350defa012 fix the\r\ncurrentLocation contract in TableAndMore. However, it turns out\r\nthere are places implicitly rely on currentLocation is always provided\r\n(e.g. CREATE TABLE AS SELECT partitioned table, with WriteMode is\r\nDIRECT_TO_TARGET_NEW_DIRECTORY)", "NaN"], ["12395", "Aria scan fixes + optimize SliceDictionaryStreamReader", "Maria Basmanova", "mbasmanova", "03/01/19, 07:15:13 PM", "Fix queries with filters on multiple columns and output columns being a subset of input columns.\r\n\r\n```\r\nSELECT\r\n   linenumber,\r\n   orderkey\r\nFROM lineitem\r\nWHERE\r\n   shipinstruct = 'TAKE BACK RETURN' AND orderkey < 10\r\n```\r\n\r\nTestAriaHiveDistributedQueries is passing.", "NaN"], ["12396", "Flush Elasticsearch testcase data loading", "Zhenxiao Luo", "zhenxiao", "02/28/19, 05:10:22 PM", "to resolve Jenkins failure @nezihyigitbasi ", "NaN"], ["12397", "Allow HiveQueryRunner to persist data to a custom path", "Ying", "yingsu00", "02/28/19, 08:33:19 PM", "Add an optional argument to HiveQueryRunner to provide a path to read\r\nand write the data and metadata to. The path must point to a writable\r\ndirectory. On first run, HiveQueryRunner will create tpch and\r\ntpch_bucketed tables and store them in the specified directory. On\r\nsubsequent runs using the same path, HiveQueryRunner will re-use the\r\ndata created before.\r\n\r\nIf no argument is provided, HiveQueryRunner will create a temporary\r\ndirectory to store data and delete it before exiting.", "NaN"], ["12398", "Calculate query peak memory usage as memory cost", "Wenlei Xie", "wenleix", "03/02/19, 10:40:32 PM", "Extracted-From: https://github.com/prestosql/presto/pull/247", "NaN"], ["12401", "Fix TestCachingOrcDataSource in presto-orc", "Ying", "yingsu00", "02/28/19, 09:03:20 PM", "NaN", "NaN"], ["12403", "Decrease logging", "Andrii Rosa", "arhimondr", "02/28/19, 06:15:42 PM", "NaN", "NaN"], ["12405", "Suppress test failures on TestAriaOrcReader", "Ying", "yingsu00", "03/04/19, 01:06:20 AM", "Currently filter push down is not supported by all StreamReaders, and\r\nTestAriaOrcReader fails on these tests. This stops the build process\r\nevery time, so we want to temprarily disable such tests.", "NaN"], ["12406", "Update Joni version to 2.1.5.2", "Rebecca Schlussel", "rschlussel", "03/05/19, 06:32:18 PM", "This contains a fix for an ArrayIndexOutOfBounds error for certain regex\r\ninputs.\r\n\r\nFixes #8711 ", "NaN"], ["12407", "Unit tests for LocalMemoryManager", "cem cayiroglu", "cemcayiroglu", "03/01/19, 07:29:27 PM", "NaN", "NaN"], ["12408", "Fix Parquet predicate pushdown for smallint, tinyint", "Nezih Yigitbasi", "nezihyigitbasi", "03/28/19, 08:12:31 PM", "Backports https://github.com/prestosql/presto/pull/131\r\nOriginal author is @findepi ~~couldn't retain authorship while applying the patch, but added the original author to all commit messages.~~", "NaN"], ["12409", "Add analyze properties system table and documentation for ANALYZE statement", "Jiexi Lin", "jessesleeping", "03/04/19, 06:27:06 PM", "The first commit adds a system table that lists all analyze properties supported in the cluster. The second commit adds documentation page for ANALYZE statement. \r\n\r\n#12400 ", "NaN"], ["12412", "Add Aria readers for boolean, byte, float, decimal and timestamp ORC types", "Maria Basmanova", "mbasmanova", "03/06/19, 09:58:47 PM", "NaN", "NaN"], ["12414", "Fix RIGHT/OUTER Join stream ordering property derivations", "Wenlei Xie", "wenleix", "03/04/19, 08:00:05 PM", "Extracted-From: https://github.com/prestosql/presto/pull/337", "NaN"], ["12420", "Allow empty string as delimiter for `split`.", null, "ssaumitra", "03/27/19, 09:39:15 PM", "Updating `split` function so that it starts to work well with empty delimiter.", "NaN"], ["12422", "ES Connector failed with SearchGuard", null, "TempleZhou", "03/08/19, 09:55:46 PM", "NaN", "NaN"], ["12423", "Implement Aria scan in LongInputStreams", "Elon Azoulay", "elonazoulay", "03/07/19, 04:56:18 PM", "Initial implemention of aria scan in LongInputStreamV1 and LongInputStreamDwrf.\r\nSome bug fixes for LongInputStreamV2.\r\n\r\nTODO:\r\nIntegrate the code from https://github.com/prestodb/presto/pull/12417 into OrcInputStream.\r", "NaN"], ["12425", "Add print_stats_for_non_join_query session property", "Vic Zhang", "viczhang861", "03/08/19, 10:36:51 PM", "Provide an optoin to only show stats for exaplain query or\r\njoin query in query plan.", "NaN"], ["12427", "Add FunctionHandle as abstraction for function resolution", "Rongrong Zhong", "rongrong", "03/06/19, 08:00:31 PM", "NaN", "NaN"], ["12429", "Fix incorrect $bucket for mismatch bucket queries", null, "mayankgarg1990", "03/26/19, 07:04:23 PM", "In case of a join of two tables with mismatching but compatible bucket counts, `$bucket` resolved to the read bucket count instead of partition's bucket count. This led to incorrect results when filters were applied on one of the original tables.", "NaN"], ["12430", "Use airlift artifact from Maven Central", "Maria Basmanova", "mbasmanova", "03/05/19, 02:16:23 AM", "* Use airlift 0.179-masha-SNAPSHOT that's available in Maven Central to allow Travis to build successfully; this artifact corresponds to https://github.com/airlift/airlift/pull/707\r\n* Fix stylecheck errors in presto-spi", "NaN"], ["12432", "Move FunctionHandle to SPI", "Yi He", "hellium01", "03/08/19, 12:23:15 AM", "This PR moves FunctionHandle and its dependency to SPI. This PR depends on #12427. This is the first part of a larger PR: #12375 ", "NaN"], ["12436", "Remove useless output from EXPLAIN", "Wenlei Xie", "wenleix", "03/06/19, 10:58:46 PM", "Before the change, EXPLAIN would print \"Cost: ?, Output: ? rows (?B)\"\r\nfor every node. This information is available only in EXPLAIN ANALYZE.\r\n\r\nExtracted-From: https://github.com/prestosql/presto/commit/ce9ecf97ed92c3c58494570bc2f17c512d2714a0\r\n\r\nOriginal PR: https://github.com/prestosql/presto/pull/397", "NaN"], ["12439", "Cherry-picking several Parquet changes from prestosql", "Nezih Yigitbasi", "nezihyigitbasi", "03/07/19, 04:57:10 AM", "NaN", "NaN"], ["12441", "Fix processing of function filters in ColumnGroupReader", "Maria Basmanova", "mbasmanova", "03/07/19, 09:44:49 PM", "- needRowNumberMap should return false when reader has no inner filter\r\nand there is no function filter attached\r\n- advance needs to clear output qSet for a reader that has no inner filter\r\nand no function filter attached; such a reader may have non-null output qSet\r\nif before filter reordering it had function filter attached\r\n\r\nAn example of affected query is:\r\n\r\n```\r\nSELECT orderkey, suppkey, partkey, linenumber\r\nFROM lineitem\r\nWHERE orderkey % 4 = 0\r\n  AND suppkey % 2 = partkey % 2\r\n  AND linenumber % 3 = orderkey % 3\r\n  AND lineitem.suppkey % 2 = lineitem.linenumber % 3\r\n```\r\n\r\nAfter these changes TestHiveDistributedQueries.testJoinPredicateMoveAround\r\nand testRightJoinEqualityInference are passing.", "NaN"], ["12442", "Support dynamic execution for broadcast joins", "Rebecca Schlussel", "rschlussel", "03/08/19, 04:14:03 PM", "NaN", "NaN"], ["12443", "Adapt Aria scan to schema evolution", "Maria Basmanova", "mbasmanova", "03/07/19, 09:45:58 PM", "Fix OrcPageSource to include all-nulls block if a column is not present in the\r\ndata file. This happens when a column is added after some data was already\r\ninserted.\r\n\r\nAfter this change TestHiveDistributedQueries.testAddColumn is passing.\r\n\r\nIncludes #12441", "NaN"], ["12444", "Refactor RowExpression and move CallExpression to FunctionHandle", "Rongrong Zhong", "rongrong", "03/26/19, 10:13:15 PM", "NaN", "NaN"], ["12445", "Fix Aria scan for IN filters", "Maria Basmanova", "mbasmanova", "03/15/19, 05:57:14 PM", "This set of changes is fixing scan for NOT IN and IN with large number of values. TestHiveDistributedQueries#testLargeIn is now passing.\r\n\r\n```\r\nSELECT orderkey FROM orders WHERE orderkey NOT IN (1, 2, 3);\r\n\r\nSELECT orderkey FROM orders WHERE orderkey IN (1, 2, 3,...5000);\r\n```", "NaN"], ["12446", "Revert \"Split special form of row expressions to separate node\"", "Yi He", "hellium01", "03/07/19, 10:40:01 PM", "This reverts commit f8b11ee4a025d0c13807f6f1f412611307844c46. The commit was still under review and was accidentally merged (not sure how this can happen since the PR is not approved). ", "NaN"], ["12447", " Fix NPE in ScanFilterAndProjectOperator", "Maria Basmanova", "mbasmanova", "03/07/19, 10:00:28 PM", "Fixes TestHiveDistributedQueries.testUnionWithAggregation", "NaN"], ["12450", "Make ST_Centroid try-friendly", "Maria Basmanova", "mbasmanova", "03/08/19, 08:43:03 PM", "Fixes #12404", "NaN"], ["12451", "Validate fragmented plan", "Andrii Rosa", "arhimondr", "03/08/19, 07:48:44 PM", "The first part of the https://github.com/prestodb/presto/pull/12419", "NaN"], ["12452", "Remove collected statitstics from plan", "Andrii Rosa", "arhimondr", "03/26/19, 06:42:21 PM", "Print collected statistics only in verbose mode", "NaN"], ["12453", "Fix checkstyle errors", "Maria Basmanova", "mbasmanova", "03/08/19, 09:03:25 PM", "NaN", "NaN"], ["12455", "Set context class loader inside Hive procedures", "Wenlei Xie", "wenleix", "03/26/19, 06:40:24 PM", "The procedures call Hadoop code which can load file systems or\r\nperform other actions that require the context class loader.\r\n\r\nExtracted-From: https://github.com/prestosql/presto/pull/414", "NaN"], ["12457", "Minor bug fix in BlackHoleConnector", "Wenlei Xie", "wenleix", "03/26/19, 06:56:36 PM", "BlackholeConnector cannot have tables with same table names in different\r\nschemas.\r\n\r\nThis is because originally BlackHoleConnector only supports `default`\r\nschema so it use the table name string to identify table. When creating\r\nschema was supported in 4e522f355ed7f0711d877e2259e489a8ebf626b9 (related PR is https://github.com/prestodb/presto/pull/9702), the\r\ntable identity should be updated to use SchemaTableName.\r", "NaN"], ["12458", "Remove unused TypeDeserializer in MemoryModule", "Wenlei Xie", "wenleix", "03/26/19, 06:55:57 PM", "NaN", "NaN"], ["12460", "Add a unit test for joins", "Timothy Meehan", "tdcmeehan", "03/14/19, 05:54:17 AM", "This currently hits the hard-coded condition in [AriaHash.java#L98-L105](https://github.com/prestodb/presto/blob/aria-research/presto-main/src/main/java/com/facebook/presto/operator/AriaHash.java#L98-L105)\r\n\r\nCC @mbasmanova @elonazoulay @yingsu00 @oerling ", "NaN"], ["12461", "Update testing-mysql-server to 8.0.12-2", "Timothy Meehan", "tdcmeehan", "03/28/19, 10:52:21 PM", "CC @mayankgarg1990 @elonazoulay ", "NaN"], ["12462", "Fix end of scan range in LongInputStreamV2", null, "oerling", "03/13/19, 05:52:32 PM", "NaN", "NaN"], ["12463", "Minor refactors in HiveClientModule and MetastoreUtil", "Shixuan Fan", "shixuan-fan", "04/15/19, 10:31:05 PM", "This is a follow up PR to address additional comments in #12386 after merging.", "NaN"], ["12464", "Introduce Temporary Table SPI", "Andrii Rosa", "arhimondr", "03/29/19, 07:42:08 PM", "First part of the #12387", "NaN"], ["12470", "Minor edits to comments", "Wenlei Xie", "wenleix", "03/30/19, 05:40:17 AM", "NaN", "NaN"], ["12471", "Format SQL statements", "Ying", "yingsu00", "03/20/19, 04:49:13 AM", "Orri used these scripts in his principia document. Formatting the scripts for a better reading experience.", "NaN"], ["12472", "Switch ImplementationDependency to use FunctionHandle", "Rongrong Zhong", "rongrong", "03/27/19, 01:00:03 AM", "On top of #12444. So only the last 4 commits are new.", "NaN"], ["12479", "Fix Aria scan issues uncovered by TestHiveIntegrationSmokeTest", "Maria Basmanova", "mbasmanova", "03/20/19, 04:57:53 PM", "These changes fix 26 tests in TestHiveIntegrationSmokeTest. There are now 5 remaining failures. TestHiveDistributedQueries continues to pass except for testInsert which is still failing.\r\n\r\nFixed tests:\r\n\r\n```\r\nTestHiveIntegrationSmokeTest.testAnalyzePartitionedTable\r\nTestHiveIntegrationSmokeTest.testBucketHiddenColumn\r\nTestHiveIntegrationSmokeTest.testCreateAndInsert\r\nTestHiveIntegrationSmokeTest.testCreatePartitionedBucketedTableAs\r\nTestHiveIntegrationSmokeTest.testCreatePartitionedBucketedTableAsFewRows\r\nTestHiveIntegrationSmokeTest.testCreatePartitionedBucketedTableAsWithUnionAll\r\nTestHiveIntegrationSmokeTest.testCreatePartitionedTable\r\nTestHiveIntegrationSmokeTest.testCreatePartitionedTableAs\r\nTestHiveIntegrationSmokeTest.testDeleteAndInsert\r\nTestHiveIntegrationSmokeTest.testDropColumn\r\nTestHiveIntegrationSmokeTest.testGroupedExecution\r\nTestHiveIntegrationSmokeTest.testIOExplain\r\nTestHiveIntegrationSmokeTest.testInsert\r\nTestHiveIntegrationSmokeTest.testInsertPartitionedBucketedTable\r\nTestHiveIntegrationSmokeTest.testInsertPartitionedBucketedTableFewRows\r\nTestHiveIntegrationSmokeTest.testInsertPartitionedBucketedTableWithUnionAll\r\nTestHiveIntegrationSmokeTest.testInsertPartitionedTable\r\nTestHiveIntegrationSmokeTest.testInsertPartitionedTableExistingPartition\r\nTestHiveIntegrationSmokeTest.testInsertPartitionedTableOverwriteExistingPartition\r\nTestHiveIntegrationSmokeTest.testMetadataDelete\r\nTestHiveIntegrationSmokeTest.testNullPartitionValues\r\nTestHiveIntegrationSmokeTest.testPartitionPerScanLimit\r\nTestHiveIntegrationSmokeTest.testPathHiddenColumn\r\nTestHiveIntegrationSmokeTest.testPredicatePushDownToTableScan\r\nTestHiveIntegrationSmokeTest.testRenameColumn\r\nTestHiveIntegrationSmokeTest.testScaleWriters\r\n```\r\n\r\nFailing tests:\r\n\r\n```\r\nTestHiveIntegrationSmokeTest.testArrays\r\nTestHiveIntegrationSmokeTest.testComplex\r\nTestHiveIntegrationSmokeTest.testMaps\r\nTestHiveIntegrationSmokeTest.testPrunePartitionFailure\r\nTestHiveIntegrationSmokeTest.testTemporalArrays\r\n```", "NaN"], ["12480", "Enable reading old partitions after bucket changes", "Rebecca Schlussel", "rschlussel", "03/28/19, 09:16:46 PM", "Creating PR to run product tests.  I'll update once tests pass and it's ready for review.", "NaN"], ["12483", "Replace ValuesNode::Expression with RowExpression", "James Sun", "highker", "04/04/19, 12:03:01 AM", "- Rebased on #12472, #12505, #12490", "NaN"], ["12487", "Fix DictionaryBlock#getSizeInBytes", "Maria Basmanova", "mbasmanova", "03/19/19, 03:36:39 PM", "Reverted Aria performance improvement in DictionaryBlock#getSizeInBytes to allow tests in presto-main to pass. This performance improvement will be re-introduced properly in #12399", "NaN"], ["12488", "Support removing remote source from ExchangeClient", "Wenlei Xie", "wenleix", "04/08/19, 10:29:33 PM", "Required by recoverable grouped execution (https://github.com/prestodb/presto/issues/12124) ", "NaN"], ["12489", "Improve Presto Verifier", "Leiqing Cai", "caithagoras", "03/28/19, 08:16:53 PM", "Improve the existing verifier.\r\n\r\n### Behavior differences\r\n- Fix the issue where result-mismatched insert queries are not reran for non-determinism check.\r\n- Fix the issue where checksum queries are treated as main queries while main queries are treated as part of the setup queries, leading to various mis-information on the UIs.\r\n- Select is rewritten into `CreateTableAsSelect` and checksum queries are used for result comparison. This removes the row count limit for Select queries verification.\r\n- Floating point columns are compared using relative errors.\r\n- Orderable array columns are applied with `array_sort` before `checksum`.\r\n- Verification happens along with rewrite, to avoid the super-long wait for the rewrite to complete before verification even starts.\r\n- Query timeout are now enforced by session properties instead of by local timer.\r\n- Add retries for query failures due to network errors and presto retryable errors.\r\n- Add configuration properties `metadataTimeout` and `checksumTimeout`, the timeout duration for different Presto queries.\r\n- Add configuration property `human-readable.log-file` to allow human readable events to be logged to file. Streamline human readable event logging.\r\n- Add integration smoke test.\r\n- Remove a bunch of unnecessary configuration properties.\r\n- Make verifier easier to configure for community users.\r\n\r\n### Changes to output event\r\n  - Add `EventStatus status`, to indicate whether is verification is succeeded, failed, skipped or failed but auto-resolved.\r\n  - Add `deterministic` to indicate whether result mismatch is due to non-determinism.\r\n  - Add `originalControlQuery` and `originalTestQuery`, the control and test queries before the rewrite.\r\n  - Add `controlChecksumQuery` and `testChecksumQuery`.\r\n  - Add `errorCode`.\r\n  - Add `mismatchedColumns`, a mapping from mismatched column names to their types.\r\n  - Remove `source`.", "NaN"], ["12490", "Move row expression to spi", "Yi He", "hellium01", "03/27/19, 06:56:43 PM", "This is part of #12375 and on top of #12472", "NaN"], ["12496", "Fixed typo in resource-groups.rst example", "Brandon Poole", "Bcpoole", "03/27/19, 09:38:49 PM", "NaN", "NaN"], ["12504", "Add CURRENT_ROLE to the list of non reserved keywords", "Andrii Rosa", "arhimondr", "03/20/19, 09:29:14 PM", "The SQL standard allows queries like:\r\n\r\n```\r\npostgres=# SELECT CURRENT_ROLE;\r\n current_role\r\n--------------\r\n postgres\r\n(1 row)\r\n```\r\n\r\nThat's why the `CURRENT_ROLE` is a reserved keyword in the SQL standard.\r\n\r\nCurrently Presto supports the `CURRENT_ROLE` keyword only in\r\n`CREATE ROLE`, `GRANT` and `REVOKE` statements. So it is possible to\r\nparse the `CURRENT_ROLE` token as a keyword in those statements, while\r\nparsing the same token as an identifier in any other queries.\r\n\r\nHowever this is only a temporary solution, as at some point the scope\r\nof the `CURRENT_ROLE` might be extended to the SELECT queries as well.\r\n\r", "NaN"], ["12505", "Simply RowExpression translation and remove unnecessary analyzeExpressionWithInput", "Yi He", "hellium01", "03/27/19, 09:58:08 PM", "On top of #12490, this is part of effort to clean up planNode. Back ported from Martin's change from prestosql. ", "NaN"], ["12516", "Export JMX stats for more split schedule block reasons", "Wenlei Xie", "wenleix", "03/23/19, 01:09:51 AM", "Export stats for split schedule blocked on\r\nMIXED_SPLIT_QUEUES_FULL_AND_WAITING_FOR_SOURCE or\r\nNO_ACTIVE_DRIVER_GROUP.", "NaN"], ["12517", "Add release notes for 0.218", "Elon Azoulay", "elonazoulay", "03/25/19, 09:31:38 PM", "NaN", "NaN"], ["12519", "Mark split schedule done after lifespan finish execution", "Wenlei Xie", "wenleix", "04/03/19, 07:45:18 PM", "\r\nWith future partial recovery support to grouped execution, finishing\r\nscheduling all known splits no longer means schedule finished, since a\r\nlifespan might fail and the engine will rewind all pre-scheduled splits.\r\n\r\nThus, under the context of grouped execution, the schedule should be\r\nconsidered as completed only after all lifespan finished execution.\r\n\r\nNote this commit removed `noMoreLifespans` interface introduced in\r\n4d56e365e4a726248179b89d3b631b3baf3fe7b2, since the new condition\r\n\"all lifespan finished execution\" is stronger than\r\n\"all lifespan finished schedule\".", "NaN"], ["12520", "Change like_pattern to be a hidden function", "Rongrong Zhong", "rongrong", "03/26/19, 08:26:14 PM", "like_pattern should be a hidden function. Currently this function is visible, and calling it would result in an error:\r\n```\r\npresto:abc> select like_pattern('%abc', '-');\r\nQuery 20190323_010558_14283_hu9zg failed: object.getClass (class io.airlift.joni.Regex) and type.getJavaType (class io.airlift.slice.Slice) do not agree\r\n```", "NaN"], ["12524", "Add Parser warnings", "Elon Azoulay", "elonazoulay", "04/05/19, 05:18:08 PM", "NaN", "NaN"], ["12526", "Make each fragment contain at most 1 TableWriterNode", "Wenlei Xie", "wenleix", "04/08/19, 12:37:19 AM", "To support grouped execution failure recovery, lifespans might be\r\nre-executed and thus write the same data multiple times. Coordinator\r\nwill guaranteed only one execution's output get committed.\r\n\r\nThis guarantee is easy to satisfy if each stage have at most one\r\nTableWriter, as coordinator can simply choose one execution per each\r\n(stage, lifespan) pair. Multiple TableWriter operators in one stage\r\ncomplicates the design of failure recovery without other benefits.\r", "NaN"], ["12527", "Revert \"Export JMX stats for more split schedule block reasons\"", "Wenlei Xie", "wenleix", "03/25/19, 06:05:35 PM", "This reverts commit f30277d7267e9b24ee03389d98921da268563dcd. (PR https://github.com/prestodb/presto/pull/12516) ", "NaN"], ["12528", "Add view_owner column to information_schema.views", "Jiexi Lin", "jessesleeping", "03/28/19, 08:38:12 PM", "NaN", "NaN"], ["12529", "Implement recoverable grouped execution", "Shixuan Fan", "shixuan-fan", "06/06/19, 09:55:09 PM", "NaN", "NaN"], ["12532", "Extract Aria scan from aria-research", "Maria Basmanova", "mbasmanova", "03/26/19, 09:06:51 PM", "NaN", "NaN"], ["12533", "Enable pushdown of complex filters on partition columns", "Maria Basmanova", "mbasmanova", "03/26/19, 10:51:22 PM", "Supersedes #12501", "NaN"], ["12534", "Initial refactoring of pushdown subscripts rule", "Maria Basmanova", "mbasmanova", "04/03/19, 02:22:55 AM", "- Extracted pushdown subscripts logic into its own rule, PushDownSubscripts,\r\nfrom PruneUnreferencedOutputs.\r\n- Replaced Subfield.PathElement with a hierarchy of classes: empty base class\r\nPathElement + NestedField, IntegerSubscript, StringSubscript sub-classes.\r\nNestedField represents subfield of a struct: c.a, IntegerSubscript - element\r\nof an array or map with integer keys: c[2], StringSubscript - element of a\r\nmap with varchar keys c[\"a\"].\r\n\r\nSupersedes #12515.", "NaN"], ["12540", "Add json plan to QueryCompletedEvent", "Rebecca Schlussel", "rschlussel", "04/03/19, 06:28:13 PM", "Useful for doing analysis on query plans for queries that have finished.", "NaN"], ["12541", "Remove unnecessary resolveFunction call", "Rongrong Zhong", "rongrong", "03/28/19, 05:09:31 PM", "NaN", "NaN"], ["12542", "Delete InternalSignatureUtils", "cem cayiroglu", "cemcayiroglu", "03/28/19, 03:33:21 AM", "NaN", "NaN"], ["12543", "Adding Code of Conduct file", "Facebook Community Bot", "facebook-github-bot", "03/28/19, 04:34:57 AM", "This is pull request was created automatically because we noticed your project was missing a Code of Conduct file.\n\nCode of Conduct files facilitate respectful and constructive communities by establishing expected behaviors for project contributors.\n\nThis PR was crafted with love by Facebook's Open Source Team.", "NaN"], ["12544", "Improve Raptor bucket recovery/reassignment logic", "Jiexi Lin", "jessesleeping", "03/29/19, 05:23:21 PM", "- Trigger bucket rebalancer after cluster restart. We still wait for a grace period to make sure all workers are back online. \r\n- Add minimum alive worker check when reassigning buckets. The limit is configurable and the default is 0.\r\n\r\nThe purpose of the minimum worker count check is to prevent unnecessary recovery when too many nodes go down and the rest of the cluster is not able to hold the data. Under such situation, we may rather fail the cluster than wasting resources recovering. \r\n\r\nNote that this check should be view as a safeguard and the configured value is prefer to be as low as possible. ", "NaN"], ["12545", "Move FunctionRegistry to StaticFunctionNamespace", "cem cayiroglu", "cemcayiroglu", "04/05/19, 11:49:56 PM", "NaN", "NaN"], ["12549", "Fix Hive table read within transaction", "Wenlei Xie", "wenleix", "03/28/19, 11:43:26 PM", "Previously reading a new created Hive unpartitioned table within the\r\ntransaction would fail when WriteMode is\r\nSTAGE_AND_MOVE_TO_TARGET_DIRECTORY.\r\n\r\nNew created partitions are augmented to prevent this. The similar fix is\r\napplied to new created unpartitioned table in this commit.", "NaN"], ["12550", "Refine Parquet schema mismatch message", "Zhenxiao Luo", "zhenxiao", "08/30/19, 12:11:58 AM", "@nezihyigitbasi ", "NaN"], ["12552", "Add ST_Length(SphericalGeography)", "Sean Barker", "seanbarker", "04/03/19, 01:42:03 AM", "Adds `ST_Length(SphericalGeography)` which computes the length of a `LINESTRING` on the surface of a spherical model of Earth's surface, i.e. the sum of great-circle distances between adjacent points along the linestring. ", "NaN"], ["12553", "Fix sporadic test failures in presto-hive modules", "Andrii Rosa", "arhimondr", "03/29/19, 05:09:04 PM", "Tests from TestHiveIntegrationSmokeTest are run simultaneously in\r\nmultiple threads. Thus the table names used in tests must be unique.", "NaN"], ["12554", "Add configuration file for stale bot", "Rebecca Schlussel", "rschlussel", "04/02/19, 07:53:05 PM", "https://github.com/probot/stale", "NaN"], ["12555", "Remove fixed width block ", "Timothy Meehan", "tdcmeehan", "04/04/19, 04:24:51 PM", "Cherry pick from prestosql.\r\n\r\nFixes #12547", "NaN"], ["12557", "Detach AST frame info from WindowNode", "James Sun", "highker", "04/01/19, 05:55:23 PM", "NaN", "NaN"], ["12559", "Add support for string dictionary encoding with row group level dictionaries", null, "oerling", "04/24/19, 11:33:56 PM", "NaN", "NaN"], ["12560", "Replace WindowNode::FunctionCall with CallExpression", "James Sun", "highker", "05/01/19, 03:02:05 AM", "No dependency", "NaN"], ["12564", "Scan support for LongDictionaryStreamReader", null, "oerling", "04/25/19, 04:20:00 PM", "Moves common functions between direct and dictionary long stream readers to a common superclass.", "NaN"], ["12565", "Backport \"Fix failure when resolving fully qualified column from view\"", "Rebecca Schlussel", "rschlussel", "04/01/19, 07:05:27 PM", "The relation type was being constructed based on the view name without catalog/schema\r\nqualifiers.", "NaN"], ["12567", "Enforce scan batch by exception on exceeding space budget", null, "oerling", "04/12/19, 05:57:50 PM", "OrcRecordReader.getNextPage tracks the average bytes per rows of\ninitial qualifying set. The batch size is adapted to stay within\nexpected soft limits. If a hard limit is exceeded an exception is\nthrown and the streams are reset to the beginning of the current row\ngroup. This behavior only works when the streams are checkpointed,\nhence ValueStreamSources are not used even if the stripe consisted of\na single row group.\n\nThis replaces the StreamReader result truncation mechanism.", "NaN"], ["12568", "Support materializing exchanges [Planner Part]", "Andrii Rosa", "arhimondr", "04/02/19, 11:35:43 PM", "First part of https://github.com/prestodb/presto/pull/12469; Part of https://github.com/prestodb/presto/issues/12387", "NaN"], ["12570", "Function metadata", "Rongrong Zhong", "rongrong", "04/10/19, 09:00:44 PM", "Introduce `FunctionMetadata` and use `FunctionMetadata` during planning. `FunctionMetadata` includes:\r\n* Name\r\n* ArgumentTypes\r\n* ReturnType\r\n* FunctionKind\r\n* Deterministic\r\n* CalledOnNullInput (call convention)", "NaN"], ["12571", "Pushdown subscript paths into project on top of scan", "Maria Basmanova", "mbasmanova", "04/09/19, 04:37:40 PM", "NaN", "NaN"], ["12572", "Add support for filters that depend on no columns or on constant columns", null, "oerling", "05/04/19, 02:48:58 AM", " ", "NaN"], ["12574", "Revert optimization of full outer join + coalesce", "Rongrong Zhong", "rongrong", "04/02/19, 07:50:44 AM", "NaN", "NaN"], ["12575", "Fix ConnectorManager dropConnection leaving AnalyzeProperty entry behind", "David Lao", "davidlao2k", "04/02/19, 06:26:26 PM", "NaN", "NaN"], ["12578", "Replace JoinNode::Expression with RowExpression", "James Sun", "highker", "05/09/19, 04:10:18 AM", "No denpendency", "NaN"], ["12582", "Simplify DistributedExecutionPlanner to SplitSourceFactory", "Andrii Rosa", "arhimondr", "04/05/19, 01:05:16 PM", "Historically `DistributedExecutionPlanner` is responsible for deriving certain execution-related properties, such as partitioning (for example, see the file 7 years ago: https://github.com/prestodb/presto/blob/ff623e753bc1aadf6945e1a076f7a62174090b8b/presto-main/src/main/java/com/facebook/presto/sql/planner/DistributedExecutionPlanner.java) . As time evolves, these responsibility are moved out (e.g. partitioning reasoning are moved to `PlanFragment`), and now `DistributedExecutionPlanner` is only used to creates split sources -- which is also the only difference between `SubPlan` and `StageExecutionPlan`. \r\n\r\n----------------------------------------\r\n\r\nTo support stage execution retry, there are basically two ways:\r\n1. Recreate the `SqlStageExeuction` for failed stages\r\n2. Reuse the existing `SqlStageExeuction` and add some kind of `restart` mechanism. \r\n\r\nGiven `SqlStageExeuction` is quite stateful (e.g. node map, exchange source, output buffer), adding `restart` is error prone for cleaning the state. Recreating the `SqlStageExeuction` is easier to reason the correctness.  This requires the creation of split sources should be deferred to creation of `SqlStageExeuction`, instead of creating at the `DistributedExecutionPlanner` time. This is the motivation for this refactor. \r\n\r\n----------------------------------------\r\n\r\nThis is also an pre-requisite to lazily initialize split source for temporary table, as required by https://github.com/prestodb/presto/issues/12387\r", "NaN"], ["12584", "Update deprecated use of Files.toString", "Nezih Yigitbasi", "nezihyigitbasi", "04/05/19, 09:56:00 PM", "NaN", "NaN"], ["12587", "Fix flakiness of TestJdbcClient.testMetadata", "Nezih Yigitbasi", "nezihyigitbasi", "04/05/19, 04:54:08 AM", "We see that this test occasionally fails with the stack below. Some unit\r\ntests in TestJdbcClient create tables in the `example` schema while the\r\nfailing test enumarates the tables in the same schema, and asserts on\r\nthe number of tables. Depending on how these runs are interleaved it's\r\npossible to get more tables than expected.\r\n\r\nThis changes fixes that by making sure that tables are created in a\r\ndifferent schema.\r\n\r\n```\r\njava.lang.AssertionError: lists don't have the same size expected [3] but found [4]\r\n\tat org.testng.Assert.fail(Assert.java:94)\r\n\tat org.testng.Assert.failNotEquals(Assert.java:513)\r\n\tat org.testng.Assert.assertEqualsImpl(Assert.java:135)\r\n\tat org.testng.Assert.assertEquals(Assert.java:116)\r\n\tat org.testng.Assert.assertEquals(Assert.java:389)\r\n\tat org.testng.Assert.assertEquals(Assert.java:556)\r\n\tat org.testng.Assert.assertEquals(Assert.java:533)\r\n\tat com.facebook.presto.plugin.jdbc.TestJdbcClient.testMetadata(TestJdbcClient.java:79)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:104)\r\n\tat org.testng.internal.Invoker.invokeMethod(Invoker.java:645)\r\n\tat org.testng.internal.Invoker.invokeTestMethod(Invoker.java:851)\r\n\tat org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1177)\r\n\tat org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:129)\r\n\tat org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:112)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\nAnother way to solve this problem is to run the test as single threaded, but I didn't want to run these tests serially as our tests already run for a long time.", "NaN"], ["12590", "Replace ProjectNode::Expression with RowExpression", "cem cayiroglu", "cemcayiroglu", "04/05/19, 04:47:26 AM", "NaN", "NaN"], ["12591", "Make Arrayblock, MapBlock, RowBlock support mayHaveNull", "Ying", "yingsu00", "04/10/19, 02:47:00 AM", "Currently the serialization/deserialization code always encode\r\n/ decode every null values in Arrayblock, MapBlock, RowBlock, \r\neven when there're no null vectors in these blocks. This can be\r\nseen in  encodeNullsAsBits() and decodeNullBits() in EncoderUtil\r\nclass. This is because these blocks don't support the Block.mayHaveNull()\r\nfunction and the default implementation always return true. By \r\nmaking these blocks support the Block.mayHaveNull() check,\r\nthe code would follow the early return route in encodeNullsAsBits() \r\nand decodeNullBits() and do not encode the null value for every position.", "NaN"], ["12592", "Fix BenchmarkPartitionedOutputOperator", "Ying", "yingsu00", "04/05/19, 03:24:57 AM", "Before this commit there was this error running this benchmark:\r\nException in thread \"main\" java.lang.IllegalStateException:\r\nlifespanCompletionCallback must be set before enqueueing data", "NaN"], ["12593", "Refactoring out Aria join from LookupJoinOperator", "Timothy Meehan", "tdcmeehan", "04/12/19, 11:07:08 PM", "Proper naming and interfaces are not yet established.  But this serves as a baseline for development against master by separating out the aria path from lookup join path.  Afterward, there should be an analogous WIP PR against master, where we'll incrementally establish interfaces and work for basic types other than the ones hardcoded in this PR.", "NaN"], ["12594", "Add optimized ORC writer to Raptor", "James Sun", "highker", "04/18/19, 11:49:05 PM", "File writer part. File rewriter will come in another PR.", "NaN"], ["12595", "Remove distributed planning time", "Andrii Rosa", "arhimondr", "04/08/19, 07:29:32 PM", "This metric is deceptive as it doesn't measure the time spent on\r\ndistributed planning. Instead it measures the time spent on opening\r\nconnector split sources.\r\n\r\nMeasuring the time spent on opening split sources is not that useful.\r\nUsually connectors enumerate splits lazily and perform only very basic\r\noperations at the moment of opening the SplitSource itself. The operations\r\nperformed at that step are very connector specific. That makes this metric\r\nnot so useful as an engine level metric.", "NaN"], ["12596", "Clarify the docs for numeric_histogram", "James Gill", "jagill", "05/24/19, 03:19:30 AM", "From the description of numeric_histogram, it was unclear what the\r\nkeys and values of the returned map were.  This clarifies that\r\nthe keys are the centers of the bin, and the entries are the\r\ncounts/total weights of the bin.\r", "NaN"], ["12599", "Fix joni regex not matching some word boundaries", "Rebecca Schlussel", "rschlussel", "04/05/19, 10:28:52 PM", "Update Joni library to 2.1.5.3 to fix a regression where regular\r\nexpressions with word boundaries were not matching boundaries at the end\r\nof the line.\r\n\r\n```\r\nExample:\r\n\r\nWithout this fix:\r\npresto> select regexp_like('test', 'test\\b');\r\n _col0\r\n-------\r\n false\r\n(1 row)\r\n\r\nWith this fix:\r\npresto> select regexp_like('test', 'test\\b');\r\n _col0\r\n-------\r\n true\r\n(1 row)\r\n```\r\n\r\nFixes #12581 ", "NaN"], ["12601", "Fix TestWarnings", "Elon Azoulay", "elonazoulay", "04/06/19, 06:39:29 AM", "NaN", "NaN"], ["12604", "Support materializing exchanges [Execution Part]", "Andrii Rosa", "arhimondr", "04/17/19, 08:57:39 PM", "NaN", "NaN"], ["12606", "Change Symbol to VariableReferenceExpression in PlanNode", "Rongrong Zhong", "rongrong", "06/11/19, 04:20:18 AM", "Step 1: Add `outputVariables` to all `PlanNode` while keeping `outputSymbols`. No logic change.\r\nStep 2: Start using `outputVariables` instead of `outputSymbols` in planning / optimization.\r\nStep 3: Remove `outputSymbols`.\r\n\r\nThis will take a while. I might or might not get it done eventually... -_- ", "NaN"], ["12607", "Clean up unused variable in BenchmarkArrayAggregation", "Ying", "yingsu00", "04/09/19, 09:30:57 AM", "NaN", "NaN"], ["12609", "Export JMX stats for more split schedule block reasons", "Wenlei Xie", "wenleix", "04/11/19, 04:31:27 AM", "Export stats for split schedule blocked on\r\nMIXED_SPLIT_QUEUES_FULL_AND_WAITING_FOR_SOURCE or\r\nNO_ACTIVE_DRIVER_GROUP.\r\n\r\nThis re-commits f30277d7267e9b24ee03389d98921da268563dcd (PR https://github.com/prestodb/presto/pull/12516), since it\r\nget reverted in 3144d703b248ba06be19ad58919fc5c7b3f4a790 (PR https://github.com/prestodb/presto/pull/12527)  due to flaky\r\ntests that block release.", "NaN"], ["12610", "Remove unnecessary type parameter in HiveSplitSource#PerBucket", "Wenlei Xie", "wenleix", "04/15/19, 06:37:25 PM", "NaN", "NaN"], ["12611", "Support partial merge pushdown ", "Wenlei Xie", "wenleix", "04/29/19, 08:11:54 PM", "## Introduction \r\n\r\nGrouped execution was introduced to Presto in https://github.com/prestodb/presto/pull/8951 to support huge join and aggregation raised in ETL pipelines. \r\n\r\nA large scale warehouse often optimize table layout by pre-bucketing tables on frequently-used join/aggregation keys to avoid the cost of data re-partitioning. However, tables often have different (yet compatible) number of buckets. \r\n\r\nFor example, an extremely huge table A might have 8192 buckets, while another large table have 1024 buckets. A full repartitioning for queries like A JOIN B is undesired because:\r\n\r\n- In-memory repartitioning loses the benefit of grouped execution and the query may exceeded maximum distributed memory allowed.\r\n- Materialized exchange (https://github.com/prestodb/presto/issues/12387) will help run the query within memory limit, however it will be very inefficient in terms of CPU and IO. \r\n\r\nhttps://github.com/prestodb/presto/pull/11749 is the first work  try to optimize this problem, and lay some fundamental building blocks to this problem. However, in practice we found the following issue impedes it from a general useful solution: (e.g. we have to ask users to opt-in, instead of enable it by default). \r\n\r\n- It couples partitioning selection and partial merge pushdown. So it can change how partitioning is chosen and  cause query to fail. For example:\r\n\r\n```\r\n          AGGR\r\n            |\r\n          JOIN\r\n         /     \\\r\n TS(2048)    TS(1024)\r\n```\r\n\r\n`AddExchange` always prefer to use left side partitioning if exists. In this case, it will cause less number of lifespan for aggregation and increase the global memory usage. \r\n\r\n\r\n\r\n- The common partitioning selection logic is opaque to engine. Thus engine cannot reason about whether this partial merge pushdown is safe or not, for example:\r\n\r\n```\r\n          JOIN\r\n         /     \\\r\n TS(1024)    AGGR(2048)\r\n                    \\\r\n                   TS(2048)\r\n```\r\n\r\nPush partial merge to the right TS can cause OOM. However, since engine doesn't know whether the common partitioning returned by Connector is a refinement or coarsening, it cannot reason which partitioning to use. \r\n\r\n## Partial Repartitioning  and Partial Merge\r\n\r\nThis PR explores an alternative attempt based on the supports built in https://github.com/prestodb/presto/pull/11749 . The idea is to efficiently support these partitioning adaptions, we need the two \"partial repartitioning\" and \"partial merge\"  exchange types introduced in SCOPE paper ([1]):  \r\n\r\n<img width=\"952\" alt=\"Screen Shot 2019-04-07 at 8 45 10 PM\" src=\"https://user-images.githubusercontent.com/799346/55697471-68c0f580-5976-11e9-92a5-aba2510e2b29.png\">\r\n\r\nConsider the same example above `A (8192) JOIN B (1024)`.  Instead of do a full re-partitioning over A or B, we can do a \"partial merge\" over A into 1024 buckets (we can also do \"partial re-partitioning\" over B, but this is beyond the scope of this PR).   \r\n\r\n\r\n## Implementation\r\n\r\nWe are going to discuss about implementation of Partial Repartitioning  and Partial Merge in the context of materialized exchange. In memory exchange would be more interesting to discuss. \r\n\r\n### Partial Repartitioning\r\n\r\nPartial repartitioning implementations is more straightforward. Since it's always going to be like this in the materialized exchange context:\r\n\r\n```\r\n      TableWriter\r\n            |\r\n      Exchange (partial repartitinoing)\r\n            |\r\n      ...............\r\n``` \r\n\r\nInstead of a remote exchange, the `ExchangeNode` can be simply expanded to a local exchange operator.  It can even be pushed into `TableWriter` if parallelism is not a concern.\r\n\r\n\r\n### Partial Merge\r\n\r\nPartial merge is more interesting since it can be handled in different ways.  For the convenience of discussion, we assuming we are trying to do partial merge from 4096 buckets to 1024 buckets. \r\n\r\n\r\n- Directly supporting it in Presto engine requires a bit more work, potential solutions:\r\n    * We can use a local exchange operator to \"merge\" the data produced from different lifespans, but the engine needs to intelligently schedule lifespans to be coalesced in the same time and on the same nodes. \r\n    * We can allow partial writes to a bucket (e.g. doing a partial merge from 4096 buckets to 1024 buckets, one lifespan can finish but the target output bucket is only 25% finished)\r\n\r\n```\r\n      TableWriter\r\n            |\r\n      Exchange (partial merge)\r\n            |\r\n      ...............\r\n``` \r\n\r\n\r\n- It can be fully eliminated by pushing down partial merge to the table scan. Note pushing through join/aggregation has the risk to increase memory usage.\r\n\r\n- If it cannot be pushed down, another way would be doing \"partial merge\" in a lazy fashion. i.e. we still writes to 4096 buckets, but in the next table scan, it reads out as 1024 buckets. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNote partial merge can be conveniently pushdown to Connector by asking an \"alternative TableLayout\". \r\n\r\n[1]: http://www.cs.columbia.edu/~jrzhou/pub/Scope-VLDBJ.pdf\r\n\r\n\r\n\r\n\r\n\r\n## Misc\r\n\r\nThe idea here is if exchange materialization is considered as the primary way to support large queries; the problem is much simplified, since all partitioning will remain the same during query execution (same number of buckets as specified by `num_hash_partitions`). \r\n\r\nIn this world, two major primitives (besides full repartitioning) are \"partial merge\" and \"partial repartitioning\" which used at beginning to adapt query specified partition count. And they can be modeled as \"partial merge pushdown\" and \"partial repartitioning pushup\".  \r\n\r\nCompatible partitioning is supported as a by-product. \r\n\r\n## Open Questions\r\n\r\nHow do Hive/Spark handles \"compatible buckets\"? \r\n\r", "NaN"], ["12612", "Allow Raptor to compress data with ZSTD", "James Sun", "highker", "04/23/19, 07:53:41 PM", "On top of #12594 ", "NaN"], ["12614", "Replace FileStream with Files.newStream", "James Petty", "pettyjamesm", "04/10/19, 09:49:05 PM", "Use Files.new{Input,Output}Stream instead of File{Input,Output}Stream where possible. FileStream classes use a finalizer to ensure the backing descriptor is closed which can lead to longer GC pause cycles when the streams are closed correctly. See [HDFS-8562](https://issues.apache.org/jira/browse/HDFS-8562) or [JENKINS-42934](https://issues.jenkins-ci.org/browse/JENKINS-42934) for instances of other projects making this change.\r\n\r\nThis is no longer an issue as of JDK 12, which [removes the finalizer](https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8213888). The usages changed in this PR can operate on any {Input,Output}Stream implementation and therefore straightforward to replace.\r\n\r", "NaN"], ["12615", "Add more RowExpression Util", "James Sun", "highker", "04/08/19, 06:09:58 PM", "Move reviewed commits from #12523 as a separate PR. The commits have already been reviewed in #12523", "NaN"], ["12616", "Backport \"Cleanup uses of node apis\"", "Wenlei Xie", "wenleix", "04/18/19, 03:36:37 AM", "Extracted-From: https://github.com/prestosql/presto/pull/578\r\n\r", "NaN"], ["12617", "Add Encryption and Compression for Local File Spills", "James Petty", "pettyjamesm", "05/14/19, 02:11:23 PM", "This PR supersedes https://github.com/prestodb/presto/pull/8801, which originally implemented spill encryption using bouncy-castle. This PR implements the the encrypt / decrypt operations using the platform-provided JCE implementation which has much better performance on most JDK implementations thanks to AES-NI intrinsics.\r\n\r\nThe behavior is controlled via `FeaturesConfig(\"experimental.spill-encryption-enabled\")` (default: off). When enabled, spill data is encrypted with a randomly generated key per `FileSingleStreamSpiller` and destroyed by `FileSingleStreamSpiller#close`.", "NaN"], ["12621", "Improve error messages in beta_cdf and inverse_beta_cdf math functions.", "Valery Yundin", "Vayu", "04/09/19, 06:54:06 PM", "This is follow-up to #11981 with improved error messages", "NaN"], ["12622", "Revert \"Make each fragment contain at most 1 TableWriterNode\"", "Wenlei Xie", "wenleix", "04/09/19, 06:33:52 PM", "This reverts commit 68642b5ba7137a2017fc2eebd7c3de66dafc74ea.\r\n\r\nIt causes regression for certain queries. In such queries, children\r\nnodes of UnionNode might not only contain TableWriterNode.\r", "NaN"], ["12624", "Add a warning to Release 0.218 about FULL OUTER JOIN regression", "Rongrong Zhong", "rongrong", "04/10/19, 05:51:29 PM", "NaN", "NaN"], ["12628", "Replace SpatialJoinNode::Expression with RowExpression", "James Sun", "highker", "05/09/19, 05:28:37 PM", "On top of #12578", "NaN"], ["12631", "Improve stale bot message", null, "mayankgarg1990", "04/11/19, 01:16:34 AM", "NaN", "NaN"], ["12634", "Don't choose right inequality joins with CBO", "Rebecca Schlussel", "rschlussel", "05/03/19, 04:23:06 PM", "Even though sometimes right inequality joins will use fewer resources,\r\nthey execute on a single node so they can have very long wall times due\r\nto a lack of parallelism for what is essentially a cross join.  Instead\r\nonly have cbo consider left outer joins, since the build side can be\r\nreplicated.  If the build table exceeds the max broadcast size, it will\r\nfall back to using the syntactic join order.", "NaN"], ["12635", "Add warnings field to coordinator v1/query endpoint", "Varun Gajjala", "varungajjala", "04/12/19, 06:06:18 PM", "We want to add warnings field to the v1/query endpoint so that its easy to retrieve all the warnings for the queries running/finished on coordinator. It will also enable us to show the warnings on the main coordinator UI page and have a sort by warning or similar thing.", "NaN"], ["12637", "Add OperatorType to FunctionMetadata", "James Sun", "highker", "04/15/19, 06:13:08 PM", "To resolve https://github.com/prestodb/presto/pull/12523#discussion_r271966647", "NaN"], ["12639", "Don't push limit through full outer joins", "Rebecca Schlussel", "rschlussel", "04/11/19, 04:58:06 PM", "Fixes a wrong results issue with limits and full outer joins.\r\nBecause full outer joins return all rows from both tables, if you push\r\ndown a limit you can get incorrect nulls in your output.\r\n\r\nExample:\r\n```sql\r\nSELECT * FROM\r\n        (VALUES (1),(2),(3)\r\n    ) AS a(id)\r\nFULL OUTER JOIN (\r\n        VALUES (2),(3), (1)\r\n    ) AS b(id)\r\n    ON a.id = b.id\r\nLIMIT\r\n    1;\r\n```\r\n\r\nFixes #12638 ", "NaN"], ["12641", "Don't push limit through full outer joins", "Rebecca Schlussel", "rschlussel", "04/11/19, 07:03:01 PM", "Fixes a wrong results issue with limits and full outer joins.\r\nBecause full outer joins return all rows from both tables, if you push\r\ndown a limit you can get incorrect nulls in your output.\r\n\r\nExample:\r\n```sql\r\nSELECT * FROM\r\n        (VALUES (1),(2),(3)\r\n    ) AS a(id)\r\nFULL OUTER JOIN (\r\n        VALUES (2),(3), (1)\r\n    ) AS b(id)\r\n    ON a.id = b.id\r\nLIMIT\r\n    1;\r\n```\r\n\r\nThis was already approved in #12639.  this pr is just to run tests on the release branch.", "NaN"], ["12643", "Rename variables in SqlTaskExecution", "Wenlei Xie", "wenleix", "04/16/19, 05:49:19 AM", "Partitioned source refers to table scan source since a dedicated driver\r\nis created per each split.\r\n\r\nUnpartitioned source refers to remote source since they share a fix\r\nset of drivers.\r\n\r\nThis commit doesn't change the name for partitioned/unpartitioned\r\nsplit.", "NaN"], ["12644", "Run rewrite, describe and checksum queries on control cluster", "Leiqing Cai", "caithagoras", "04/16/19, 06:07:48 PM", "NaN", "NaN"], ["12645", "Use Builder to construct ParsingOptions in Verifier", "Leiqing Cai", "caithagoras", "04/15/19, 05:21:42 AM", "The one-argument constructor has been depreacted.", "NaN"], ["12647", "Allow to change compression codec with a session property", "Andrii Rosa", "arhimondr", "05/01/19, 02:58:45 PM", "Required by https://github.com/prestodb/presto/issues/12387", "NaN"], ["12650", "Make FunctionHandle an interface", "Rongrong Zhong", "rongrong", "04/17/19, 11:18:02 PM", "Also introducing FunctionNamespace as an interface. Currently only including bare minimum APIs.", "NaN"], ["12651", "Emit checksum query ID in the Verifier output event", "Leiqing Cai", "caithagoras", "04/15/19, 05:25:00 AM", "During the process of debugging row count mismatch, we found it useful to have the verifier log checksum query id.", "NaN"], ["12652", "Auto-resolve EXCEEDED_TIME_LIMIT regardless of CPU time increase", "Leiqing Cai", "caithagoras", "04/18/19, 06:34:58 PM", "Until we're able to identify the cause of huge CPU time difference between control and test queries and to reduce it, we should allow the verifier to automatically resolve those failures to minimize noise.", "NaN"], ["12654", "Encode PlanFragmentId as string in JSON", "Andrii Rosa", "arhimondr", "04/12/19, 06:29:24 PM", "The \"Create stage id based on the plan fragment id\" commit changed the\r\nway the PlanFragmentId is encoded in JSON:\r\n\r\n326f805\r\n\r\nBefore this patch the value had been encoded as a String, and after\r\nthis patch the PlanFragmentId started to be encoded as a number.\r\n\r\nThis breaks the \"Live Plan\" UI, as the \"Live Plan\" controller doesn't\r\nmatch the remote source ids that are still encoded as strings\r\n(see JsonRenderedNode):\r\n\r\nhttps://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/sql/planner/planPrinter/JsonRenderer.java#L53\r\nhttps://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/sql/planner/planPrinter/JsonRenderer.java#L64\r\nhttps://github.com/prestodb/presto/blob/master/presto-main/src/main/resources/webapp/src/components/LivePlan.jsx#L238\r\n\r\nAn alternative solution is to encode JsonRenderedNode#remoteSources\r\nas a number as well. However, since all other ids (PlanNodeId, StageId)\r\nare encoded as a string, it seems to be more convenient to also have\r\nthe PlanFragmentId encoded as a string.", "NaN"], ["12656", " Allow materializing partitioning inferred by join", "Wenlei Xie", "wenleix", "04/24/19, 05:53:06 AM", "A loose ends from https://github.com/prestodb/presto/pull/12568, the first commit are squashed from https://github.com/prestodb/presto/pull/12604. Thus it's part of #12387 :) ", "NaN"], ["12658", "Check partition handle argument in {Raptor, Thrift}SplitSource", "Wenlei Xie", "wenleix", "04/13/19, 02:57:59 AM", "These two connectors doesn't support addressable split group.", "NaN"], ["12663", "Detach SymbolReference from SetOperationNode", "James Sun", "highker", "04/15/19, 02:09:38 PM", "NaN", "NaN"], ["12666", "Detach SampledRelation from SampleNode", "James Sun", "highker", "04/15/19, 02:07:59 PM", "NaN", "NaN"], ["12667", "Cache hive metastore to optimize writing bucketed table", "Vic Zhang", "viczhang861", "04/16/19, 02:24:38 PM", "When write to bucketed table, each bucket will call metastore to get\r\npartition information. Use CachingHiveMetastore to cache partition.\r\n\r\nTested in production using a bucketed table with 1638 buckets,\r\n1.6K metastore calls were made without this diff.", "NaN"], ["12669", "Add support for readPreferenceTags in mongodb", null, "ciscoring", "04/18/19, 07:03:17 PM", "Add support for readPreferenceTags that lead the mongodb connector to read a specific sharded cluster to configuration properties. The properties are split tag sets as a character '&' and specified a tag set as a comma-separated list of colon-separated key-value pairs. For example, mongodb.read-preference-tags=dc:east,use:reporting&use:reporting", "NaN"], ["12670", "Replace FilterNode::Expression with RowExpression", "James Sun", "highker", "04/17/19, 04:59:56 AM", "copied from #12523. Github accidentally closed my PR and disallow me to reopen it.", "NaN"], ["12671", "Add DomainTranslator to ConnectorContext", "James Sun", "highker", "04/22/19, 08:54:06 PM", "On top of #12670 and part of #12650.\r", "NaN"], ["12673", "Fix spatial join on top of colocated right join", "Maria Basmanova", "mbasmanova", "04/17/19, 03:08:11 PM", "Reference counting for the spatial index didn't take into account a possibility\r\nof having multiple SpatialJoinOperatorFactories. This caused the index to be\r\ndeleted before all the probe operators were done processing.\r\n\r\nThe fix is to increment reference count by 1 for each duplicate factory.\r\n\r\nFixes #12672", "NaN"], ["12674", "Remove tableLayoutHandle", "Yi He", "hellium01", "04/30/19, 01:31:20 AM", "This PR is to remove tableLayoutHandle completely from PlanNodes and metadata APIs.  \r\n\r\nFor background and future works, see the following comments:\r\n\r\n- https://github.com/prestodb/presto/pull/12674#issuecomment-486825431\r\n- https://github.com/prestodb/presto/pull/12674#issuecomment-485493020 \r", "NaN"], ["12677", "Add rewind API for SplitSource", "Shixuan Fan", "shixuan-fan", "04/27/19, 12:35:34 AM", "This is a prerequisite API for recoverable grouped execution\r\n\r\nExtracted from https://github.com/prestodb/presto/pull/12529 to avoid blocking other changes on `HiveSplitSource`  . Required by https://github.com/prestodb/presto/issues/12124", "NaN"], ["12678", "Mark unpartitioned table in HiveWrittenPartitions", "Shixuan Fan", "shixuan-fan", "04/17/19, 08:38:15 PM", "We already use \"\\<UNPARTITIONED\\>\" for unpartitioned table input, and\r\nthis commit would unify the representation of unpartitioned table in\r\noutput as well.", "NaN"], ["12680", "Remove the SubType list from JsonTypeInfo on PlanNode", "Yi He", "hellium01", "04/29/19, 08:57:27 PM", "This type list is only for making sure name serialized is same as given\r\nshortened readable names. However, it makes expanding PlanNode types \r\nvery difficult. \r\n\r\nTest seems OK but need to confirm if there is something else outside presto depends on plan json (which shouldn't). ", "NaN"], ["12682", "Simplify table scan node collection in PlanFragment", "Wenlei Xie", "wenleix", "05/23/19, 09:04:20 PM", "PlanFragment has 3 different collections of table scan node:\r\n\r\n- List<PlanNodeId> partitionedSources: represents scheduling order\r\n- Set<PlanNodeId> partitionedSourcesSet: used to quickly tell whether a\r\n    plan node id is a table scan node from HttpRemoteTask.\r\n- Set<PlanNode> partitionedSourceNodes: used in both HttpRemoteTask,\r\n    QueryStateMachine and StageStateMachine. However, the latter two use\r\n    case can be easily eliminated.\r\n\r\nOnly the first collection is required to be in PlanFragment.", "NaN"], ["12684", "Fix flaky test TestExchangeClient#testRemoveRemoteSource", "Wenlei Xie", "wenleix", "04/22/19, 11:26:09 PM", "Resolves https://github.com/prestodb/presto/issues/12676", "NaN"], ["12685", "Remove redundant offset parameter in primitive type getter", "Wenlei Xie", "wenleix", "04/26/19, 08:03:56 PM", "Historically, the offset parameter in block primitive type\r\ngetter method (getByte, getShort, getInt, getLong) are mainly\r\nused for encode/decode complex types before\r\n`InterleavedBlock` is introduced.\r\n\r\nNow we have natural columnar representations for complex types,\r\nthe offset parameter is no longer used except in `LongDecimalType`,\r\nwhere `getLong(position, SIZE_OF_LONG)` is used to retrieve\r\nthe high 64 bit int a 128 bit unscaled decimal value.\r\n\r\nWIP to revive https://github.com/prestodb/presto/pull/9477 . `getLong` requires a different solution. ", "NaN"], ["12687", "Move filter reordering and budget enforcement flags into Hive connector", "Maria Basmanova", "mbasmanova", "04/18/19, 06:04:38 PM", "NaN", "NaN"], ["12688", "Support more intermidiate aggregations for materialized exchanges", "Andrii Rosa", "arhimondr", "04/23/19, 11:25:48 PM", "NaN", "NaN"], ["12690", "Make max error in approx_set tunable", "Ben MR", "blrnw3", "05/13/19, 03:02:19 PM", "Adds a new version: approx_set(x, e)\r\nAlso for empty_approx_set(e)\r\n\r\nThis brings it in line with the approx_distinct functionality, although I kept the existing difference in default max standard error (0.01625 vs 0.023 i.e. 4096 vs 2048 buckets).\r\n\r\nIssue: #12506", "NaN"], ["12691", "Move removeRemoteSource from RemoteTaskFactory to RemoteTask", "Wenlei Xie", "wenleix", "04/23/19, 05:52:18 AM", "Loose ends as discussed in https://github.com/prestodb/presto/pull/12488#discussion_r271532854 ; required by recoverable grouped execution (https://github.com/prestodb/presto/issues/12124)", "NaN"], ["12694", "Store RuntimeExceptions in ErrorSet", "Maria Basmanova", "mbasmanova", "04/26/19, 02:56:57 PM", "NaN", "NaN"], ["12696", "Move StaticFunctionHandle to presto-main", "Rongrong Zhong", "rongrong", "04/19/19, 05:29:48 AM", "Didn't realized I just moved it to `package com.facebook.presto.metadata` in presto-spi...", "NaN"], ["12698", "Use nexus-staging-maven-plugin for publishing artifacts", null, "mayankgarg1990", "04/24/19, 12:06:38 AM", "As a part of the release process, move to using the `nexus-staging-maven-plugin` plugin to publish the maven artifacts to open. This is currently gated by using the `-Pdeploy-to-ossrh` flag.\r\n\r\nThe motivation behind this change is to automate some of the steps while deploying the release artifacts to nexus. The plugin understands nexus repositories, ensures that all packages in a build go to a single nexus staging repository and can also close and release the repo without going to the UI.\r\n\r\nIn order to make this work, I had move `presto-server` from `provisio-maven-plugin` to `maven-assembly-plugin`. `provisio-maven-plugin` moves the lifecycle to `takari-lifecycle-plugin` and that is something that the `nexus-staging-maven-plugin` does not intercept and hence were missed being added to the repository. Clearly the older solution was better than the new one - however - given that we release more often as compared to adding new entries to the tar I feel that this is an acceptable tradeoff.\r\n\r\nLooking for feedback on this approach and if others have a better idea to approach this problem.", "NaN"], ["12699", "Support JdbcErrorCode to Verifier exception classifier", "Leiqing Cai", "caithagoras", "04/23/19, 06:44:44 PM", "Also, mark DBC_ERROR as retryable.", "NaN"], ["12702", "Apply expression optimization for RowExpressionInterpreter::optimize", "James Sun", "highker", "04/23/19, 07:55:13 AM", "NaN", "NaN"], ["12705", "Add release notes for 0.219", "Leiqing Cai", "caithagoras", "04/25/19, 01:37:45 AM", "NaN", "NaN"], ["12706", "Expose basic function info to connectors", "James Sun", "highker", "04/23/19, 02:57:13 AM", "NaN", "NaN"], ["12707", "Add STRRPOS and modify STRPOS functions as per syntax", "Sagar Sumit", "codope", "05/09/19, 06:16:23 PM", "Implements issue [10771](https://github.com/prestodb/presto/issues/10771) according to the updated syntax.\r\n@rongrong Please review this PR. Sorry, the previous PR [12664](https://github.com/prestodb/presto/pull/12664) got polluted by mistake. Please close that. I did my changes in new branch and rebased on top of master.\r", "NaN"], ["12709", "Closes #12708 -- Clarify split_to_map documentation", "Michael Chirico", "MichaelChirico", "04/24/19, 06:55:57 AM", "#12708 Straightforward PR.\r\n\r\nShould something be added to the release notes?\r\n\r\nLet me know any other touch-up needed.", "NaN"], ["12710", "Make Aggregation use RowExpression instead", "Yi He", "hellium01", "06/14/19, 01:55:47 AM", "This will remove the dependency to Expression in following plan nodes:\r\n```\r\nAggregationNode\r\nTableWriterNode\r\nTableFinishNode\r\n```\r\nIt is splitted to small commits  that may need to squash together later. \r", "NaN"], ["12716", "Introduce RowExpressionRewriter", "James Sun", "highker", "04/24/19, 02:18:50 AM", "NaN", "NaN"], ["12718", "Detach Node from LateralJoinNode and ApplyNode", "Yuan Mei", "curcur", "04/25/19, 01:50:13 AM", "Detach Node from LateralJoinNode and ApplyNode and use a error message instead to propagate position (line:column) information. ", "NaN"], ["12719", "Enforce number of tasks per stage limit for grouped execution", "Wenlei Xie", "wenleix", "05/06/19, 01:40:32 AM", "Test query:\r\n\r\n```\r\npresto:tpch_bucketed> select * from orders JOIN customer USING(custkey);\r\n```\r\nAnd set `max_tasks_per_stage=3` (introduced in https://github.com/prestodb/presto/commit/8f7449ab77fb8ef5701c253a839e743c7bbcb47b) \r\n\r\nBefore: \r\n\r\n![Screen Shot 2019-04-23 at 2 49 41 PM](https://user-images.githubusercontent.com/799346/56621409-343d7280-65e1-11e9-8bc0-4c6f37caa91e.png)\r\n\r\n\r\nAfter:\r\n\r\n![Screen Shot 2019-04-23 at 3 58 35 PM](https://user-images.githubusercontent.com/799346/56621414-37386300-65e1-11e9-8e09-3951a9622727.png)\r\n\r\n\r", "NaN"], ["12720", "Add subfield support to DomainTranslator", "Maria Basmanova", "mbasmanova", "04/26/19, 02:57:55 PM", "Modify DomainTranslator to support subfields, e.g. `a[2] > 10`.\r\n\r\nPart of #12585", "NaN"], ["12724", "Fix equality inference for VARCHAR predicates", "Andrii Rosa", "arhimondr", "05/02/19, 02:21:20 PM", "Equality inference for VARCHAR predicates has been broken since the\r\nintroduction of the bounded VARCHAR type (e.g.: VARCHAR(2)).\r\n\r\nThe actual expression tree for the predicate like `c_varchar = 'abc'`\r\n(assuming that the c_varchar column is of unbounded VARCHAR type) is\r\n`c_varchar = CAST('abc' AS VARCHAR)`.\r\n\r\nEquality cannot be inferred through a CAST in a generic case, as\r\ncasts like `cast(JSON 'null' AS ...)` and `try_cast` may return NULL\r\nfor a non null input.\r\n\r\nHowever casts from VARCHAR(n) to VARCHAR(m) where m > n are type only.\r\nType only casts are not being evaluated by execution, thus cannot change\r\nthe input in any way.", "NaN"], ["12727", "Fix multiple result verification issues in verifier", "Leiqing Cai", "caithagoras", "04/30/19, 04:02:24 PM", "NaN", "NaN"], ["12728", "Handle null constants for FilterStatsCalculator", "James Sun", "highker", "04/26/19, 04:55:24 AM", "NaN", "NaN"], ["12729", "Add PredicateCompiler to SPI", "Maria Basmanova", "mbasmanova", "05/15/19, 02:42:18 PM", "NaN", "NaN"], ["12730", "Mark ABANDONED_TASK as retryable in Verifier", "Leiqing Cai", "caithagoras", "04/29/19, 06:52:26 PM", "NaN", "NaN"], ["12731", "Remove isAggregationFunction from FunctionManager and Metadata", "Rongrong Zhong", "rongrong", "05/01/19, 08:47:59 PM", "NaN", "NaN"], ["12736", "Move subfields information to tablescan", "Pratham", "phd3", "05/31/19, 02:43:44 PM", "@mbasmanova Added changes to store subfields pruning information in TableScanNode instead of a function call.", "NaN"], ["12738", "Limit query submition threads", "Andrii Rosa", "arhimondr", "08/06/19, 08:24:35 PM", "Analysis is the part of the submission and when analysing Presto communicates\r\nwith the metastore using the blocking IO. Thus the maximum number of threads\r\nthat handle submission remains high (twice the number of CPUs).\r\n\r\nCallbacks of the FailedQueryExecution and the ResourceGroupManager are supposed\r\nto be lightweight and low latency. That why the unbounded executor remains to be\r\nused to handle those.", "NaN"], ["12741", "Mark HIVE_WRITER_OPEN_ERROR and THRIFT_SERVICE_CONNECTION_ERROR as retryable in Verifier", "Leiqing Cai", "caithagoras", "04/30/19, 01:59:16 AM", "We have seen those 2 types of transient failures in nightly Verifier runs.", "NaN"], ["12743", "Function resolution", "Rongrong Zhong", "rongrong", "09/04/19, 06:34:34 PM", "NaN", "NaN"], ["12744", "Use primitive double in RemoteTaskStats#IncrementalAverage", "Wenlei Xie", "wenleix", "05/22/19, 08:56:52 PM", "Method add() is synchronized thus `average` can be a primitive double\r\ninstead of an AtomicDouble.", "NaN"], ["12746", "Properly close TestingMySqlServer in Verifier tests", "Leiqing Cai", "caithagoras", "04/30/19, 06:34:01 PM", "NaN", "NaN"], ["12747", "Replace Expression with RowExpression in assignment ", "Yi He", "hellium01", "06/18/19, 08:58:39 PM", "A update on top of #12715. Will squash after review. Some utility was borrowed from another PR which will be removed once rebased on top of each other. ", "NaN"], ["12750", "Allow failure resolver to be disabled in Verifier", "Leiqing Cai", "caithagoras", "05/01/19, 06:26:18 PM", "NaN", "NaN"], ["12753", "Minor refactoring and fixes", "Shixuan Fan", "shixuan-fan", "05/02/19, 09:03:32 PM", "Extracted from #12529 ", "NaN"], ["12754", "Improve OrcWriter by estimating page logical size", "Vic Zhang", "viczhang861", "05/07/19, 07:34:00 PM", "OrcWriter is slow when calculating logical size for DictionaryBlock.#11717\r\n\r\nCalculate accurate logical size for each page chunk is expensive\r\nand not necessary, estimate maximal number of rows a chunk can hold\r\nassuming data size for each row is similar.\r\n\r\nTested by profiling a query that writes a lot data,  the time spent on\r\ngetLogicalSizeInBytes() decreased from 65% to 14%\r\n\r\nAlso tried not to sample but calculate total size for input page instead.\r\nHowever, no improvement observed suggesting the original binary\r\nsearch loop is not the root cause.", "NaN"], ["12755", "Rewrite unsupported comparison exception in array_sort ", "Leiqing Cai", "caithagoras", "05/01/19, 10:52:07 PM", "PrestoException with NOT_SUPPORTED error code may be thrown when\r\narray_sort compare the elements, not allowing TRY block to catch\r\nthe failure.", "NaN"], ["12761", "Avoid exception message duplication in PageResponseHandler", "Wenlei Xie", "wenleix", "05/08/19, 01:27:30 PM", "When a PageTransportErrorException is thrown, today it will be wrapped\r\nin another PageTransportErrorException with the original exception\r\nmessage copied.\r\n\r\nThis is unnecessary since the original message is contained in cause.\r\nWhen the exception message is long (e.g. it may contain stacktrace from\r\nthe remote host), useful information can be omitted.", "NaN"], ["12762", "Prepare to expose FilterNode, TableScanNode, and TableHandle in SPI", "James Sun", "highker", "05/20/19, 06:29:52 PM", "There is a major temporary table refactoring @wenleix @arhimondr ", "NaN"], ["12768", "Entropy UDF", "Ami Tavory", "atavory", "05/21/19, 11:34:43 PM", "Following #12700  and talks with @rongrong, I'd like to ask to pull the entropy UDF.\r\n\r\nMotivation\r\n------------\r\n\r\n[Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) is a measure of disorder and uncertainty. \r\n\r\nSay you have a number of web pages, each accessed by approximately 100 visits last month. The visits of one page might be \"organized\", in the sense that all 100 accesses came from a small number of users, while the visits of another page might be \"disorganized\", in the sense that the 100 accesses were distributed over many more users. In this case, entropy quantifies how \"disorganized\" the accesses are, by taking a column of counts per user (summing up to about 100, in this example), and giving a single number.\r\n\r\nEntropy is also used as the basis of several other calculations in machine learning and statistics.\r\n\r\nPossible inputs and output\r\n------------------------------\r\n\r\n  entropy(BIGINT) -> DOUBLE\r\n  entropy(INT) -> DOUBLE\r\n\r\nDescription\r\n------------\r\n\r\nCompute the normalized entropy of a series of counts.  The input is assumed\r\n to be a column of counts of each value's occurrence . The entropy is normalized in the sense that\r\n the counts are first normalized so that the sum of all counts equals one\r\n (i.e., it is converted to a probability distribution).  Any NULLs in the\r\n column are ignored.  If the column contains negative values then NULL is\r\n returned.  If the total count of entries of in the column is zero then zero\r\n is returned.\r\n\r\n Note that the entropy is computed using base-2 log.", "NaN"], ["12769", "Log intermediate io separately", "Andrii Rosa", "arhimondr", "05/10/19, 03:11:38 PM", "NaN", "NaN"], ["12770", "Add ExpressionOptimizer to RowExpressionService", "James Sun", "highker", "05/06/19, 10:32:57 PM", "ExpressionOptimizer helps to simplify a RowExpression into a minimum\r\nform. For example, `length(query) > 100` will be translated into\r\n`GREATER_THAN(length(query), CAST(Slice{base=[B@47199eaa, address=47, length=3}))`;\r\nAfter optimization, it will be `GREATER_THAN(length(query), 100)`", "NaN"], ["12772", "Avoid statically importing TupleDomain.{none, all}", "Wenlei Xie", "wenleix", "05/06/19, 06:05:06 PM", "NaN", "NaN"], ["12773", "Fix incorrect partitioned join after PredicatePushDown", "Rebecca Schlussel", "rschlussel", "06/05/19, 12:27:36 PM", "PredicatePushDown used the original join distribution type when it\r\ncreates a new join node.  However, if predicate push down removed all\r\nequijoin criteria after DetermineJoinDistributionType already ran, then\r\nthe join type could be invalid leading to an assertion failure in the\r\njoin node.\r\n\r\nFixes #12354 \r\n\r\nI still need to figure out how to test it because none of our test tables have small int types and i'm not sure how to reproduce it with the existing tables. We either need to\r\n1. create a new test table (and not run for tpch?)\r\n2. find another way to reproduce. \r\n3. just have a product test", "NaN"], ["12777", "Handle inList with null for FilterStatsCalculator", "James Sun", "highker", "05/07/19, 06:14:40 PM", "inList in FilterStatsCalculator will be translated into a disjunction of\r\nequals. Need to handle nulls for comparisons.", "NaN"], ["12780", "Reduce Hive split memory usage to prevent EXCEEDED_SPLIT_BUFFERING_LIMIT errors", "Rebecca Schlussel", "rschlussel", "05/22/19, 11:48:24 PM", "During grouped execution for queries with lots of buckets/partitions, we can encounter EXCEEDED_SPLIT_BUFFERING_LIMIT errors for the Hive connector because all splits get created at once.  This PR reduces the memory usage of each split by\r\n1. extracting partition level information and counting it only once\r\n2. storing just the end of each block rather than both start and end\r\n3. not storing block addresses unless its required for scheduling\r\n\r\n\r\nSee https://github.com/prestodb/presto/wiki/HiveSplitSource-and-Grouped-Execution for more details and background\r\n\r\n----------------------------\r\n\r\nSplit schedule notes can be found in https://github.com/prestodb/presto/pull/12780#pullrequestreview-236183935", "NaN"], ["12782", "Add comments to ExchangeClient and improve tests", "Wenlei Xie", "wenleix", "05/09/19, 03:27:02 PM", "NaN", "NaN"], ["12783", "Print arithmetic and comparison expressions in human-readable format.", "Akshay Pall", "AkshayPall", "05/10/19, 04:18:45 PM", "Previously, all functions (CallExpressions) would be printed as `$function_name$(ARG1, ARG2, ..., ARGN)` but now arithmetic and comparison expressions will print with the operand between arguments eg, `(#1) + (BIGINT 5)`. Additional changes were made to pass a `FunctionManager` instance to `RowExpressionFormatter` instead of instantiating one in the class.", "NaN"], ["12786", "Classification Metric UDFs", "Ami Tavory", "atavory", "05/30/19, 02:30:09 PM", "Following #12732 and talks with @rongrong, I'd like to request pulling a Precision/Recall UDF.\r\n\r\n```\r\n.. function:: classification_miss_rate(buckets, y, x, weight) -> array<double>\r\n\r\n     Computes the miss-rate part of the receiver operator curve with up to ``buckets`` number of buckets. Returns\r\n    an array of miss-rate values. ``y`` should be a boolean outcome value; ``x`` should be predictions, each\r\n    between 0 and 1; ``weight`` should be non-negative values, indicating the weight of the instance.\r\n\r\n     To get an ROC map, use this in conjunction with :func:`classification_recall`:\r\n\r\n     .. code-block:: none\r\n\r\n         MAP(classification_recall(200, outcome, prediction), classification_miss_rate(200, outcome, prediction))\r\n\r\n .. function:: classification_precision(buckets, y, x) -> array<double>\r\n\r\n     This function is equivalent to the variant of\r\n    :func:`classification_precision` that takes a ``weight``, with a per-item weight of ``1``.\r\n\r\n .. function:: classification_precision(buckets, y, x, weight) -> array<double>\r\n\r\n     Computes the precision part of the precision-recall curve with up to ``buckets`` number of buckets. Returns\r\n    an array of precision values. ``y`` should be a boolean outcome value; ``x`` should be predictions, each\r\n    between 0 and 1; ``weight`` should be non-negative values, indicating the weight of the instance.\r\n\r\n     To get a map of recall to precision, use this in conjunction with :func:`classification_recall`:\r\n\r\n     .. code-block:: none\r\n\r\n         MAP(classification_recall(200, outcome, prediction), classification_precision(200, outcome, prediction))\r\n\r\n .. function:: classification_precision(buckets, y, x) -> array<double>\r\n\r\n     This function is equivalent to the variant of\r\n    :func:`classification_precision` that takes a ``weight``, with a per-item weight of ``1``.\r\n\r\n .. function:: classification_recall(buckets, y, x, weight) -> array<double>\r\n\r\n     Computes the recall part of the precision-recall curve or the receiver operator charateristic curve\r\n    with up to ``buckets`` number of buckets. Returns an array of recall values.\r\n    ``y`` should be a boolean outcome value; ``x`` should be predictions, each\r\n    between 0 and 1; ``weight`` should be non-negative values, indicating the weight of the instance.\r\n\r\n     To get a map of recall to precision, use this in conjunction with :func:`classification_recall`:\r\n\r\n     .. code-block:: none\r\n\r\n         MAP(classification_recall(200, outcome, prediction), classification_precision(200, outcome, prediction))\r\n\r\n .. function:: classification_recall(buckets, y, x) -> array<double>\r\n\r\n     This function is equivalent to the variant of\r\n    :func:`classification_recall` that takes a ``weight``, with a per-item weight of ``1``.\r\n\r\n .. function:: classification_thresholds(buckets, y, x) -> array<double>\r\n\r\n     Computes the thresholds part of the precision-recall curve with up to ``buckets`` number of buckets. Returns\r\n    an array of thresholds. ``y`` should be a boolean outcome value; ``x`` should be predictions, each\r\n    between 0 and 1.\r\n\r\n     To get a map of thresholds to precision, use this in conjunction with :func:`classification_precision`:\r\n\r\n     .. code-block:: none\r\n\r\n         MAP(classification_thresholds(200, outcome, prediction), classification_precision(200, outcome, prediction))\r\n\r\n     To get a map of thresholds to recall, use this in conjunction with :func:`classification_recall`:\r\n\r\n     .. code-block:: none\r\n\r\n         MAP(classification_thresholds(200, outcome, prediction), classification_recall(200, outcome, prediction))\r\n```", "NaN"], ["12787", "Introduce channels for staging partition write and commit", "Shixuan Fan", "shixuan-fan", "05/15/19, 12:02:13 AM", "Extracted from #12529 ; part of #12124\r\n\r\nWe removed hive session property `staging_file_writing_enabled`, instead, we let the engine decide when to write and commit staging partitions. The engine will use `PageSink` as the channel to tell the connector to write staging partitions, and use `Metadata` to commit the partitions. In this PR we introduced these two channels, and they will be used in #12529.", "NaN"], ["12788", "Revert \"Trigger bucket balancer after cluster restart\"", "Jiexi Lin", "jessesleeping", "05/10/19, 08:47:38 PM", "A forced rebalance can have unexpected outcome when restarting cluster\r\nto mitigate issues.", "NaN"], ["12789", "Flatten FunctionCall as direct fields in AggregationNode#Aggregation", "Yi He", "hellium01", "05/15/19, 01:52:09 AM", "This is the first 3 commits of #12710", "NaN"], ["12792", "Add block flattening", "Timothy Meehan", "tdcmeehan", "06/14/19, 06:05:56 PM", "# High level goal\r\n\r\nJoin, repartitioning, and other future enhancements, need data access with minimal overhead for efficient access.  Experiments have shown nontrivial overhead from accessing deeply nested blocks and boundary checks over data which is not elided by the JVM.  We want \"fast track\" data access which trades safety for speed, with the idea that this API may require more care and testing.\r\n\r\n# New components\r\n\r\n## BlockFlattener\r\n\r\nReturns a Block structure which is nested at most one level.  Nested structures, for the time being, are considered Dictionary and RLE blocks.  The idea is to flatten nested structures so that they are at most one level deep.  This promises better efficiency when operating over tight loops due to a lower cache miss rate.\r\n\r\n## BlockLease\r\n\r\nRepeated array allocation has nontrivial overhead, in particular when combined with block decoding/flattening, as this requires repeated array allocation when rewriting the nested ids map.  BlockLease is introduced as an `AutoCloseable` resources that is designed to return arrays for later reuse when finished, and `Supplier<Block>` to access the Block from within the try-with-resources.\r\n\r\n## ArrayAllocator\r\n\r\nA simple interface for creating and returning primitive arrays.  It may be used with a `BlockLease` to return an array for reuse when the lease has expired.  This class is designed to live at the operator level to avoid repeated array allocation over several iterations of calls to an operator.\r\n\r\n## UncheckedBlock\r\n\r\nAccess over primitive types over blocks incurs nontrivial overhead from boundary checks which are not elided by the JVM.  Access over primitive arrays proves to have the best performance, as for example the JVM will elide most boundary checks in loops to the end of the loops.  We would like a compromise where we can get the performance of raw primitive arrays but with the encapsulation and convenience of Blocks.\r\n\r\nThe idea of UncheckedBlock is to hoist out boundary checks, so we pay the cost once in a loop rather than per item of the loop.  To accomplish this, we expose the offset of the block in UncheckedBlock--this provides the start of the iteration.  Once the offset is exposed, which is where the initial data lives in the underlying array, we exposed `getXXXUnchecked` methods to provide quick access which is strictly limited to underlying array access.  These methods will be inlined by the JVM into direct array access, giving us a good compromise of performance, with the slight inconvenience of a downcast to `UncheckedBlock`.\r\n\r\nWhen complete, any block which is not RLE or Dictionary could be cast to UncheckedBlock, where a parallel API could be used to provide fast track access to the data in the Blocks.\r\n\r\n## Decisions made\r\n\r\n1. A decision was made that the Block interface is already heavily polluted, and adding unsafe accessor methods will further pollute this interface.\r\n2. A decision was made to have a clear delineation of the \"safe\" word of Block into the \"Unsafe\" world of UncheckedBlock.  Meaning, you must do something inconvenient in order to obtain the necessary methods on UncheckedBlock, which signals that you're aware of the risks of coding errors or other issues.  i.e. UcheckedBlock is a \"sharp instrument\" which requires particular care.\r\n\r", "NaN"], ["12793", "Reorder the lookup RecordSet in ThriftIndexedTpchService", "Yi He", "hellium01", "05/12/19, 07:11:53 PM", "IndexedTable is cached with a fixed order. To do a lookup, we should\r\nalways reorder input RecordSet to match with it.", "NaN"], ["12800", "Add TableCommitContext for TableWriterOperator and DeleteOperator", "Shixuan Fan", "shixuan-fan", "05/20/19, 09:49:03 PM", "TableCommitContext includes lifespan, stageId, taskId,\r\ncommitGranularity and lastPage. This context is included in\r\noutput pages of TableWriterOperator and DeleteOperator, which\r\nwill be used for page deduplication in TableFinishOperator for\r\nrecoverable grouped execution.\r\n\r\nExtracted from #12529 ", "NaN"], ["12807", "Enable materialized execution for window functions and mark_distinct", "Andrii Rosa", "arhimondr", "05/17/19, 04:55:55 PM", "NaN", "NaN"], ["12808", "Add TableStatistics in QueryInputMetadata for Join Query", "Vic Zhang", "viczhang861", "08/01/19, 07:12:05 PM", "Table stats availablity is important for cost-based optimization\r\nof join query. Add it to QueryInputMetadata class.\r\n\r\nTest is added to show example of TableStatisitcs in json format.", "NaN"], ["12809", "Support exchange materialization for union node", "Wenlei Xie", "wenleix", "05/20/19, 05:09:47 PM", "NaN", "NaN"], ["12810", "Move LogicalRowExpressions to SPI", "Maria Basmanova", "mbasmanova", "05/16/19, 03:28:02 PM", "CC: @oerling @tdcmeehan @yingsu00 @elonazoulay @wenleix ", "NaN"], ["12811", "Fix TestOrcPageSourceMemoryTracking", "Maria Basmanova", "mbasmanova", "05/16/19, 03:27:38 PM", "Aria code path relies on partition columns to be at the end of the list as they are in Hive. TestOrcPageSourceMemoryTracking was failing because it used to create a synthetic table where a partition column appears before a regular column. The fix is to move partition column after the regular column.", "NaN"], ["12812", "Add release notes for 0.220", "Leiqing Cai", "caithagoras", "05/17/19, 08:24:32 PM", "NaN", "NaN"], ["12816", "Add icon for warnings to query list view", "Varun Gajjala", "varungajjala", "05/21/19, 07:15:46 PM", "- Add a small yellow box beside the query id if the query has warnings\r\n- Search query ids by warnings\r\n<img width=\"1677\" alt=\"Screen Shot 2019-05-16 at 12 39 30 PM\" src=\"https://user-images.githubusercontent.com/9701790/57882316-45406480-77d8-11e9-95da-92fc1453bbb3.png\">\r\n<img width=\"1675\" alt=\"Screen Shot 2019-05-16 at 12 40 03 PM\" src=\"https://user-images.githubusercontent.com/9701790/57882319-47a2be80-77d8-11e9-9537-fc68c2b02b2c.png\">\r\n\r", "NaN"], ["12820", "Compare OrderingScheme with OrderBy in AggregationFunctionMatcher", "Yi He", "hellium01", "05/21/19, 05:35:16 PM", "NaN", "NaN"], ["12823", "[GeoSpatial] Add xxHash64 operator to Bing Tiles", "James Gill", "jagill", "06/04/19, 05:40:29 PM", "In particular, this enables operators like approx_distinct which\r\nrequires the object to be xxHashable.\r\n\r\nFixes #12822 ", "NaN"], ["12825", "Minor Bugfix to Doc of BlockBuilder's closeEntry", "Ami Tavory", "atavory", "05/20/19, 05:45:16 PM", "NaN", "NaN"], ["12827", "Fix HiveQueryRunner.createBucketedSession", "Maria Basmanova", "mbasmanova", "05/20/19, 03:48:23 PM", "NaN", "NaN"], ["12831", "Use factory methods for ScheduleResult", "Wenlei Xie", "wenleix", "05/21/19, 02:37:30 AM", "NaN", "NaN"], ["12836", "Move PlanNodeId and allocator to SPI", "James Sun", "highker", "05/20/19, 04:32:32 AM", "Getting prepared for the connector optimizer interface\r\n```java\r\nPlanNode optimize(\r\n    PlanNode maxSubplan,\r\n    ConnectorSession session,\r\n    VariableAllocator variableAllocator,\r\n    PlanNodeIdAllocator planNodeIdAllocator);\r\n```\r\n\r\n\r\nThis is similar to\r\n\r\n```java\r\npublic interface PlanOptimizer\r\n{\r\n    PlanNode optimize(PlanNode plan,\r\n            Session session,\r\n            TypeProvider types,\r\n            SymbolAllocator symbolAllocator,\r\n            PlanNodeIdAllocator idAllocator,\r\n            WarningCollector warningCollector);\r\n}", "NaN"], ["12838", "Move checkstyle checks to Presto", "Timothy Meehan", "tdcmeehan", "05/21/19, 02:26:27 PM", "It's more flexible to leave them in Presto.", "NaN"], ["12839", "Cleanup temporary table on rollback", "Andrii Rosa", "arhimondr", "05/21/19, 02:09:17 PM", "All temporary table write attempts must be cleaned up on both,\r\nrollback and commit.", "NaN"], ["12840", "Make StageScheduler notify all tasks are scheduled", "Wenlei Xie", "wenleix", "06/04/19, 06:44:12 AM", "There are three major responsibilities for `SourcePartitionedScheduler`:\r\n- Split schedule\r\n- Task schedule and stage management\r\n- Lifespan schedule\r\n\r\nNote `SourcePartitionedScheduler` can be used either as a `StageScheduler` or `SourceScheduler`. In both case it needs to take care about split schedule. \r\n\r\n- When it's used as a `StageScheduler`, it's always used for unpartitioned stage (i.e. JOIN/AGGREGATE over *remote* source. \r\n  - It needs to take care about task schedule\r\n  - It doesn't need to take care about lifespan schedule\r\n\r\n- When it's used as a `SourceScheduler`, it's used by a `FixedSourcePartitionedScheduler` to schedule table scan sources.\r\n   * It doesn't need to take care about task schedule (although today it still do so) \r\n   * It need to take care about lifespan schedule (behemoth warning!)\r\n\r\nHistorically, `FixedSourcePartitionedScheduler` doesn't call `stage.transitionToSchedulingSplits()`. And it  relies on `SourcePartitionedScheduler` to do this stage management,  even `SourcePartitionedScheduler` is used as a `SourceScheduler`.  This is confusing and error-prone, as it should be `StageScheduler`'s responsibility to do stage management .\r\n\r\nThis commit makes `FixedSourcePartitionedScheduler` to call `stage.transitionToSchedulingSplits()`. As a result, it release the needs for `SourcePartitionedScheduler` from doing this for grouped execution (In the context of grouped execution, `SourcePartitionedScheduler` is guaranteed to be used as a `SourceScheduler`). \r\n\r\nIn principle, `SourcePartitionedScheduler`  doesn't need to do stage management at all  as long as it's used as a `SourceScheduler` (even for ungrouped execution).  This is not done in this commit. \r\n\r\n`FixedCountScheduler` doesn't need to  call `stage.transitionToSchedulingSplits()`  since it there will be no table splits to schedule.", "NaN"], ["12842", "Support synthetic column names in Subfield", "Maria Basmanova", "mbasmanova", "05/21/19, 06:55:55 PM", "Make $-sign a valid character in subfield path to support filters on $bucket\r\nand $path columns, e.g. SELECT * FROM t WHERE $bucket = 1.\r\n\r\nCC: @oerling @yingsu00 @arhimondr @wenleix ", "NaN"], ["12843", "Replace SubfieldPath with Subfield", "Maria Basmanova", "mbasmanova", "05/21/19, 08:20:53 PM", "NaN", "NaN"], ["12844", "Fix race condition in the SqlQueryScheduler", "Andrii Rosa", "arhimondr", "05/22/19, 12:45:14 AM", "The `scheduling` is supposed to ensure that only a single\r\nscheduling happens at any moment of a time.\r\n\r\nIf the finally block is executed after the next streaming section\r\nhas already started scheduling, the \"scheduling\" flag will be set\r\nto \"false\" despite the scheduling of the second streaming section\r\nis still in process.", "NaN"], ["12845", "Fix a time format string in the tests.", "Nelson Elhage", "nelhage", "05/22/19, 08:07:25 PM", "`%M` is the code for the current minute; `%m` is the month. `%Y-%M` is thus never a format string you want.\r\n\r\nThis isn't actually a bug since this is testing that the code errors, and won't actually examine the contents of the format, but I found it in a large-scale grep for this common bug; fixing it resolves the slim chance of someone cargo-culting it, or doing the same grep and having to do the same work I did to ignore it.", "NaN"], ["12851", "Pushdown filter into Hive connector", "Maria Basmanova", "mbasmanova", "05/24/19, 01:23:57 AM", "The new pushdownFilter metadata API is used to pushdown filter into Hive connector in the form of RowExpression. The connector decomposes the filter into conjuncts of two types: (1) range filter on an entire column or a subfield; (2) non-trivial expression that applies to a distinct set of columns or subfields. The connector adaptively re-arranges the order of columns to scan so that most efficient filters (drop most rows fastest) are applied first. The columns with no filters are scanned last and only rows that passed all the filters are extracted.\r\n\r\nThe pushed down filter is stored in HiveTableLayoutHandle in three pieces:\r\n\r\n* `TupleDomain<Subfield> domainPredicate` - a set of range filters that apply to entire columns of subfields\r\n* `RowExpression remainingPredicate` - the rest of the filter\r\n* `List<HiveColumnHandle> predicateColumns` - columns referenced by the filter; these columns need to be scanned by may not need to be projected; these columns may not be present in the TableScanNode after filter pushdown\r\n\r\nThis information is propagated to the workers via HiveSplit.\r\n\r\nAt this point, range filters on partition and synthetic columns ($bucket and $path) are supported, but more complex expressions are not.", "NaN"], ["12853", "Fix 0.220 release notes header", "Rebecca Schlussel", "rschlussel", "05/22/19, 10:46:15 PM", "just noticed this typo with the wrong release version.", "NaN"], ["12854", "Add checkstyle rule to avoid statically importing all and none", "Wenlei Xie", "wenleix", "05/24/19, 03:03:52 AM", "See also https://github.com/prestodb/presto/pull/12772", "NaN"], ["12856", "Eliminate negation in logical expressions", "James Sun", "highker", "05/24/19, 12:30:42 AM", "Ideally we may have CNF for filter pushdown. But given the complexity of exploding the expression sizes, we don't exactly know if that worth the effort. ", "NaN"], ["12859", "Improve InternalHiveSplit memory usage estimate", "Andrii Rosa", "arhimondr", "05/23/19, 03:52:17 PM", "NaN", "NaN"], ["12861", "Fix planPrinter and JSON serialization on AggregationNode#Aggregation", "Yi He", "hellium01", "05/24/19, 12:48:10 AM", "Commit 13e1d0bc changed the way aggregation is formatted and introduced a bug in JSON serialization of Aggregation (Issue #12860). \r", "NaN"], ["12862", "Further trim PageSourceOptions", "Maria Basmanova", "mbasmanova", "05/28/19, 02:32:30 PM", "Removed outputChannels and internalChannels from PageSourceOptions. These used to specify which columns to project out, but the same information is provided by the `columns` argument in ConnectorPageSourceProvider#createPageSource.\r\n\r\nAlso, updated session properties to turn Aria off by default.\r\n\r\ndepends on #12851", "NaN"], ["12863", "Bring back presto-bytecode", "James Sun", "highker", "05/31/19, 09:40:38 PM", "NaN", "NaN"], ["12867", "Move getPathDomain to HiveSplitManager", "Maria Basmanova", "mbasmanova", "05/28/19, 06:23:09 PM", "Filter pushdown implementation in Hive connector involves changing HiveTableLayoutHandle to replace `TupleDomain<? extends ColumnHandle> compactEffectivePredicate` with 3 new fields:\r\n\r\n```\r\n// Range filters on entire columns and subfields extracted from the filter (non-compacted)\r\nTupleDomain<Subfield> domainPredicate;\r\n\r\n// The remaining filter. AND-ing this with domainPredicate give full filter.\r\nRowExpression remainingPredicate;\r\n\r\n// Columns referenced in domainPredicate or remainingPredicate. \r\n// These columns may overlap columns that will be projected out (e.g. columns listed in the TableScanNode).\r\nMap<String, HiveColumnHandle> predicateColumns;\r\n```\r\n\r\nExtracting range filters on $path column will be more involved and would be more convenient to do in HiveSplitManager as opposed to InternalHiveSplitFactory. This change makes InternalHiveSplitFactory accept `Optional<Domain> pathDomain` instead of more opaque `TupleDomain<? extends ColumnHandle> compactEffectivePredicate`.", "NaN"], ["12868", "Backport \"Create Block directly if the Data stream in ORC file is null\"", null, "aweisberg", "06/01/19, 02:36:04 AM", "Backport of https://github.com/prestosql/presto/pull/229\r\n\r\nCloses: #12413", "NaN"], ["12869", "Show completed and total lifespans on Web UI ", "Wenlei Xie", "wenleix", "05/31/19, 08:56:36 PM", "<img width=\"301\" alt=\"Screen Shot 2019-05-24 at 12 00 53 AM\" src=\"https://user-images.githubusercontent.com/799346/58308776-56c8d400-7db7-11e9-8e54-2d79a2e9398e.png\">\r\n\r\n\r\nExpect to have some test failures and will fix them.", "NaN"], ["12870", "Add merge_hll", "Gaston Arguindegui", "gastonar", "06/07/19, 09:47:09 PM", "Allows HyperLogLog objects to be merged from array.", "NaN"], ["12873", "Minor refactor to SourcePartitionedScheduler", "Wenlei Xie", "wenleix", "05/31/19, 09:04:19 PM", "NaN", "NaN"], ["12874", "Update properties.rst", "emhlbmc", "emhlbmc", "06/02/19, 07:08:23 AM", "The default value of \"query.max-total-memory\" should be  \"query.max-memory * 2\"", "NaN"], ["12875", "Introduce pushdownFilter SPI", "Maria Basmanova", "mbasmanova", "06/05/19, 02:38:29 PM", "Introduce `pushdownFilter` metadata API to enable more efficient filter processing for ORC data inside of the Hive connector. Also, introduce `isPushdownFilterSupported` API to allow connectors to opt-in into the new functionality. By default, `isPushdownFilterSupported` returns `false` and `pushdownFilter` is not invoked. When `isPushdownFilterSupported` returns true, the engine invokes `pushdownFilter` instead of `getTableLayouts` API. The connector encodes the pushed down filter into the `ConnectorTableLayoutHandle`.\r\n\r\nThis PR introduces the new APIs, updates `PickTableLayouts` optimizer rule to use them and provides a minimal implementation for the Hive connector.\r\n\r\nThe implementation in Hive connector covers primarily the optimizer part and there is no logic to evaluate the pushed down filter during the execution yet. The information about the filter is propagated via `HiveSplit` and this PR includes the necessary plumbing. The original code path is modified slightly to compact range filters during the execution instead of planning. This is done to reduce the discrepancy between two code paths in the planning phase.\r\n\r\nAside from `PickTableLayouts` rule, there are a number of places that invoke `getTableLayout` API. These will be updated in subsequent PRs.", "NaN"], ["12876", "Upgrade to testing-mysql-server 8.0.12-4", "Leiqing Cai", "caithagoras", "05/29/19, 04:15:26 AM", "Increase command timeouts from 30s to 90s.", "NaN"], ["12878", "Small refactor to  PlanFragment ", "Andrii Rosa", "arhimondr", "05/31/19, 06:47:44 PM", "NaN", "NaN"], ["12881", "Add RowExpressionSymbolInliner", "Yi He", "hellium01", "06/07/19, 12:14:23 AM", "Add utility to inline symbol in RowExpression that will be used in #12710", "NaN"], ["12888", "Add bitwise shift operations", "Gaston Arguindegui", "gastonar", "06/13/19, 06:17:38 PM", "Allows bitwise shift operations including arithmetic shift right, logical shift right, and shift left.", "NaN"], ["12896", "Avoid using containsAll for tasks in SqlStageExecution", "Shengpei Zhang", "tastynoodle", "06/07/19, 08:50:17 PM", "`containsAll()` is `O(n)`. This causes lock contention on coordinator and cause potential reliability issues. \r\n\r\n Since `finishedTasks` is a subset of `allTasks`, we can simply compare the size.", "NaN"], ["12897", "override hashcode and equals in FunctionMetadata", "Yuan Mei", "curcur", "06/04/19, 06:23:14 AM", "Need to override hashcode() and equals() in FunctionMetadata to register whether a function is pushable or not (using FunctionMetadata as key in a map).\r", "NaN"], ["12901", "Fix ORC stripe skipping when using bloom filter", "Dilip Singh Kasana", "dilipkasana", "09/18/19, 08:47:03 PM", "Fixes #12900\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n-----------------\r\n* Fix ORC stripe skipping when using bloom filter\r\n```", "NaN"], ["12902", "Change jar version to work with branch based release model", null, "mayankgarg1990", "06/04/19, 06:44:50 PM", "The previous model involved using git-describe which broke as we\r\nmoved to branch based releases since all of our tags are now on the\r\nrelease branches rather than master.\r\n\r\nEven before that, the versions generated used the previous version\r\nnumber of the release rather than the current maven version.\r\n\r\nChanged that to now use a combination of maven version and commit id.", "NaN"], ["12903", "Limit the number of concurrent plan sections", "Rebecca Schlussel", "rschlussel", "06/06/19, 09:39:12 PM", "Limit the number of streaming plan sections that can run at once.\r\n\r\nRelated to https://github.com/prestodb/presto/issues/12387", "NaN"], ["12904", "Add Experimental annotation in SPI", "Timothy Meehan", "tdcmeehan", "06/05/19, 12:58:48 AM", "CC: @rongrong @hellium01 \r", "NaN"], ["12910", "Replace all apiNote with Experimental annotation in SPI", "James Sun", "highker", "06/06/19, 01:07:51 AM", "apiNote is the legacy way to indicate an experimental interface. Replace\r\nthe usage with new Experimental annotation.", "NaN"], ["12919", "Throw exceptions for unsupported scalar function annotation", "Yuan Mei", "curcur", "06/08/19, 05:30:50 PM", "NaN", "NaN"], ["12920", "Apply connector-specified optimization in PlanOptimizers", "James Sun", "highker", "06/17/19, 11:17:44 PM", "NaN", "NaN"], ["12925", "Rename LogicalRowExpressions::TRUE/FALSE to TRUE/FALSE_CONSTANT", "James Sun", "highker", "06/11/19, 06:29:51 AM", "TRUE/FALSE collide with java.lang.Boolean.TRUE/FALSE\r\n\r\nThis goes after #12606", "NaN"], ["12926", "Add PushdownSubfields optimizer rule", "Maria Basmanova", "mbasmanova", "06/14/19, 07:19:23 PM", "The new rule enables subfield pruning in the connectors by identifying\r\nreferenced subfields and passing them to the connector. The rule identifies\r\narray subscripts, map keys, and nested fields in structs.\r\n\r\nDepends on #12606.\r\n\r\nPart of #12585\r\n\r\nCC: @aweisberg @rongrong @yingsu00 @wenleix ", "NaN"], ["12927", "Hide list of tasks in query details by default", "Andrii Rosa", "arhimondr", "06/12/19, 03:01:13 PM", "Most of the queries have thousands of tasks on a real production cluster.\r\n\r\nFor example even a simple aggregation query with 3 stages on a real production\r\ncluster has ~1k tasks.\r\n\r\nTrying to display this excessive number of elements makes browser not responsive.\r\n\r\nIdeally tasks have to be expanded per stage, when the stage in the stage menu is\r\nexpanded. This change is not complex, but at the same time it is not trivial. Thus,\r\nas a temporary solution, just to prevent browser from dying, this patch disables\r\nlist of tasks rendering by default.", "NaN"], ["12930", "Fix decimal division bug", "Andrii Rosa", "arhimondr", "06/12/19, 03:01:40 PM", "NaN", "NaN"], ["12931", "Add release notes for 0.221", "Rebecca Schlussel", "rschlussel", "06/13/19, 04:23:29 PM", "NaN", "NaN"], ["12934", "Support grouped execution for eligible table scans", "Shixuan Fan", "shixuan-fan", "06/17/19, 07:49:28 PM", "This is controlled by session property grouped_execution_for_all_eligible_table_scans.\r\n\r\nRelated to https://github.com/prestodb/presto/issues/12124", "NaN"], ["12936", "Enable tuple domain extraction for subfields", null, "aweisberg", "06/18/19, 08:01:48 PM", "Fixes: #12917", "NaN"], ["12937", "Fix checkArgument message in AggregationMetadata", "Wenlei Xie", "wenleix", "06/12/19, 11:09:55 PM", "NaN", "NaN"], ["12938", "[Hive Connector] Convert TupleDomain into executable filter", "Maria Basmanova", "mbasmanova", "06/21/19, 08:41:31 PM", "Convert TupleDomain into a filter that a StreamReader can apply while decoding values from the ORC.\r\n\r\nCC: @aweisberg ", "NaN"], ["12939", "Symbol cleanup", "Rongrong Zhong", "rongrong", "07/27/19, 06:33:09 AM", "NaN", "NaN"], ["12946", "Add optimization for queries with FULL OUTER JOIN + COALESCE to remove unnecessary shuffle", "Rongrong Zhong", "rongrong", "09/09/19, 08:49:12 PM", "Reimplement #12586 on top of VariableReferenceExpression / RowExpression.\r\n\r\n```\r\n== Release Notes ==\r\nGeneral Changes\r\n* Improve performance for queries with FULL OUTER JOIN where join keys have the :func:COALESCE function applied.\r\n```", "NaN"], ["12947", "Inline SymbolToSymbolTranslator", "Yi He", "hellium01", "06/14/19, 07:20:46 AM", "SymbolToSymbolTranslator is no long needed after we have variable to variable mapping.", "NaN"], ["12948", "Support nested fields in Elasticsearch Connector", "Zhenxiao Luo", "zhenxiao", "08/15/19, 07:47:09 PM", "Fix nested fields in Elasticsearch Connector\r\nCC @nezihyigitbasi @wenleix @jwfcps @ebyhr @knikel ", "NaN"], ["12951", "Increase methods visibility in AbstractTestHiveClient", "Andrii Rosa", "arhimondr", "06/19/19, 02:35:28 PM", "NaN", "NaN"], ["12952", "Adding long polling for task info", "cem cayiroglu", "cemcayiroglu", "06/26/19, 11:45:28 PM", "Task status updates are causing coordinator to overload in large\r\nclusters.", "NaN"], ["12953", "Create correct identity translation for PropertyDeriviations", "Rongrong Zhong", "rongrong", "06/16/19, 11:21:29 PM", "Fixes #12950", "NaN"], ["12958", "Reduce visibility of HivePageSource methods", "Maria Basmanova", "mbasmanova", "06/17/19, 05:52:01 PM", "NaN", "NaN"], ["12959", "Fixes for Classification ROC", "Ami Tavory", "atavory", "06/26/19, 08:25:27 PM", "Minor fixes for Classification ROC related UDFs\r\n\r\n1. Use fall-out for ROC\r\n2. Fixed miss-rate\r\n3. Improved docs\r\n\r\nUDFs:\r\n\r\nclassification_miss_rate\r\nclassification_fall_out\r\nclassification_precision\r\nclassification_recall\r\nclassification_thresholds", "NaN"], ["12960", "Move translateExpressions below HashGenerationOptimizer", "Yi He", "hellium01", "07/02/19, 01:14:29 AM", "On top of https://github.com/prestodb/presto/pull/12747", "NaN"], ["12961", "Add t-digest", "Gaston Arguindegui", "gastonar", "07/20/19, 12:06:40 AM", "Add t-digest to Presto, which allows faster and more accurate results when calculating quantiles while saving space in memory. In reference to issue #12929.\r\n\r\n# Note\r\n\r\nWe make intentional decision to just copy the existing implementation for now. See https://github.com/prestodb/presto/pull/12961#pullrequestreview-251927260 and https://github.com/prestodb/presto/pull/12961#issuecomment-510696010", "NaN"], ["12964", "[WIP] Remove pushdownFilter interface", "James Sun", "highker", "06/20/19, 12:28:15 AM", "NaN", "NaN"], ["12965", "Two fixes to connector optimizer", "James Sun", "highker", "06/18/19, 08:53:30 PM", "NaN", "NaN"], ["12966", "Fixed-Range Histograms", "Ami Tavory", "atavory", "08/01/19, 06:26:53 AM", "Fixed-range histograms:\r\n\r\n1. Mapping values to weights\r\n2. Mapping values and weights, to the number of times these combinations were seen\r\n\r\nThis is used for various classification metrics, as well as differential entropy.", "NaN"], ["12967", "Fixup unaddressed comments", "Yi He", "hellium01", "06/18/19, 11:24:50 PM", "Some comments unaddressed in: #12747", "NaN"], ["12968", "Fix type mismatch in AddExchanges.Rewriter.visitUnnest", "Rongrong Zhong", "rongrong", "06/20/19, 04:07:46 PM", "NaN", "NaN"], ["12969", "Remove TODO in TranslateExpressions", "James Sun", "highker", "06/19/19, 03:54:52 AM", "PlanNode has been completely migrated from Expression to RowExpression.\r\nThe TODO is no longer needed.", "NaN"], ["12973", "Backport information_schema should not fail for illegal storage format", "Stephen Pascual", "sjpascual", "06/21/19, 05:33:46 PM", "This PR backports the fix for querying the information_schema using the hive plugin:  https://github.com/prestosql/presto/pull/568/", "NaN"], ["12974", "Allow configuring the Glue catalog id", "Stephen Pascual", "sjpascual", "06/21/19, 05:54:35 PM", "NaN", "NaN"], ["12975", "Minor cleanups to Metadata SPI", "Wenlei Xie", "wenleix", "06/20/19, 12:40:07 AM", "NaN", "NaN"], ["12976", "Update drift to com.facebook.drift 1.19", "Yi He", "hellium01", "06/24/19, 10:11:30 PM", "Update presto to use drift provided from\r\nhttps://github.com/prestodb/drift instead.", "NaN"], ["12977", "Avoid binding to hive catalog in TestHiveIntegrationSmokeTest", "Wenlei Xie", "wenleix", "06/20/19, 05:41:44 PM", "Private connectors based on Hive connector can have tests inherited from\r\nTestHiveIntegrationSmokeTest, where the catalog is not \"hive\".", "NaN"], ["12978", "Revert \"[WIP] Remove pushdownFilter interface\"", "Yuan Mei", "curcur", "06/20/19, 12:30:54 AM", "Reverts prestodb/presto#12964", "NaN"], ["12979", "Remove unused SymbolsExtractor::extractUniqueVariable", "James Sun", "highker", "06/20/19, 06:36:44 AM", "NaN", "NaN"], ["12980", "Fix changes after assignment and variable refactor", "Yi He", "hellium01", "06/24/19, 09:26:09 PM", "Because the two refactor happened around same time, some changes needed to adjust to use both VariableReference as key of assignment and RowExpression as value of assignment. ", "NaN"], ["12982", "Move row expression up step2", "Yi He", "hellium01", "08/02/19, 03:12:43 PM", "NaN", "NaN"], ["12984", "Remove unused variables from StreamReaders", "Maria Basmanova", "mbasmanova", "06/20/19, 05:51:58 PM", "NaN", "NaN"], ["12985", "add example of UNNEST for a MAP column to docs", "Thomas J. Leeper", "leepface", "06/25/19, 08:56:54 PM", "Adds example of UNNEST operation on MAP column, which is mentioned as possible but no example is given", "NaN"], ["12991", "Introduce SelectiveStreamReader ", "Maria Basmanova", "mbasmanova", "07/01/19, 03:17:31 PM", "Depends on #12938\r\n\r\nPart of #12585", "NaN"], ["12992", "Use getLocationService() for AbstractTestHiveClient", "Shixuan Fan", "shixuan-fan", "06/21/19, 10:02:49 PM", "Using locationService member variable directly doesn't work when a test\r\noverwrites getLocationService().", "NaN"], ["12993", "Add empty Hive connector plan optimizer", null, "Phelodas", "06/24/19, 12:30:13 AM", "Changes to Hive Connector in order to enable the Connector Plan Optimization", "NaN"], ["12994", "Fix connector optimizer registration in PlanOptimizers", "Jiexi Lin", "jessesleeping", "06/22/19, 02:05:32 AM", "Use a supplier method instead of an object to initialize PlanOptimizers.", "NaN"], ["12995", "Improve testIllegalStorageFormatDuringTableScan", "Shixuan Fan", "shixuan-fan", "06/22/19, 01:17:51 AM", "Provide both schema name and table name when listing columns. It\r\nis not necessary to list all tables to achieve the goal of the test.", "NaN"], ["12999", "Move ValuesNode to SPI", "James Sun", "highker", "06/24/19, 06:27:40 PM", "NaN", "NaN"], ["13007", "Add sanity checker to check no expression left after optimization", "Ke", "kewang1024", "07/02/19, 01:24:38 AM", "NaN", "NaN"], ["13008", "Support recoverable grouped execution for unbucketed table", "Shixuan Fan", "shixuan-fan", "07/25/19, 07:32:18 PM", "Supercedes https://github.com/prestodb/presto/pull/12099", "NaN"], ["13012", "Restore d3.js for timeline view", null, "mayankgarg1990", "06/26/19, 06:37:24 PM", "Resolves #12361\r\n\r\nThis file was removed in `e2aa9e1` but the timeline.html file still depends on it. Hence restoring it for now.", "NaN"], ["13013", "Add SelectiveStreamReaders for BIGINT, INTEGER, SMALLINT and DATE types", "Maria Basmanova", "mbasmanova", "07/16/19, 10:47:16 PM", "Depends on #12991\r\n\r\nPart of #12585", "NaN"], ["13017", "Unify PlanBuilder::join with only RowExpression", "Ke", "kewang1024", "06/27/19, 01:33:09 AM", "\u2026ssion", "NaN"], ["13018", "Fix a typo in the description of a session property", "Eric Xiao", "ericxiao251", "06/27/19, 01:25:19 PM", "I saw a typo in the description of a session property that I was using. Creating a PR to fix it :D.", "NaN"], ["13019", "Remove temporary table from TableScanNode", "Rebecca Schlussel", "rschlussel", "06/27/19, 10:42:23 PM", "Make all split sources lazy so that we don't need to keep track of\r\ntemporary tables in the TableScanNode\r\n\r\nFixes #12776 ", "NaN"], ["13020", "Do not throw when exception happens in JdbcPageSink::abort", "Rongrong Zhong", "rongrong", "06/28/19, 06:31:38 PM", "Fixes #10644.\r\n\r\nBased on https://github.com/prestosql/presto/pull/479", "NaN"], ["13026", "Add CNF/DNF conversion methods to LogicalRowExpressions", "Akshay Pall", "AkshayPall", "07/02/19, 10:49:41 PM", "This is meant to be used in filter and project pushdown for connectors.\r\nIn the future, adding a method to simplify/remove redundancies should\r\nbe added.", "NaN"], ["13028", "Add documentation on how to use `USING`", "Tal Galili", "talgalili", "07/01/19, 04:08:28 PM", "(4th attempt at this point, combining github online/desktop and terminal did not go well for me...)\r\n\r", "NaN"], ["13029", "Lazily construct error message for CheckState", "Jiexi Lin", "jessesleeping", "06/28/19, 12:47:57 AM", "NaN", "NaN"], ["13031", "Support multibyte character in date_format function", "Yuya Ebihara", "ebyhr", "06/28/19, 05:41:28 PM", "Fix #13022 ", "NaN"], ["13034", "Improve TestHiveRecoverableGroupedExecution", "Shixuan Fan", "shixuan-fan", "07/03/19, 06:42:18 PM", "Extracted from #13008 ", "NaN"], ["13035", "Add functions to compute differential entropy", "Ami Tavory", "atavory", "09/18/19, 05:13:39 PM", "Add a set of differential_entropy functions to compute differential entropy using 4 different methods: \r\n- reservoir sampling, \r\n- weighted reservoir sampling,\r\n- histogram maximum likelihood, \r\n- histogram jacknife-corrected estimation.\r\n\r\nThese implementations have different tradeoffs, and are left to the user. See the documentation (aggregates.rst) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n-----------------\r\n* Add :function:`differential_entropy` functions to compute differential entropy\r\n```", "NaN"], ["13036", "Improve error message when resource group selector cannot be loaded", "Wenlei Xie", "wenleix", "08/29/19, 10:07:06 PM", "```\r\nNo release notes\r\n```", "NaN"], ["13039", "Upgrade to testing-mysql-server 8.0.12-5", "Leiqing Cai", "caithagoras", "07/01/19, 09:03:39 PM", "NaN", "NaN"], ["13041", "Support resubmitting certain verification failures", "Leiqing Cai", "caithagoras", "07/25/19, 09:11:27 PM", "Verification failed with one of the following Presto error will be re-queued. Maximum number of re-queue can be configured through `verification-resubmission.limit`\r\n- HIVE_PARTITION_DROPPED_DURING_QUERY\r\n- HIVE_TABLE_DROPPED_DURING_QUERY", "NaN"], ["13042", "Move histogram tests to the right package", null, "bhhari", "07/02/19, 09:40:09 PM", "These 2 test files should belong to the aggregate package", "NaN"], ["13043", "Support table property override in QueryRewriter", "Leiqing Cai", "caithagoras", "07/09/19, 09:33:33 PM", "NaN", "NaN"], ["13047", "Move row expression above last projectionPushDown", "Yi He", "hellium01", "08/22/19, 10:39:10 PM", "```\r\nRelease note\r\n-------\r\nno user affecting change.  \r\n```\r\nOn top of https://github.com/prestodb/presto/pull/12982", "NaN"], ["13051", "Move RowExpressionNodeInliner to SPI", "Maria Basmanova", "mbasmanova", "07/03/19, 11:15:41 PM", "NaN", "NaN"], ["13052", "Remove ValuesNode verification in ApplyConnectorOptimization", "Shixuan Fan", "shixuan-fan", "07/03/19, 06:17:26 PM", "IndexSourceNode could also hit this code path and the planning would\r\nfail. This verification does not seem useful so we'd rather remove\r\nit instead of enhancing it.", "NaN"], ["13054", "Remove unused import in ApplyConnectorOptimization", "Shixuan Fan", "shixuan-fan", "07/03/19, 08:07:23 PM", "NaN", "NaN"], ["13056", "Add support for Row subscript", null, "bhhari", "07/26/19, 12:45:29 AM", "Backported from https://github.com/prestosql/presto/pull/895\r\n\r\ncc : @wenleix ", "NaN"], ["13057", "Remove unused systemMemoryContext variable from XxxBatchStreamReader", "Maria Basmanova", "mbasmanova", "07/09/19, 02:09:57 PM", "NaN", "NaN"], ["13058", "Fix scalar function example doc", "Lizz", "myl2821", "07/09/19, 06:09:48 PM", "should claim `calledOnNullInput = true` to pass the condition check", "NaN"], ["13064", "Allow special character in MongoDB password", "Yuya Ebihara", "ebyhr", "09/03/19, 04:56:20 PM", "Fix #13048 ", "NaN"], ["13066", "Enable OrcSelectiveRecordReader to evaluate arbitrary filters", "Maria Basmanova", "mbasmanova", "07/23/19, 05:16:09 PM", "With these changes `OrcSelectiveRecordReader` is able to evaluate arbitrary filters. It scans columns with TupleDomain filters first, then scans all the columns that provide input to complex filters and applies complex filters, and finally scans the remaining columns. A future PR will introduce logic to collect selectivity and performance stats on all filters and dynamically change the order in which the filters are applied.\r\n\r\nDepends on #13013\r\n\r\nPart of #12585", "NaN"], ["13069", "Add config to limit buckets for grouped execution", "Rebecca Schlussel", "rschlussel", "07/30/19, 02:45:25 PM", "NaN", "NaN"], ["13070", "Make HiveQueryRunner logging succinct", "Ying", "yingsu00", "07/09/19, 11:09:56 PM", "Reduce logging to avoid hitting 4MB limit on the log size in Travis.\r\nThis limit is reached when more tests using HiveQueryRunner are added.\r\nThe Hive config values were removed, and logging level for several\r\nclasses was increased.\r\n\r\nExample log before the change: https://api.travis-ci.org/v3/job/554950027/log.txt\r\nExample log after the change: https://api.travis-ci.org/v3/job/555581109/log.txt", "NaN"], ["13071", "Fix variable name in Verifier", "Leiqing Cai", "caithagoras", "07/10/19, 06:16:40 PM", "addresses https://github.com/prestodb/presto/pull/13043#discussion_r301752788", "NaN"], ["13072", "Add support for partition and hidden columns to OrcSelectivePageSource", "Maria Basmanova", "mbasmanova", "07/25/19, 04:58:21 PM", "Depends on #13066", "NaN"], ["13073", "Fix long-to-int cast in LiteralInterpreter", "James Sun", "highker", "07/10/19, 03:53:18 PM", "ConstantExpression takes long as its value for some numeric types. It\r\nshould avoid direct cast to integers.", "NaN"], ["13078", "IN predicate for strings", null, "oerling", "07/19/19, 12:11:40 AM", "Adds BytesValues for efficient set membership filter for a string column.\nUses a linear hash table and Bloom filter for set membership checking and works without allocating memory.\nAdds a low-cost hash function over a byte[] that bypasses using Slice for passing a string v iew and uses a cheaper hash function than XxHash64 from Slice.", "NaN"], ["13079", "Use Flatbush RTree for Spatial Joins", "James Gill", "jagill", "08/22/19, 06:22:34 AM", "There are memory problems with spatial joins, potentially due to the\r\nlarge number of objects created by the JTS RTree.  A Flatbush is an\r\nRTree with a low memory footprint, minimal heap allocation, and fast\r\nlookup.  As the name would suggest, it is very flat -- the tree itself\r\nis just a DoubleBigArray, with a couple other small fields.  Lookup is\r\nalso pretty fast: the main difficulty is some index ninjitsu to handle\r\nparent/child relationships.\r\n\r\nThis PR is branched off of #13235, which contains formatting and\r\nother changes that are good independent of this one.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral\r\n--------\r\n* Use Hilbert Packed RTree (Flatbush) instead of the JTS STR RTree for spatial joins.  The Flatbush is a flattened tree that uses vastly fewer heap allocations, and has faster build and query times.\r\n```", "NaN"], ["13082", "Disable PushdownSubfields rule", "Maria Basmanova", "mbasmanova", "07/11/19, 08:04:06 PM", "Turns out PushdownSubfields doesn't interoperate with CBO yet. PushdownSubfields\r\nchanges ColumnHandles, but TableScanStatsRule uses them to look up statistics in\r\na map keyed by unmodified ColumnHandles.\r\n\r\nIntroduced pushdown_subfields_enabled session property with default value 'false'\r\nto control whether PushdownSubfields rule runs or not.", "NaN"], ["13083", "Improve error message when $bucket column is not available", "Andrii Rosa", "arhimondr", "07/11/19, 09:02:07 PM", "NaN", "NaN"], ["13084", "Fix bitwise operations documentation", "Gaston Arguindegui", "gastonar", "07/15/19, 11:52:49 PM", "NaN", "NaN"], ["13085", "Improve exchange materialization configuration experience", "Andrii Rosa", "arhimondr", "07/17/19, 09:01:18 PM", "Improve error messages. Always keep bucketed execution enabled for temporary tables.\r\n\r\nRelated to https://github.com/prestodb/presto/issues/12387, Resolves https://github.com/prestodb/presto/issues/13003", "NaN"], ["13086", "Upgrade kafka version 1.1.1", "Joao Boto", "eskabetxe", "07/12/19, 10:40:08 PM", "upgrade kafka version to the most recent posible", "NaN"], ["13087", "Resolve tables/schemas case-insensitively in JDBC connectors", "Ke", "kewang1024", "08/20/19, 11:07:49 PM", "Fixes #3470\r\nRelates to #2863\r\nIssues #12657\r\n\r\nChanges ported from prestosql/presto#614", "NaN"], ["13089", "Fix hardcoded catalog name in testIgnoreTableBucketing()", "Jiexi Lin", "jessesleeping", "07/15/19, 04:48:06 PM", "NaN", "NaN"], ["13090", "Change section schedulng approach", "Andrii Rosa", "arhimondr", "08/10/19, 12:28:34 PM", "Related to #12387\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13094", "Fix variable expression serde", "James Sun", "highker", "07/16/19, 08:04:03 PM", "Variables were serialized as \"name(type)\". This conflicts with some\r\nspecial literal naming convention like \"$literal$array(integer)\", which\r\nis a legit name. Fix this by using angle brackets \"name<type>\".", "NaN"], ["13096", "Check remote exchange partitioning when planning Union", "Andrii Rosa", "arhimondr", "07/29/19, 03:41:51 PM", "Single fragment cannot contain remote sources with incompatible partitioning\r\nhandles", "NaN"], ["13099", "Add SelectiveStreamReader for LIST type", "Maria Basmanova", "mbasmanova", "08/13/19, 12:08:48 AM", "Introduce SelectiveStreamReader for LIST type with support for top level filters (`c IS NULL` and `c IS NOT NULL`) and range filters on elements multiple levels deep (`c[1] = 5`, `c[1][2] > 10`). \r\n\r\nThe values of LIST type are encoded using 3 streams: \r\n- is-null (present) stream has an entry for each row which specified whether the value in the row is null or not\r\n- lengths stream has an entry for each non-null row which specified the size of the array value; length of zero indicates an empty array\r\n- data stream represents a flat list of array elements\r\n\r\n<img width=\"811\" alt=\"Screen Shot 2019-07-25 at 10 16 26 AM\" src=\"https://user-images.githubusercontent.com/27965151/61881866-5b982a00-aec5-11e9-8a5c-85b6dd693d37.png\">\r\n\r\n`ListSelectiveStreamReader` reads the is-null and lengths streams and generates an array of positions in the data stream to read. It then creates a `PositionalFilter` that applies range filters to the *right* positions in the data stream.\r\n\r\n<img width=\"809\" alt=\"Screen Shot 2019-07-25 at 10 16 37 AM\" src=\"https://user-images.githubusercontent.com/27965151/61881869-5dfa8400-aec5-11e9-9354-02849d8d22c3.png\">\r\n\r\nList types can be composed to create nested lists:\r\n\r\n<img width=\"932\" alt=\"Screen Shot 2019-07-25 at 10 41 15 AM\" src=\"https://user-images.githubusercontent.com/27965151/61883722-fba38280-aec8-11e9-95d2-b7201ec2dfb0.png\">\r\n\r\nIn such cases, multiple `ListSelectiveStreamReader`s work together. Each reader is responsible for one level in the hierarchy. Reader at a higher level provides `NullsFilter` to the reader and the next level to allow for applying IS [NOT] NULL filters at mid levels, e.g. `a[1] is null AND a[2][3] > 10`.\r\n\r\nDepends on #13072 \r\n\r\nPart of #12585", "NaN"], ["13100", "Add release notes for 0.222", null, "mayankgarg1990", "07/18/19, 09:32:39 PM", "Issue #12945", "NaN"], ["13101", "Make BlockFlattener tests single threaded", "Timothy Meehan", "tdcmeehan", "07/18/19, 09:21:31 PM", "@rschlussel @mayankgarg1990 @mbasmanova \r\n\r\nThe allocator in this test is intentionally reused across all tests to help catch problems related to requesting/releasing arrays.  Therefore I'd prefer to keep it that way and just ensure access is single threaded.", "NaN"], ["13107", "Math mode in restructured text docs", "Ami Tavory", "atavory", "08/06/19, 09:08:35 PM", "The docs contain some math that is marked as code. reStructuredText [supports math for several years](https://github.com/atavory/presto), so this diff changes two instances to use the `math` tag.\r\n\r\n![image](https://user-images.githubusercontent.com/7824605/61710765-352c9000-ad07-11e9-9c9c-291de5946afc.png)\r", "NaN"], ["13109", "Add release notes for 0.223", "Varun Gajjala", "varungajjala", "07/25/19, 07:09:13 PM", "#13103", "NaN"], ["13110", "Secure nextUri with a slug", "Rebecca Schlussel", "rschlussel", "07/23/19, 09:27:10 PM", "Backport https://github.com/prestosql/presto/pull/561", "NaN"], ["13112", "Row expression predicate pushdown", "Yi He", "hellium01", "10/20/19, 06:03:23 AM", "Pushdown RowExpression predicates. On top of #13560", "NaN"], ["13117", "Fix accidental removal nested type only cast", "Yi He", "hellium01", "07/24/19, 02:58:23 AM", "This is a short term fix for #13116. \r\nWe should rethink how we handle type only cast(which will be tracked on #12891)\r", "NaN"], ["13120", "Fix QueryContext.getAdditionalFailureInfo bug", "Ke", "kewang1024", "08/22/19, 05:38:13 PM", "For queryAllocations(a map) that tracks all the Operator and\r\nits consuming memory for per query, it consists of a special key\r\nFORCE_FREE_OPERATION with a negative number as its value\r\nfor memory management purpose. When reading queryAllocations,\r\nwe should skip key FORCE_FREE_OPERATION.", "NaN"], ["13123", "Fix CPU time and Scheduled time in PlanPrinter", "Yuan Mei", "curcur", "07/25/19, 05:02:21 PM", "NaN", "NaN"], ["13124", "Export the reason of skipping test cases in Verifier", "Leiqing Cai", "caithagoras", "07/25/19, 08:38:03 PM", "NaN", "NaN"], ["13127", "Remove typeOnly optimization from SqlToRowExpressionOptimizer", "Yi He", "hellium01", "07/27/19, 01:08:08 AM", "This is a permanent fix for https://github.com/prestodb/presto/issues/13116", "NaN"], ["13128", "Fix query failure for certain collated join", "Wenlei Xie", "wenleix", "07/31/19, 05:18:47 AM", "Colodated join fails under the following conditions:\r\n- Colocated join\r\n- Ungrouped execution\r\n- Probe side is empty\r\n\r\nThis failure happens prior to 4d56e365e4a726248179b89d3b631b3baf3fe7b2.\r\nWhile this commit aims at fixing some corner cases for grouped\r\nexecution, it also solves this ungrouped execution as a by-product.\r\n\r\nThe later commit 054c7d2884531e119c5251478055f45158cfe622 effectively\r\nreverts 4d56e365e4a726248179b89d3b631b3baf3fe7b2 while solving the\r\noriginal grouped execution issue with a different approach.. However,\r\nit does introduce regression for collated join + ungrouped execution.", "NaN"], ["13130", "Fix authorship for row subscript", "James Sun", "highker", "07/26/19, 08:39:45 PM", "Authorship is not set correclty in https://github.com/prestodb/presto/pull/13056\r\n\r\nThe PR is to correct the authorship", "NaN"], ["13134", "Add extra credentials", "Ke", "kewang1024", "07/27/19, 09:22:12 PM", "Changes ported from prestosql/presto#124\r\n\r", "NaN"], ["13135", "Symbol cleanup", "Rongrong Zhong", "rongrong", "07/31/19, 01:28:08 AM", "More clean up. At this point all reference to `Symbol` in planner are related to `Expression`, which probably makes more sense to wait until the `Expression` to `RowExpression` refactoring is done. I'll continue with reducing `Symbol` in tests.", "NaN"], ["13136", "Pass session to JdbcClient#toWriteMapping", "Ke", "kewang1024", "07/31/19, 12:05:31 AM", "Co-Authored-By: Ke Wang <ke1024@fb.com>", "NaN"], ["13137", "Update properties reference to add oom killer policy", "Nezih Yigitbasi", "nezihyigitbasi", "08/07/19, 03:09:27 AM", "NaN", "NaN"], ["13138", "Add RemoveRedundantIdentityProjections after connector optimizer", "James Sun", "highker", "07/30/19, 07:07:49 PM", "NaN", "NaN"], ["13140", "Optionally ignore session zone from client", null, "aweisberg", "08/05/19, 01:38:59 PM", "Backport of https://github.com/prestosql/presto/pull/1164\r\n\r\nPR:\r\n\r\nCo-authored-by: Ariel Weisberg <aweisberg@fb.com>", "NaN"], ["13141", "Create empty files in parallel for bucketed table", "Leon Yang", "lnyng", "07/31/19, 08:29:40 PM", "Buckets of a bucketed table in Hive were initialized as empty files serially,\r\nwhich is very slow.\r\n\r\nNow the empty file creation logic in HiveMetadata is factored out as\r\nHiveEmptyFileCreator, which creates files in parallel in a configurable\r\nway similar to HiveStagingFileCommitter. By default the executor has\r\nmaximum 100 threads to create files, and can be configured by variable\r\nhive.max-concurrent-file-creates. Considering a 50ms file creation\r\nlatency, this feature significantly improves the performance.", "NaN"], ["13144", "Backport - Handle race in QueryStateTimer", "Rebecca Schlussel", "rschlussel", "07/30/19, 03:01:33 PM", "In rare cases two threads can attempt to update the timing states with\r\ndifferent \"now\" times, and this can result in a negative duration.\r\n\r\nFixes #13142 ", "NaN"], ["13145", "Disable TestHiveRecoverableGroupedExecution", "Shixuan Fan", "shixuan-fan", "07/30/19, 02:54:14 PM", "TestHiveRecoverableGroupedExecution has become flaky and it is\r\nvery likely due to force closing worker in DistributedQueryRunner\r\nwon't close cleanly and thus impact later tests. We should\r\nreintroduce this test when we could deflake it, for example,\r\nhave a better way to inject failure.", "NaN"], ["13146", "Add release notes template", "Leiqing Cai", "caithagoras", "08/02/19, 07:18:09 PM", "```\r\n== NO RELEASE NOTES ==\r\n```", "NaN"], ["13154", "Add Planning Time in Query Details Page", null, "aweisberg", "07/31/19, 04:30:23 PM", "Cherry-pick of https://github.com/prestosql/presto/pull/1115\r\n\r\nCo-authored-by: Ariel Weisberg <aweisberg@fb.com>", "NaN"], ["13155", "Cherry-pick documentation for cost, statistics, CBO", null, "aweisberg", "01/07/20, 08:50:07 PM", "Cherry-pick of prestosql/presto#127\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13157", "Fix NPE in spatial joins with GeometryCollections", "James Gill", "jagill", "08/12/19, 06:51:22 PM", "The code which gets a (JTS) Envelope from a GeometryCollection\r\nraised an NPE: getting an Esri Envelope from an OgcGeometry that\r\nwraps a GeometryCollection returns null.  We correct for this\r\nin the GeometryUtils.getEnvelope; this commit delegates getting\r\nthe Esri envelope to that function, and converts it to a JTS\r\nEnvelope in GeometryUtils.  It also adds some tests, including the\r\nbreaking GeometryCollection (a reduced test case that caused the\r\nbug in production).", "NaN"], ["13158", " Improve Presto Query Page UI", "Yupeng Zhang", "nausicaasnow", "08/06/19, 08:57:16 PM", "Distribute the tasks into every stage and only display when expanding stage summary, which is to avoid tasks flushing, the auto-fresh behaviors of tasks are now consistent with stage's.\r\n\r\nThe 'None' is removed from task filter, the default value now is 'ALL'.\r\n\r\n![image](https://user-images.githubusercontent.com/13233335/62167261-ac4dbf80-b2d7-11e9-9cb6-a0865c5062ee.png)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/13233335/62167224-9809c280-b2d7-11e9-8312-edd7e46d3dce.png)\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Display tasks per stage on the query page\r\n```\r", "NaN"], ["13159", "Fix travis product test stability", "Rebecca Schlussel", "rschlussel", "08/06/19, 03:55:18 PM", "This is a cherry-pick of fixes related to https://github.com/prestosql/presto/issues/683 with the goal of improving the stability of product tests on travis.  In particularly hopefully by improving the memory usage, there will be fewer failures due to no output received. ", "NaN"], ["13165", "Compact tupple domain before sending", "Andrii Rosa", "arhimondr", "07/31/19, 06:09:29 PM", "TupleDomain compaction reduces a long list of well specified values to a\r\nrange if the number of values threshold is crossed.\r\n\r\nThe compaction has been moved as part of the 76ea27e commit.\r\n\r\neffectivePredicate is carried to a worker with a HiveSplit. HiveSplit\r\nmust be serialized into JSON on the coordinator. Without a compaction\r\nthe serialization of large tupple domains (100+ values) results in\r\nexcessive CPU usage on coordinator.", "NaN"], ["13166", "Add revocable memory and reserved memory pool docs to spill", null, "aweisberg", "08/12/19, 01:12:24 PM", "Cherry-pick of https://github.com/prestosql/presto/pull/285 and\r\nhttps://github.com/prestosql/presto/pull/1226\r\n\r\nCo-authored-by: praveenkrishna <praveenkrishna@tutanota.com>\r\nCo-authored-by: Ariel Weisberg <aweisberg@fb.com>", "NaN"], ["13169", "Fix display of Non-Heap memory usage in UI", null, "aweisberg", "08/12/19, 01:12:50 PM", "Backport of https://github.com/prestosql/presto/pull/1225\r\n\r\nCo-authored-by: Ariel Weisberg <aweisberg@fb.com>", "NaN"], ["13170", "Compact effective predicate for partitioned table", "Andrii Rosa", "arhimondr", "07/31/19, 09:15:35 PM", "NaN", "NaN"], ["13171", "Release 0.223.1 backport", "Andrii Rosa", "arhimondr", "07/31/19, 08:25:14 PM", "NaN", "NaN"], ["13172", "Fix flaky testVirtualBucketing()", "Jiexi Lin", "jessesleeping", "08/01/19, 03:15:35 PM", "Instead of asserting on results we assert the plan shape.\r\n\r\n#13152 ", "NaN"], ["13173", "Add logging for hanging tests", "Rebecca Schlussel", "rschlussel", "08/08/19, 06:24:04 PM", "Cherry-picked from https://github.com/prestosql/presto/pull/840. This will enable us to gather more information for #13168 ", "NaN"], ["13175", "Throw exception when creating an empty spatial partition", "James Gill", "jagill", "08/14/19, 08:25:01 PM", "If the spatial partition query yielded 0 rows, an empty spatial\r\npartition would be made.  It's unclear what an empty spatial partition\r\nis!  Correspondingly, it was serialized as null, and then caused errors\r\nduring deserialization and use.\r\n\r\nThis throws an exception in the case that the spatial partition\r\nis trying to be created with no rows.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral\r\n--------\r\n\r\n* Throw an exception when a spatial partition is made with 0 rows. \r\n```", "NaN"], ["13176", "Pass JdbcIdentity To DriverConnectionFactory#openConnection", "Ke", "kewang1024", "08/06/19, 01:29:55 AM", "Co-Authored-By: Ke Wang <ke1024@fb.com>", "NaN"], ["13181", "Enable setQueryMaxTotalMemory for TestingTaskContext", "Ying", "yingsu00", "08/02/19, 12:14:57 AM", "Adding setQueryMaxTotalMemory to avoid \"Query exceeded per-node total memory limit of 512MB\" error when running the new BenchmarkPartitionedOutputOperator", "NaN"], ["13183", "Optimize PartitionedOutputOperator", "Ying", "yingsu00", "10/10/19, 05:31:00 AM", "Github issue #13015\r\n\r\nThe new OptimizedPartitionedOutputOperator achieves better performance by\r\nskipping block building. It appends data directly to output buffers, then wraps\r\nthese buffers into SerializedPage.\r\n\r\nInternal shadow run shows 2x improvement in the operator CPU usage, and 5%\r\nCPU improvement over all queries.\r\n\r\nBenchmark results show 1.5x - 3.7x improvements on 5000 pages (unit ms/op):\r\n\r\n|Type\t\t\t|\t     hasNull | channels | baseline | optimized | Gain (x) |\r\n|-------------------| --------------- | -------- | --------- | ---------- | --------|\r\n| BIGINT\t\t\t|\t\tFALSE |\t1\t |  2,677 \t |  882 |\t 3.0 |\r\n| BIGINT\t\t\t|\t\tFALSE |\t2\t|  3,961 \t| 1,084 |\t 3.7 |\r\n| BIGINT\t\t | \t\tTRUE | \t1 | \t 3,348 |  \t 1,245 |  \t 2.7 | \r\n | BIGINT\t\t | \t\t\tTRUE | \t2 | \t 4,375  | \t 1,557  | \t 2.8 | \r\n | BOOLEAN\t | \t\t\t\tFALSE\t | 1\t |  2,915 \t |    873  | \t 3.3 | \r\n | BOOLEAN\t | \t\t\t\tFALSE\t | 2\t  | 3,818 \t |  1,140  |  3.3 | \r\n | BOOLEAN\t | \t\t\t\tTRUE | \t1\t |  3,003 \t  | 1,112 \t |  2.7 | \r\n | BOOLEAN\t | \t\t\t\tTRUE | \t2\t |  4,084 \t |  1,553 \t |  2.6 | \r\n | INTEGER\t | \t\t\t\tFALSE | \t1 | \t 2,804  | \t   954  | \t 2.9 | \r\n | INTEGER\t | \t\t\t\tFALSE | \t2\t |  3,921 |  \t 1,101  | \t 3.6 | \r\n | INTEGER\t | \t\t\t\t TRUE\t | 1\t |  3,143  | \t 1,181  | \t 2.7 | \r\n | INTEGER\t | \t\t\t\tTRUE\t | 2\t  | 4,442 \t |  1,542  | \t 2.9 | \r\n | LONG_DECIMAL\t | \t\t\tFALSE | \t1\t |  3,697 \t  | 1,118 \t |  3.3 | \r\n | LONG_DECIMAL\t | \t\t\tFALSE | \t2 | \t 5,550 \t |  1,476 \t |  3.8 | \r\n | LONG_DECIMAL\t | \t\t\tTRUE | \t1\t |  3,654  | \t 1,311  | \t 2.8 | \r\n | LONG_DECIMAL\t | \t\t\tTRUE\t | 2\t  | 6,089  | \t 1,821  | \t 3.3 | \r\n | SMALLINT\t | \t\t\tFALSE\t | 1\t |  2,821 \t |  1,032  | \t 2.7 | \r\n | SMALLINT\t | \t\t\tFALSE\t | 2\t  | 4,007 \t |  1,239 \t  | 3.2 | \r\n | SMALLINT\t | \t\t\tTRUE | \t1\t |  2,908 \t |  1,141 \t  | 2.5 | \r\n | SMALLINT\t | \t\t\tTRUE | \t2\t |  4,244 \t  | 1,580 \t |  2.7 | \r\n | VARCHAR\t | \t\t\t\tFALSE | \t1\t |  5,821 \t  | 1,972 \t |  3.0 | \r\n | VARCHAR\t | \t\t\t\tFALSE | \t2\t | 10,021   |  3,117 \t |  3.2 | \r\n | VARCHAR\t | \t\t\tTRUE\t | 1\t  | 5,732  | \t 2,226  | \t 2.6 | \r\n | VARCHAR\t | \t\t\t\tTRUE | \t2\t |  9,484 \t  | 3,436 \t  | 2.8 | \r\n | DICTIONARY(BIGINT)\t | \t\tFALSE\t | 1\t |  1,694 \t    | 628  |  2.7 | \r\n | DICTIONARY(BIGINT)\t\t | \tFALSE | \t2\t |  2,450 \t |    822  | \t 3.0 | \r\n | DICTIONARY(BIGINT)\t\t | \tTRUE | \t1\t |  1,940 \t |    823  | \t 2.4 | \r\n | DICTIONARY(BIGINT)\t\t | \tTRUE | \t2\t  | 2,960  | \t 1,096  | \t 2.7 | \r\n | RLE(BIGINT)\t\t | \t\tFALSE | \t1 | \t 1,663  | \t   592 |  \t 2.8 | \r\n | RLE(BIGINT)\t\t | \t\tFALSE | \t2 | \t 2,404  | \t   753  | \t 3.2 | \r\n | RLE(BIGINT)\t\t | \t\tTRUE | \t1 | \t 1,645  | \t   684 |  \t 2.4 | \r\n | RLE(BIGINT)\t\t | \t\tTRUE | \t2 | \t 2,429  | \t   851  | \t 2.9 | \r\n | ARRAY(BIGINT)\t | \t\t\tFALSE | \t1 | \t 1,128  | \t   660 |  \t 1.7 | \r\n | ARRAY(BIGINT)\t | \t\t\tFALSE | \t2 | \t 1,924  | \t   958  | \t 2.0 | \r\n | ARRAY(BIGINT)\t\t | \t\tTRUE | \t1 | \t 1,190  | \t   702  | \t 1.7 | \r\n | ARRAY(BIGINT) | \t\t\t\tTRUE | \t2 | \t 2,002 |  \t 1,160 |  \t 1.7 | \r\n | ARRAY(VARCHAR) | \t\t\t\tFALSE | \t1 | \t 2,049  | \t 1,204 |  \t 1.7 | \r\n | ARRAY(VARCHAR) | \t\t\t\tFALSE | \t2 | \t 3,937 |  \t 2,162 |  \t 1.8 | \r\n | ARRAY(VARCHAR) | \t\t\t\tTRUE | \t1 | \t 1,913 |  \t 1,216  | \t 1.6 | \r\n | ARRAY(VARCHAR) | \t\t\t\tTRUE | \t2 | \t 3,482 |  \t 2,177  | \t 1.6 | \r\n | ARRAY(ARRAY(BIGINT)) | \t\t\tFALSE | \t1 | \t 2,148  | \t 1,290 |  \t 1.7 | \r\n | ARRAY(ARRAY(BIGINT)) | \t\t\tFALSE | \t2 | \t 4,271  | \t 2,430 |  \t 1.8 | \r\n | ARRAY(ARRAY(BIGINT)) | \t\t\tTRUE | \t1 | \t 2,122  | \t 1,441  | \t 1.5 | \r\n | ARRAY(ARRAY(BIGINT)) | \t\t\tTRUE | \t2 | \t 3,937 |  \t 2,495 |  \t 1.6 | \r\n | MAP(BIGINT,BIGINT) | \t\t\tFALSE | \t1 | \t 2,266 |  \t   838 |  \t 2.7 | \r\n | MAP(BIGINT,BIGINT) | \t\t\tFALSE | \t2 | \t 4,468 |  \t 1,441  | \t 3.1 | \r\n | MAP(BIGINT,BIGINT) | \t\t\tTRUE | \t1 | \t 2,137  | \t   936 |  \t 2.3 | \r\n | MAP(BIGINT,BIGINT) | \t\t\tTRUE | \t2 | \t 4,165 |  \t 1,644 |  \t 2.5 | \r\n | MAP(BIGINT,MAP(BIGINT,BIGINT)) | \t\tFALSE | \t1 | \t 5,941 |  \t 2,154 |  \t 2.8 | \r\n | MAP(BIGINT,MAP(BIGINT,BIGINT)) | \t\tFALSE | \t2 | \t11,707 |    4,564  | \t 2.6 | \r\n | MAP(BIGINT,MAP(BIGINT,BIGINT)) | \t\tTRUE | \t1 | \t 5,301  | \t 2,060  | \t 2.6 | \r\n | MAP(BIGINT,MAP(BIGINT,BIGINT)) | \t\tTRUE | \t2 | \t10,225  |   4,169 |  \t 2.5 | \r\n | ROW(BIGINT,BIGINT) | \t\t\tFALSE | \t1 | \t 1,239 |  \t   575  | \t 2.2 | \r\n | ROW(BIGINT,BIGINT) | \t\t\tFALSE | \t2 | \t 2,289  | \t   933  | \t 2.5 | \r\n | ROW(BIGINT,BIGINT) | \t\t\tTRUE | \t1 | \t 1,354  | \t   721  | \t 1.9 | \r\n | ROW(BIGINT,BIGINT)\t | \t\tTRUE | \t2 | \t 2,380  | \t 1,121  | \t 2.1 | \r\n | ROW(ARRAY(BIGINT),ARRAY(BIGINT)) | \tFALSE | \t1 | \t 2,349 |  \t 1,238 |  \t 1.9 | \r\n | ROW(ARRAY(BIGINT),ARRAY(BIGINT)) | \tFALSE | \t2 | \t 4,647  | \t 2,476  | \t 1.9 | \r\n | ROW(ARRAY(BIGINT),ARRAY(BIGINT)) | \tTRUE | \t1 | \t 2,412  | \t 1,568  | \t 1.5 | \r\n | ROW(ARRAY(BIGINT),ARRAY(BIGINT)) | \tTRUE | \t2 | \t 4,698  | \t 3,093  | \t 1.5 | \r\n\r\n### Updates\r\nSept 30 2019\r\n - Fixed \"position is not valid\" for MapBlock\r\n - Removed FIXED_WIDTH_TYPE_SERIALIZED_BYTES in OptimizedPartitionedOutputOperator\r\n - Removed \"Add getRetainedSizeInBytes to BlockFlattener\". This is because the arrays are already accounted for in the DecodeBlock.\r\n\r\nSept 26 2019\r\n - Added BlockEncodingBuffer as an interface and AbstractBlockEncodingBuffer as a super class to all XXXBlockEncodingBuffer.\r\n - Renamed XXXBlockEncodingBuffers to XXXBlockEncodingBuffer\r\n - Reordered methods in the BlockEncodingBuffer and AbstractBlockEncodingBuffer and all XXXBlockEncodingBuffer\r\n - Added memory tracking for BlockFlattener, which is owned by OptimizedPartitionedOutputOperator.PagePartitioner\r\n - Introduced ExpansionOption enum in Arrays\r\n - Removed \"Copy JvmUtils to presto-spi\"\r\n - Addressed other comments from @arhimondr \r\n\r\nSept 23 2019\r\n - Fixed  GENERIC_INTERNAL_ERROR \"Invalid position x in block with y positions\" in accumulateRowSizes()) in two commits 3a08823 and 83ca97\r\n - Fixed  INVALID_CAST_ARGUMENT in ae0277d25c\r\n\r\nSept 16 2019\r\n - Changed SerializedPage size from 200B to 10000B for TestHiveDistributedQueriesWithOptimizedRepartitioning to make it run faster.\r\n\r\nSept 13 2019\r\nMoved the operator and BlockEncodingBuffers to operator/repartition folder\r\n\r\nSept 10 2019\r\n  - Removed columnarArrayBaseOffset in populateNestedPositions() from ArrayBlockEncodingBuffers and MapBlockEncodingBuffers after df67792817 is merged (Adjust getOffset output in Columnar classes for trimmed element blocks)\r\n\r\nSept 09 2019\r\n - Fixed a bug where calculateRowSizes fails for REAL type.\r\n - Removed early initialization on OptimizedPartitionedOutputOperator#PartitionBuffer.positions and OptimizedPartitionedOutputOperator#PartitionBuffer.rowSizes(now renamed to serializedRowSizeInBytes) to fix small queries performance regression. Internal shadow run on cluster with 340 nodes show 2x CPU gain in the operator and no visible regression in small queries.\r\n\r\n### TODO\r\n - We need to decided whether to use the ColumnarArray/Map/Row objects to expose the offsets and child blocks from the Array/Map/Row Blocks, and whether to do the same to VariableWidthBlock. Based on what we agree we will make the change accordingly.\r\n - We want connectors to be able to provide custom BlockEncodingBuffer implementations for the custom blocks. One way of doing this is to move the BlockEncodingBuffer implementations to presto-spi but these buffers are coupled with the operator and are stateful. How to achieve the goal is an open question yet.\r\n - PartitionedOutputOperator reports incorrect output data size #11770\r\n\r\n### Previous WIP PR \r\nhttps://github.com/prestodb/presto/pull/13032", "NaN"], ["13184", "Move TranslateExpression above SimplifyRowExpression", "Yi He", "hellium01", "09/27/19, 02:34:01 AM", "On top of #13047 (after commit \"Move TranslateExpressions above last projectionPushDown\"). \r\n```\r\n== NO RELEASE NOTE ==", "NaN"], ["13187", "Improvements to classification metric docs", "Ami Tavory", "atavory", "08/12/19, 05:43:53 PM", "Improvements to classification-metric UDF docs:\r\n\r\n1. Better instructions using `CROSS JOIN UNNEST` instead of `MAP`\r\n2. Code examples\r\n3. Separate subsection to group related functions\r\n<img width=\"1073\" alt=\"Screen Shot 2019-08-10 at 23 08 34\" src=\"https://user-images.githubusercontent.com/7824605/62826428-ff6f1e80-bbc3-11e9-89cf-34729df72e01.png\">\r\n<img width=\"1154\" alt=\"Screen Shot 2019-08-10 at 23 08 45\" src=\"https://user-images.githubusercontent.com/7824605/62826429-ff6f1e80-bbc3-11e9-97cb-5c03f3271e52.png\">\r\n<img width=\"1219\" alt=\"Screen Shot 2019-08-10 at 23 08 54\" src=\"https://user-images.githubusercontent.com/7824605/62826431-1c0b5680-bbc4-11e9-9c48-8b9d93c95c79.png\">\r\n\r\n\r\n\r", "NaN"], ["13189", "Move LimitNode to SPI", "Akshay Pall", "AkshayPall", "08/15/19, 12:25:35 AM", "Move LimitNode, TopNNode, and OrderingScheme to presto-spi module. This\r\nwill allow visiting and thus pushdown of limits at the connector level.\r\n\r\nFiles with real changes (not just imports):\r\n- ApplyConnectorOptimization.java\r\n- Util.java\r\n- PlanVisitor.java\r\n- LimitNode.java\r\n- TopNNode.java\r\n- OrderingScheme.java\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13190", "Decrease default maximum concurrent materializations", "Andrii Rosa", "arhimondr", "08/10/19, 12:28:54 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13192", "Using reStructuredText citations", "Ami Tavory", "atavory", "08/07/19, 08:51:18 PM", "Using rst [citations](http://docutils.sourceforge.net/docs/user/rst/quickref.html#citations).", "NaN"], ["13193", " Add ByteSelectiveStreamReader", null, "bhhari", "08/09/19, 08:10:57 PM", "This commit adds a new SelectiveStreamReader for TINYINT type.\r\nFilters the values during the read.\r\nThis PR is part 1 of the #13174", "NaN"], ["13195", "Add release note for 0.224", "Shixuan Fan", "shixuan-fan", "08/12/19, 02:10:29 PM", "#13129 ", "NaN"], ["13196", "GCP related backports", "Zhenxiao Luo", "zhenxiao", "08/15/19, 07:28:50 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* JSON key file used to access Google Cloud Storage\r\nhive.gcs.json-key-file-path=/path/to/gcs_keyfile.json\r\n\r\n* Use client-provided OAuth token to access Google Cloud Storage\r\nhive.gcs.use-access-token=false\r\n```\r", "NaN"], ["13199", "Remove BeforeMethod annotation from parallel test", "Rebecca Schlussel", "rschlussel", "08/09/19, 06:12:11 PM", "I missed one in #13173 \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13206", "Multiple Verifier Improvements", "Leiqing Cai", "caithagoras", "08/15/19, 10:55:56 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support for overriding session properties for all queries in a suite.\r\n* Add cluster type and retryable information for ``QueryFailure``.\r\n* Add final query failure information to Verifier output event.\r\n```\r", "NaN"], ["13207", " Refactor DirectoryLister", "Timothy Meehan", "tdcmeehan", "08/16/19, 04:09:04 PM", "Push directory recursion into DirectoryLister to allow for a\r\nhypothetical implementation of DirectoryLister to make better decisions\r\nas to when to recurse subdirectories and when to simply list files.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13208", "Add DoubleSelectiveStreamReader", "Sahar Massachi", "sayhar", "08/29/19, 06:21:36 PM", "Fixes #13191\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n````", "NaN"], ["13209", "Fix the Raptor connector to error out on unsupported type", "Sreeni Viswanadha", "kaikalur", "08/13/19, 11:45:09 PM", "Please fill in the release notes towards the bottom of the PR description. See [README](https://github.com/prestodb/presto/blob/master/README.md#release-notes) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nRaptor changes\r\n* Error out when creating tables with unsupported types like ROW type.\r\n\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13210", "Run allocationListener on trySetBytes and close", null, "aweisberg", "08/13/19, 05:59:36 PM", "Co-authored-by: Ariel Weisberg <aweisberg@fb.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix leak in operator peak memory computations. (:issue:`13210`)\r\n```", "NaN"], ["13213", "Add TimestampSelectiveStreamReader", null, "bhhari", "08/14/19, 08:02:17 PM", "```\r\nBenchmark                                       (typeSignature)  (withNulls)  Mode  Cnt  Score    Error  Units\r\nBenchmarkSelectiveStreamReaders.read                  timestamp         true  avgt   60  0.115 \u00b1  0.001   s/op\r\nBenchmarkSelectiveStreamReaders.read                  timestamp        false  avgt   60  0.177 \u00b1  0.002   s/op\r\nBenchmarkSelectiveStreamReaders.readAllNull           timestamp          N/A  avgt   60  0.003 \u00b1  0.001   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter        timestamp         true  avgt   60  0.114 \u00b1  0.001   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter        timestamp        false  avgt   60  0.198 \u00b1  0.004   s/op\r\n\r\nBenchmark                                          Mode  Cnt  Score   Error  Units\r\nBenchmarkBatchStreamReaders.readTimestampNoNull    avgt   60  0.188 \u00b1 0.005   s/op\r\nBenchmarkBatchStreamReaders.readTimestampWithNull  avgt   60  0.166 \u00b1 0.003   s/op\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13214", "Make subfield pruning work with CBO", "Maria Basmanova", "mbasmanova", "08/14/19, 04:00:19 PM", "Fix the issue discussed in #13082 \r\n\r\n```\r\n== RELEASE NOTES ==\r\nSPI Changes\r\n-------------\r\n* Add columnHandles parameter to `ConnectorMetadata.getTableStatistics` method. \r\nThe new parameter allows connectors to implement efficient pruning of statistics to the \r\ndesired list of columns and subfields and fixes compatibility issue between subfield pruning \r\nand CBO (#13082).\r\n```\r", "NaN"], ["13215", "Add FloatSelectiveStreamReader", null, "bhhari", "08/22/19, 09:48:36 PM", "```\r\nBenchmark                                       (typeSignature)  (withNulls)  Mode  Cnt  Score    Error  Units\r\nBenchmarkSelectiveStreamReaders.read                       real         true  avgt   20  0.134 \u00b1  0.005   s/op\r\nBenchmarkSelectiveStreamReaders.read                       real        false  avgt   20  0.199 \u00b1  0.003   s/op\r\nBenchmarkSelectiveStreamReaders.readAllNull                real          N/A  avgt   20  0.003 \u00b1  0.001   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter             real         true  avgt   20  0.148 \u00b1  0.012   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter             real        false  avgt   20  0.217 \u00b1  0.014   s/op\r\n\r\nBenchmark                                      Mode  Cnt  Score   Error  Units\r\nBenchmarkBatchStreamReaders.readFloatNoNull    avgt   60  0.227 \u00b1 0.006   s/op\r\nBenchmarkBatchStreamReaders.readFloatWithNull  avgt   60  0.170 \u00b1 0.003   s/op\r\n ```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13217", "Remove predicate information from HiveSplit", "Maria Basmanova", "mbasmanova", "08/15/19, 07:52:44 AM", "Pushed down filter used to be transported from coordinator to workers via splits. For complex filter expressions, serializing these expressions for each split consumed a lot of CPU on the coordinator and made it non-responsive. This change is to remove predicate information from the split and propagate it via `TableHandle#layout` which is sent only once for each task as part of the plan fragment (`TableScanNode#table`).\r\n\r\nSee also: #13165\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n-----------\r\n\r\n* Add `TableHandle` parameter to `ConnectorPageSourceProvider#createPageSource` method. This gives connector access to `ConnectorTableLayoutHandle` during execution.\r\n```", "NaN"], ["13222", "Cherry pick 142", null, "aweisberg", "08/14/19, 12:32:33 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Improve performance for positioned reads on S3 :pr:`13222`\r\n```", "NaN"], ["13224", "Fix reporting of output data size for PartitionedOutputOperator", "Ajay George", "ajaygeorge", "08/20/19, 01:11:32 PM", "This PR is to fix the PartitionedOutputOperator's incorrect output data size.\r\nWe used to record the output size of the PartitionedOutputOperator the same as input size. This could be wrong for pages containing Dictionary/RLE blocks because the Dictionary/RLE wrapping would be ripped out during repartitioning, or when rows are replicated. We now record the output size based on the actual output sizes of the pageBuilders.\r\n\r\nFixes #11770\r\n\r\n`EXPLAIN ANALYZE` command now shows the correct data size.\r\n\r\ne.g.\r\n```\r\nEXPLAIN ANALYZE \r\nSELECT checksum(l.shipmode) \r\nFROM lineitem l, partsupp p \r\nWHERE l.partkey = p.partkey AND l.suppkey = p.suppkey AND p.availqty < 1000;\r\n```\r\nBefore this fix \r\n```\r\nFragment 3 [SOURCE]\r\n     CPU: 162.60ms, Scheduled: 289.28ms, Input: 60175 rows (1.27MB); per task: avg.: 20058.33 std.dev.: 25721.42, Output: 60175 rows (1.78MB)\r\n```\r\nAfter this fix\r\n\r\n```\r\nFragment 3 [SOURCE]\r\n     CPU: 188.25ms, Scheduled: 310.65ms, Input: 60175 rows (1.27MB); per task: avg.: 20058.33 std.dev.: 19657.87, Output: 60175 rows (2.08MB)\r\n```\r\nAfter the fix the Output size is correctly recorded as 2.08 MB instead of the original 1.78 MB\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral\r\n--------\r\n* Fix the issue that fragment's output size was under-recorded for cases where input data is dictionary or run-length encoded, or the rows are replicated(#11770)\r\n```\r\n\r\n\r", "NaN"], ["13225", "Point release notes template and README to Release Notes Guidelines", "Leiqing Cai", "caithagoras", "08/14/19, 09:25:43 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13227", "Update SCM in pom.xml to point to prestodb", null, "aweisberg", "08/14/19, 05:08:46 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13228", "Enforce task count limit", "Vic Zhang", "viczhang861", "08/21/19, 11:59:06 AM", "Too many tasks cause stability issues like frequent GC,  for query that exceeds an unreasonable high threshold,  fail that single query to maintain cluster in a  healthy state\r\n\r\nConcern:  task count check is performed with a delay of one second,   is it possible that multiple queries generate a lot tasks (e.g., more than 100K) in less than one second?\r\n\r\nTest:  tested in production and expensive query is successfully killed\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add configuration parameters experimental.max-total-running-task-count and experimental.max-query-running-task-count to control the maximal number of tasks for all queries and a single query, respectively\r\n\r", "NaN"], ["13229", "Merge TableWriter output before sending to TableFinishOperator", "Andrii Rosa", "arhimondr", "09/03/19, 08:46:30 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13232", "Reuse compressionBuffer when serializing pages", "Ying", "yingsu00", "09/03/19, 02:32:28 PM", "BenchmarkPartitionedOutputOperator shows 8% gain in elapsed time:\r\n\r\nAlways allocate compressionBuffer (existing behavior):\r\nBenchmark                                   Mode  Cnt     Score     Error  Units\r\nBenchmarkPartitionedOutputOperator.addPage  avgt   20  4749.447 \u00b1 380.463  ms/op\r\n\r\nReuse compressionBuffer (current commit):\r\nBenchmark                                   Mode  Cnt     Score     Error  Units\r\nBenchmarkPartitionedOutputOperator.addPage  avgt   30  4380.171 \u00b1 148.011  ms/op\r\n\r\nBenchmarkCompressToByteBuffer shows using ByteBuffer and byte[] as compression\r\nbuffer has similar performance. We used byte[] because the code is cleaner.\r\n\r\nBenchmark                                          Mode  Cnt    Score   Error  Units\r\nBenchmarkCompressToByteBuffer.compressToByteArray  avgt   60  181.677 \u00b1 9.927  us/op\r\nBenchmarkCompressToByteBuffer.compressToByteBuffer  avgt   60  189.371 \u00b1 3.747  us/op\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Reuse compression buffer in PagesSerde when serializing pages.\r\n```\r", "NaN"], ["13236", "Exclude commons-logging from util-hadoop", "Leiqing Cai", "caithagoras", "08/15/19, 11:18:22 PM", "`commons-logging:commons-logging` collides with `org.slf4j:jcl-over-slf4j`, the logging library that we're using in Presto.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13237", " Remove Hadoop ORC writer from Raptor", "James Sun", "highker", "08/16/19, 07:13:41 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nRaptor Changes\r\n* Removed legacy ORC writer. Config `storage.orc.optimized-writer-stage` is enabled by default with `DISABLED` option removed.\r\n\r\n\r", "NaN"], ["13240", "Add StructSelectiveStreamReader", "Maria Basmanova", "mbasmanova", "08/23/19, 06:35:27 PM", "This is basic implementation of SelectiveStreamReader for ROW type that \r\nsupports the following:\r\n- extracting only specified rows;\r\n- top-level IS NULL and IS NOT NULL filters;\r\n- subfield pruning\r\n- structs nested within other structs or arrays.\r\n\r\nSupport for range filters on nested fields will be added in future PRs.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13241", "Abstract Raptor file system to use HDFS", "James Sun", "highker", "08/21/19, 04:31:46 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13243", "Add Procedure in Raptor to trigger bucket balancer", "Pratik Dhandharia", "pdhandharia", "08/17/19, 12:08:35 AM", "This PR is targetted to add a procedure in Raptor to trigger bucket balancer: trigger_bucket_balancer()\r\n\r\n```\r\n== RELEASE NOTES ==\r\nRaptor Changes\r\n* Create a new Procedure: TriggerBucketBalancerProcedure that implements procedure interface.\r\n* Modified the RaptorConnector & RaptorConnectorFactory to implement procedures.\r\n```", "NaN"], ["13246", "Use ConnectorTableLayoutHandle instead of TableHandle for createPageS\u2026", "James Sun", "highker", "08/17/19, 01:46:17 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13250", "Multiple Verifier Improvements", "Leiqing Cai", "caithagoras", "09/17/19, 01:15:59 AM", "This PR addresses several issues we are seeing during release verification. There are 3 parts:\r\n- Refactor and modularize Verifier:\r\n  - Move `PrestoAction` and `QueryRewrite` into separate packages\r\n  - Refactor `PrestoAction` so that it will be easier to run queries on a single cluster in the future, for benchmarking purpose.\r\n- Auto-resolve ``HIVE_TOO_MANY_OPEN_PARTITIONS``\r\n- Improve determinism analysis\r\n  - Improve handling queries with ``LIMIT`` clause.\r\n  - Export ``Determinism`` as an enum.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support for auto-resolving query failures with ``HIVE_TOO_MANY_OPEN_PARTITIONS`` error.\r\n* Add support to perform additional determinism analysis for queries with ``LIMIT`` clause.\r\n* Add detailed determinism analysis result to ``VerifierOutputEvent``.\r\n```\r", "NaN"], ["13251", "Update spatial index memory while building R-Tree", "Vic Zhang", "viczhang861", "08/22/19, 11:08:16 PM", "Update memory usage during construction of rtree\r\n\r\nBefore this change:  large spatial query fail with internal communication error\r\nAfter this change : same query failed with message \"Query exceeded distributed user memory limit of 5TB\"\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n-----------------\r\n* Improve memory tracking for geospatial query indexing and prevent worker node down due to out of memory\r\n```", "NaN"], ["13257", "elasticsearch.scroll-timeout example correction", "\u00d6mer Faruk Karakaya", "farukarakaya", "08/31/19, 03:07:30 PM", "originally mentioned [here](https://github.com/prestodb/prestodb.github.io/pull/42) \r\n@aweisberg ", "NaN"], ["13262", "Cache row group column stats in ORC column writer", "James Sun", "highker", "08/23/19, 10:58:05 PM", "**We found huge regression in prod due to column stats calculation when writing large files**\r\n\r\nAn ORC file can be large containing thousands of row groups. In some\r\ncases, 50% CPU is spent on getting the retained size for column writers.\r\nCache the row group stats to save CPU.\r\n\r\nPlease fill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Fix high CPU usage when writing ORC files with too many row groups.\r\n```\r", "NaN"], ["13264", "Fix docs for geospatial functions", "James Gill", "jagill", "08/28/19, 10:10:30 AM", "Two functions had an incorrect return type (`Geometry` instead of\r\n`array(Geometry)`).  Make formatting more consistent, and rearrange\r\nsome paragraphs for clarity.\r\n\r\n== NO RELEASE NOTE ==\r", "NaN"], ["13268", "Disable TestMySqlCaseInsensitiveMapping", "Ke", "kewang1024", "08/23/19, 03:42:09 PM", "Due to lack of docker support in Jenkins environment\r\nDisable the test for now\r", "NaN"], ["13271", "Subfield pruning in Parquet", "Zhenxiao Luo", "zhenxiao", "09/13/19, 12:27:14 AM", "@mbasmanova @nezihyigitbasi \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n--------------\r\n*  Add subfield pruning to reading of Parquet files so that only necessary subfields are extracts from struct columns.\r\n```", "NaN"], ["13272", "Mark method as alwaysRun when closing TestingMySqlServer", "Leiqing Cai", "caithagoras", "08/23/19, 06:42:56 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13273", "Make PrestoS3InputStream#read comply with read method contract on EOF", null, "aweisberg", "08/26/19, 02:26:34 PM", "Read should return -1 when no data was read but EOF is reached\r\n\r\nCherry-pick of https://github.com/prestosql/presto/pull/1293\r\n\r\nCo-authored-by: Ariel Weisberg <aweisberg@fb.com>\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13275", "Increase jvm heap for mvn build", null, "bhhari", "08/23/19, 10:49:21 PM", "this should solve the occasional build error due to GC overhead\r\n\r\n```\r\n[INFO] Compiling 792 source files to /home/travis/build/prestodb/presto/presto-main/target/test-classes\r\n\r\nTerminating due to java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n```", "NaN"], ["13277", "Add MapSelectiveStreamReader", "Maria Basmanova", "mbasmanova", "09/04/19, 06:24:58 PM", "Basic implementation of SelectiveStreamReader for MAP type that \r\nsupports the following:\r\n- extracting only specified rows;\r\n- top-level IS NULL and IS NOT NULL filters;\r\n- subfield pruning (implemented as a filter on key column).\r\n\r\nSupport for flat maps and range filters on nested fields will be added in future PRs.\r\n\r\nDepends on #13240\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13278", " Fix handling of non-deterministic filters in SelectiveStreamReaders", null, "bhhari", "08/28/19, 06:21:39 PM", "NaN", "NaN"], ["13281", "Update fixed-precision docs to reflect constants parsed as DECIMAL", null, "aweisberg", "08/23/19, 07:00:03 PM", "Since 0.198 the default behavior is that decimal literals without an\r\nexplicit type specifier (e.g. 1.2) are treated as DECIMAL where\r\nprior to 0.198 they were treated as DOUBLE.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nThis has been tripping up users see #13256 and #13150.", "NaN"], ["13283", "imported combinations function from prestosql, complete with tests an\u2026", "Laila Wahedi", "lwahedi", "11/26/19, 11:17:04 PM", "Ported combinations function from prestosql. Added documentation example showing n=3. Code mostly from: https://github.com/prestosql/presto/pull/718/commits/2a55f5891833c85b780730237ee4132d6bee8509\r\n\r\nNOTE: this is my first time, and I need help testing before this pr should be accepted. \r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Added combinations function, a function that returns n combinations of values in an array, up to n=5 \r\n```", "NaN"], ["13284", "Move Arrays class from presto-orc to presto-array", "Ying", "yingsu00", "08/27/19, 03:03:55 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13286", "Adds testLength to TupleDomainFilter", null, "oerling", "08/27/19, 10:25:59 AM", "This allows prefiltering strings and later lists and maps based on length.\nWe note that testLength of a PositionalFilter is true and has no effect on the position for simplicity. This could also advance the positional filter if false and leave it at position if true.", "NaN"], ["13288", "Add .mailmap to increase release stats accuracy", null, "aweisberg", "08/27/19, 03:34:55 PM", "Release stats use git-log to identify the number of authors and committers.\r\nSince email addresses aren't always consistent the number can be off.\r\nAdding a .mailmap allows git-log to optionally group more commits under\r\nthe right person.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13289", "Optimize single-column FilterFunctions to only run on distinct values for DictionaryBlocks", null, "oerling", "09/05/19, 10:44:23 PM", "Note that the DictionaryBlocks are produced by scan only for string columns. This therefore depends on the introduction of a selective slice dictionary reader.", "NaN"], ["13290", "Remove unused OrcReader field maxReadSize", "Ying", "yingsu00", "08/28/19, 10:06:55 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13291", "Remove unused systemMemoryContext from OrcBatchRecordReader", "Ying", "yingsu00", "08/28/19, 11:05:21 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13292", "Add test for PrestoS3InputStream positional read EOS", null, "aweisberg", "08/30/19, 04:16:21 PM", "6d296f90688cd79d6c2cdcdf4c665121f1d61d06 fixed a bug where it returned 0\r\non EOS but didn't add a test case.\r\n\r\nPlease fill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13296", "Add pushed down filter to the output of EXPLAIN", "Maria Basmanova", "mbasmanova", "09/05/19, 07:06:36 PM", "Add filters pushed down into the Hive connector (by setting `hive.pushdown-filter-enabled=true`) to the output of EXPLAIN command under `LAYOUT` section of `TableScan`.\r\n\r\n```\r\nEXPLAIN SELECT orderkey FROM lineitem WHERE linenumber % 2 = 0\r\n            \r\n- Output[orderkey] => [orderkey:bigint]\r\n        Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 541575.00}\r\n    - RemoteStreamingExchange[GATHER] => [orderkey:bigint]\r\n            Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 541575.00}\r\n        - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.lineitem{filter=((linenumber) % (INTEGER 2)) = (INTEGER 0)}]'}] => [orderkey:bigint]\r\n                Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 0.00}\r\n                LAYOUT: tpch.lineitem{filter=((linenumber) % (INTEGER 2)) = (INTEGER 0)}\r\n                orderkey := orderkey:bigint:0:REGULAR\r\n)\r\n\r\nEXPLAIN SELECT orderkey FROM lineitem WHERE orderkey < 1000 AND linenumber % 2 = 0\r\n            \r\n- Output[orderkey] => [orderkey:bigint]\r\n        Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 541575.00}\r\n    - RemoteStreamingExchange[GATHER] => [orderkey:bigint]\r\n            Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 541575.00}\r\n        - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.lineitem{filter=((linenumber) % (INTEGER 2)) = (INTEGER 0), domains={orderkey=[ [(<min>, 1000)] ]}}]'}] => [orderkey:bigint]\r\n                Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 0.00}\r\n                LAYOUT: tpch.lineitem{filter=((linenumber) % (INTEGER 2)) = (INTEGER 0), domains={orderkey=[ [(<min>, 1000)] ]}}\r\n                orderkey := orderkey:bigint:0:REGULAR\r\n)\r\n\r\nEXPLAIN SELECT orderkey FROM lineitem WHERE linenumber > 3\r\n            \r\n- Output[orderkey] => [orderkey:bigint]\r\n        Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 541575.00}\r\n    - RemoteStreamingExchange[GATHER] => [orderkey:bigint]\r\n            Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 541575.00}\r\n        - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.lineitem{domains={linenumber=[ [(3, <max>)] ]}}]'}] => [orderkey:bigint]\r\n                Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 0.00}\r\n                LAYOUT: tpch.lineitem{domains={linenumber=[ [(3, <max>)] ]}}\r\n                orderkey := orderkey:bigint:0:REGULAR\r\n)\r\n\r\nEXPLAIN SELECT orderkey FROM lineitem WHERE linenumber IN (1, 2, 3)\r\n            \r\n- Output[orderkey] => [orderkey:bigint]\r\n        Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 541575.00}\r\n    - RemoteStreamingExchange[GATHER] => [orderkey:bigint]\r\n            Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 541575.00}\r\n        - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.lineitem{domains={linenumber=[ [[1], [2], [3]] ]}}]'}] => [orderkey:bigint]\r\n                Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 0.00}\r\n                LAYOUT: tpch.lineitem{domains={linenumber=[ [[1], [2], [3]] ]}}\r\n                orderkey := orderkey:bigint:0:REGULAR\r\n)\r\n\r\nEXPLAIN SELECT orderkey FROM lineitem\r\n\r\n- Output[orderkey] => [orderkey:bigint]\r\n        Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 541575.00}\r\n    - RemoteStreamingExchange[GATHER] => [orderkey:bigint]\r\n            Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 541575.00}\r\n        - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.lineitem{}]'}] => [orderkey:bigint]\r\n                Estimates: {rows: 60175 (528.88kB), cpu: 541575.00, memory: 0.00, network: 0.00}\r\n                LAYOUT: tpch.lineitem{}\r\n                orderkey := orderkey:bigint:0:REGULAR\r\n)\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13306", "Add DecimalSelectiveReader", null, "bhhari", "09/11/19, 06:38:15 PM", "```\r\n===NEW===\r\nBenchmark                                       (typeSignature)  (withNulls)  Mode  Cnt  Score    Error  Units\r\nBenchmarkSelectiveStreamReaders.read              decimal(10,5)         true  avgt   20  0.234 \u00b1  0.008   s/op\r\nBenchmarkSelectiveStreamReaders.read              decimal(10,5)        false  avgt   20  0.471 \u00b1  0.015   s/op\r\nBenchmarkSelectiveStreamReaders.readAllNull       decimal(10,5)          N/A  avgt   20  0.002 \u00b1  0.001   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter    decimal(10,5)         true  avgt   20  0.277 \u00b1  0.009   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter    decimal(10,5)        false  avgt   20  0.466 \u00b1  0.018   s/op\r\n\r\nBenchmark                                             Mode  Cnt  Score   Error  Units\r\nBenchmarkBatchStreamReaders.readShortDecimalNoNull    avgt   60  0.481 \u00b1 0.009   s/op\r\nBenchmarkBatchStreamReaders.readShortDecimalWithNull  avgt   60  0.301 \u00b1 0.005   s/op\r\n\r\nBenchmark                                       (typeSignature)  (withNulls)  Mode  Cnt  Score    Error  Units\r\nBenchmarkSelectiveStreamReaders.read             decimal(30,10)         true  avgt   20  0.288 \u00b1  0.008   s/op\r\nBenchmarkSelectiveStreamReaders.read             decimal(30,10)        false  avgt   20  0.446 \u00b1  0.025   s/op\r\nBenchmarkSelectiveStreamReaders.readAllNull      decimal(30,10)          N/A  avgt   20  0.002 \u00b1  0.001   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter   decimal(30,10)         true  avgt   20  0.350 \u00b1  0.011   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter   decimal(30,10)        false  avgt   20  0.585 \u00b1  0.036   s/op\r\n\r\nBenchmark                                            Mode  Cnt  Score   Error  Units\r\nBenchmarkBatchStreamReaders.readLongDecimalNoNull    avgt   60  0.645 \u00b1 0.013   s/op\r\nBenchmarkBatchStreamReaders.readLongDecimalWithNull  avgt   60  0.483 \u00b1 0.010   s/op\r\n\r\n===OLD===\r\nBenchmark                                       (typeSignature)  (withNulls)  Mode  Cnt  Score    Error  Units\r\nBenchmarkSelectiveStreamReaders.read              decimal(10,5)         true  avgt   20  0.255 \u00b1  0.008   s/op\r\nBenchmarkSelectiveStreamReaders.read              decimal(10,5)        false  avgt   20  0.465 \u00b1  0.022   s/op\r\nBenchmarkSelectiveStreamReaders.readAllNull       decimal(10,5)          N/A  avgt   20  0.002 \u00b1  0.001   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter    decimal(10,5)         true  avgt   20  0.466 \u00b1  0.013   s/op\r\n\r\n\r\n# Run complete. Total time: 00:04:09\r\n\r\nBenchmark                                             Mode  Cnt  Score   Error  Units\r\nBenchmarkBatchStreamReaders.readShortDecimalNoNull    avgt   60  0.490 \u00b1 0.014   s/op\r\nBenchmarkBatchStreamReaders.readShortDecimalWithNull  avgt   60  0.344 \u00b1 0.026   s/op\r\n\r\n\r\n\r\nBenchmarkSelectiveStreamReaders.read             decimal(30,10)         true  avgt   20  0.385 \u00b1  0.009   s/op\r\nBenchmarkSelectiveStreamReaders.read             decimal(30,10)        false  avgt   20  0.611 \u00b1  0.012   s/op\r\nBenchmarkSelectiveStreamReaders.readAllNull      decimal(30,10)          N/A  avgt   20  0.003 \u00b1  0.001   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter   decimal(30,10)         true  avgt   20  0.430 \u00b1  0.011   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter   decimal(30,10)        false  avgt   20  0.678 \u00b1  0.008   s/op\r\n\r\n\r\nBenchmark                                            Mode  Cnt  Score   Error  Units\r\nBenchmarkBatchStreamReaders.readLongDecimalNoNull    avgt   60  0.636 \u00b1 0.009   s/op\r\nBenchmarkBatchStreamReaders.readLongDecimalWithNull  avgt   60  0.437 \u00b1 0.013   s/op\r\n```\r\n== NO RELEASE NOTE ==", "NaN"], ["13307", "Allow anonymous rows in Hive temporary tables", "Andrii Rosa", "arhimondr", "08/30/19, 02:50:46 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nIncrease the coverage for exchange materialization (https://github.com/prestodb/presto/issues/12387) ", "NaN"], ["13310", "Improve condition for ignoring bucketing for hive", "Rebecca Schlussel", "rschlussel", "09/04/19, 02:46:51 PM", "Don't ignore bucketing for queries that use the bucket column and consider the\r\nbucket filter when calculating the total number of buckets\r\n\r\nPlease fill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Fix computation for number of buckets accessed for `max_buckets_for_grouped_execution`\r\n* Fix a bug where the bucket column was not available if `max_buckets_for_grouped_execution` was exceeded\r\n```", "NaN"], ["13312", "Add debug support for offline table and partition", "Wenlei Xie", "wenleix", "09/05/19, 01:13:49 AM", "Please fill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\n\r\nHive Changes\r\n* Add debug mode that allows reading from offline table and partition. This is controlled by session property `offline_data_debug_mode_enabled `\r\n```\r\n\r", "NaN"], ["13313", "Add release note for 0.225", "Vic Zhang", "viczhang861", "09/03/19, 03:01:42 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13314", "Use Optional instead of null when getting Parquet types", "Zhenxiao Luo", "zhenxiao", "08/30/19, 08:39:35 PM", "@nezihyigitbasi @arhimondr follow previous discussion, use optional instead of null when getting Parquet types\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13318", "Add FileOpener interface in Hive connector", "Shixuan Fan", "shixuan-fan", "10/25/19, 06:23:13 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13319", "Use DictionaryBlocks for Unnest Operator", "Ajay George", "ajaygeorge", "09/09/19, 06:02:06 PM", "Use DictionaryBlocks for Unnest Operator\r\n\r\nFixes #13121 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Use DictionaryBlocks for Unnest Operator\r\n```\r", "NaN"], ["13321", "Simplify batch stream readers for all nulls", "Ying", "yingsu00", "09/03/19, 07:22:57 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13323", "Fix ST_Centroid and ST_Buffer for tiny geometries", "James Gill", "jagill", "09/04/19, 02:59:27 AM", "Version 2.2.3 has two fixes of interest: \r\n- correct computation of centroids of very small polygons - https://github.com/Esri/geometry-api-java/pull/227\r\n- correct computation of buffers around small polygons - https://github.com/Esri/geometry-api-java/pull/243\r\n\r\nFixes #13194 and #10629\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n-----------------\r\n* Fix ST_Buffer for tiny geometries (#13194)\r\n* Fix ST_Centroid for tiny geometries (#10629)\r\n```\r", "NaN"], ["13324", "[easy] Fix ShardCleaner not running on HDFS", "James Sun", "highker", "09/03/19, 05:15:41 AM", "ShardCleaner needs to clean up files even on HDFS.\r\n\r\nPlease fill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13329", "Fix grouped execution tagging for table scan", "Shixuan Fan", "shixuan-fan", "09/04/19, 02:50:13 AM", "Addresses #13322\r\nWe should return incapable for table scan in the following two cases:\r\n- TablePartitioing is empty\r\n- TablePartitioning is not empty but it only contains one\r\n   NotPartitionedPartitionHandle in the partition handle list\r\n    \r\nThe first case is already covered, and this commit is aiming\r\nat fixing the second case.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix bug that causes failure when reading unpartitioned table from some connectors with session property ``grouped_execution_for_eligible_table_scans`` turned on.\r\n```\r", "NaN"], ["13331", "Inline LocalQueryRunner#translateExpressions", "Wenlei Xie", "wenleix", "09/05/19, 03:50:10 AM", "This is only used in tests to construct a plan optimizer that\r\ntranslates all OriginalExpression in planNodes to RowExpression.\r\nAnd all the information obtained from public methods of\r\nLocalQueryRunner. Thus it doesn't have to be a method of\r\nLocalQueryRunner.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13332", "Report filter selectivity for pushed down filters", "Maria Basmanova", "mbasmanova", "09/20/19, 12:09:49 PM", "- Add `ConnectorPageSource#getCompletedPositions` SPI to fetch number of rows processed by the connector and use this value in TableScanOperator and ScanFilterProjectOperator to populate `OperatorStats#rawInputPositions`.\r\n- Update PlanPrinter to report filter selectivity for TableScanNode.\r\n\r\n```\r\nselect orderkey from lineitem where orderkey % 2 = 0\r\n\r\n     - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.lineitem{filter=((orderkey) % (CAST(VARCHAR 2 AS bigint))) = (CAST(VARCHAR 0 AS bigint))}]'}, grouped = false] => [orderkey:bigint] \r\n             CPU: 32.00ms (100.00%), Scheduled: 39.00ms (100.00%), Output: 30050 rows (264.11kB)                                                                                                                                                                                                                           \r\n             Input avg.: 15025.00 rows, Input std.dev.: 0.65%                                                                                                                                                                                                                                                              \r\n             LAYOUT: tpch.lineitem{filter=((orderkey) % (CAST(VARCHAR 2 AS bigint))) = (CAST(VARCHAR 0 AS bigint))}                                                                                                                                                                                                        \r\n             orderkey := orderkey:bigint:0:REGULAR                                                                                                                                                                                                                                                                         \r\n             Input: 30050 rows (264.11kB), Filtered: 0.00%                                                                                                                                                                                                                                                                 \r\n             Raw input: 60175 rows (1.38MB), Filtered: 50.06%                                                                                                                                                                                                                                                              \r\n\r\n\r\nselect orderkey from lineitem where linenumber < 2\r\n\r\n     - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.lineitem{domains={linenumber=[ [(<min>, 2)] ]}}]'}, grouped = false] => [orderkey:bigint] \r\n             CPU: 29.00ms (100.00%), Scheduled: 90.00ms (100.00%), Output: 15000 rows (131.83kB)                                                                                                                                                                                 \r\n             Input avg.: 7500.00 rows, Input std.dev.: 0.16%                                                                                                                                                                                                                     \r\n             LAYOUT: tpch.lineitem{domains={linenumber=[ [(<min>, 2)] ]}}                                                                                                                                                                                                        \r\n             orderkey := orderkey:bigint:0:REGULAR                                                                                                                                                                                                                               \r\n             Input: 15000 rows (131.83kB), Filtered: 0.00%                                                                                                                                                                                                                       \r\n             Raw input: 60175 rows (1.38MB), Filtered: 75.07%  \r\n\r\n\r\nselect orderkey from lineitem\r\n\r\n     - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.lineitem{}]'}, grouped = false] => [orderkey:bigint] \r\n             CPU: 16.00ms (100.00%), Scheduled: 18.00ms (100.00%), Output: 60175 rows (528.88kB)                                                                                                                                            \r\n             Input avg.: 30087.50 rows, Input std.dev.: 0.50%                                                                                                                                                                               \r\n             LAYOUT: tpch.lineitem{}                                                                                                                                                                                                        \r\n             orderkey := orderkey:bigint:0:REGULAR                                                                                                                                                                                          \r\n             Input: 60175 rows (528.88kB), Filtered: 0.00% \r\n\r\n\r\nselect orderkey + 1 from lineitem where linenumber < 2\r\n\r\n     - ScanProject[table = TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.lineitem{domains={linenumber=[ [(<min>, 2)] ]}}]'}, grouped = false] => [expr:bigint] \r\n             CPU: 20.00ms (100.00%), Scheduled: 23.00ms (100.00%), Output: 15000 rows (131.83kB)                                                                                                                                                                                       \r\n             Input avg.: 7500.00 rows, Input std.dev.: 0.16%                                                                                                                                                                                                                           \r\n             expr := (orderkey) + (CAST(VARCHAR 1 AS bigint))                                                                                                                                                                                                                          \r\n             LAYOUT: tpch.lineitem{domains={linenumber=[ [(<min>, 2)] ]}}                                                                                                                                                                                                              \r\n             orderkey := orderkey:bigint:0:REGULAR                                                                                                                                                                                                                                     \r\n             Input: 15000 rows (131.83kB), Filtered: 0.00%                                                                                                                                                                                                                             \r\n             Raw input: 60175 rows (1.38MB), Filtered: 75.07%   \r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13334", "Add query task count to statistics field of QueryCompletedEvent", "Vic Zhang", "viczhang861", "09/05/19, 05:11:02 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add peak running task count to statistics field of QueryCompletedEvent\r\n```", "NaN"], ["13335", "Extend OrcTester to enable testing of filters on subfields", "Maria Basmanova", "mbasmanova", "09/05/19, 03:12:23 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13336", "Fix reading empty and all-nulls lists with filters", "Maria Basmanova", "mbasmanova", "09/05/19, 04:39:43 PM", "Fixes #13253\r\n\r\nDepends on #13335\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13339", "Move request slug from path to query", null, "aweisberg", "09/05/19, 08:03:57 PM", "This prevents the airlift HTTP server from logging the slug in the http request log.\r\n\r\nThis turns out to work because if you look at io.airlift.http.server.HttpRequestEvent it invokes org.eclipse.jetty.server.Request.getRequestURI which does not as its name implies retrieve the entire URI.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13341", "Add SliceSelectiveStreamReader", null, "bhhari", "09/25/19, 08:05:47 PM", "Add SliceSelectiveStreamReader\r\n\r\n```\r\nBenchmark                                       (typeSignature)  (withNulls)  Mode  Cnt  Score   Error  Units\r\nBenchmarkSelectiveStreamReaders.read             varchar_direct        false  avgt   20  0.360 \u00b1 0.006   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter   varchar_direct        false  avgt   20  0.467 \u00b1 0.013   s/op\r\n\r\n\r\nBenchmark                               (typeSignature)  (withNulls)  Mode  Cnt  Score   Error  Units\r\nBenchmarkBatchStreamReaders.readBlocks   varchar_direct         NONE  avgt   60  0.358 \u00b1 0.014   s/op\r\n\r\n\r\nBenchmark                                          (typeSignature)  (withNulls)  Mode  Cnt  Score    Error  Units\r\nBenchmarkSelectiveStreamReaders.read            varchar_dictionary         true  avgt   20  0.044 \u00b1  0.002   s/op\r\nBenchmarkSelectiveStreamReaders.read            varchar_dictionary        false  avgt   20  0.044 \u00b1  0.002   s/op\r\nBenchmarkSelectiveStreamReaders.readAllNull     varchar_dictionary          N/A  avgt   20  0.003 \u00b1  0.001   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter  varchar_dictionary         true  avgt   20  0.175 \u00b1  0.009   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter  varchar_dictionary        false  avgt   20  0.299 \u00b1  0.007   s/op\r\n\r\n\r\n\r\nBenchmark                                  (typeSignature)  (withNulls)  Mode  Cnt  Score    Error  Units\r\nBenchmarkBatchStreamReaders.readBlocks  varchar_dictionary      PARTIAL  avgt   60  0.045 \u00b1  0.001   s/op\r\nBenchmarkBatchStreamReaders.readBlocks  varchar_dictionary         NONE  avgt   60  0.029 \u00b1  0.001   s/op\r\nBenchmarkBatchStreamReaders.readBlocks  varchar_dictionary          ALL  avgt   60  0.023 \u00b1  0.001   s/op\r\n\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13343", "Include full OrcType in StreamDescriptor", null, "bhhari", "09/06/19, 11:56:08 PM", "SelectiveStreamReaders for varchars and decimals need additional type information such as precision, scale and length. \r\n \r\nSee #13306\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13344", "Materialize unknown type", "Andrii Rosa", "arhimondr", "09/18/19, 05:05:22 PM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13348", "Improve test coverage for selective map reader and fix uncovered bugs", "Maria Basmanova", "mbasmanova", "09/09/19, 09:46:13 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13353", "Add test coverage for StructSelectiveStreamReader.compactValues", "Maria Basmanova", "mbasmanova", "09/11/19, 06:13:29 PM", "Depends on #13348\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13354", "Add use-exact-partitioning-enabled session property", null, "aweisberg", "01/16/20, 03:52:37 PM", "When enabled this forces exchanges to occur unless the partitioning matches exactly\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add use_exact_partitioning session property that forces repartitioning if repartitioning is possible\r\n```", "NaN"], ["13357", "Improve BenchmarkBatchStreamReaders", "Ying", "yingsu00", "09/12/19, 02:13:49 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\n### Updates\r\nSept 09 2019\r\n\r\n- Added @SuppressWarnings(\"unused\") to typeSignature and withNulls in bb70985c25 Simplify BenchmarkBatchStreamReaders by introducing Params and Nulls enum\r\n- Resolved review comments and updated commit messages on\r\n\r\n  - 2f5d1d10a7 Avoid creating OrcDataSource and OrcReader in each iteration\r\n  - 71883d9648 Use per-row metric in BenchmarkBatchStreamReaders\r\n  - d3b9f3a745 Add benchmark for SliceBatchDirectStreamReader\r\n  - ad2ac122fe Simplify BenchmarkBatchStreamReaders\r\n\r", "NaN"], ["13358", "Update access modifiers in BenchmarkSelectiveStreamReaders", "Ying", "yingsu00", "01/21/20, 09:53:52 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13364", "Add a new S3FileSystemType to Hive connector", "Bin Fan", "apc999", "09/12/19, 09:40:34 PM", "* Add a new `S3FileSystemType`  to serve URLs with s3 scheme. Currently, input with URLs like `s3://bucket/path` can only be served by `PrestoS3FileSystem` or EMR FS class (i.e., `com.amazon.ws.emr.hadoop.fs.EmrFileSystem`). In addition to these two possible choices, a `RuntimeException` will be thrown. This PR enables Presto to be served by additional services (e.g., Alluxio as a caching layer on top of S3 but without change HMS). \r\n\r\nParticularly, users can update `etc/config.properties`\r\n\r\n```\r\nhive.s3-file-system-type=HADOOP_DEFAULT\r\n```\r\n\r\nand update `core-site.xml`\r\n\r\n```\r\n<property>\r\n  <name>fs.s3.impl</name>\r\n  <value>alluxio.hadoop.ShimFileSystem</value>\r\n</property>\r\n```\r\nAs a result, end users can transparently benefit from the caching from customized implementation (e.g., Alluxio in my example) for Presto.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add an option to switch file system on S3 to allow users to configure their own ones by setting the config `hive.s3-file-system-type=HADOOP_DEFAULT` and providing file system dependency in `core-site.xml`.\r\n```\r", "NaN"], ["13370", "Improve filter order in selective ORC reader", "Sahar Massachi", "sayhar", "09/24/19, 10:49:30 PM", "```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13371", "Fix MapBlock.ensureHashTableLoaded", "Ying", "yingsu00", "09/12/19, 09:09:58 PM", "createMapBlockInternal takes positionCount as the logical positionCount\r\nbut the hashtables need to be built on the entire underlying MapBlock.\r\nSince offsets.length may be larger than the actual valid number of offsets,\r\nwe need to know the number of hashtables from somewhere else. This commit\r\nadds expectedHashTableCount to HashTables and uses it in\r\nMapBlock.ensureHashTableLoaded()\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13372", "Fix issue \"PlanPrinter prints \"PartialSort\" for the only sort when distributed sort is enabled\"", "Xiaohe Dong", "dongxiaohe", "09/11/19, 07:48:03 PM", "Fix issue related in https://github.com/prestodb/presto/issues/12087\r\n\r\nThe logic can be explained like if there is no AddExchange happened (no data exchange between the node), the plan display should be Sort. Otherwise, it would display PartialSort. From my understanding, this way we don't need to fetch any information from session property to do the validation.\r\n\r\n@rschlussel \r\n\r\nGeneral Changes\r\n* Add isPartial (default `false`) in SortNode constructor and keep it immutable.\r\n* The value `true` would be only set by AddExchanges, so it would display PartialSort in this case.\r\n* Remove session property checking in PlanPrinter, since the value is extracted from SortNode directly.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13374", "Close OrcDataSource in OrcPageFileRewriter", "Jiexi Lin", "jessesleeping", "09/10/19, 11:44:39 PM", "Fix fd leak in Raptor during compaction.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nRaptor Changes\r\n* Fix file descriptor memory leak during shard compaction, which was introduced in 0.219.\r\n```", "NaN"], ["13375", "Use recursiveDirWalkerEnabled in getVirtuallyBucketedSplits", "Shixuan Fan", "shixuan-fan", "09/11/19, 05:41:29 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13377", "Allow to configure partitioned table write concurrency independently", "Andrii Rosa", "arhimondr", "09/20/19, 03:18:00 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add `task_partitioned_writer_count` session property to allow to set number of concurrent writers for partitioned (bucketed) writes independently.\r\n```\r", "NaN"], ["13379", "Add peak memory distribution among tasks of each stage", "Vic Zhang", "viczhang861", "10/24/19, 04:38:31 PM", "Resolves https://github.com/prestodb/presto/issues/13327\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add peak total memory distribution among tasks of each stage\r\n\r\n```\r", "NaN"], ["13382", "Implement IPPREFIX type", null, "mikemcbrearty", "09/16/19, 09:59:22 PM", "Addresses issue #13274 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add ``IPPREFIX`` type and :func:`ip_prefix` functions\r\n```", "NaN"], ["13384", "Function implementation", "Rongrong Zhong", "rongrong", "10/29/19, 05:08:25 AM", "Ready for pre-review on high level. I'll probably use some data structures introduced in #13484 and add tests once it's merged.", "NaN"], ["13388", "Fix method handle binding in ExpressionInterpreter", "Rongrong Zhong", "rongrong", "09/12/19, 08:24:46 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13389", "Clean up RowExpressionInterpreter", "Yi He", "hellium01", "09/14/19, 06:54:22 AM", "This is the part of https://github.com/prestodb/presto/pull/13184, in which we are trying to clean up the RowExpressionOptimizer to make it works properly in different query stages. \r\n\r\nVerifier tests on 100K queries passed. \r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13392", "Hash Builder Memory", "Sahar Massachi", "sayhar", "10/03/19, 06:28:41 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13396", "Add selective stream reader for flat maps", "Maria Basmanova", "mbasmanova", "09/17/19, 04:52:40 PM", "The reader supports IS NULL and IS NOT NULL filters and subfield pruning.\r\nFilters on subfields are not supported yet.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13403", "Fix getExtent npe", "James Gill", "jagill", "09/17/19, 12:20:25 AM", "Fixes https://github.com/prestodb/presto/issues/13395\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n-----------------\r\n* Fix NullPointerException in spatial join with geometry collections in the build side\r\n```", "NaN"], ["13404", "Move Projection Node to SPI", "Saksham", "sachdevs", "09/20/19, 09:04:41 PM", "In preparation for aggregation/projection pushdown, move ProjectNode plan node to SPI. As ProjectNode uses the Assignments class, I had to consider moving it to SPI as well. To me this move makes sense, however let me know if I should be doing this in an alternate way.\r\n\r\nAggregationNode will be added to SPI in a separate PR as it is slightly more complicated to refactor it.\r\n\r\nTODO: ~~depending on what we do with the Assignments class, move the Assignments test into the SPI test directory.~~ Confirmed with James, we do not want to do this.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral \r\n* Move `ProjectNode` to SPI. Connectors can now pushdown project to table scan.\r\n```\r", "NaN"], ["13405", "Reduce the identifier length in generated code for nested columns", "Venki Korukanti", "vkorukanti", "11/05/19, 08:54:41 PM", "(porting https://github.com/prestosql/presto/pull/537)\r\n\r\nCurrently the type signature is used as name which could be long if the\r\ntype is a nested type. We recently hit the limit of identifier length in\r\nASM [1] on one of the wide tables which has nested column deferences in\r\nproject/filter.\r\n\r\nUse the existing logic up to some size limit (up to 20 bytes), and after that use `type.getTypeSignature().getBase()`. This reduces the size of the code generated too.\r\n\r\nBytecode for the test query used in unittests:\r\nBefore fix: https://gist.github.com/vkorukanti/64bfab1842c192fdb21f12f54006def3#file-before_fix\r\nAfter fix: https://gist.github.com/vkorukanti/64bfab1842c192fdb21f12f54006def3#file-after-fix\r\n\r\n[1] https://gitlab.ow2.org/asm/asm/blob/master/asm/src/main/java/org/objectweb/asm/ByteVector.java#L246\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix compilation errors for expressions over types containing an extremely\r\n  large number of nested types.\r\n```", "NaN"], ["13407", "Fix struct and map selective readers for filter-only columns", "Maria Basmanova", "mbasmanova", "09/19/19, 01:20:59 AM", "Fix queries where complex type columns are used for filtering but are not projected out. \r\n\r\nFor example: `SELECT a FROM t WHERE b IS NOT NULL`, where `b` is a map or a struct.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13409", "Do not extract range filters for complex types", "Maria Basmanova", "mbasmanova", "09/19/19, 06:04:36 PM", "Selective readers cannot handle range filters other than null checks for\r\ncomplex types (arrays, maps, and structs), e.g. a = ARRAY[1, 2, 3]. This\r\nchange extends DomainTranslator.ColumnExtractor to provide values domain\r\nand updates HiveMetadata to provide an implementation of ColumnExtractor\r\nthat excludes non-null check filters for complex types.\r\n\r\nFixes #13295\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13410", "Revert SQL path", "Rongrong Zhong", "rongrong", "09/19/19, 09:14:27 PM", "We don't really use path at the moment. My understanding is that we introduced this to be used in SQL function resolution (use sql path to define the namespace and order of resolution). However, from discussions with various people, it doesn't seem to bring enough benefit to justify this. It's probably better to just require all reference to non-builtin functions to be referred to by its fully qualified name to remove ambiguity and throw otherwise. In that case, we are not expecting to use SQL PATH in any feature any time soon, and keeping these around just adds dead code and maintenance cost. In terms of format, I personally prefer to merge all commits into one. But if people prefer to revert each commit individually, I can also go back and fix each commit to be error-free.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Removing syntax support for `SET PATH`, `CURRENT_PATH` (Presto server do not use this information)\r\n```\r", "NaN"], ["13411", "Add release notes for 0.226", null, "neeradsomanchi", "09/21/19, 12:10:57 AM", "Reference issue: #13340\r\n\r\n== NO RELEASE NOTE ==", "NaN"], ["13412", "Support DROP FUNCTION", "Leiqing Cai", "caithagoras", "12/03/19, 02:57:33 AM", "Syntax Reference: https://github.com/prestodb/presto/issues/13415\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral\r\n* Add support for ``DROP FUNCTION``.\r\n```", "NaN"], ["13416", "Wait for indexes to load for IndexJoin", "Rebecca Schlussel", "rschlussel", "09/25/19, 06:19:06 PM", "Wait for indexes to load for IndexJoins until a configured time\r\nout. Add configuration property index-loader-timeout and session\r\nproperty index_loader_timeout to configure this timeout.  Previously\r\nqueries would fail with the error \"driver should never block\" if the\r\nindex server for the index join was slow. (Fixes #12826)\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix a bug where index joins would fail with the error \"driver should never block\".  Now queries will wait until a configured timeout for the indexes to load.  The timeout can be configured with the configuration property index-loader-timeout and the session property index_loader_timeout. \r\n```\r", "NaN"], ["13417", "Add support for missing columns to OrcSelectiveRecordReader", null, "oerling", "09/19/19, 11:39:30 PM", "Adds adaptation to missing columns in OrcSelectiveRecordReader. Evaluates TupleDomainFilters against these and fails the whole split if any filter on a missing column fails with null.\r\nTests interaction of schema evolution with prefilled (e.g. partitioning) columns.\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13418", "Reduce running time of presto-hive Travis job", "Maria Basmanova", "mbasmanova", "09/19/19, 06:04:02 PM", "presto-hive Travis job regularly times out after 1 hour and 20 minutes. This change is to move some of the longer running tests into separate jobs. Specifically, move TestHivePushdownFilterQueries and TestParquetDistributedQueries into their own jobs. With these changes presto-hive job completes in about 45 min.\r\n\r\n<img width=\"1282\" alt=\"Screen Shot 2019-09-18 at 10 42 10 PM\" src=\"https://user-images.githubusercontent.com/27965151/65209301-ac5f7580-da65-11e9-9edc-1c4f11272b5a.png\">\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13419", "Use LambdaBytecodeGenerator for all lambda bytecode generation", "Rongrong Zhong", "rongrong", "09/19/19, 12:27:43 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13424", "Fix selective readers for arrays of maps", "Maria Basmanova", "mbasmanova", "09/20/19, 11:22:18 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13425", "Prepare for whole stage retries", "Andrii Rosa", "arhimondr", "10/02/19, 04:59:37 PM", "Introduce stage execution attempt concept. Extend TaskId with the stage execution id . This is required for stage retry (https://github.com/prestodb/presto/issues/13438) \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13426", "Move AggregationNode to SPI", "Saksham", "sachdevs", "09/24/19, 07:20:56 AM", "Follow up after #13404, depends on that one first.\r\nIn preparation for plan pushdown, move Aggregation plan node to SPI.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral \r\n* Move `AggregationNode` to SPI. Connectors can now pushdown aggregation to table scan.\r\n```\r", "NaN"], ["13428", "Add clientInfo to HdfsContext", "Nikhil Collooru", "NikhilCollooru", "09/23/19, 11:36:20 PM", "Send additional client Info as part of the HdfsContext. This will help the underlying system to get more info about the requesting client. This is an optional field in HdfsContext. \r\nAlso expose getClientInfo() method in ConnectSession SPI to help retrieve this new client info.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13429", "Unit test validating IllegalState Exception thrown from OrcWriter", "Jacob Lammott", "golammott", "09/25/19, 12:45:06 AM", "If an exception is thrown in `OrcWriter` its internal `ColumnWriters` can be marked as closed, while the `OrcFileWriter` and `OrcStorageManager` still views the writers as open. \r\n\r\nThis fixes that issue by marking `OrcWriter` as closed whenever the internal `ColumnWriters` are closed.\r\n\r\nIssue: #13400 \r\n\r\n```\r\n== NO RELEASE NOTES ==\r\n```", "NaN"], ["13430", "Add subfield pruning for arrays", "Maria Basmanova", "mbasmanova", "09/25/19, 02:12:43 PM", "Prune arrays after the last required index.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13433", "Fix queries on tables with multiple partitioning columns", null, "oerling", "09/24/19, 12:59:31 AM", "Hive selective page source logic uses hiveColumnIndex to distinguish\r\nbetween columns. All partitioning columns have a hiveColumnIndex of\r\n-1. Therefore, assign consecutive negative column numbers to all\r\npartitioning HiveColumnHandles.\r\n\r\nAlternatively all references to columns in selective page sources\r\nwould have to be by name.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13434", "Add common scalar reads to OrcInputStream", null, "oerling", "10/09/19, 11:26:56 PM", "Adds an optimized read path for varints, floats and doubles. This\r\navoids copying and bounds checking for consecutive bytes.\r\n\r\nRemoves the use of Slice from OrcInputStream. Keeps the decompression\r\nbuffer separate from the Slice read from the compressed input. The\r\nbacking byte[] of the compressed input Slice is directly used as the\r\nbuffer when there is no compression, avoiding needless copy.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n--------------\r\n * Streamline ORC read path for varints and fixed width numbers.\r\n```", "NaN"], ["13439", "Return error when files created per writer per query exceeds 'storage.max-allowed-files-per-writer' limit", "Nikhil Collooru", "NikhilCollooru", "09/24/19, 10:04:56 PM", "Introduce  new config parameter storage.max-allowed-files-per-writer . For a given INSERT query, this parameter limits the number of files that can be created per worker for a query. This way we can avoid excessive pressure on metadata commit and backup requests caused by bad INSERT queries.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nRaptor Changes\r\n* Add a new config parameter 'storage.max-allowed-files-per-writer' that limits the number of files created per writer per query.\r\n\r\n```\r", "NaN"], ["13440", "Added scaffolding for query plan optimization in JDBC connector", "Saksham", "sachdevs", "09/25/19, 05:57:50 AM", "Currently these new classes are not linked to anything, in the follow up PRs I will implement the remaining functionality for plan optimization in the JDBC connector.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13444", "Add support for filters on struct subfields", "Maria Basmanova", "mbasmanova", "09/25/19, 08:05:01 PM", "Supports filters on struct subfields at any level, e.g. `a.b.c = 5`.\r\n\r\nSupport for structs nested inside an array or map, e.g. `a[1].b.c = 5`,\r\nwill come in future commits.\r\n\r\nDepends on #13430 \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13448", "PartitionedOutputOperator optimizations - part1", "Ying", "yingsu00", "09/25/19, 09:06:33 PM", "This is the first part of https://github.com/prestodb/presto/pull/13183\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13450", "Evalute filter once per Dictionary value", null, "bhhari", "10/05/19, 06:03:22 AM", "```\r\n# Before\r\n\r\nBenchmark                                          (typeSignature)  (withNulls)  Mode  Cnt  Score   Error  Units\r\nBenchmarkSelectiveStreamReaders.read            varchar_dictionary         true  avgt   20  0.053 \u00b1 0.002   s/op\r\nBenchmarkSelectiveStreamReaders.read            varchar_dictionary        false  avgt   20  0.053 \u00b1 0.003   s/op\r\nBenchmarkSelectiveStreamReaders.readAllNull     varchar_dictionary          N/A  avgt   20  0.009 \u00b1 0.001   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter  varchar_dictionary         true  avgt   20  0.174 \u00b1 0.006   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter  varchar_dictionary        false  avgt   20  0.311 \u00b1 0.032   s/op\r\n\r\n#With optimization\r\n\r\nBenchmark                                          (typeSignature)  (withNulls)  Mode  Cnt  Score    Error  Units\r\nBenchmarkSelectiveStreamReaders.read            varchar_dictionary         true  avgt   20  0.057 \u00b1  0.004   s/op\r\nBenchmarkSelectiveStreamReaders.read            varchar_dictionary        false  avgt   20  0.052 \u00b1  0.002   s/op\r\nBenchmarkSelectiveStreamReaders.readAllNull     varchar_dictionary          N/A  avgt   20  0.009 \u00b1  0.001   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter  varchar_dictionary         true  avgt   20  0.078 \u00b1  0.005   s/op\r\nBenchmarkSelectiveStreamReaders.readWithFilter  varchar_dictionary        false  avgt   20  0.092 \u00b1  0.006   s/op\r\n\r\n\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13451", "Optimize PlanFragment Serialization", "Nikhil Collooru", "NikhilCollooru", "10/28/19, 08:27:37 PM", "Do not send PlanFragment's  jsonRepresentation, statsAndCosts to workers as they don't need it. \r\nThey are only for the coordinator UI purpose.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13452", "Reduce excessive memory usage by ExchangeClient", null, "aweisberg", "10/11/19, 07:58:10 PM", "ExchangeClient when it first runs will send a request for a page to hash_partition_count nodes in the cluster. This resulted in an instance with 10 gigabytes of data in ExchangeClients and another 8 gigabytes in flight in jetty. This was causing full GCs since I was testing with a 40 gigabyte heap.\r\n\r\nPreviously the average response size calculation would reset to 0 every time a 0 is sampled. This occurs quite often when a 0 response is received from any partition that doesn't have data. This is expected to cause ExchangeClient to poll very aggressively.\r\n\r\nThis change has ExchangeClient track the expected averageResponseSize using an [exponential moving average](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average). This is initialized to 1 megabyte (default page size). The alpha is defaults to 0.1.\r\n\r\nWhen deciding how many nodes to contact the EMA is used and can function as a proxy for how hard it is to find more data. The lower the value returned by the EMA the more nodes will be contacted. This mimics the current behavior which might not be ideal, but at least now a 0 doesn't bring the average down to 0 immediately.\r\n\r\nThe number of bytes requested is set to `min(averageResponseSize * 2, maxResponseSize)`. This makes the exchange client more likely to only request a single (or fewer) pages if it ends up contacting many nodes. Previously it would always use maxResponseSize which poorly bounded the amount of data that could be returned.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Reduce excessive memory usage by ExchangeClient\r\n```", "NaN"], ["13453", "Optimize HiveSplit serialization", "Andrii Rosa", "arhimondr", "10/01/19, 12:00:07 AM", "Serializing \"schema\" for every splits is very expensive. Instead of creating it on the coordinator it can be re-constructed on a worker.\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Improve cpu load on coordinator by reducing the cost of serializing ``HiveSplit``s\r\n```\r", "NaN"], ["13458", "Enabled flat map tests for varchar and varbinary keys", "Maria Basmanova", "mbasmanova", "09/26/19, 06:49:00 PM", "#13341 added selective reader for varchar, char and varbinary types. This allows the flat map reader to work for maps with keys of these types.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13460", "More spatial joins", "James Gill", "jagill", "09/27/19, 07:30:15 PM", "Enable spatial joins (broadcast and distributed) for ST_Equals, ST_Overlaps, ST_Crosses, and ST_Touches.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n-----------------\r\n* Enable spatial joins (broadcast and distributed) for ST_Equals, ST_Overlaps, ST_Crosses, and ST_Touches.\r\n```", "NaN"], ["13462", "Refactor Block#getObject to getBlock", "Dmitry Vinnik", "dmitryvinn", "09/27/19, 02:36:28 AM", "== RELEASE NOTES ==\r\n\r\n```\r\nSPI Changes:\r\n\r\nRename `Block#getObject` to `Block#getBlock` and remove unnecessary `clazz` parameter.\r\n```", "NaN"], ["13464", "Fix NPE in MapDirectSelectiveStreamReader", "Maria Basmanova", "mbasmanova", "09/27/19, 08:34:52 AM", "Fixes #13463\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13466", "Fix NPE in LongDirectSelectiveStreamReader", "Maria Basmanova", "mbasmanova", "09/27/19, 08:34:36 AM", "Fixes #13465\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13467", "Write temp files into temp directory for sorted table", "Shixuan Fan", "shixuan-fan", "10/02/19, 01:27:50 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13468", "Fix ArrayIndexOutOfBoundsException in LongDictionarySelectiveStreamReader", "Maria Basmanova", "mbasmanova", "09/27/19, 08:34:26 AM", "Fixed the following failure that occurred in a verifier run:\r\n\r\n```\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 15\r\n    at com.facebook.presto.orc.reader.LongDictionarySelectiveStreamReader.read(LongDictionarySelectiveStreamReader.java:155)\r\n    at com.facebook.presto.orc.reader.LongSelectiveStreamReader.read(LongSelectiveStreamReader.java:116)\r\n    at com.facebook.presto.orc.OrcSelectiveRecordReader.getNextPage(OrcSelectiveRecordReader.java:298)\r\n    at com.facebook.presto.hive.orc.OrcSelectivePageSource.getNextPage(OrcSelectivePageSource.java:84)\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13471", "De-flake TestUnweightedDoubleReservoirSample#testMany", "Ami Tavory", "atavory", "10/09/19, 06:14:56 AM", "Weighted reservoir sampling tests, some of which were randomized, sporadically failed. Adjusted the bounds to account for many times the tests run.\r\n\r\nFixes #13459\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13472", "Allow zoom 0 Bing Tiles", "James Gill", "jagill", "10/03/19, 02:50:55 PM", "These are not part of the strict spec, but are conceptually\r\nconsistent and absorb a common edge case in work flows.\r\n\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Allow Bing Tiles at zoom level 0\r", "NaN"], ["13473", "[Not for Review] Prepare to merge #13393", "Wenlei Xie", "wenleix", "09/28/19, 05:24:36 AM", "Cherry-pick of https://github.com/prestosql/presto/pull/58\r\n\r\nCo-authored-by: Amit Chopra <amitchopra@fb.com>\r\n```\r\n== RELEASE NOTES ==\r\nHive Connector Changes\r\n* Reduce GC pressure from Parquet reader by constraining the maximum column read size.\r\n\r\n```\r\n\r", "NaN"], ["13474", "Use uuid for temp file when writing sorted table", "Shixuan Fan", "shixuan-fan", "09/28/19, 12:24:19 AM", "Currently the temp file name for sorted table is in the format of\r\nquery_id + bucket_number + increasing_number_from_0. This has the\r\npotential of leading to corruption when a query is running in\r\nrecoverable grouped execution mode. Assuming a task \"fake died\" and\r\nwe schedule the bucket to another task, then these two tasks will\r\nbe working on the same set of temp file names.\r\n\r\nTo fix this, instead of using an increasing number from 0, we use\r\nuuid to avoid name clashing.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Fix a bug that might lead to corruption when writing sorted table in\r\n  recoverable grouped execution mode.\r\n```", "NaN"], ["13476", "Use TableWriterMergeNode to check recoverability", "Shixuan Fan", "shixuan-fan", "09/30/19, 05:22:54 PM", "After we introduced TableWriterMergeNode, the root node of writer\r\nfragment is TableWriterMergeNode, so we should use TableWriterMergeNode\r\nto check recoverability.\r\n\r\nTo make sure this is properly tested, this change would also enable\r\nonly two tests in TestHiveRecoverableGroupedExecution. This test is\r\nknown to be flaky when we run too many tests because GC would slow\r\ndown query execution and cause timeout. Only enabling one seems\r\na reasonable compromise.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13477", "Support max_tasks_per_stage for scan", "cem cayiroglu", "cemcayiroglu", "10/24/19, 01:52:10 AM", "stage.max-tasks-per-stage configuration property can be used by to limit the number of tasks for scan.  \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n\r\n* Respect stage.max-tasks-per-stage to limit number of tasks for scan.\r\n```\r\n\r", "NaN"], ["13479", "The example table creating have a syntax error", "Peng Yu", "yupbank", "09/30/19, 11:27:24 PM", " \r\n== NO RELEASE NOTE ==\r\n ", "NaN"], ["13481", "Add syntax support for CREATE FUNCTION", "Leiqing Cai", "caithagoras", "10/08/19, 04:53:32 AM", "Introduce CREATE FUNCTION syntax as according to https://github.com/prestodb/presto/issues/13254\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13482", "Mention formatting guidelines in the pull request template", null, "aweisberg", "10/01/19, 12:00:42 AM", "Please fill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13484", "Support transaction in FunctionNamespaceManager", "Leiqing Cai", "caithagoras", "10/10/19, 07:01:47 PM", "Highlights of the PR:\r\n- `InMemoryFunctionNamespaceManager` for testing purpose.\r\n  - Requires proper definition for `createFunction`.\r\n  - Requires proper definition for `SqlInvokedRegularFunction`\r\n- Transaction support for `FunctionNamespaceManager`.\r\n  - For this PR, transaction support is limited to the read-only method `#getFunctions`.\r\n- Caching support for `FunctionNamespaceManager<SqlInvokedRegularFunction>`\r\n  - Covers both `#getFunctions` and `#getFunctionMetadata`\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13485", "Add session property for nested subfield filters", null, "bhhari", "10/22/19, 02:26:11 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13487", "Optimize PartitionedOutputOperator - part 2", "Ying", "yingsu00", "10/02/19, 12:15:06 AM", "The commits in this PR were cherry-picked from https://github.com/prestodb/presto/pull/13183 and were reviewed there.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13489", "Removes interface FileRewriter #13478", "Dennis Shirazi", "dennisshirazi", "10/03/19, 11:43:45 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13491", "Implement interfaces for RowExpressionTranslation", "Saksham", "sachdevs", "10/15/19, 03:24:30 AM", "Implement Row Expression translation interfaces in new module presto-expression.\r\n\r\nNOTE: See an example implementation using these interfaces in #13526 \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13492", "Remove unused variable in ServerMainModule#setup", "Wenlei Xie", "wenleix", "10/10/19, 03:23:52 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13493", "Do not fail FINISHING queries", "Andrii Rosa", "arhimondr", "10/28/19, 04:48:24 PM", "When the query state enters FINISHING, the only thing remaining is to commit\r\nthe transaction. It should only be failed if the transaction commit fails.\r\n\r\nFor example, if a query like `SELECT ... LIMIT 1` returns 1 row as a result,  the query is considered `FINISHING`, and the only thing left - is to commit the current transaction. As long as the client gets a single row, there's no reason to fail the query if, let's say, subsequent scan of the remaining rows fails. The only thing that the query should be failed for - is if the transaction commit fails. \r\n\r\nPractically speaking this fixes the unexpected failures in a case when:\r\n\r\n1. Query has finished\r\n2. Query is committing\r\n3. The transaction is gone\r\n4. The transactional metastore is gone\r\n5. Some unfinished scans still try to enumerate some splits\r\n6. Since the transaction / transactional metastore are gone - the split enumeration fails\r\n7. Failed split enumeration fails the `FINISHING` query \r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13494", "Use empty Envelope instead of null in deserialization", "James Gill", "jagill", "10/03/19, 06:29:45 PM", "When deserializing the envelope of an empty geometry, it's semantically\r\nmore intuitive to return an empty envelope, instead of a null.\r\n\r\n== NO RELEASE NOTE ==\r", "NaN"], ["13497", "Rename test -> execution", "Ajay George", "ajaygeorge", "10/15/19, 06:13:37 PM", "\r\n`beginTest` and `endTest` naming can be confusing since the `setUp` and `tearDown` invocations use the same code path. Proposal is to rename it to a more generic `beginExecution` and `endExecution`\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13499", "Limit size of Expression/RowExpression during constant folding", "Yi He", "hellium01", "10/11/19, 02:42:19 AM", "This PR put a limit on the size of constant we can generate when we do constant folding.  \r\n\r\nFixes issue: https://github.com/prestodb/presto/issues/13360\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve coordinator stability on parsing functions that may create large constant arrays/maps/structs (e.g., `SEQUENCE`, `REPEAT`, `ARRAY[...]`, etc) by delaying the evaluation on these functions on workers.\r\n```\r", "NaN"], ["13500", "Fix MapTransformValueFunction to handle exceptions and reset Blockbuilder before throwing", "Nikhil Collooru", "NikhilCollooru", "10/05/19, 07:40:52 AM", "TRANSFORM_VALUES fails even when put inside a TRY block. So add code in MapTransformValueFunction to try-catch exceptions that occur during transform and handle it gracefully (by reseting/closing the BlockBuilder ) so that the next transform of the TRY doesn't see an inconsistent state.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13501", "Add cache for orc file tail information", "Shixuan Fan", "shixuan-fan", "10/17/19, 03:14:20 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13504", "Adding presto-pinot with pushdown working", "Devesh Agrawal", "agrawaldevesh", "10/26/19, 05:42:18 AM", "Adding presto-pinot with pushdown\r\n\r\nThis presto-pinot connector works as follows:\r\n* It builds upon the new plan connector optimizer framework to push the\r\n  maximal query to the pinot system.\r\n* By default, aggregation and limit queries are send to the pinot broker,\r\n  while just filter queries are send to the servers parallelly. This is\r\n  useful when joining against another (say hive) table wherein the join\r\n  needs to be parallelized and segments from pinot (fetched directly from\r\n  the pinot servers) need to be joined with hive splits. If you wish to\r\n  not send any queries to the broker, please set the catalog session\r\n  property prefer_broker_queries to false.\r\n* You can disable parallel queries (useful for low latency deployment)\r\n  we do at Uber, by setting the catalog property\r\n  forbid_segment_queries=false.\r\n* The connector needs a way to talk to the controller (to fetch the\r\n  brokers and the table schema). You can either provide the controller\r\n  URI or the URI of the Muttley proxy (an RPC proxy mechanism we use at\r\n  Uber). It is also possible to use the pinot-rest-proxy instead of\r\n  going to the pinot-broker (again something we use for our test pinot\r\n  clusters at Uber).\r\n\r\nIn addition the pinot connector would split the filter expressions such\r\nthat expressions that can be pushed down to pinot are done so, and those\r\nthat aren't are handled in presto.\r\n\r\nIt handles the full range of pinot PQL and respects pinot's weirdnesses\r\nto guarantee that all results are still in the expected SQL semantics.\r\n\r\nUsage: See the newly added connector/pinot.rst documentation file.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n* Added a new Pinot connector\r\n\r\nSPI Changes\r\n* Split `ConnectorPlanOptimizerProvider` to have two phases for connectors to participate query optimization: `LOGICAL` and `PHYSICAL`. The two phases correspond to pre- and post-shuffle optimization respectively.\r\n```\r", "NaN"], ["13505", "Intersect pushdown predicates", null, "oerling", "10/16/19, 09:06:13 PM", "Predicates can be pushed down into a scan multiple times, for example\nwhen a CTE has local predicates initially and acquires new predicates\nfrom the enclosing context. Adds an intersect method to\nDomainTranslater.ExtractionResult.", "NaN"], ["13507", "Move BeginTableWrite logic to the scheduler", "Rebecca Schlussel", "rschlussel", "10/23/19, 05:48:53 PM", " BeginTableWrite is a giant hack, that prepares for writing a table at the end of planning.  For full stage retries (#13438), we need to be able to restart a table write. In preparation for full stage retries, this pr moves the BeginTableWrite logic to the scheduler, where it can be restarted on retry.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13508", "Close ColumnWriters in OrcWriter when IOException is thrown", "Jacob Lammott", "golammott", "10/08/19, 09:28:23 PM", "If `OrcWriter` throws IOException its internal ColumnWriters can be marked as closed, while the `OrcFileWriter` and `OrcStorageManager` still view the writers as open. This can lead to `IllegalStateExceptions` from subsequent calls.\r\n\r\nThis fix adds Try-Catch-Finally logic to force ColumnWriters to reset when an IOException is thrown, and thus preventing `IllegalStateExceptions`.\r\n\r\nIssue: #13400 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Fix ORC writer rollback failure due to exception thrown during rollback.\r\n```\r", "NaN"], ["13509", "Disable TestHiveRecoverableGroupedExecution", "Shixuan Fan", "shixuan-fan", "10/08/19, 01:03:14 AM", "This test is still flaky even if only enabling one test, which\r\nmeans it needs more investigation.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13512", "Fix doc typos at Array Functions and Operators", "Akira Tameoka", "zettaittenani", "10/09/19, 06:13:17 PM", "I was reading the doc below and it seems to be typo.\r\nhttps://prestodb.github.io/docs/current/functions/array.html\r\n\r\n```\r\narrays_overlap(x, y) \u2192 boolean\r\nTests if arrays x and y have any any non-null elements in common. Returns null if there are no non-null elements in common but either array contains null.\r\n                             ~~~~~~~\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13513", "Handle empty string columns in slice reader", null, "bhhari", "10/14/19, 10:29:04 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13516", "Fix NPE in LogDictionarySelectiveStreamReader", null, "bhhari", "10/09/19, 12:32:42 AM", "``` == NO RELEASE NOTE == ```", "NaN"], ["13517", "Fix typo in HandleResolver", "Leiqing Cai", "caithagoras", "10/09/19, 01:00:40 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13518", "Handle null dataStream in SliceDictionary reader", null, "bhhari", "10/09/19, 12:32:13 AM", "```\n== NO RELEASE NOTE ==\n```", "NaN"], ["13519", "Move LIMIT 0 rule to its dedicated optimization phase", null, "godofskies", "10/10/19, 09:20:25 PM", "Presto has the `MergeLimitWithSort`, `MergeLimitWithTopN` optimizer rules that\r\nare transforming ORDER BY + LIMIT into specialized nodes (e.g: `TopNNode`,\r\n`MergeLimitWithDistinct`) that are then expanded into specialized operators\r\n(such as `TopNOperator`) that can execute TopN operation (ORDER BY + LIMIT)\r\nefficiently.\r\n\r\nThere's another rule, the `EvaluateZeroLimit` that replaces the LimitNode with\r\na static empty values tupple.\r\n\r\nThe problem is that these rules have non deterministic order as all of them are\r\nexecuted within a single iterative optimizer loop. If the replacement of the\r\n`LimitNode` happens before the `EvaluateZeroLimit`, the `LIMIT 0` operation is\r\nnot getting optimized.\r\n\r\nThis commit fixes the issue by moving the `EvaluateZeroLimit` to its own\r\noptimization phase. We also eliminate the plan time construction of the\r\nTopNNode and allow the optimization phase to determine the best way to combine\r\nLIMIT nodes with ORDER BY node.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Optimization provided for LIMIT 0 queries\r\n```", "NaN"], ["13521", "Move RowExpression translation above AddExchange", "James Sun", "highker", "10/22/19, 02:31:05 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13523", "Generate unique index for PartitionColumns", null, "bhhari", "10/18/19, 06:30:29 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13525", "Add assertion in TableFinishOperator", "Shixuan Fan", "shixuan-fan", "10/10/19, 04:00:02 PM", "When running in recoverable grouped execution mode, We should\r\nfail the query if we received a page for a committed lifespan\r\nand stage from the same task whose written data is committed.\r\nOtherwise we would silently corrupt the data.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13526", "Implement Basic Example For Jdbc Row Expression Translation", "Saksham", "sachdevs", "10/22/19, 12:50:17 AM", "See #13491 for interface that this is built on.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nNO RELEASE NOTE\r\n```", "NaN"], ["13527", "Passing X_Forwarded_For in header from proxy server", "Swapnil", "swapsmagic", "10/17/19, 04:24:52 PM", "Presto Proxy doesn't pass client address and so presto cluster thinks proxy being the initiating client. As part of this change, proxy will start sending client address in X_Forwarded_For header so it can be used by Presto cluster and log it correctly.\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* start forwarding X_Forwarded_For in header from Proxy\r", "NaN"], ["13528", "Resolve SMILE encoding issue with JsonRaw value", "Nikhil Collooru", "NikhilCollooru", "10/14/19, 06:59:34 PM", "SMILE does not support serializing raw values. TaskFinishInfo has connectorOutputMetadata as a raw value and hence SMILE errors. Made code changes to workaround this. \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13530", "Add utilities for Row expression predicate pushdown", "Yi He", "hellium01", "10/16/19, 06:45:26 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13531", "Fix table scan results with disjoint pushdown filters", null, "bhhari", "10/26/19, 03:57:34 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13532", "Minor cleanup on SubfieldExtractor::toColumnExtractor", "James Sun", "highker", "10/11/19, 02:41:34 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13535", "Support using remote HDFS as storage in Raptor", "Jiexi Lin", "jessesleeping", "10/22/19, 09:27:08 PM", "TBD\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nRaptor Changes\r\n* Change `storage.data-directory` from path to URI. For existing deployment on local flash, a scheme header \"file://\" should be added to the original config value.\r\n* Change error code name `RAPTOR_LOCAL_FILE_SYSTEM_ERROR` to `RAPTOR_FILE_SYSTEM_ERROR`.\r\n```\r", "NaN"], ["13537", "Update testing-mysql-server dependency", "Wenlei Xie", "wenleix", "10/11/19, 07:29:15 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13540", "Add a check to update outputPositions", null, "bhhari", "10/14/19, 05:44:55 PM", "This check was missing for DecimalReaders. This is required check whether we can modify the outputPositions array while compacting values.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13541", "Fix selective readers output positions when reading null streams", null, "bhhari", "10/17/19, 04:24:01 AM", "if there are 3 columns that have filters\r\n1 col  ---> produced 1, 2, 3 positions\r\n2 col ---> all nulls with filter(null allowed) should return back 1, 2, 3 positions but because of the bug that we are copying the positions in this case we are returning 0,0,0\r\n3 col ---> apply filter on position output from 2nd col (1,2,3)\r\n\r\nThe fix is to copy copy the positions passed to outputPositions in this case.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13543", "Add document for ip_prefix function", "Ying", "yingsu00", "10/15/19, 02:52:14 AM", "Add missing document for https://github.com/prestodb/presto/pull/13382\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13544", "Add release notes for 0.227", "Ying", "yingsu00", "10/16/19, 07:10:09 PM", "Missing release notes:\r\nhttps://github.com/prestodb/presto/issues/13490", "NaN"], ["13546", "Move SetOperationNode to SPI", "James Sun", "highker", "10/22/19, 04:01:11 AM", "```\r\n== RELEASE NOTES ==\r\nGeneral \r\n* Move `SetOperationNode` to SPI. Connectors can now alter query plan to have union, intersect, and except.\r\n```\r", "NaN"], ["13549", "Update to Facebook Airlift", "Wenlei Xie", "wenleix", "10/17/19, 07:21:25 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13551", "Remove RowExpression utils from SPI", "James Sun", "highker", "10/15/19, 06:25:41 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n* Removed most `RowExpression` utilities to module presto-expressions.\r", "NaN"], ["13554", "Minor deserialization bug fix in TableFinishInfo", "Nikhil Collooru", "NikhilCollooru", "10/16/19, 07:01:06 AM", "Minor deserialization bug fix in TableFinishInfo\r\nThe bug is introduced in 7f3633ab10ef3a8bb14b1b4a7d579ac6b407a90b\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13556", "Reset DictionaryBlock values after getBlock", null, "bhhari", "10/17/19, 04:27:12 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13557", "Fix testBytes in TupleDomainFilter", null, "bhhari", "10/16/19, 06:36:07 PM", "This was causing incorrect rows to be selected.\r\nThe testBytes check was incorrectly returning true if the length matches and first char is same.\r\nchanged the place where we return.\r\n\r\nFixes #13559\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13558", "Include error message in SimpleHttpResonseHandler for SMILE", "Nikhil Collooru", "NikhilCollooru", "10/18/19, 08:51:03 PM", "This resolves github issue #12835 . When exception occurs, the responsebytes of the Response have the stack trace. Including this trace in the error message helps debugging the cause when SMILE encoding is enabled.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13560", "Add RowExpressionPredicateExtractor", "Yi He", "hellium01", "10/16/19, 11:20:14 PM", "Separate out from PR #13112\r\n```\r\nNo release note\r\n```", "NaN"], ["13561", "Support CREATE FUNCTION execution", "Leiqing Cai", "caithagoras", "10/30/19, 11:26:17 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for ``CREATE FUNCTION``\r\n```", "NaN"], ["13562", "Add release note for 0.228", "Rongrong Zhong", "rongrong", "10/28/19, 11:35:54 PM", "# Missing Release Notes\r\n## Rebecca Schlussel\r\n- [x] all checked\r\n- https://github.com/prestodb/presto/pull/13472 Allow zoom 0 Bing Tiles\r\n## Wenlei Xie\r\n- [x] all checked\r\n- https://github.com/prestodb/presto/pull/13183 Optimize PartitionedOutputOperator\r\n\r\n# Extracted Release Notes\r\n- #13434 James Sun: Add common scalar reads to OrcInputStream\r\n  - Streamline ORC read path for varints and fixed width numbers.\r\n- #13452 Andrii Rosa: Reduce excessive memory usage by ExchangeClient\r\n  - Reduce excessive memory usage by ExchangeClient\r\n- #13499 Wenlei Xie: Limit size of Expression/RowExpression during constant folding\r\n  - Improve coordinator stability on parsing functions that may create large constant arrays/maps/structs (e.g., `SEQUENCE`, `REPEAT`, `ARRAY[...]`, etc) by delaying the evaluation on these functions on workers.\r\n- #13508 James Sun: Close ColumnWriters in OrcWriter when IOException is thrown\r\n  - Fix ORC writer rollback failure due to exception thrown during rollback.\r\n- #13519 Andrii Rosa: Move LIMIT 0 rule to its dedicated optimization phase\r\n  - Optimization provided for LIMIT 0 queries\r\n- #13551 James Sun: Remove RowExpression utils from SPI\r\n  - Removed most `RowExpression` utilities to module presto-expressions.\r", "NaN"], ["13565", "Fix compareRanges logic", null, "bhhari", "10/20/19, 08:39:31 AM", "Fixes #13564 \r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13570", "Fix Flaky TestTransactionManager.testExpiration", "cem cayiroglu", "cemcayiroglu", "10/23/19, 02:21:48 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13574", "Refactor Differential Entropy for Mutual Information Classification", "Ami Tavory", "atavory", "11/15/19, 04:24:36 PM", "== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Move strategy factory code from udf to base class of strategy\r\n* Add population counts/weights to reservoir samples\r\n\r\nTogether with #13203, this forms the basis for mutual information classification udfs #13163.", "NaN"], ["13575", "Add cache for stripe metadata", "Shixuan Fan", "shixuan-fan", "10/31/19, 01:22:06 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13578", "Fix SliceReader ArrayOutOfBounds when presentStream is null", null, "bhhari", "10/22/19, 05:42:35 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13581", "Revert reserved words introduced in CREATE FUNCTION syntax support", "Leiqing Cai", "caithagoras", "10/22/19, 07:50:45 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13582", "Export round trip count for update/info/status", "Wenlei Xie", "wenleix", "10/27/19, 05:41:06 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13584", "Fail query with large taskUpdate size", "Nikhil Collooru", "NikhilCollooru", "10/24/19, 09:01:03 PM", "The max taskUpdate size can be set using a new parameter\r\nexperimental.internal-communication.max-task-update-size.\r\nAlso add code changes to monitor the size of taskUpdates\r\nthat do not have the plan info. For this we sample\r\n1 in 100 to avoid concurrency problems.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add configuration property ``experimental.internal-communication.max-task-update-size`` to limit the size of the taskUpdate.\r\n```\r", "NaN"], ["13591", "Add TestHivePushdownDistributedQueries", null, "bhhari", "11/07/19, 01:33:15 AM", "depends on https://github.com/prestodb/presto/pull/13636\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13592", "Add common InternalCommunicationConfig to turn on HTTPS", "Yi He", "hellium01", "10/26/19, 04:08:41 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* added configs to use common HTTPS settings for internal communications\r\n```\r", "NaN"], ["13593", "Do not overcommit memory in HashBuilderOperator", "Andrii Rosa", "arhimondr", "10/23/19, 10:08:45 PM", "For the joins where the build side consumes the input from a stateful\r\noperator (Aggregation, WindowFunction, etc...) overcomitting memory\r\nresults in overall peak memory increase leading to query failures\r\ndue to exceeding the overall distributed memory limit.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13594", "Parquet Dictionary Predicate Pushdown Fixes", "James Petty", "pettyjamesm", "10/24/19, 12:27:42 AM", "Port of https://github.com/prestosql/presto/pull/1846.\r\n\r\nParquet dictionary pushdown was refactored in prestodb/presto#6892 to remove a nested loop iteration but accidentally left the inner loop break statement behind. This meant that dictionary predicate pushdown would read at most 1 dictionary.\r\n\r\nIn addition to fixing the pushdown behavior, this PR adds support for checking the dictionary pushdown on each column skipping additional dictionary reads once the block can already be filtered. \r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Fix parquet predicate pushdown on dictionaries to consider more than just the first predicate column\r\n* Improve parquet predicate pushdown on dictionaries to avoid reading additional data after successfully eliminating a block \r\n```\r", "NaN"], ["13595", "Reliable Resource Group With Versioning", "Swapnil", "swapsmagic", "06/03/20, 08:10:54 PM", "Problem:\r\nIn case of resource group changes, if we make any leaf resource group to be intermediate resource group by introducing new resource group, all the queries assigned to the old resource group before the change and are queued, will fail when they are being processed. And the reason for that, we fail all queries which try to run on an intermediate resource group. But in this scenario, we should allow those queries to be run on intermediate resource group and not fail.\r\n\r\nSolution:\r\nIntroducing versioning to Resource Groups. Please find more information on the behavior of the resource group versioning as below. \r\n- With every change in the resource group, Resource group manager will create a new version of each of the resource groups. \r\n- Queries queued on the old version will move to the new version if the new version resource group is still a leaf node. Otherwise, queued queries stays with old version and continue getting executed as per the old resource group contraints (i.e. hard concurrency)\r\n- For every new query to run, resource group manager first checks in the older version if any pending query eligible to run and runs them first before moving to the new version.\r\n- For any new resource group, when running query it checks total running query which includes all previous version queries as well. And if that still within the constraint, then only allow the new resource group. While queuing queries, it only checks it's existing limit and queue query accordingly. \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Introducing Resource Group Versioning\r\n\r\nBug Fixes\r\n* Existing resource group was not reliably handle transition of leaf resource group internal and vice versa. Coordinator restart was needed to handle it reliably. ", "NaN"], ["13597", "Refactor SqlQueryScheduler", "Rebecca Schlussel", "rschlussel", "10/25/19, 08:15:32 PM", "Refactorings in SqlQueryScheduler in preparation for stage retries (https://github.com/prestodb/presto/issues/13438)\r\n\r\n* Create SqlStageExecutionInfo so we can get rid of the hacky way\r\n  StageLinkages and StageSchedulers were created\r\n* Separate out the parts of createStreamingLinkedStageExecutions that\r\n  create the bucketToPartition, StageSchedulers, and child stages so\r\n  that they aren't interleaved anymore. This refactoring changed the list of\r\n  stageExecutions from a preorder to a postorder representation of the tree,\r\n  so all the places that expected the root at the head of the list now look\r\n  at the end of the list.\r\n* Get rid of the stageExecutions, stageLinkages and\r\n  stageSchedulers fields and instead have a single map of\r\n  SqlStageExecutionInfos\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13601", "Modularize Hive Metastore", "Islam Ismailov", "islamismailov", "11/08/19, 08:56:03 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Metastore interface is separated into a separate module to reduce monolithicness \r\n```\r", "NaN"], ["13602", "Improve BatchStreamReader - Part 1", "Ying", "yingsu00", "11/13/19, 11:18:46 PM", "Cherry-pick of https://github.com/prestosql/presto/pull/555\r\n\r\nThe memory tracking for StreamReader was firstly introduced in 97552478c9 Add memory tracking for StreamReader local buffers and then removed in\r\n303cecacf7 Remove unnecessary StreamReader.close method\r\n47423bbb2c Remove unused systemMemoryContext from OrcBatchRecordReader\r\nbe749f6197 Remove unused systemMemoryContext variable from XxxBatchStreamReader\r\n\r\nBenchmarkBatchStreamReaders results:\r\n\r\nBefore\r\nBenchmark                                  (typeSignature)  (withNulls)  Mode  Cnt    Score   Error  Units\r\nBenchmarkBatchStreamReaders.readBlocks             boolean      PARTIAL  avgt   60   45.432 \u00b1 0.720  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             boolean         NONE  avgt   60   34.396 \u00b1 0.629  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             boolean          ALL  avgt   60    7.894 \u00b1 0.166  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             tinyint      PARTIAL  avgt   60   47.784 \u00b1 1.106  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             tinyint         NONE  avgt   60   39.331 \u00b1 0.747  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             tinyint          ALL  avgt   60    8.269 \u00b1 0.217  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks            smallint      PARTIAL  avgt   60   87.624 \u00b1 3.163  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks            smallint         NONE  avgt   60   93.390 \u00b1 2.122  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks            smallint          ALL  avgt   60    8.886 \u00b1 0.289  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             integer      PARTIAL  avgt   60   97.352 \u00b1 2.974  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             integer         NONE  avgt   60  106.238 \u00b1 2.092  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             integer          ALL  avgt   60    9.017 \u00b1 0.301  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks              bigint      PARTIAL  avgt   60  111.976 \u00b1 3.586  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks              bigint         NONE  avgt   60  133.045 \u00b1 4.189  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks              bigint          ALL  avgt   60    8.871 \u00b1 0.322  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks           timestamp      PARTIAL  avgt   60  165.562 \u00b1 5.700  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks           timestamp         NONE  avgt   60  238.276 \u00b1 7.139  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks           timestamp          ALL  avgt   60    9.239 \u00b1 0.506  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks      varchar_direct      PARTIAL  avgt   60  208.502 \u00b1 3.426  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks      varchar_direct         NONE  avgt   60  279.045 \u00b1 6.075  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks      varchar_direct          ALL  avgt   60   32.427 \u00b1 0.826  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks  varchar_dictionary      PARTIAL  avgt   60   52.460 \u00b1 1.217  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks  varchar_dictionary         NONE  avgt   60   38.757 \u00b1 0.596  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks  varchar_dictionary          ALL  avgt   60   32.064 \u00b1 0.518  ms/op\r\n\r\nAfter\r\nBenchmark                                  (typeSignature)  (withNulls)  Mode  Cnt    Score   Error  Units\r\nBenchmarkBatchStreamReaders.readBlocks             boolean      PARTIAL  avgt   60   26.017 \u00b1 0.436  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             boolean         NONE  avgt   60   14.980 \u00b1 0.255  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             boolean          ALL  avgt   60    8.180 \u00b1 0.195  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             tinyint      PARTIAL  avgt   60   27.405 \u00b1 0.604  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             tinyint         NONE  avgt   60   16.941 \u00b1 2.228  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             tinyint          ALL  avgt   60    7.880 \u00b1 0.181  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks            smallint      PARTIAL  avgt   60   43.038 \u00b1 1.083  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks            smallint         NONE  avgt   60   49.573 \u00b1 2.417  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks            smallint          ALL  avgt   60    8.062 \u00b1 0.105  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             integer      PARTIAL  avgt   60   49.302 \u00b1 0.955  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             integer         NONE  avgt   60   56.010 \u00b1 1.357  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks             integer          ALL  avgt   60    8.114 \u00b1 0.265  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks              bigint      PARTIAL  avgt   60   60.625 \u00b1 1.964  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks              bigint         NONE  avgt   60   82.082 \u00b1 4.071  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks              bigint          ALL  avgt   60    8.117 \u00b1 0.136  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks           timestamp      PARTIAL  avgt   60  131.245 \u00b1 3.605  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks           timestamp         NONE  avgt   60  199.041 \u00b1 4.008  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks           timestamp          ALL  avgt   60    8.134 \u00b1 0.155  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks      varchar_direct      PARTIAL  avgt   60   61.129 \u00b1 1.331  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks      varchar_direct         NONE  avgt   60   71.723 \u00b1 2.255  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks      varchar_direct          ALL  avgt   60   21.339 \u00b1 0.940  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks  varchar_dictionary      PARTIAL  avgt   60   55.705 \u00b1 1.959  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks  varchar_dictionary         NONE  avgt   60   29.450 \u00b1 0.479  ms/op\r\nBenchmarkBatchStreamReaders.readBlocks  varchar_dictionary          ALL  avgt   60   20.122 \u00b1 0.396  ms/op\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Improve BatchStreamReader performance\r\n```\r", "NaN"], ["13603", "Optimize ByteSelectiveStreamReader when reading contiguous rows with no nulls and no filter", "Ying", "yingsu00", "11/12/19, 05:40:33 AM", "JMH benchmark results show 2x improvement when there is no nulls and no filters.\r\n\r\n    Before\r\n    Benchmark                             (typeSignature)  (withNulls)  Mode  Cnt  Score   Error  Units\r\n    BenchmarkSelectiveStreamReaders.read          tinyint         true  avgt   20  0.045 \u00b1 0.001   s/op\r\n    BenchmarkSelectiveStreamReaders.read          tinyint        false  avgt   20  0.039 \u00b1 0.001   s/op\r\n\r\n    After\r\n    Benchmark                             (typeSignature)  (withNulls)  Mode  Cnt  Score   Error  Units\r\n    BenchmarkSelectiveStreamReaders.read          tinyint         true  avgt   20  0.043 \u00b1 0.002   s/op\r\n    BenchmarkSelectiveStreamReaders.read          tinyint        false  avgt   20  0.018 \u00b1 0.001   s/op\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13604", "Use JTS instead of ESRI for geometries, Parts 1 and 2", "James Gill", "jagill", "12/09/19, 07:02:27 PM", "I am in the process of refactoring the geospatial functions to use JTS instead of Esri.  Why?\r\n\r\n* JTS is a more standard library, that is compatible with PostGIS and GEOS and most geometry libraries.\r\n* It conforms to the ISO standards better.\r\n* Serialization is about 80% of the CPU for many geometrical functions, and\r\n* JTS serializes about 20% faster than ESRI, and has the potential of serializing 10-20x faster.\r\n\r\nSince geometries are always converted to/from slices, it's fine for some functions to use Esri and some to use JTS (this is the case already).  This PR converts most of the \"easy\" functions (those that don't involve much in the way of logical or numerical changes).  However, JTS uses the standard order for polygon rings (counter-clockwise) instead of Esri's clockwise.  This means that many of the tests need to have the order of the vertices reversed.  I've isolated all of those changes into the commit that uses JTS for conversion from/to WKT.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Use JTS instead of Esri for many geometrical operations.\r\n  + Polygon WKTs must have closed loops.  Previously Esri would close the loops for you.\r\n  + Certain other invalid geometries will fail to be created from WKTs, such as\r\n    `LINESTRING(0 0, 0 0, 0 0)`.\r\n  + Returned WKTs may have a different point order.\r\n  + Fixes incorrect calculation of extreme points in certain cases.\r\n```\r", "NaN"], ["13606", "Fix recoverable grouped execution + tablewritermerge", "Andrii Rosa", "arhimondr", "10/31/19, 05:33:38 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13607", "Use fast varint read path in LongInputStreamV1", null, "oerling", "10/28/19, 02:29:22 AM", "Do not decode varints that are skipped over and do not maintain a\nbuffer of possibly unused values. Use the OrcInputStream varint\nmethods to read or skip only what is needed.", "NaN"], ["13609", "Fix double and float NaN comparisons", null, "bhhari", "11/06/19, 12:45:01 AM", "Fix comparison for NaNs in double and floats\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13610", "Add non-linear transformation for structural hash", "Wenlei Xie", "wenleix", "10/31/19, 05:05:26 PM", "Both structural hash and checksum are linear functions to the hash\r\nvalues of input elements. This makes vulnerable to hash collision\r\nattack by exchanging exchanging some input values .\r\n(e.g. values in the same nested column)\r\n\r\nThis commit avoids such checksum collision by adding non-linear\r\ntransformation at the end of structural hash computation.\r\n\r\nExample for checksum collision:\r\n\r\n```SQL\r\nSELECT checksum(value)\r\nFROM (VALUES\r\n    ARRAY['a', '1'],\r\n    ARRAY['b', '2']\r\n) AS t(value)\r\n```\r\n\r\nvs.\r\n\r\n```SQL\r\nSELECT checksum(value)\r\nFROM (VALUES\r\n    ARRAY['a', '2'],\r\n    ARRAY['b', '1']\r\n) AS t(value)\r\n```\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve hash value computation for structrual types to avoid collision.\r", "NaN"], ["13611", "Improve HashGenerator.getPartition performance", "Ying", "yingsu00", "10/29/19, 04:16:16 AM", "Change the mod operation to bit shifting in HashGenerator.getPartition.\r\nBenchmarkPartitionedOutputOperator shows 35% gain.\r\n\r\nBigint Before:\r\nBenchmark                                            (hasNull)  Mode  Cnt    Score     Error  Units\r\nBenchmarkPartitionedOutputOperator.optimizedAddPage       true  avgt   30  847.520 \u00b1 111.003  ms/op\r\nBenchmarkPartitionedOutputOperator.optimizedAddPage      false  avgt   30  820.443 \u00b1  63.596  ms/op\r\n\r\nBigint After:\r\nBenchmark                                            (hasNull)  Mode  Cnt    Score    Error  Units\r\nBenchmarkPartitionedOutputOperator.optimizedAddPage       true  avgt   30  565.254 \u00b1 64.455  ms/op\r\nBenchmarkPartitionedOutputOperator.optimizedAddPage      false  avgt   30  558.286 \u00b1 47.540  ms/op\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13612", "Add geospatial function line_interpolate_point", "James Gill", "jagill", "11/01/19, 01:17:32 AM", "One of our most requested functions.  Given a linestring and a double\r\nbetween 0 and 1, give the point at that fractional distance along the\r\nline.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n-----------------\r\n* Add geospatial function line_interpolate_point\r\n```\r", "NaN"], ["13613", "Fix hive columnIndex with default partition", null, "bhhari", "10/29/19, 02:35:29 AM", "Fix invalid partition column index lookup. Incase of a null partition we are creating a unnecessary streamReader which results in a negative index.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13614", "Improve geometry tests", "James Gill", "jagill", "10/28/19, 06:08:42 PM", "This PR cleans up some test cases.  In particular, many of our polygons in our test cases were constructed in a non-standard way (without the final, closing vertex) which is not to spec and not accepted by the dominant geometry library (JTS).\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13615", "Revert HashBuilderOperator changes", "Rongrong Zhong", "rongrong", "10/28/19, 07:56:13 PM", "This caused 10% memory regression on some queries. Consider the potential production impact it's probably better to revert it for now.", "NaN"], ["13616", "Add geospatial expand_envelope function.", "James Gill", "jagill", "11/05/19, 06:07:00 PM", "This creates an envelope that is the minimum bounding rectangle of a\r\nGeometry, expanded by a float distance.  This is vastly cheaper than\r\nST_Envelope(ST_Buffer(geometry, distance)), and significantly cheaper\r\nthan ST_Envelope(ST_Buffer(ST_Envelope(geometry), distance)).\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n-----------------\r\n* Add expand_envelope function to return a geometry's envelope expanded by a distance.\r\n```", "NaN"], ["13617", "Fix interactions between filter pushdown and CBO", "Sahar Massachi", "sayhar", "11/09/19, 01:12:18 AM", "Update Hive connector to have statistics returned from the `ConnectorMetadata.getTableStatistics` API reflect the pushed down filter.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13618", "Use URI in raptor.properties", "Jiexi Lin", "jessesleeping", "10/29/19, 02:34:11 AM", "Related to #13249 \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13625", "[easy] Remove unused FeatureConfig from PlanOptimizers", "James Sun", "highker", "10/30/19, 01:19:56 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13627", "Fix SliceSelectiveReader comparison for empty strings and presentStream", null, "bhhari", "11/01/19, 12:18:14 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13630", "Fix getBlockView for all-null SliceDictionarySelectiveReader", null, "oerling", "10/31/19, 02:16:55 PM", "NaN", "NaN"], ["13631", "Fix position count of prefilled Blocks", null, "oerling", "10/31/19, 02:18:03 PM", "Downstream operators, e.g. table writer, expect Blocks on a Page to\nhave the same position count.", "NaN"], ["13633", "Fix outputPositions in List- and MapFlatSelectiveStreamReader in case of all null values", null, "bhhari", "11/05/19, 04:48:46 AM", "List- and MapFlatSelectiveStreamReader returned junk values from `getReadPositions` when all values are null.  This PR is fixing that ands a sanity check for the return value of `getReadPositions` to the caller.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13634", "Remove TypeProvider from FilterStatsCalculator.filterStats", "Sahar Massachi", "sayhar", "10/31/19, 06:06:05 PM", "Types are no longer needed!\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13635", "Fix NPE in SimpleNodeSelector", "Andrii Rosa", "arhimondr", "10/31/19, 05:24:25 PM", "When node disappears (for example due to a slow heartbeat or due to the\r\nfailure) the NPE can happen in SimpleNodeSelector, as the node mapping\r\nfor the existing task won't be found in the node map.\r\n\r\nExample stacktrace:\r\n```\r\nCaused by: java.lang.NullPointerException\r\n\tat java.base/java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:946)\r\n\tat com.facebook.presto.execution.NodeTaskMap.createOrGetNodeTasks(NodeTaskMap.java:62)\r\n\tat com.facebook.presto.execution.NodeTaskMap.getPartitionedSplitsOnNode(NodeTaskMap.java:52)\r\n\tat java.base/java.util.HashMap.computeIfAbsent(HashMap.java:1138)\r\n\tat com.facebook.presto.execution.scheduler.NodeAssignmentStats.getTotalSplitCount(NodeAssignmentStats.java:50)\r\n\tat com.facebook.presto.execution.scheduler.SimpleNodeSelector.computeAssignments(SimpleNodeSelector.java:134)\r\n\tat com.facebook.presto.execution.scheduler.DynamicSplitPlacementPolicy.computeAssignments(DynamicSplitPlacementPolicy.java:41)\r\n\tat com.facebook.presto.execution.scheduler.SourcePartitionedScheduler.schedule(SourcePartitionedScheduler.java:271)\r\n\tat com.facebook.presto.execution.scheduler.SourcePartitionedScheduler$1.schedule(SourcePartitionedScheduler.java:146)\r\n\tat com.facebook.presto.execution.scheduler.SqlQueryScheduler.schedule(SqlQueryScheduler.java:747)\r\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:514)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n```\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13636", "Allow reading non-ORC tables when pushdown filter is enabled ", null, "bhhari", "11/06/19, 12:44:04 AM", "capture whether pushdown is enabled at query plan time and pass it into tableLayout to use it during execution\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13638", "WIP: Parquet Reader Optimizations", "Venki Korukanti", "vkorukanti", "05/27/20, 11:56:02 PM", "Adds new ColumnReaders for types: INT32, INT64, INT96, VarBinary, Boolean (nested, array, and non-nested). Remaining: Fixed Len Binary, Decimal\r\nSupported encodings: RLE, PACKED, DICTIONARY. Delta encodings support is not yet added.\r\n\r\nMain changes in the new column readers are:\r\n * Batch reads from Decoders\r\n * Use ArrayBlock implementations instead of BlockBuilder\r\n * New RLE Decoders that contains both the dictionary and values decoder. Dictionary lookup only once for the RLE block.\r\n * Separate non-nested and nested column readers\r\n    * non-Nested columns don\u2019t need RL\r\n    * non-Nested and non-nullable columns don\u2019t need to read DL\r\n * Avoid using temp byte arrays in case of VarBinary\r\n * Skip values without decoding\r\n\r\nAlso adds the benchmarks (based on ORC benchmarking code)\r\n\r\nBenchmark | Mode | Old | Error | New | Error\r\n-- | -- | -- | -- | -- | --\r\nreadBooleanNoNull | avgt | 39.44 | \u00b10.702 | 1.523 | \u00b10.046\r\nreadBooleanWithNull | avgt | 52.746 | \u00b11.459 | 11.511 | \u00b10.239\r\nreadInt32NoNull | avgt | 59.116 | \u00b11.423 | 9.968 | \u00b11.031\r\nreadInt32WithNull | avgt | 60.933 | \u00b11.277 | 16.179 | \u00b10.32\r\nreadInt64NoNull | avgt | 77.31 | \u00b13.451 | 20.309 | \u00b11.277\r\nreadInt64WithNull | avgt | 67.296 | \u00b11.5 | 21.531 | \u00b10.548\r\nreadInt96NoNull | avgt | 126.638 | \u00b15.213 | 44.831 | \u00b12.071\r\nreadInt96WithNull | avgt | 93.29 | \u00b12.707 | 33.348 | \u00b10.767\r\nreadListBooleanWithNull | avgt | 159.107 | \u00b12.767 | 96.307 | \u00b11.79\r\nreadListInt32WithNull | avgt | 171.035 | \u00b13.819 | 99.612 | \u00b11.651\r\nreadListInt64WithNull | avgt | 185.385 | \u00b14.309 | 109.098 | \u00b13.58\r\nreadListInt96WithNull | avgt | 227.756 | \u00b15.58 | 127.24 | \u00b13.924\r\nreadListSliceDictionaryWithNull | avgt | 188.426 | \u00b13.223 | 105.013 | \u00b12.163\r\nreadSliceDictionaryNoNull | avgt | 76.132 | \u00b11.778 | 15.625 | \u00b10.324\r\nreadSliceDictionaryWithNull | avgt | 72.929 | \u00b11.871 | 18.103 | \u00b10.358\r\nreadStructBooleanWithNull | avgt | 61.284 | \u00b11.271 | 28.969 | \u00b10.784\r\nreadStructInt32WithNull | avgt | 71.697 | \u00b11.827 | 35.646 | \u00b10.762\r\nreadStructInt64WithNull | avgt | 78.999 | \u00b11.826 | 42.849 | \u00b10.769\r\nreadStructInt96WithNull | avgt | 104.609 | \u00b12.012 | 54.361 | \u00b11.225\r\nreadStructSliceDictionaryWithNull | avgt | 85.545 | \u00b11.879 | 38.806 | \u00b10.92\r\n\r\n\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13644", "Introduce caching file system", "James Sun", "highker", "11/21/19, 01:25:42 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nCache Changes\r\n* Introduced new cache module to allow using local disk for to cache files from remote file systems.\r\n\r\nRaptor Changes\r\n* Allow Raptor to read data from HDFS while caching the files on local disks.\r\n```\r", "NaN"], ["13645", "Fix GeometryToBingTiles for certain geometries", "James Gill", "jagill", "11/05/19, 09:27:34 PM", "Geometries that had a max longitude equal to `BingTile.MIN_LONGITUDE`\r\nor min latitude equal to `BingTile.MAX_LATITUDE` would originally\r\nnot get tiled, and with 0.227 would throw an exception.  This was due to\r\n\"trimming\" a SE tile to negative tileX or tileY.  This change ensures\r\nthat the SE tile cannot be west or north or the NW tile, preventing\r\nthose issues.\r\n\r\nFix #13639\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix geometry_to_bing_tile for geometries at -180 longitude or 85.05112878 latitude.\r\n```\r", "NaN"], ["13646", "Add JSON annotations to SqlFunctionId", "Leiqing Cai", "caithagoras", "11/05/19, 12:51:56 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13647", "Output verifier events for queries being skipped by pre-filters", "Leiqing Cai", "caithagoras", "11/07/19, 12:26:30 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add skipped verification results to the output for queries being filtered.\r\n```\r", "NaN"], ["13650", "Optimize lambda body when possible", "Yi He", "hellium01", "11/28/19, 01:45:49 AM", "Commit 98f05bf deprecated `ExpressionOptimizer` without porting the\r\nhandling of lambda body into `RowExpressionInterpreter`. This commit\r\nadded optimization on Lambda definition body so that expensive\r\noperation within lambda can be replaced with constant.\r\n\r\nFixes #13648\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix regression on lambda evaluation (Issue#13648).\r\n```\r", "NaN"], ["13651", "Make commitPartition an async operation", "Shixuan Fan", "shixuan-fan", "11/20/19, 12:40:22 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n* Change ``ConnectorMetadata#commitPartition`` into async operation, and rename it to ``ConnectorMetadata#commitPartitionAsync``\r\n```", "NaN"], ["13652", "fix organizationDiscoveryIntervalMillis bug", "Ke", "kewang1024", "11/06/19, 12:47:02 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nRaptor Changes\r\n* Fix organizer not running scheduled job due to misreading `storage.organization-discovery-interval` config.\r\n\r\n\r\n```", "NaN"], ["13653", "Fix PRESTO_EXTRA_CREDENTIAL parsing", "Yi He", "hellium01", "11/07/19, 01:47:55 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improved PRESTO_EXTRA_CREDENTIAL header parsing to allow value contain multiple '=' and urlEncode characters.\r\n\r\n\r\n```\r\n\r", "NaN"], ["13654", "Revert \"Add non-linear transformation for structural hash\"", "Wenlei Xie", "wenleix", "11/06/19, 12:47:43 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13655", "Fix selective readers to make a copy of input positions ", null, "bhhari", "11/07/19, 07:07:04 AM", "Each reader has a mutable array of output positions. read() populates this\r\narray and returns it via getReadPositions() to the caller who is passing it to\r\nthe read() method of the next reader as \"input positions\". Subsequent calls to\r\ngetBlockView() or getBlock() may modify that array by removing positions\r\nfiltered out by the other readers. \r\n\r\nReaders without filters used to hold on to the input positions array assuming\r\nthe array won't change. This assumption is incorrect and may cause a reader to\r\nget into incosistent state. This change makes it so that all readers always\r\nmake a copy of input positions in the read() method and never rely on that\r\narray to not change.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13658", "Reset values after getBlock in LongSelectiveStreamReader", null, "bhhari", "11/07/19, 07:06:50 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13659", "Enable testing of schema changes with filter pushdown enabled", "Maria Basmanova", "mbasmanova", "11/07/19, 05:55:33 PM", "Add a version of TestHiveClientInMemoryMetastore with filter pushdown enabled. This test enabled testing of coercions and bucket adaptation for the Aria scan. The support for coercions and bucket adaptation will come in future PRs.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13660", "Transaction Sharing across Connectors", "Saksham", "sachdevs", "11/22/19, 09:57:15 PM", "Do not review, just for tracking purposes.\r\n\r\nSee referenced PR for most of the details.", "NaN"], ["13661", "Support supplying benchmark queries through MySQL", "Nikhil Collooru", "NikhilCollooru", "11/25/19, 11:45:37 PM", "- Create a new module presto-benchmark-runner to enable running benchmark queries on Presto.\r\n- Add code changes to able to connect and retrieve the Benchmark suite information from MySql.\r\n- QuerySet is a set of queries. All the queries of a Benchmark suite belong to a single query set. \r\n- Each benchmark might have multiple phases. And each phase will have queries and an execution strategy that describes as to how to execute them. \r\n- If the execution strategy is CONCURRENT , then all the queries in the phase will be executed at the same time. If the execution strategy is STREAM, then one query from each stream will be executed at once.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13662", "Add basic support for schema evolution to OrcSelectiveRecordReader", "Maria Basmanova", "mbasmanova", "11/08/19, 10:09:34 AM", "Add support for schema evolution for columns with no range filters. Support for coercions on columns with range filters will come in a future PR.\r\n\r\nDepends on #13659\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13666", "Do not prune subfields when filter uses entire column", "Maria Basmanova", "mbasmanova", "11/08/19, 10:10:00 AM", "Fix queries where entire column is used in a filter, but is not being\r\nprojected out. For example: SELECT a[1] FROM t WHERE cardinality(a) = 1\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13668", "Fix IS DISTINCT FROM for decimals with precision > 18", "Michael Styles", "ptkool", "01/02/20, 03:13:09 PM", "The current logic in method `distinctBlockPositionLongLong` compares the left value to itself. This PR corrects this by assigning the proper values to `rightLow` and `rightHigh`.\r\n\r\nFixes https://github.com/prestodb/presto/issues/13667.\r", "NaN"], ["13672", "Replace Joda-Time libraries with java.time", "Ajay George", "ajaygeorge", "11/15/19, 06:49:07 PM", "Since Java 8 we have java.time packages which are equivalent to Joda and\r\nthe recommendation from the author of the Joda-Time is to migrate to\r\njava.time(JSR-310) library.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13674", "Allow range filters on columns with coercions", "Maria Basmanova", "mbasmanova", "11/11/19, 07:33:42 PM", "Coercion is now supported for columns with any type of filter and regardless of whether column is filter-only or is being projected out.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13675", "Fix filter pushdown for non-partitioned tables", "Maria Basmanova", "mbasmanova", "11/08/19, 09:16:22 PM", "When query reads from multiple tables filter pushdown may happen multiple times. First, we get filter pushdown based on analyzing the query structure. This pushdown involves pruning partitions using Metastore. The result, the list of valid partition keys, is stored in `TableScanNode.currentConstraint`. When there is a join, this info is used by `PredicatePushDown` rule to copy filters from one side of the join to the other, if appropriate. This triggers second pushdown.\r\n\r\nSubsequent filter pushdowns must combine the filter being pushed down with the one that was pushed down earlier. This logic didn't work for unpartitioned tables, because HivePartitionManager incorrectly set `enforcedConstraint` to `none` for unpartitioned tables. \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13676", "Add support for scanning RC files within an ORC table", null, "bhhari", "11/16/19, 02:58:03 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13678", "Move out the entire Hive metastore", "James Sun", "highker", "11/11/19, 07:20:26 PM", "The metastore will be used by both Raptor and Hive\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13680", " Fix the size of all-null blocks returned from selective readers", null, "bhhari", "11/12/19, 09:23:33 PM", "When all data in a column is null, `SelectiveStreamReader.getBlock()` may return a block with more positions when requested. If this column happens to be inside of a struct and there are other non-null columns, RowBlock constructor will throw an exception: `IllegalArgumentException: length of field blocks differ`\r\n\r\n```\r\n@Test\r\npublic void testAllNulls()\r\n{\r\n    getQueryRunner().execute(\"CREATE TABLE test_all_nulls AS SELECT orderkey, cast(row(null, 1) AS ROW(null_boolean BOOLEAN, a INTEGER)) as struct FROM orders\");\r\n\r\n    try {\r\n        assertQuery(\"SELECT * FROM test_all_nulls WHERE struct IS NOT NULL AND orderkey % 2 = 0\", \"SELECT orderkey, (null, 1) FROM orders WHERE orderkey % 2 = 0\");\r\n    }\r\n    finally {\r\n        getQueryRunner().execute(\"DROP TABLE test_all_nulls\");\r\n    }\r\n}\r\n```\r\n\r\nThis test fails with an exception in RowBlock: \u201clength of field blocks differ\u201d\r\n \r\n```\r\nCaused by: java.lang.IllegalArgumentException: length of field blocks differ: field 0: 2, block 1: 1\r\n         at com.facebook.presto.spi.block.RowBlock.validateConstructorArguments(RowBlock.java:93)\r\n         at com.facebook.presto.spi.block.RowBlock.fromFieldBlocks(RowBlock.java:52)\r\n         at com.facebook.presto.orc.reader.StructSelectiveStreamReader.getBlock(StructSelectiveStreamReader.java:427)\r\n         at com.facebook.presto.orc.OrcSelectiveRecordReader.getNextPage(OrcSelectiveRecordReader.java:393)\r\n         at com.facebook.presto.hive.orc.OrcSelectivePageSource.getNextPage(OrcSelectivePageSource.java:84)\r\n         ... 12 more\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13684", "Add support for bucket adaptation", "Maria Basmanova", "mbasmanova", "11/13/19, 08:36:13 PM", "Turn bucket adaptation into filter function and run it along with other filters.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13686", "Add release notes for 0.229", "Leiqing Cai", "caithagoras", "11/15/19, 02:54:33 AM", "# Missing Release Notes\n## Ajay George\n- [x] https://github.com/jzmq/presto/pull/41 [pull] master from prestodb:master\n- [x] https://github.com/jzmq/presto/pull/18 [pull] master from prestodb:master\n\n## James A. Gill\n- [x] c006c8075038aa5ba73f4f7b388fa8b2ab7da245 Fix GeometryToBingTiles for certain geometries\n\n## Ke Wang\n- [x] b4752ccb4ee2a1717df903d1a0fed110a2df6b18 fix organizationDiscoveryIntervalMillis bug\n\n## Leiqing Cai\n- [x] 7e12d427f748776c47a38eec8f406d528e208246 Add JSON annotations to SqlFunctionId\n\n## Rongrong Zhong\n- [x] https://github.com/prestodb/presto/pull/13384 Function implementation (Merged by: Rongrong Zhong)\n- [x] https://github.com/prestodb/presto/pull/13615 Revert HashBuilderOperator changes (Merged by: Rongrong Zhong)\n\n## Saksham Sachdev\n- [x] https://github.com/prestodb/presto/pull/13526 Implement Basic Example For Jdbc Row Expression Translation (Merged by: James Sun)\n\n## Yi He\n- [x] https://github.com/prestodb/presto/pull/13112 Row expression predicate pushdown (Merged by: James Sun)\n- [x] https://github.com/prestodb/presto/pull/13560 Add RowExpressionPredicateExtractor (Merged by: James Sun)\n\n## oerling@fb.com\n- [x] https://github.com/prestodb/presto/pull/13631 Fix position count of prefilled Blocks (Merged by: Maria Basmanova)\n- [x] https://github.com/prestodb/presto/pull/13630 Fix getBlockView for all-null SliceDictionarySelectiveReader (Merged by: Maria Basmanova)\n- [x] https://github.com/prestodb/presto/pull/13607 Use fast varint read path in LongInputStreamV1 (Merged by: James Sun)\n- [x] https://github.com/prestodb/presto/pull/13505 Intersect pushdown predicates (Merged by: Rebecca Schlussel)\n\n# Extracted Release Notes\n- #13379 (Author: Vic Zhang): Add peak memory distribution among tasks of each stage\n  - Add peak total memory distribution among tasks of each stage.\n- #13477 (Author: Cem Cayiroglu): Support max_tasks_per_stage for scan\n  - Respect stage.max-tasks-per-stage to limit number of tasks for scan.\n- #13504 (Author: Devesh Agrawal): Adding presto-pinot with pushdown working\n  - Added a new Pinot connector.\n  - Split `ConnectorPlanOptimizerProvider` to have two phases for connectors to participate query optimization: `LOGICAL` and `PHYSICAL`. The two phases correspond to pre- and post-shuffle optimization respectively.\n- #13527 (Author: Swapnil Tailor): Passing X_Forwarded_For in header from proxy server\n  - Start forwarding X_Forwarded_For in header from Proxy.\n- #13535 (Author: Jiexi Lin): Support using remote HDFS as storage in Raptor\n  - Change `storage.data-directory` from path to URI. For existing deployment on local flash, a scheme header \"file://\" should be added to the original config value.\n  - Change error code name `RAPTOR_LOCAL_FILE_SYSTEM_ERROR` to `RAPTOR_FILE_SYSTEM_ERROR`.\n- #13546 (Author: James Sun): Move SetOperationNode to SPI\n  - Move `SetOperationNode` to SPI. Connectors can now alter query plan to have union, intersect, and except.\n- #13561 (Author: Leiqing Cai): Support CREATE FUNCTION execution\n  - Add support for ``CREATE FUNCTION``.\n- #13584 (Author: Nikhil Collooru): Fail query with large taskUpdate size\n  - Add configuration property ``experimental.internal-communication.max-task-update-size`` to limit the size of the taskUpdate.\n- #13592 (Author: Yi He): Add common InternalCommunicationConfig to turn on HTTPS\n  - Added configs to use common HTTPS settings for internal communications.\n- #13594 (Author: James Petty): Parquet Dictionary Predicate Pushdown Fixes\n  - Fix parquet predicate pushdown on dictionaries to consider more than just the first predicate column.\n  - Improve parquet predicate pushdown on dictionaries to avoid reading additional data after successfully eliminating a block.\n- #13610 (Author: Wenlei Xie): Add non-linear transformation for structural hash\n  - Improve hash value computation for structrual types to avoid collision.\n- #13612 (Author: James Gill): Add geospatial function line_interpolate_point\n  - Add geospatial function line_interpolate_point.\n\n# All Commits\n- b4752ccb4ee2a1717df903d1a0fed110a2df6b18 fix organizationDiscoveryIntervalMillis bug (Ke Wang)\n- c006c8075038aa5ba73f4f7b388fa8b2ab7da245 Fix GeometryToBingTiles for certain geometries (James A. Gill)\n- 7e12d427f748776c47a38eec8f406d528e208246 Add JSON annotations to SqlFunctionId (Leiqing Cai)\n- fbdbeeb03c508f628ef294a05538aa3f583e24f6 Add geospatial function line_interpolate_point (James Gill)\n- f424e1b17c6c617f5523c9c739d23101bdf695e7 Fix slice reader reading empty strings column (Bhavani Hari)\n- 5d8105c835aedeca5791a9ece27057e5b9d2049f Fix slice reader check for presentStream (Bhavani Hari)\n- b82b56b3e868742b9a72784f61f512cb36c888d4 Remove TypeProvider from FilterStatsCalculator.filterStats (Sahar Massachi)\n- 34c846e37a5545f359e868dd19d658096877ec40 Ensure TableFinishOperator input is ordered (Andrii Rosa)\n- 62bf6986d1955e164de11a627f79c39859bb83cf Remove synchronized method in FixedSourcePartitionedScheduler (Shixuan Fan)\n- c61d6bb048881d44dbbd9f9aefb6e5bcb4e010e1 Make updateTaskStatus not synchronized within SqlStageExecution (Andrii Rosa)\n- be1db83343333f65c4c8612991fa964ec712354d Fix recoverable grouped execution for task_writer_count=1 (Andrii Rosa)\n- dd00be96c20b31d09130b1186d106a52b7536225 Improve TestHiveRecoverableGroupedExecution (Andrii Rosa)\n- 91fb5c1aa82b7ad30f0ebba34f209580ac216bda Fix NPE in SimpleNodeSelector (Andrii Rosa)\n- 9fd0400fcb9e305b426e717198b1d870f29e90c0 Add non-linear transformation for structural hash (Wenlei Xie)\n- 241392469199c058aab4f6bdf3944a3cfa2e4068 Fix position count of prefilled Blocks (oerling@fb.com)\n- 1373a9ff23ef7e1c96ea901682f7d3fc7ba41a7f Fix getBlockView for all-null SliceDictionarySelectiveReader (oerling@fb.com)\n- 808043fe0b009c9c1122fa20766ddb104ae71fc6 Add cache for stripe metadata (Shixuan Fan)\n- 177a680faafebd3d969e1647701ed55c22023e80 Move cache related class into separate package (Shixuan Fan)\n- e1f0dd6d420a514b0e2d5db672b1e3958b7e2908 Remove unused method in OrcTestingUtil (Shixuan Fan)\n- f24a4524b9cbe9a1b29e341ba2110ec707ba5caf Fix an issue when fetching function implementation in FunctionManager (Leiqing Cai)\n- dabecccdda52aa56224dacd7f7d63a2d76c0eba6 Support CREATE FUNCTION execution (Leiqing Cai)\n- 866f66577903dbbb55a412e32857ff46aa03a186 Add support for refreshing functions cache in namespace manager (Leiqing Cai)\n- 109ad5e3ce322e7ef19a5cb683882c60503e032f Rename FunctionNamespaceManager#rollback to #abort (Leiqing Cai)\n- 3b7f14306a6f82b603d335ac90be45a1b5dabb2d Add ParametersAreNonnullByDefault annotation to cache loading methods (Leiqing Cai)\n- 02b8c0e2abe15825409e197e01b638080d1d5da3 Move SqlInvokedRegularFunctionTestUtils to presto-sql-functions (Leiqing Cai)\n- e81b812b91b019e589ca1520eef22cdaf3735fbc Move classes related to SQL-invoked functions into spi (Leiqing Cai)\n- a13b4c57bd828d91764771b412c499a6223d3dbb Rename SqlInvokedRegularFunction#comment to description (Leiqing Cai)\n- dd7f571d2d27c049384c14d72fdf3cbdef346c0d Require function namespace to be exactly 2 parts (Leiqing Cai)\n- 4051fd7ac165d841ac03ff01adca954862589968 Move spi.relation.FullyQualifiedName to spi.QualifiedFunctionName (Leiqing Cai)\n- ab9d0ec22584ce1ab134ef2580514ab4791b6f25 Remove unused FeatureConfig from PlanOptimizers (James Sun)\n- 694bd5dc61c77bf10fe83c6df3438bf92996997a Support SQL function execution (Rongrong Zhong)\n- d47c75d0203013eafbc3b54afea88c17a23d60ed Add ScalarFunctionImplementation abstraction (Rongrong Zhong)\n- 7294fa5018239f6f145ad9eef30634ac5884e77c Add funcion implementation type and argument names to FunctionMetadata (Rongrong Zhong)\n- 04ead21d85198984cb3fdedb078603fa0f62196c Rename ScalarFunctionImplementation to BuiltInScalarFunctionImplementation (Rongrong Zhong)\n- b0645420b27f47dbc7feb67186621a9cc712cba5 Add SqlFunctionProperties to bytecode compiler cache keys (Rongrong Zhong)\n- a5701caa56da7447be42dc4e388cb649e799a9e5 Remove Session from SqlToRowExpressionTranslator (Rongrong Zhong)\n- a80dea946b982eecf7a4eb718fdc5d18094aa4f7 Factor out Expression related session configs into SqlFunctionProperties (Rongrong Zhong)\n- 297e2bac161281adc1e9b4601fbd8a6e188b38c0 Fix error message in FunctionManager (Rongrong Zhong)\n- c1a0c65d85987b13beb20f1b09343760beb5f3f9 Check call convention once in RowExpressionInterpreter.visitCall (Rongrong Zhong)\n- 096c1def104088264d2d3ce3432c5b8ab64ab678 Check call convention once in ExpressionInterpreter.visitFunctionCall (Rongrong Zhong)\n- 929e8a90e855952175101b350951c479ae8c6b48 Remove TypeProvider from LocalExecutionPlanContext (Rongrong Zhong)\n- b54d2483c76afe2d7b57c7ae56c356ba25440e88 Improve HashGenerator.getPartition performance (Ying Su)\n- 898037b78649c2c16314eb154ea1b765be3b7a1e Fix hive columnIndex with default partition (Bhavani Hari)\n- 052e0eb6b79e235d234fb3cbed6a07fdd57d8ab6 Use URI in raptor.properties (Jiexi Lin)\n- 2be543e0e1a993b8c4619247c14b25c43cf1a6df Optimize PlanFragment serialization (Nikhil Collooru)\n- af3b9dd74de1c31cbd71fa7a78ddb673185ff263 Revert \" Include predicted size of built hash in HashBuilderOperator\" (Rongrong Zhong)\n- 2404248fca3f28ff8d7cd0a7c52b6a16bf11d56e Revert \"Do not overcommit memory in HashBuilderOperator\" (Rongrong Zhong)\n- b0fed6e50b842deedf5ca9d63ff0909ce16de7a5 Make Polygon WKTs valid in test cases (James A. Gill)\n- d28e5de3f932510dae5c52568b0ce7fcddc895fd Improve Geometry Serialization Tests (James Gill)\n- de1dda7a12921d65fdeb320ef266bd20f4efb24a Improve error message when transaction is gone (Andrii Rosa)\n- a140fba74e14c612ecaaa83b1ee78be2c6c3c760 Refactor Hive transaction management not to use metastore getter (Andrii Rosa)\n- e6b768774317690ec14d82cc5b2dd8de8b9f4444 Do not update query failureCause when query is FINISHING (Andrii Rosa)\n- 2b5711cb7540a84411120124a25f6ac3ace88214 Do not fail finishing query unless the transaction failed (Andrii Rosa)\n- aa47ca31869fcc7b1480772d5bf0af4e536fe913 Check if transaction still exist on transitioning to failed query (Andrii Rosa)\n- d75bd6b888399af7e055fad207be4b38289ef1e9 Use fast varint read path in LongInputStreamV1 (oerling@fb.com)\n- 5a143aa42845b4c34af6ac8d617f3989ca3e041f Export round trip count for update/info/status (Wenlei Xie)\n- 78f24709b80f17bc7b7f6a0317cd9ee0515c638e Adding presto-pinot with pushdown (Devesh Agrawal)\n- 6a7c4c81957cb4a85bc316bacf597c9fbe568f34 Add common InternalCommunicationConfig to turn on HTTPS (Yi He)\n- 0be3619feda6f86f3bcad7fc7cabab49be56494b Fix table scan results with disjoint pushdown filters (Bhavani Hari)\n- 2b79a586d18477faa64f4bb6addabf2f8e8d68f5 Refactor SqlStageExecution creation (Rebecca Schlussel)\n- ad92616433964f38f1d463a8091b56ece94f14db Small cleanups of SqlQueryScheduler (Rebecca Schlussel)\n- 314dd12c9d9efbd36b478b25636a3b832a2a0428 Add FileOpener interface in Hive connector (Shixuan Fan)\n- 6098d199bd1c8f0e03cb342d91d88525048c77f8 Remove unused static variable in TestBackgroundHiveSplitLoader (Shixuan Fan)\n- 2de30ec442f1ef9ded4e658ddbbc300ecaaf62c9 Fail query with large taskUpdate size (Nikhil Collooru)\n- 88416e540a15e12bab7325474dca68f0cd20b794 Add peak memory distribution among tasks of each stage (Vic Zhang)\n- a97dd9406206a92754998072f4e908f6615b32f0 Rename StageCpuDistribution to ResourceDistribution (Vic Zhang)\n- 6dbfefcb7aa82e7864ab751f7bed72de8bd9e542 Add peak total memory to TaskStats (Vic Zhang)\n- 947b5b90bbf8c4d66dcff5432cd9625c115e6918 Add PartitionObjectBuilder interface for Hive (Ajay George)\n- 93506020afa3e290618f438e01a2554e3e6bb0e4 Support max_tasks_per_stage for scan (Cem Cayiroglu)\n- a71977ea0a3b0eb6488578e839afedb64bbcde58 Stop Parquet Dictionary Reads Eagerly in Predicate Pushdown (James Petty)\n- d257b0c46a7e377bae7a919d607070a0fbbbaca0 Fix Parquet Predicate Pushdown on Dictionaries (James Petty)\n- fcd6f04d84887d9b51013353567f509d3efcf01e Do not overcommit memory in HashBuilderOperator (Andrii Rosa)\n- a38dbfa332f4aee567d05814f20574cf7ac4999d Remove DeleteHandle from DeleteNode (Rebecca Schlussel)\n- b581ffb91ac8a7707ba5f85004745d5801e1b0c2 Use TableHandle directly in StatisticsWriterNode (Rebecca Schlussel)\n- 928a23437a4e4cdece2bf3e6cebf7814767ba7fe Move BeginTableWrite logic to scheduler (Rebecca Schlussel)\n- 7ebe52287206aef8ddf91d60630d2c9cedf95f99 Simplify code dealing with WriterTargets (Rebecca Schlussel)\n- 5b8ec08aadf8226616c3e922deda5e81375c6522 Add ConnectorId to CreateName (Rebecca Schlussel)\n- 94c73cc0be438a1437de59997b309a41b6b55971 Make WriterTargets optional in plan nodes (Rebecca Schlussel)\n- efbd394cc3b910b9aa5b60f1b6954823f0cff726 Replace DeleteHandle with TableHandle for metadata delete (Rebecca Schlussel)\n- 48c06932c9c032a2ac3b557b9babef6e56c232a7 Remove unused field from LocalExecutionPlanner (Rebecca Schlussel)\n- a9fa7b6c266fe30c2f42ba575b01542d9258b08c Remove test usage of TableScan json constructor (Rebecca Schlussel)\n- 44bc89782cc7bdc8b02b037608bbf63d5628f700 Fix Flaky TestTransactionManager.testExpiration (Cem Cayiroglu)\n- fbb7794517b124e373af863a423aad9f68a259b3 Support using HDFS as storage in Raptor (Jiexi Lin)\n- 9fb46ce831b4a20fa7e91e29ab6701a36e4ed037 Revert reserved words introduced in CREATE FUNCTION syntax support (Leiqing Cai)\n- 0e7847b69106120b7d437707ac9412da2b0778d5 Fix SliceReader ArrayOutOfBounds when presentStream is null (Bhavani Hari)\n- a6fd61b38336d57dc85ffa2cd336cbed13c1712a Move RowExpression translation above AddExchange (James Sun)\n- 55c3f7c43437a247f122053fe5b54b7bb20a1a76 Rename SymbolResolver to VariableResolver (James Sun)\n- 332d48418da295b0da4de3e34f9c164486cabb73 Move SetOperationNode to SPI (James Sun)\n- 7491f2038cc55a0feba5c6d44e44a5f2b2ae4bfd Add session property for nested subfield filters (Bhavani Hari)\n- f62dfdd47f83105e7695275bbf2b6d5f21b34b17 Hanlde isNull filter for Array type (Bhavani Hari)\n- 9092cb7b41638de8c5ec4046b632be89f66c5e7b Add example for Jdbc Connector Row Expression Translation (Saksham Sachdev)\n- e3694a0dd43403d2f5ae56107582c937f7408be7 Fix compareRanges logic (Bhavani Hari)\n- 49df615aaffddab28f47144951c83c003dedab2f Move TranslateExpressions below AddExchange (Yi He)\n- 4f74189285f63bb6146a9e0bd3ab9b5a3ec362d0 Move TranslateExpressions above last predicatePushdown (Yi He)\n- 434357405e40c4e573178d3d20e28cd0c57f98cf Include error message in SimpleHttpResonseHandler for SMILE (Nikhil Collooru)\n- a51c1e3cabb20b162e8c108611daf5327be68073 Generate unique index for PartitionColumns (Bhavani Hari)\n- 90cfbd2345f546087fce57932852e05efcd3895f Passing X_Forwarded_For from proxy (Swapnil Tailor)\n- 8d5d5e67e1e2276e9e2a1fc02f471e6d0a020c89 Update to Facebook airlift (Wenlei Xie)\n- 184fb504592d8ba432d6904ae0dd36ec34aa9457 Add ignored dependencies in presto-hive (Wenlei Xie)\n- cff2383bb95827c61a65e39e1d359dcd3e6ee93d Add more dependencies for HiveGcsModule (Wenlei Xie)\n- 442a46f6c5c0a69bc415259fffaf3624b0ab41e6 Fix undeclared dependency used for HTTP constants (David Phillips)\n- 42e7b22d5e4362072858f2f5c30fad913effa686 Use ValidationAssertions in TestPowerOfTwoValidator (David Phillips)\n- 7e2f60dc9ab7ae9b68dccfa20d0c5ae4b34ab7cd Reset DictionaryBlock values after getBlock (Bhavani Hari)\n- 270a40f087435d6ca7548d0f2d73a79021e2dd08 Fix selective readers output positions when reading null streams (Bhavani Hari)\n- c1b2b2e480afbcdc5215cb2d19352840f770da15 Add cache for orc file tail information (Shixuan Fan)\n- 418d63b634f9af16e4d8831f2f86c6018cb02a3e Add RowExpressionPredicateExtractor (Yi He)\n- 9a6d4de9a7b0389ca1b7807e164ff5759e69a983 Intersect pushdown predicates (oerling@fb.com)\n- 3c8584ff76c6d435773790a3b31597d689f90aed prepare for next development iteration - 0.229-SNAPSHOT (Rongrong Zhong)", "NaN"], ["13689", "Fix subfield pruning for legacy unnest", "Maria Basmanova", "mbasmanova", "11/12/19, 09:27:12 PM", "With `legacy_unnest=true` we used to generate subfield paths with extra `.field`, e.g. `array_of_structs[*].field.linenumber` instead of `array_of_structs[*].linenumber`. These paths would cause struct reader to prune *all* the fields and return null.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13690", "Fix getTableStatistics with subfield pruning", "Maria Basmanova", "mbasmanova", "11/13/19, 07:07:41 PM", "Fix `java.lang.IllegalArgumentException: Multiple entries with same key` error in `getTableStatistics` when a complex-type column is used in a filter and is being projected out of scan using only a subset of subfields.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13691", "Enable testGroupByKeyPredicatePushdown with filter pushdown", "Maria Basmanova", "mbasmanova", "11/12/19, 07:40:34 PM", "The fix for the underlying issue was added in #13675\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13695", "Fix build break introduced by 9633991927", "Ying", "yingsu00", "11/14/19, 01:06:23 AM", "Fix build break introduced by `9633991927 Improve ORC byte reader`\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13697", "Wrap OrcReader parameters into OrcReaderOptions", "Gaurav Mittal", "gauravkm", "11/19/19, 04:35:43 AM", "Currently adding any new config param to OrcReader is quite painful. I have created a OrcReaderConfig to serve the purpose of making this easier in the future. The change would need to be made in one place rather than doing it across all layers.\r\n\r\nI am trying to add a config param to control whether the zstd decompression should use the JNI version or the Java version.\r\n\r\ncc: @highker ", "NaN"], ["13699", "HMS Impersonation Access and breakdown metrics by hosts", "Curt", "BlueStalker", "04/06/20, 11:03:47 PM", "Please make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Impersonation Access by using HMS delegation token\r\n* Enable multi HMS instances load balancing and breakdown metrics by HMS hosts.\r\n```", "NaN"], ["13700", "Evaluate pushed down predicate on partition columns to prune partitions", "Maria Basmanova", "mbasmanova", "11/16/19, 05:51:31 PM", "This enables partition pruning on queries like `SELECT * FROM t WHERE date(ds) = date('2019-11-01')`\r\n\r\nFixes #12916\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13702", "Remove unused types from CostCalculator", "James Sun", "highker", "11/14/19, 06:20:03 PM", "CostCalculator now only deals with RowExpression. TypeProvider is no\r\nlonger needed.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13703", "Run filters without inputs first in OrcSelectiveStreamReader", "Sahar Massachi", "sayhar", "11/17/19, 03:38:05 PM", "Filters without any inputs are usually less expensive and\r\nmore productive than other filters, so it is more efficient\r\nto run them first.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13704", "Add support for JNI based decompression for zstd files", "Gaurav Mittal", "gauravkm", "11/19/19, 11:16:33 PM", "Initial results and estimates point to 10% improvement in CPU by using JNI.\r\n\r\nThe feature is controlled by a flag, that is false by default.\r\n\r\nI tried to split the pull requests but there is no way to stack them across different forks.", "NaN"], ["13705", "Fix filter pushdown for DELETE queries", "Maria Basmanova", "mbasmanova", "11/16/19, 05:50:47 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13709", "Update Kafka connector to 2.3.1", "Da Cheng", "dcfocus", "01/30/20, 06:34:32 AM", "This PR improves Kafka connector implementation and performance.\r\n\r\n1. Replaced SimpleConsumer with KafkaConsumer for Kafka connector.\r\n2. Push collecting offsets for each partition from coordinator to workers to resolve the bottleneck.", "NaN"], ["13710", "Allow range filter on subfields of structs with coercion", "Maria Basmanova", "mbasmanova", "11/18/19, 03:08:46 PM", "Fixes `UnsupportedOperationException` for queries with range filters on struct subfields when applied to tables with updated schema.\r\n\r\n```\r\nCaused by: java.lang.UnsupportedOperationException: Range filers on array types are not supported\r\n\tat com.facebook.presto.hive.HiveCoercer$StructCoercer.toCoercingFilter(HiveCoercer.java:479)\r\n\tat com.facebook.presto.hive.orc.OrcSelectivePageSourceFactory.toTupleDomainFilters(OrcSelectivePageSourceFactory.java:545)\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13714", "Reset allNulls flag in SliceDictionaryXxx#read", "Maria Basmanova", "mbasmanova", "11/18/19, 03:04:15 PM", "allNulls flag was not being reset in SliceDictionarySelectiveReader#read() causing the reader to return nulls for all batches after the first all-null batch.\r\n\r\nSliceDictionarySelectiveReader didn't have this problem, but reset logic there was confusing and different from other readers. Hence, updates that reader to be consistent with the rest.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13715", "Skip map entries with null keys", "Maria Basmanova", "mbasmanova", "11/18/19, 03:04:01 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13716", "Fix Values are too large error message", "Maria Basmanova", "mbasmanova", "11/18/19, 08:19:17 PM", "Fix number of columns in `Values are too large` error message. It used to be always zero.\r\n\r\n`Caused by: com.facebook.presto.spi.PrestoException: Values in column \"value\" are too large to process for Presto. 0 column values are larger than 1GB [...]`\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13717", "Support retries of streaming sections", "Rebecca Schlussel", "rschlussel", "02/07/20, 01:58:14 AM", "Addresses https://github.com/prestodb/presto/issues/13438\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n*  Add support for retrying failed stages from a materialized point instead of failing the entire query.  The number of retries allowed can be configured using the configuration property max-stage-retries and session property max_stage_retries. The default value is zero.  To take advantage of this feature, exchange_materialization_strategy must be set to 'ALL'.\r\n\r\n* Add configuration property use-legacy-scheduler and session property use_legacy_scheduler to use a version of the query scheduler from before refactorings to enable full stage retries.  The default value is false. This is a temporary property to provide an easy way to roll back in case of bugs in the new scheduler.  This property will be removed in a couple releases once we have confidence in the stability of the new scheduler.\r\n```\r\n\r\n\r", "NaN"], ["13718", "Fix deserialization error in verifier", "Nikhil Collooru", "NikhilCollooru", "12/11/19, 08:10:39 PM", "Stats class doesn't have a constructor that helps deserializing. So instead of \r\ndeserializing into List<Stats> objects, deserialize them into a \r\nList<Map<String,Object>>. Because all we want is just the size of list. \r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13719", "Fix StructReader outputPositions copied from nested streams", null, "bhhari", "11/19/19, 11:38:55 AM", "Same fix as in #13655 applied to struct reader.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13723", "Fix size type when calculating retained size.", "Ying", "yingsu00", "11/22/19, 07:26:05 AM", "There're some cases where the decodedBlock is ColumnarArray whose elementBlock is large DictionaryBlock. For such cases, the retained size, if in int type, would overflow.\r\n\r\nNote that this doesn't fix the problem where the big DictionaryBlock was generated: https://github.com/prestodb/presto/issues/13722\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13725", "Add support for filter functions on columns with coercions", null, "bhhari", "11/26/19, 05:18:04 AM", "Filter expressions are referring to columns from the latest table schema, hence, variables in the variableToInput mapping in OrcSelectivePageSourceFactory must use column types from the latest table schema, not from the partition schema.\r\n\r\n```\r\nCaused by: com.facebook.presto.spi.PrestoException: Error opening Hive split xxx java.lang.UnsupportedOperationException\r\n\tat com.facebook.presto.hive.orc.OrcSelectivePageSourceFactory.createOrcPageSource(OrcSelectivePageSourceFactory.java:395)\r\n\tat com.facebook.presto.hive.orc.DwrfSelectivePageSourceFactory.createPageSource(DwrfSelectivePageSourceFactory.java:115)\r\n\tat com.facebook.presto.hive.HivePageSourceProvider.createSelectivePageSource(HivePageSourceProvider.java:211)\r\n\tat com.facebook.presto.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:117)\r\n\tat com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:51)\r\n\tat com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:58)\r\n\tat com.facebook.presto.operator.ScanFilterAndProjectOperator.getOutput(ScanFilterAndProjectOperator.java:226)\r\nCaused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.UnsupportedOperationException\r\n\t...\r\n\tat com.facebook.presto.hive.orc.OrcSelectivePageSourceFactory.toFilterFunctions(OrcSelectivePageSourceFactory.java:606)\r\n\tat com.facebook.presto.hive.orc.OrcSelectivePageSourceFactory.createOrcPageSource(OrcSelectivePageSourceFactory.java:357)\r\n\t... 17 more\r\nCaused by: java.lang.UnsupportedOperationException\r\n\tat com.facebook.presto.sql.gen.InputReferenceCompiler.visitVariableReference(InputReferenceCompiler.java:98)\r\n\tat com.facebook.presto.sql.gen.InputReferenceCompiler.visitVariableReference(InputReferenceCompiler.java:43)\r\n\tat com.facebook.presto.sql.gen.RowExpressionCompiler$Visitor.visitVariableReference(RowExpressionCompiler.java:237)\r\n\tat com.facebook.presto.sql.gen.RowExpressionCompiler$Visitor.visitVariableReference(RowExpressionCompiler.java:98)\r\n\tat com.facebook.presto.spi.relation.VariableReferenceExpression.accept(VariableReferenceExpression.java:71)\r\n\t...\r\n\tat com.facebook.presto.sql.gen.RowExpressionPredicateCompiler.lambda$new$0(RowExpressionPredicateCompiler.java:89)\r\n\tat com.google.common.cache.CacheLoader$FunctionToCacheLoader.load(CacheLoader.java:165)\r\n```\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13726", "Construct QualifiedFunctionName with CatalogSchemaName and function name", "Vic Zhang", "viczhang861", "11/26/19, 11:46:31 PM", "Do not use dot separated string to create QualifiedFunctionName to lower the chance of throwing runtime error\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13727", "Rename session variable to be more specific", null, "bhhari", "11/22/19, 07:28:16 AM", "Session property `nested_columns_filter_enabled` enables range filters on elements of arrays and maps, but has no effects on structs. Hence, renamed to `range_filters_on_subscripts_enabled`.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13729", "Implement Parallel Partition Pruning for Glue Hive Metastore", "Anoop Johnson", "anoopj", "12/06/19, 01:33:00 AM", "```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Add support for parallel partition fetching for the Glue metastore.\r\n```", "NaN"], ["13732", "Update cla info", "Brian Warner", "brianwarner", "11/21/19, 07:15:00 PM", "Please make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nUpdate the CONTRIBUTING.md file to remove reference to the Facebook CLA, and instead describe the EasyCLA process.\r\n\r\nSigned-off-by: Brian Warner <brian@bdwarner.com>", "NaN"], ["13733", "Make restProxyServiceForQuery nullable for Pinot connector", "Devesh Agrawal", "agrawaldevesh", "11/22/19, 05:57:18 AM", "Summary: I think we missed making it nullable in the first version of\r\nthis code. Indeed it should be nullable, because its only call is\r\ninside of Optional.ofNullable(...). This setting is kind of Uber\r\ninternal because of the way Pinot is deployed there and its not\r\npublicized in https://prestodb.io/docs/current/connector/pinot.html.\r\n\r\nMake it nullable as intended.\r\n\r\nTest Plan: Start the presto server locally with a pinot property file\r\nnot having this setting and paired with a locally (docker) running pinot\r\ninstallation.\r\n\r\nWould fix https://github.com/prestodb/presto/issues/13724\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13734", "Add all_match, any_match and none_match functions for arrays", "Wenlei Xie", "wenleix", "11/22/19, 05:47:00 AM", "Backported from https://github.com/prestosql/presto/pull/1045\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add all_match(), any_match(), and none_match() functions.\r\n```\r", "NaN"], ["13736", "Minor fixes to documents", "Wenlei Xie", "wenleix", "11/22/19, 07:39:37 PM", " * Reorder array functions alphabetically.\r\n * Remove extra space in function type to keep consistent.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13739", "Revert \"Optimize PlanFragment serialization\"", "Nikhil Collooru", "NikhilCollooru", "11/22/19, 08:12:24 PM", "This reverts commit 325c37d57f1b661ea8dc9ddea9372b3fe34302d1.\r\nReverting the original commit because we are unable to see the LivePlan for the queries \r\non the coordinator UI.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13741", "Enable creating table with table_supports_delta_delete property for raptor", "Ke", "kewang1024", "11/26/19, 10:12:10 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nRaptor Changes\r\n* Add table_supports_delta_delete property in Raptor to allow deletion happening in background. DELETE queries in Raptor can now delete data logically but relying on compactors to delete physical data.\r\n```\r", "NaN"], ["13742", "Use distinct table names in TestHiveRecoverableGroupedExecution", "Shixuan Fan", "shixuan-fan", "11/25/19, 03:03:03 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13743", "Add Alluxio client jar to hive-hadoop2", "Bin Fan", "apc999", "12/03/19, 06:02:47 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add an Alluxio client jar to `plugin/hive-hadoop2/` (as a runtime dependency) \r\n  to avoid copying Alluxio client jar to all Presto servers manually to connect to Alluxio.\r\n```\r\n\r", "NaN"], ["13744", "Merge files in chunks for LocalRangeCacheManager", null, "sujay-jain", "11/27/19, 02:19:22 AM", "This PR includes following changes to the ```LocalRangeCacheManager.java```\r\n\r\n- merge files in chunks during cache write to avoid reading excessively large data into memory at once\r\n- avoiding extra copy from ```data``` to ```remainingCacheFileBytes``` and then to newFile by using ```RandomAccessFile``` \r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13745", "Introduce warnings for nonReserved words moving to Reserved", null, "sujay-jain", "11/27/19, 11:34:59 PM", "Fix for https://github.com/prestodb/presto/issues/13586 \r\nOpen to suggestions for a better way to keep the set of the words. Opted for a static set due to the temporary nature of the warning, and the assumption that this list would not change often. Downside being that this approach is not very extendible. \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13746", "Optimize UNNEST", "Ying", "yingsu00", "04/20/20, 02:37:40 PM", "Updates on 01/21/2019:\r\n- Removed ArrayOfRowUnnestBuilder\r\n- Addressed other comments from Masha and Andrii\r\n\r\nUpdates on 12/28/2019:\r\n- Added appendNull() to Block interface in 252dc6072b\r\n- Removed UnnestBuilderUtils which contained insertNull to all Blocks\r\n- Refactored Unnester to Interface in b9c03f49fa\r\n- Removed GenericUnnestBuilder class and directly use UnnestBuilder\r\n- Resolved other comments from @mbasmanova and @arhimondr \r\n\r\nResolves https://github.com/prestodb/presto/issues/13751\r\n\r\nImprove performance of UnnestOperator by using vectorized execution and bulk copy of the blocks. JMH benchmark shows 2.3x-10.x improvement over baseline (unit is us/op):\r\n\r\nNo nulls, no ordinality\r\n\r\nReplicated     | Unnest                |         before        |   after | gain\r\n-------------|-----------------|-----------------|-------|-----\r\nbigint  | array(integer)         |                   1,477.6      |      167.0    | 8.8\r\nbigint  |  array(varchar)        |                    1,594.9       |     165.8   |  9.6\r\nbigint   | map(varchar,varchar)      |                1,535.4     |       187.5   |  8.2\r\nbigint   | array(array(varchar))    |                 1,499.7     |       171.2   |  8.8\r\nbigint   | array(row(varchar,varchar,varchar))    |   1,682.7          |  201.7    | 8.3\r\nvarchar  | array(integer)                |            1,457.0        |    172.8  |   8.4\r\nvarchar |  array(varchar)               |             1,483.1        |    170.7   |  8.7\r\nvarchar  | map(varchar,varchar)           |           1,586.4     |       203.9  |   7.8\r\nvarchar  | array(array(varchar))     |                1,489.8  |          169.0 |    8.8\r\nvarchar  | array(row(varchar,varchar,varchar))    |   1,681.0         |   204.8 |    8.2\r\n\r\nNo nulls, with ordinality\r\n\r\nReplicated  | Unnest          |                       before      |       after   | gain\r\n-------------|-----------------|-----------------|-------|-----\r\nbigint  |  array(integer)        |                    1,590.6        |    284.4  |   5.6\r\nbigint |   array(varchar)    |                        2,987.8  |          276.5 |    10.8\r\nbigint   | map(varchar,varchar)  |                    1,685.4  |          303.2  |   5.6\r\nbigint  |  array(array(varchar))    |                 1,613.3      |      310.9  |   5.2\r\nbigint   | array(row(varchar,varchar,varchar))  |     1,865.8  |          343.8 |    5.4\r\nvarchar  | array(integer) |                           1,609.8 |           280.7 |    5.7\r\nvarchar |  array(varchar)    |                        1,605.9   |         291.9   |  5.5\r\nvarchar  | map(varchar,varchar)  |                    1,639.1 |           317.8 |    5.2\r\nvarchar  | array(array(varchar))   |                  1,593.8  |          315.1  |   5.1\r\nvarchar  | array(row(varchar,varchar,varchar)) |      1,840.4 |           341.5 |    5.4\r\n\r\nNo ordinality, 2 unnested columns\r\n\r\nnullsRatio  | Replicated   |  Unnest       |               before    |       after |   gain\r\n-------------|-----------------|-----------------|-------|-----|--------\r\n0      |   bigint  |  array(bigint)+array(bigint)   |     2,153.7     |      581.5  |   3.7\r\n0      |   bigint  |  array(varchar)+array(varchar)   |   4,799.6   |        615.4  |   7.8\r\n0     |    varchar  | array(bigint)+array(bigint)   |     2,105.1     |      669.8   |  3.1\r\n0      |   varchar |  array(varchar)+array(varchar)  |    4,839.5    |       620.5   |  7.8\r\n0.2    |   bigint  |  array(bigint)+array(bigint)   |     1,884.3       |    523.3    | 3.6\r\n0.2    |   bigint    | array(varchar)+array(varchar)  |    1,867.9       |    496.9    | 3.8\r\n0.2     |  varchar  |  array(bigint)+array(bigint)    |    1,871.2         |  529.8    | 3.5\r\n0.2     |  varchar   | array(varchar)+array(varchar)   |   1,884.2      |     516.2   |  3.7\r\n\r\nArray of rows with 5% nulls in RowBlock\r\n\r\nnullsRatio  | Replicated   |  Unnest            |                 before      |      after  |  gain\r\n-------------|-----------------|-----------------|-------|-----|--------\r\n0      |   bigint |   array(row(varchar,varchar,varchar))   |     6,462.5     |      777.5   |  8.3\r\n0     |    varchar |  array(row(varchar,varchar,varchar))   |     6,720.0     |      702.7  |   9.6\r\n0.2    |   bigint   | array(row(varchar,varchar,varchar))    |    1,732.1        |   720.1    | 2.4\r\n0.2    |   varchar  | array(row(varchar,varchar,varchar))   |     1,747.5       |    743.8   |  2.3\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve performance of UnnestOperator\r\n\r\n```\r", "NaN"], ["13747", "Fix error type for unsupported correlated subquery", "Vic Zhang", "viczhang861", "11/25/19, 07:18:53 PM", "Part of #13935\r\n\r\nThis reverts previous change of error type in\r\nhttps://github.com/prestodb/presto/commit/5f72687a4b09abc30648985e5cd5a4db0735bcc5\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13748", "Fix SQL function compilation failure", "Rongrong Zhong", "rongrong", "11/26/19, 12:26:31 AM", "Fixes #13737\r\n\r\nRowExpression compilation for SQL function could fail when the input\r\nis not referenced in function body.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix SQL function compilation failure when the function parameter is not referenced in function body.\r\n\r\n```\r", "NaN"], ["13749", "Consuming max buffer size from the session properties ", "Rohit Jain", "jainxrohit", "12/06/19, 09:39:49 PM", "Consuming max buffer size from the session properties instead of configs in the Raptor page sink provider.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13754", "Fix filter pushdown for always-true range filter on a boolean column", null, "bhhari", "11/27/19, 03:51:13 PM", "Fixes filter pushdown for an always-true range filter on a boolean column. The following query fails without the fix:\r\n\r\n`select * from t where b = true or b = false or b is null`\r\n\r\n```\r\nCaused by: java.lang.IllegalArgumentException: Unexpected range of ALL values\r\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:141)\r\n\tat com.facebook.presto.orc.TupleDomainFilterUtils.createBooleanFilter(TupleDomainFilterUtils.java:148)\r\n\tat com.facebook.presto.orc.TupleDomainFilterUtils.toFilter(TupleDomainFilterUtils.java:91)\r\n\tat com.google.common.collect.Maps$9.transformEntry(Maps.java:1950)\r\n\tat com.google.common.collect.Maps$12.getValue(Maps.java:1991)\r\n\tat com.facebook.presto.hive.orc.OrcSelectivePageSourceFactory.toTupleDomainFilters(OrcSelectivePageSourceFactory.java:552)\r\n\tat com.facebook.presto.hive.orc.OrcSelectivePageSourceFactory.createOrcPageSource(OrcSelectivePageSourceFactory.java:328)\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13756", "Raptor read and write with delta delete functionality", "Ke", "kewang1024", "12/18/19, 03:34:12 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nRaptor Changes\r\n* Add the ability to read and write with delta deletes.\r\n```\r", "NaN"], ["13757", "Support running benchmark queries concurrently", "Nikhil Collooru", "NikhilCollooru", "12/20/19, 06:07:39 PM", "- Support running benchmarks having CONCURRENT as query executing strategy for all its phases.\r\n- Add ability to retry queries if they failed initially with cluster connection errors.\r\n- Add event clients to export the benchmark results\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13758", "Verifier: Improve determinism analysis and checksum query recording", "Leiqing Cai", "caithagoras", "12/04/19, 02:26:38 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n-----------------\r\n* Fix an issue where checksum query text and ID are not recorded if the checksum query fails.\r\n* Add new columns ``control_session_properties`` and `test_session_properties` to ``verifier_queries``, and remove column ``session_properties_json``. The value of the removed column can be copied to the two new columns for the schema change.\r\n* Add details of determinism analysis runs to the output.\r\n* Add configuration property ``max-determinism-analysis-runs`` to control maximum number of determinism analysis runs in case of column mismatch.\r\n* Add configuration property ``run-teardown-for-determinism-analysis`` to allow disabling teardown for determinism analysis runs.\r\n```\r", "NaN"], ["13760", "Presto on Spark Initial Commit", "Andrii Rosa", "arhimondr", "01/14/20, 05:30:30 PM", "For https://github.com/prestodb/presto/issues/13856\r\nDesign Doc: https://docs.google.com/document/d/1aURQWDY1NJZ7xPS6jnsFcQY7pOIqHGkHzJrcPu-42Tk/\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13764", "Respect null-call clause in SQL function execution", "Rongrong Zhong", "rongrong", "11/28/19, 02:20:16 AM", "fixes #13761 \r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix SQL function execution not respecting null-call clause\r\n```", "NaN"], ["13765", "Fix slice size overflow in DictionaryBlock.getIds()", "Ying", "yingsu00", "12/01/19, 08:01:21 PM", "When OptimizedPartitionedOutputOperator receives DictionaryBlock of\r\nDictionaryBlock and the dictionary's ids array is over 500 million rows,\r\nthe BlockFlattener would throw an exception in DictionaryBlock.getIds()\r\nbecause it's trying to wrap the large ids array into a Slice but the\r\nsize limit of a Slice is 2GB. This fix introduces ImmutableIntArray\r\nand uses it as the return type of DictionaryBlock.getIds().\r\n\r\nRelated issue: https://github.com/prestodb/presto/issues/13722\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13766", "Add support for column names with hyphen to SubfieldTokenizer", "Maria Basmanova", "mbasmanova", "12/01/19, 07:59:58 PM", "Fixes #13759\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13767", "Fail creation of table with unsupported bucket count", "Vic Zhang", "viczhang861", "12/05/19, 10:46:33 PM", "This commit moves bucket_count check from scheduling stage to query analyzer and throw user error.\r\nHere is stack trace of current error https://gist.github.com/viczhang861/95f02041f595637450c4035098ccd841\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13768", "Fix and optimize stripe metadata cache", "Shixuan Fan", "shixuan-fan", "11/27/19, 10:49:02 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13769", "Fix materialized exchange planning with filter pushdown", "Maria Basmanova", "mbasmanova", "12/02/19, 05:09:01 PM", "- Make sure to include pushed down range filters in ConnectorTableLayout#predicate.\r\n- Populate constant values for Partitioning#arguments if corresponding column is not\r\nprojected out of table scan and has fixed value (due to pushed down x = 5 filter)\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13773", "Fix backward compatible issue of Raptor delta delete", "Ke", "kewang1024", "12/02/19, 08:15:06 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13774", "Adding GcStatusMonitor to log Major GCs", null, "aweisberg", "12/12/19, 12:00:32 AM", "Replaces #13495 which I can't seem to push to anymore.\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13779", "Add support for column names with slashes to SubfieldTokenizer", "Maria Basmanova", "mbasmanova", "12/02/19, 09:14:31 PM", "Fixes #13759\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13780", "Invert if condition for better readability", "Ajay George", "ajaygeorge", "12/02/19, 10:28:55 PM", "This refactoring inverts the if condition in removeDuplicates() method\r\nfor better readability of code and aligns with the intent described in\r\nthe javadoc for the method\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13782", "Use LazyBlock for columns with no filters", "Maria Basmanova", "mbasmanova", "12/03/19, 06:43:38 AM", "In cases when table scan is followed by projection with a conditional expression, it could\r\nbe beneficial to delay loading some columns as they may never be needed. For example, if\r\na = 2 is very selective (less than 1 in a 1000), using LazyBlock for b allows us to avoid\r\nreading b for pages with no hits.\r\n\r\n`SELECT COUNT_IF(a = 2 AND cardinality(b) > 4 AND b[4] = 7)`\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13784", "Cast constants in SQL function to proper types", "Rongrong Zhong", "rongrong", "12/03/19, 05:44:01 AM", "Fixes #13786.\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix SQL function byte code compilation error for functions implemented with constants\r\n```\r", "NaN"], ["13785", "Fix flaky test TestRetryDriver", "Leiqing Cai", "caithagoras", "12/04/19, 02:26:10 AM", "Fixes intermittent failure:\r\n\r\n```\r\njava.lang.IllegalStateException: There already is a start record for test: com.facebook.presto.verifier.retry.TestRetryDriver::setup\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13787", "List non built-in functions in SHOW FUNCTIONS", "Leiqing Cai", "caithagoras", "12/03/19, 05:44:30 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support to list non-builtin functions in SHOW FUNCTIONS.\r\n  The feature can be turned on by the configuration property ``list-non-built-in-functions``.\r\n```\r", "NaN"], ["13789", "Check column exist before alter table for raptor delta delete", "Ke", "kewang1024", "12/03/19, 11:47:39 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13790", "Make subfield name matching case insensitive", "Maria Basmanova", "mbasmanova", "12/03/19, 08:03:51 PM", "Before this fix, with subfield pruning enabled, the following\r\nquery returned NULLs:\r\n\r\n`SELECT INFO.ORDERKEY FROM lineitem_ex`\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13791", "Update hive.rst with Alluxio docs", "Haoyuan Li", "haoyuan", "12/03/19, 11:11:53 PM", "Documentation for this PR: https://github.com/prestodb/presto/pull/13743\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13792", "Create NestedField using lowercase field name", "Maria Basmanova", "mbasmanova", "12/03/19, 08:40:02 PM", "Fix subfield pruning for queries that refer to the same subfield using\r\ndifferent word casings:.\r\n\r\n`SELECT s.a, s.A FROM t`\r\n\r\nWithout the fix the query above generated two subfield paths: s.a and s.A.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13796", "Make SQL function work for input parameters with lambda", "Rongrong Zhong", "rongrong", "12/04/19, 07:27:24 PM", "Fixes #13795.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix SQL function compilation error when input parameters contain lambda.\r\n```", "NaN"], ["13797", "Make TestUpgradeMetadata single threaded", "Leiqing Cai", "caithagoras", "12/04/19, 02:38:24 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13798", "Add a method to compare SqlInvokedFunction", "Leiqing Cai", "caithagoras", "12/05/19, 07:09:11 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13799", "Support ALTER FUNCTION", "Leiqing Cai", "caithagoras", "01/14/20, 10:29:40 PM", "ALTER FUNCTION Syntax:\r\n```\r\n<alter routine statement> ::=\r\n    ALTER <specific routine designator> <alter routine characteristics>\r\n\r\n<specific routine designator> ::=\r\n    <routine type> <member name>\r\n\r\n<routine type> ::=\r\n    FUNCTION\r\n\r\n<member name> ::=\r\n    <schema qualified routine name> [ <data type list> ]\r\n\r\n<alter routine characteristic> ::=\r\n    <null-call clause>\r\n```\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for ``ALTER FUNCTION``.\r\n```\r", "NaN"], ["13802", "Optimize and refactor array_distinct, array_filter and array_sort ", "Sreeni Viswanadha", "kaikalur", "12/13/19, 07:00:52 PM", "  * Avoid null checks when mayHaveNulls is false\r\n  * Avoid creating new blocks when not needed\r\n      * when a filter keeps all elements\r\n      * distinct on an already distinct array\r\n      * sort on a sorted array\r\n  * Cleanup the array_sort algorithm to be more efficient\r\n  * Removed page_builder use and made the functions static\r\n  * Added simple tests\r\n\r\nOverall all the operations are anywhere 5%-30% faster.\r\n\r\nBenchmark results (best numbers for before and after):\r\n\r\nBenchmarkArrayFilter.benchmark  avgt   20  26.636 \u00b1 1.571  ns/op\r\nBenchmarkArrayFilter.benchmark  avgt   20  24.918 \u00b1 1.113  ns/op\r\n\r\nBenchmarkArraySort.arraySort      avgt   20  78.128 \u00b1 5.007  ns/op\r\nBenchmarkArraySort.arraySort      avgt   20  48.125 \u00b1 3.626  ns/op\r\n\r\nBenchmarkArrayDistinct.arrayDistinct avgt 20   35.702 \u00b1 1.024  ns/op\r\nBenchmarkArrayDistinct.arrayDistinct avgt   20  33.535 \u00b1 1.060  ns/op\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13804", "Enable bucket pruning for IN predicates", "Gautam Parai", "gparai", "12/07/19, 07:31:49 AM", "Given table `t` bucketed by column `x` and a query `SELECT * FROM t WHERE x = 5`, Presto compute bucket ID for x = 5 and reduces table scan to just that bucket. This technique is called bucket pruning.\r\n\r\nThis change extends bucket pruning to predicates where multiple values match. For queries with predicates like `x IN (5, 7)` and `x = 5 OR x = 7`, Presto will compute bucket IDs for values 5 and 7 and reduce table scan to just these buckets. \r\n\r\nThis logic doesn't apply to predicates like `x BETWEEN 5 AND 7` or `x > 5 AND x < 10`.\r\n\r\nCC: @prestodb/aria \r\n\r\n== NO RELEASE NOTE ==\r", "NaN"], ["13805", "Reduce Memory used by Finished AsyncQueue Instances", "James Petty", "pettyjamesm", "12/09/19, 05:41:15 AM", "`ArrayDeque` instances have no way to shrink their backing array which can grow large during query execution and is retained by way of references in `QueryTracker` after completion. This resets the `AsyncQueue` elements field after finishing to reduce heap usage.\r\n\r\nPort of https://github.com/prestosql/presto/pull/2200\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13806", "Fix NaN comparison", null, "bhhari", "12/18/19, 08:29:33 AM", "Any floating point comparison with NaN should return false, but `!=` and `<>` operators should return true:\r\n\r\n```\r\nNaN  != 1 -> true\r\nNaN != NaN -> true\r\n\r\nselect * from t where value = 1  -> doesn't return NaNs\r\nselect * from  t where value <> 1 -> returns NaNs\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13808", "Fix pushdown for filters on multiple subfields of a struct", "Maria Basmanova", "mbasmanova", "12/05/19, 05:23:33 PM", "A filter on multiple subfields of a struct would fail with an error:\r\n\r\n`SELECT count(*) FROM t WHERE a.x IS NOT NULL AND a.y IS NOT NULL`\r\n\r\n```\r\njava.lang.IllegalArgumentException: Every argument must have a unique mapping. c.c maps to com.facebook.presto.spi.predicate.Domain@8c1adaec and com.facebook.presto.spi.predicate.Domain@99b87377\r\n\r\n\tat com.facebook.presto.spi.predicate.TupleDomain.transform(TupleDomain.java:395)\r\n\tat com.facebook.presto.hive.HiveMetadata.getTableLayout(HiveMetadata.java:2173)\r\n\tat com.facebook.presto.hive.HiveMetadata.pushdownFilter(HiveMetadata.java:1924)\r\n\tat com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorMetadata.pushdownFilter(ClassLoaderSafeConnectorMetadata.java:105)\r\n\tat com.facebook.presto.metadata.MetadataManager.pushdownFilter(MetadataManager.java:446)\r\n\tat com.facebook.presto.sql.planner.iterative.rule.PickTableLayout.pushPredicateIntoTableScan(PickTableLayout.java:336)\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13815", "Revert \"Consuming max buffer size from the session properties \"", "Saksham", "sachdevs", "12/06/19, 11:33:59 PM", "Reverts prestodb/presto#13749", "NaN"], ["13816", "Move logical optimization above ExtractSpatialJoins", "Saksham", "sachdevs", "12/19/19, 08:55:23 PM", "Refactor ExtractSpatialJoins to use RowExpressions\r\n\r\nTODO: Run verifier.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13818", " Support PathFilter when fetching splits for Hudi tables", "Bhavani Sudha Saktheeswaran", "bhasudha", "02/11/20, 02:56:56 PM", "This PR is corresponds to the issue - https://github.com/prestodb/presto/issues/13511\r\nSummary:\r\n    - Introduce PathFilter support in DirectoryLister interface and HiveFileIterator\r\n    - Unit test HiveFileIterator using PathFilter\r\n    - Plug specific PathFilter implementation for HoodieParquetInputFormat\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13819", "Adaptive filter reordering in scan", "Maria Basmanova", "mbasmanova", "12/10/19, 08:56:23 PM", "We are seeing a performance regression for filters with multiple conjuncts on different columns and \r\nthe first conjunct being highly selective.\r\n\r\n`SELECT * FROM t WHERE f(a) AND g(b)`\r\n\r\nIn these cases, in baseline, LazyBlocks for the 2nd conjunct are not loaded (or loaded rarely). In Aria, we had all filter functions evaluated together after all inputs for all of them are read. In the query above, we read columns a and b, then evaluate f(a), then f(b). \r\n\r\nThis PR introduces a few changes:\r\n- Evaluate each filter function as soon as all the inputs are read. This way, f(a) above will be evaluated before column b is read.\r\n- Change the initial order of columns to (1) columns with range filters; (2) columns providing inputs to filter functions; (3) columns without filters. In group (2), columns are ordered by type (simply types first): bool, integers, floats, strings, arrays/maps/structs.\r\n- Introduce adaptive filter reordering. Track number of rows dropped per unit of time for each function, then put most productive (drops rows fastest) first.\r\n\r\nCC: @prestodb/aria \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13820", "Sort OOM killer log output", null, "aweisberg", "12/12/19, 04:55:57 PM", "This is a quick change to make it easier to surface interesting information first. \r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* OOM killer log output sorted to put memory heavy nodes and queries first\r\n```", "NaN"], ["13821", "Minor refactor to TableFinishOperator", "Wenlei Xie", "wenleix", "12/10/19, 07:39:36 AM", "* Add comment and state check for lifespan commits\r\n* Reorganize if-branches\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13823", "Extend ST_Points to support major Well-Known spatial objects", "Yifeng Jiang", "uprush", "02/13/20, 11:52:45 PM", "Extend ST_Points to support POINT, LINESTRING, POLYGON, MULTIPOINT,\r\nMULTILINESTRING, MULTIPOLYGON and GEOMETRYCOLLECTION.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeospatial Changes\r\n* Improve ST_Points to add support for major Well-Known spatial objects.\r\n  ST_Points now supports POINT, LINESTRING, POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON and GEOMETRYCOLLECTION.\r\n```\r", "NaN"], ["13824", "Use JTS for ST_Buffer", "James Gill", "jagill", "12/18/19, 07:20:33 PM", "Convert ST_Buffer implementation to use JTS.  JTS is generally more efficient (see benchmarks below).  Also, ESRI returns an empty geometry when you buffer by small distances (< 1e-9);\r\nJTS handles these correctly.\r\n\r\nJTS\r\nBenchmark                                        Mode  Cnt        Score       Error  Units\r\nBenchmarkSTBuffer.stBufferPoint                  avgt   20       14.221       2.408  us/op\r\nBenchmarkSTBuffer.stBufferMultiPointSparse       avgt   20      810.414      43.980  us/op\r\nBenchmarkSTBuffer.stBufferMultiPointDense        avgt   20    18539.600     998.499  us/op\r\nBenchmarkSTBuffer.stBufferMultiPointReallyDense  avgt   20  1650058.575   71342.838  us/op\r\nBenchmarkSTBuffer.stBufferLineStringCircle       avgt   20      562.464      26.039  us/op\r\nBenchmarkSTBuffer.stBufferLineStringDense        avgt   20   316479.586   29001.318  us/op\r\nBenchmarkSTBuffer.stBufferPolygonSimple          avgt   20       11.892       0.711  us/op\r\nBenchmarkSTBuffer.stBufferPolygonNormal          avgt   20    46390.604    2958.109  us/op\r\nBenchmarkSTBuffer.stBufferPolygonDense           avgt   20    72822.143    6841.961  us/op\r\n\r\nEsri\r\nBenchmark                                        Mode  Cnt        Score        Error  Units\r\nBenchmarkSTBuffer.stBufferPoint                  avgt   20        8.290        0.541  us/op\r\nBenchmarkSTBuffer.stBufferMultiPointSparse       avgt   20    16521.536      818.169  us/op\r\nBenchmarkSTBuffer.stBufferMultiPointDense        avgt   20   450394.639    94490.620  us/op\r\nBenchmarkSTBuffer.stBufferMultiPointReallyDense  avgt   20  3856960.148   300968.259  us/op\r\nBenchmarkSTBuffer.stBufferLineStringCircle       avgt   20    13470.522     1308.101  us/op\r\nBenchmarkSTBuffer.stBufferLineStringDense        avgt   20   440081.641    37944.614  us/op\r\nBenchmarkSTBuffer.stBufferPolygonSimple          avgt   20       21.796        0.871  us/op\r\nBenchmarkSTBuffer.stBufferPolygonNormal          avgt   20    16648.597     1348.367  us/op\r\nBenchmarkSTBuffer.stBufferPolygonDense           avgt   20  1159728.778    27607.808  us/op\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Use more efficient implementation for ST_Buffer.  This produces fewer buffer points on rounded corners, which will produce very similar but different results.  JTS also better handles buffering with small (<1e-9) distances.\r\n```\r", "NaN"], ["13827", "Optimize zero row files creation for bucketed tables", "Andrii Rosa", "arhimondr", "12/11/19, 12:59:47 AM", "Hive bucketed table format requires zero row files to be created\r\nfor empty buckets.\r\n\r\nCreating zero files in a loop is very inefficient, and can significantly\r\nincrease query latency. Thus the file creation is done by multiple threads\r\nin parallel.\r\n\r\nRunning multiple threads is required, as FileSystem interface doesn't\r\nprovide non blocking IO methods for the file creation.\r\n\r\nTo provide reasonable latency for cases when the number of buckets is high\r\n(e.g. 4000++), some large thread pools might be created (e.g.: 100+ threads).\r\n\r\nRunning CPU intensive computation with such a high concurrency is generally\r\nnot desirable. CPU intensive tasks may steal precious CPU quanta from other,\r\nmore important processes in presto coordinator, resulting in overall cluster\r\ninstability.\r\n\r\nThis patch minimizes the amount of work that has to be done by each of these\r\nthreads, effectively limiting the thread pool usage just to waiting on blocking\r\nIO.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13829", "Fix SQL formatting for CREATE FUNCTION", "Leiqing Cai", "caithagoras", "12/12/19, 07:48:35 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13831", "Fix BooleanInputStream#getSetBits", "Maria Basmanova", "mbasmanova", "12/10/19, 05:02:56 PM", "An alternative to #13830 (fix for getSetBits is borrowed from getUnsetBits).\r\n\r\nFixes #13828\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13834", "Remove Redundant Parquet Column Index Lookups", "James Petty", "pettyjamesm", "12/11/19, 03:22:25 PM", "Previously, the parquet column index was repeatedly looked up to\r\ndetermine whether the column exists in the underlying file. This\r\nis unnecessary since this already occurs during the field creation\r\nand the presence or absence of a Field entry a sufficient check.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13839", "Enable subfield pruning to pass through arbitrary() function", "Gautam Parai", "gparai", "12/13/19, 03:57:59 PM", "Subfield pruning applies to queries accessing columns of complex types: maps, arrays and structs.\r\nQueries that use only some indices, keys or fields of these columns are optimized so that only\r\nnecessary data is extracted during the table scan. This PR enables subfield pruning to pass\r\nthrough the `arbitrary()` aggregate function. \r\n\r\nFor example, in the following query only subfield `a` will be extracted from struct `c` during table scan.\r\n\r\n```\r\nCREATE TABLE t(id bigint, c row(a bigint, b varchar));\r\n\r\nSELECT id, arbitrary(c).a \r\nFROM t \r\nGROUP BY 1;\r\n``` \r\n\r\n== NO RELEASE NOTE ==\r", "NaN"], ["13841", "Populate basic query info for skipped events in Verifier", "Leiqing Cai", "caithagoras", "12/11/19, 09:52:49 AM", "When a verification is skipped, we can still populate the control/test catalog, schema, and original query.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13842", "Fix function namespace manager instantiation failure", "Leiqing Cai", "caithagoras", "12/12/19, 07:33:36 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix an issue where server fails to start when two function namespace managers of the same type are specified.\r\n```\r", "NaN"], ["13849", "Raptor background jobs", "Ke", "kewang1024", "12/20/19, 11:52:16 PM", "```\r\n== Raptor ==\r\n- Add delta delete support in Raptor. If table property \"table_supports_delta_delete\" is set to true\r\nwhen creating a Raptor table, DELETE query will write down \"tombstones\" of deleted data instead \r\nof rewriting files immediately. \r\n```\r", "NaN"], ["13851", "Create zero row files without overwrite flag", "Andrii Rosa", "arhimondr", "12/11/19, 11:09:15 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13852", "Fix boolean block regression", "Maria Basmanova", "mbasmanova", "12/12/19, 02:37:00 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13853", "Skip verification when checksum query fails to compile", "Leiqing Cai", "caithagoras", "12/12/19, 09:27:41 PM", "When the target table has too many floating point columns, checksum\r\nquery ends up with too many columns with window functions, causing\r\nCOMPILER_FAILURE error code. This case cannot be properly handled by\r\nthe Verifier and thus should be skipped for now.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nVerifier Changes\r\n* Improve Verifier to skip verification when checksum query fails to compile.\r\n```\r", "NaN"], ["13854", "Support table write commit in Presto on Spark", "Wenlei Xie", "wenleix", "04/27/20, 10:46:37 PM", "This is required by Presto-on-Spark (https://github.com/prestodb/presto/issues/13856) in case there is job failures/retry. Data written by failed tasks shouldn't be visible. ", "NaN"], ["13857", "Fix schema mismatch w/Parquet INT64 & Timestamp", null, "aweisberg", "12/12/19, 08:04:43 PM", "Schema checks are too strict and prevent timestamp type from being read as int64 from Parquet files.\r\nFixes: #13855\r\n\r\n```\r\n== RELEASE NOTES ==\r\nParquet Changes\r\n* Fix schema mismatch w/Parquet INT64 & Timestamp \r\n```", "NaN"], ["13858", "Get PrestoS3FileSystem to work with the AWS Default Credentials Provider", "Anoop Johnson", "anoopj", "12/26/19, 06:20:40 PM", "DefaultAWSCredentialsProviderChain is frequently used by AWS customers\r\nand it provides access from a documented list of sources. This\r\nespecially makes it easier to run Presto on non-EC2 hosts where you\r\ndon't have the instance profile. (e.g. Macs, during development).\r\n\r\nSee:\r\nhttps://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Get PrestoS3FileSystem to work with the AWS Default Credentials Provider\r\n```\r\n\r", "NaN"], ["13860", "Disable TestDistributedSpilledQueries#testLimitWithJoin", "Leiqing Cai", "caithagoras", "12/13/19, 12:21:21 AM", "Tracking in https://github.com/prestodb/presto/issues/13859\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13862", "Increase timeout in TestBackupManager#testCorruption", "Leiqing Cai", "caithagoras", "12/17/19, 06:25:06 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13866", "Optimize IN filter using bitmask", "Maria Basmanova", "mbasmanova", "12/17/19, 02:44:29 PM", "Fixes #13865\r\n\r\n```\r\nBenchmark                                (numValues)  Mode  Cnt  Score    Error  Units\r\nBenchmarkBigintValues.lookupInBitmask           5000  avgt   20  0.020 \u00b1  0.001  ms/op\r\nBenchmarkBigintValues.lookupInBitmask           1000  avgt   20  0.016 \u00b1  0.001  ms/op\r\nBenchmarkBigintValues.lookupInBitmask            100  avgt   20  0.015 \u00b1  0.002  ms/op\r\nBenchmarkBigintValues.lookupInBitmask             10  avgt   20  0.017 \u00b1  0.001  ms/op\r\nBenchmarkBigintValues.lookupInHashTable         5000  avgt   20  0.100 \u00b1  0.002  ms/op\r\nBenchmarkBigintValues.lookupInHashTable         1000  avgt   20  0.087 \u00b1  0.001  ms/op\r\nBenchmarkBigintValues.lookupInHashTable          100  avgt   20  0.033 \u00b1  0.001  ms/op\r\nBenchmarkBigintValues.lookupInHashTable           10  avgt   20  0.038 \u00b1  0.001  ms/op```\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13871", "Add approx_percentile forms with accuracy", "James Gill", "jagill", "01/10/20, 07:29:07 PM", "Previously you could only specify an accuracy to approx_percentile if\r\nyou also specified weights.  This adds an accuracy option for all forms.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add forms of approx_percentile accepting an accuracy parameter.\r\n\r\n```\r", "NaN"], ["13872", "Cache results of TupleDomainFilterUtils::toFilter", "Maria Basmanova", "mbasmanova", "01/02/20, 03:04:39 PM", "Large IN predicates are expensive to convert to TupleDomainFilter. Hence, we'd like to convert these once per task as opposed to once per split.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13873", "Fix raw input stats for ScanFilterAndProject", "James Petty", "pettyjamesm", "01/07/20, 01:35:12 AM", "Previously, `LazyBlock`s loaded from a `PageSource` would update the\r\nprocessed input bytes upon being loaded, but would not recompute the raw input\r\nbytes. The next call to getOutput would update the raw input stats, but if that never occurs then the reads were left uncounted.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Fix ScanFilterAndProjectOperator raw input bytes accounting for LazyBlocks\r\n```\r", "NaN"], ["13874", "Optimize array_join by supporting PROVIDED_BLOCKBUILDER convention", "Wenlei Xie", "wenleix", "12/20/19, 08:06:32 AM", "Please make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Optimizer performance for array_join\r", "NaN"], ["13877", "Add session property to disable adaptive filter reordering", "Maria Basmanova", "mbasmanova", "01/02/20, 03:05:14 PM", "Sometimes adaptive filter reordering is not able to pick optimal filter order. We observed this in a query with a filter containing dozens of conjuncts on a single column plus some conjuncts on other columns:\r\n```\r\nc NOT LIKE '%cats%' \r\nAND c NOT LIKE '%dogs%` \r\nAND c NOT LIKE ...\r\nAND b LIKE '%heads%' \r\nAND b NOT LIKE '%tails%' \r\nAND c NOT LIKE `%crocodiles%\r\nAND c NOT LIKE ...\r\n```\r\n\r\nIn that particular case optimal filter order is to evaluate some filter on `c`, then some on `b`, then some more on `c`. Adaptive filter reordering currently is not able to generate that order as all conjuncts on a given column are always evaluated together. \r\n\r\nThe new session property `hive.adaptive_filter_reordering_enabled` allows to disable adaptive filter reordering and evaluate filter in the order specified by the query. By default, adaptive filter reordering is enabled.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13878", "Add query integrity check in AccessControlManager", null, "mayankgarg1990", "12/26/19, 06:55:16 PM", "Resubmitting #13632\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n* Add API to check if the query is unexpectedly modified using the credentials passed in the identity\r\n```", "NaN"], ["13879", "Keep sum stat for string when min/max stat is too long in ORC writer", "Islam Ismailov", "islamismailov", "12/26/19, 07:38:19 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Output (sum) string stat even if min/max values are too long. This is needed for the read-path to be able to better estimate the size of row.\r\n```", "NaN"], ["13884", "Fix requiredField logic in StructSelectiveStreamReader", null, "bhhari", "12/21/19, 01:47:11 PM", "This fixes the following exception\r\n\r\n```\r\nCaused by: java.lang.IllegalArgumentException: filter must be present if outputRequired is false\r\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:141)\r\n\tat com.facebook.presto.orc.reader.DoubleSelectiveStreamReader.<init>(DoubleSelectiveStreamReader.java:92)\r\n\tat com.facebook.presto.orc.reader.SelectiveStreamReaders.createStreamReader(SelectiveStreamReaders.java:70)\r\n\tat com.facebook.presto.orc.reader.StructSelectiveStreamReader.<init>(StructSelectiveStreamReader.java:164)\r\n\tat com.facebook.presto.orc.reader.SelectiveStreamReaders.createStreamReader(SelectiveStreamReaders.java:84)\r\n\tat com.facebook.presto.orc.OrcSelectiveRecordReader.createStreamReaders(OrcSelectiveRecordReader.java:581)\r\n\tat com.facebook.presto.orc.OrcSelectiveRecordReader.<init>(OrcSelectiveRecordReader.java:176)\r\n\tat com.facebook.presto.orc.OrcReader.createSelectiveRecordReader(OrcReader.java:234)\r\n\tat com.facebook.presto.hive.orc.OrcSelectivePageSourceFactory.createOrcPageSource(OrcSelectivePageSourceFactory.java:363)\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13887", "Propagate deprecated.legacy-map-subscript to selective map reader", "Maria Basmanova", "mbasmanova", "01/16/20, 05:53:41 PM", "`deprecated.legacy-map-subscript` configuration properties affects the behavior of map subscript operator. When `cat` key doesn't exist, `m[\"cat\"]` returns null if `deprecated.legacy-map-subscript` is true and throws an exception if `deprecated.legacy-map-subscript` is false.\r\n\r\nMapSelectiveStreamReader needs access to this property to properly implement pushdown of range filters on elements.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13888", "Cache optimized RowExpression predicate", null, "bhhari", "12/23/19, 05:16:11 AM", "The RowExpressionOptimizer creates a new RowExpression\r\nevery time optimize is called. This change caches the\r\noptimized RowExpression at worker level for all the\r\nsplits which is efficient. This will also fix the issue\r\nwhere the predicateCache is having cache misses in case\r\nof a predicate with LikeFunction as the new RowExpression\r\ncreated does not correctly implement hashcode method\r\nbecause of the joni Regex.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13894", "Support internal communication with thrift", "James Sun", "highker", "01/17/20, 06:13:59 PM", "HTTP is too unreliable to use for internal communication. Add thrift support.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Allow Presto nodes to shuffle data with Thrift protocol. Use config `internal-communication.task-communication-protocol` to control between HTTP and Thrift.\r\n* Allow Presto nodes to announce state with Thrift protocol. Use config `internal-communication.server-info-communication-protocol` to control between HTTP and Thrift.\r\n```\r", "NaN"], ["13897", "Eliminate duplicate table names in TestRaptorIntegrationSmokeTest", "Ke", "kewang1024", "12/27/19, 04:01:24 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13898", "Add support for get partition names by filter", "Ajay George", "ajaygeorge", "01/10/20, 11:38:23 PM", "```\n== RELEASE NOTES ==\n\nGeneral Changes\n* Add support for get partition names by filter\n\n```", "NaN"], ["13899", "Correct doc for redis.key-prefix-schema-table config", "Deleted user", "ghost", "01/02/20, 06:00:46 PM", "Update the description of `redis.key-prefix-schema-table` property to use the correct key prefix syntax of `schema-name:table-name:` (add missing colon at the end) and document the behavior for default schemas.\r\n\r\nFixes #12142\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13901", "Fix invalid plan for repeated lambdas in order by", "Rebecca Schlussel", "rschlussel", "01/09/20, 04:35:52 PM", "Presto was creating invalid plans for queries with duplicate lambda\r\nexpressions in the order by clause because the expressions were\r\nconsidered \"equal\" for some purposes, but not equal when converting the lambdas\r\nto symbols.  This caused one of the expressions to not be mapped to an input\r\nsymbol. To resolve this, we don't deduplicate even seemingly equal expressions\r\nin the order by clause so that we maintain translations for all the\r\nexpressions.\r\n\r\nExample query:\r\n```\r\nSELECT\r\n  COUNT(*)\r\nFROM\r\n  (values ARRAY['a', 'b']) as t(col1)\r\nORDER BY\r\n  IF(\r\n    SUM(\r\n      REDUCE(\r\n        col1,\r\n        ROW(0),\r\n        (\"l\", \"r\") -> \"l\",\r\n        \"x\" -> 1\r\n      )\r\n    ) > 0,\r\n    COUNT(*),\r\n    SUM(\r\n      REDUCE(\r\n        col1,\r\n        ROW(0),\r\n        (\"l\", \"r\") -> \"l\",\r\n        \"x\" -> 1\r\n      )\r\n    )\r\n  )\r\n```\r\n\r\n\r\nThis fixes the second part of #10694 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix failures caused by invalid plans for queries with repeated lambda expressions in the order by clause.\r\n```\r", "NaN"], ["13902", "[easy] Clean up arrays in ORC dictionary after close", "James Sun", "highker", "01/02/20, 06:28:48 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13904", "Add caching file system to hive connector", "Rohit Jain", "jainxrohit", "01/08/20, 01:34:24 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Allow reading data from HDFS while caching the fetched data on local disks. Turn on the feature by specifying the cache directory config `cache.base-directory`.\r\n```", "NaN"], ["13905", "Add max buffer size to Raptor session properties", "Rohit Jain", "jainxrohit", "12/31/19, 04:37:28 AM", "Consuming max buffer size from the session properties instead of configs in the Raptor page sink provider.\r", "NaN"], ["13907", "Support non-blocking IO for page transport", "Vic Zhang", "viczhang861", "03/03/20, 01:30:34 AM", "To reviewers:\r\n  Feature is turned on by default for testing purpose,  feature should be off by default.  Don't merge until I fix this.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add configuration property `exchange.async-page-transport-enabled` to turn on non-blocking IO for page transport.\r\n* Add configuration property `exchange.async-page-transport-timeout` for server side timeout.\r\n* Use URL prefix '/v1/task/async' for async page transport request. \r\n\r\n```\r", "NaN"], ["13911", "Make BenchmarkSelectiveStreamReaders load LazyBlock", "Eugene Kalenkovich", "UncleGene", "01/02/20, 06:20:45 PM", "PR #13782 introduced LazyBlock in OrcSelectiveRecordReader. This change ensures that the block is loaded for benchmarking.\r\n\r\n== NO RELEASE NOTE ==", "NaN"], ["13914", "Optimize BooleanSelectiveStreamReader no filter reads", "Eugene Kalenkovich", "UncleGene", "01/03/20, 02:25:37 AM", "Similar to #13603\r\n\r\nPart of #13848\r\n\r\nOptimize boolean reader when reading contiguous rows with no nulls and no filter\r\n\r\nJMH benchmark results show 2x improvement when there is no nulls and no filters.\r\n\r\n    Before:\r\n    Benchmark                             (typeSignature)  (withNulls)  Mode  Cnt  Score   Error  Units\r\n    BenchmarkSelectiveStreamReaders.read          boolean        false  avgt   20  0.042 \u00b1 0.001   s/op\r\n\r\n    After:\r\n    Benchmark                             (typeSignature)  (withNulls)  Mode  Cnt  Score   Error  Units\r\n    BenchmarkSelectiveStreamReaders.read          boolean        false  avgt   20  0.022 \u00b1 0.001   s/op\r\n\r\n== NO RELEASE NOTE ==\r\n\r", "NaN"], ["13915", "Add release notes for 0.230", "Leiqing Cai", "caithagoras", "01/08/20, 08:19:46 PM", "# Missing Release Notes\n## Ami Tavory\n- [x] https://github.com/prestodb/presto/pull/13574 Refactor Differential Entropy for Mutual Information Classification (Merged by: Rebecca Schlussel)\n\n## Andrii Rosa\n- [x] 621eda421c851f5fc126a84be71afd3737b370d6 Set boolean value to 0 if the position is null\n- [x] c4462915ae5a5ee4d6009a3bd84c00933060e8b8 Check aggregation mask values for null\n\n## Gaurav Mittal\n- [x] https://github.com/prestodb/presto/pull/13704 Add support for JNI based decompression for zstd files (Merged by: James Sun)\n- [x] https://github.com/prestodb/presto/pull/13697 Wrap OrcReader parameters into OrcReaderOptions (Merged by: James Sun)\n\n## Leiqing Cai\n- [x] ec9f4778b726128dc1b14a72efb9ce7cd8774856 Disable TestDistributedSpilledQueries#testLimitWithJoin\n- [x] f189ca7dabb0044725bd59ab395b5152ffebe332 Make TestUpgradeMetadata single threaded\n\n## Masha Basmanova\n- [x] 08dba521159b03f98d803dbf6b55e828906254d9 Fix selective stream readers\n- [x] 20653eabb90fee53d455dd18fce40dafc8702d9e Fix BooleanInputStream#getSetBits\n- [x] c4d0c7f3fd1b11ac90f4dce9484f6685424caa02 Fix pushdown for filters on multiple subfields of a struct\n- [x] 9d60d38328a2b4e4ef2eff3b31fdfda452dcfce7 Create NestedField using lowercase field name\n- [x] 2b6e9749629bd41d4beff6809a050761e2a43f18 Make subfield name matching case insensitive\n\n## Sahar Massachi\n- [x] https://github.com/mayunlei/presto/pull/2 merge \n\n## Saksham Sachdev\n- [x] https://github.com/prestodb/presto/pull/13660 Transaction Sharing across Connectors (Merged by: James Sun)\n\n# Extracted Release Notes\n- #13283 (Author: Laila A. Wahedi): imported combinations function from prestosql, complete with tests an\u2026\n  - Added combinations function, a function that returns n combinations of values in an array, up to n=5.\n- #13405 (Author: Venki Korukanti): Reduce the identifier length in generated code for nested columns\n  - Fix compilation errors for expressions over types containing an extremely large number of nested types.\n- #13412 (Author: Leiqing Cai): Support DROP FUNCTION\n  - Add support for ``DROP FUNCTION``.\n- #13601 (Author: Islam Ismailov): Modularize Hive Metastore\n  - Metastore interface is separated into a separate module to reduce monolithicness.\n- #13602 (Author: Ying Su): Improve BatchStreamReader - Part 1\n  - Improve BatchStreamReader performance.\n- #13616 (Author: James A. Gill): Add geospatial expand_envelope function.\n  - Add expand_envelope function to return a geometry's envelope expanded by a distance.\n- #13644 (Author: James Sun): Introduce caching file system\n  - Introduced new cache module to allow using local disk for to cache files from remote file systems.\n  - Allow Raptor to read data from HDFS while caching the files on local disks.\n- #13645 (Author: James A. Gill): Fix GeometryToBingTiles for certain geometries\n  - Fix geometry_to_bing_tile for geometries at -180 longitude or 85.05112878 latitude.\n- #13647 (Author: Leiqing Cai): Output verifier events for queries being skipped by pre-filters\n  - Add skipped verification results to the output for queries being filtered.\n- #13650 (Author: Yi He): Optimize lambda body when possible\n  - Fix regression on lambda evaluation (Issue#13648).\n- #13651 (Author: Shixuan Fan): Make commitPartition an async operation\n  - Change ``ConnectorMetadata#commitPartition`` into async operation, and rename it to ``ConnectorMetadata#commitPartitionAsync``.\n- #13652 (Author: Ke Wang): fix organizationDiscoveryIntervalMillis bug\n  - Fix organizer not running scheduled job due to misreading `storage.organization-discovery-interval` config.\n- #13653 (Author: Yi He): Fix PRESTO_EXTRA_CREDENTIAL parsing\n  - Improved PRESTO_EXTRA_CREDENTIAL header parsing to allow value contain multiple '=' and urlEncode characters.\n- #13734 (Author: Wenlei Xie): Add all_match, any_match and none_match functions for arrays\n  - Add all_match(), any_match(), and none_match() functions.\n- #13741 (Author: Ke Wang): Enable creating table with table_supports_delta_delete property for raptor\n  - Add table_supports_delta_delete property in Raptor to allow deletion happening in background. DELETE queries in Raptor can now delete data logically but relying on compactors to delete physical data.\n- #13743 (Author: Bin Fan): Add Alluxio client jar to hive-hadoop2\n  - Add an Alluxio client jar to `plugin/hive-hadoop2/` (as a runtime dependency) to avoid copying Alluxio client jar to all Presto servers manually to connect to Alluxio.\n- #13748 (Author: Rongrong Zhong): Fix SQL function compilation failure\n  - Fix SQL function compilation failure when the function parameter is not referenced in function body.\n- #13764 (Author: Rongrong Zhong): Respect null-call clause in SQL function execution\n  - Fix SQL function execution not respecting null-call clause.\n- #13784 (Author: Rongrong Zhong): Cast constants in SQL function to proper types\n  - Fix SQL function byte code compilation error for functions implemented with constants.\n- #13787 (Author: Leiqing Cai): List non built-in functions in SHOW FUNCTIONS\n  - Add support to list non-builtin functions in SHOW FUNCTIONS. The feature can be turned on by the configuration property ``list-non-built-in-functions``.\n\n# All Commits\n- ec9f4778b726128dc1b14a72efb9ce7cd8774856 Disable TestDistributedSpilledQueries#testLimitWithJoin (Leiqing Cai)\n- 08dba521159b03f98d803dbf6b55e828906254d9 Fix selective stream readers (Masha Basmanova)\n- 621eda421c851f5fc126a84be71afd3737b370d6 Set boolean value to 0 if the position is null (Andrii Rosa)\n- c4462915ae5a5ee4d6009a3bd84c00933060e8b8 Check aggregation mask values for null (Andrii Rosa)\n- 20653eabb90fee53d455dd18fce40dafc8702d9e Fix BooleanInputStream#getSetBits (Masha Basmanova)\n- c4d0c7f3fd1b11ac90f4dce9484f6685424caa02 Fix pushdown for filters on multiple subfields of a struct (Masha Basmanova)\n- f189ca7dabb0044725bd59ab395b5152ffebe332 Make TestUpgradeMetadata single threaded (Leiqing Cai)\n- 9d60d38328a2b4e4ef2eff3b31fdfda452dcfce7 Create NestedField using lowercase field name (Masha Basmanova)\n- 2b6e9749629bd41d4beff6809a050761e2a43f18 Make subfield name matching case insensitive (Masha Basmanova)\n- c5c3c670f3ba1552752740473fb07355ac52a2ae Use LazyBlock for columns with no filters (Masha Basmanova)\n- 23e47735069360373b25e6604e460dec38353ce4 Add Alluxio client jar to hive-hadoop2 (Bin Fan)\n- 2b7baf23d57ca858be569d7a69d6a2054c29eb4a List non built-in functions in SHOW FUNCTIONS (Leiqing Cai)\n- 7236e03b2806dc0ace1b6a522106b123fff24886 Cast constants in SQL function to proper types (Rongrong Zhong)\n- b35550cb99c994239aa978054961555cb90a6cd2 Remove some usages of QualifiedFunctionName#of(String) (Leiqing Cai)\n- cb1b514a2a2e2dabbe9e9000f736702a78802973 Throw SYNTAX_ERROR when creating function with invalid name (Leiqing Cai)\n- 943b4d0dddd266eb9615146e96bdd29f535b7ba2 Support DROP FUNCTION execution (Leiqing Cai)\n- 71a23da4413adb97b26a132f085a00dbb7259328 Support dropFunction in FunctionNamespaceManager (Leiqing Cai)\n- 8138d2f2b1ca83d0ef90b2ebeda8a6e4e83c0a57 Add syntax support for DROP FUNCTION (Leiqing Cai)\n- eb47fcfe1121e82155dede9cb543156453c96f9d Remove print statement in TestFunctionNamespaceManager (Leiqing Cai)\n- 7c26b6e0fd5763a542e8c1062a7e80e4d9310f67 Invert if condition for better readability (Ajay George)\n- 1fbfb8e1c2d3869bee8fa8d25b86da77f6de3363 Add support for column names with slashes to SubfieldTokenizer (Masha Basmanova)\n- 0f1fe83486c8fa8f1377dbf7480dc4c9587a2389 Fix backward compatible issue of Raptor delta delete (Ke Wang)\n- d254d5f9464ada40b976653d8ae2b6fcc6997d01 Fix materialized exchange planning with filter pushdown (Masha Basmanova)\n- df0fd4def8d04c4a7abdbb8ad64c978e3af913f0 Fix slice size overflow in DictionaryBlock.getIds() (Ying Su)\n- 80613f9cbedd0fbd8ca08580ea56eee51ac2be55 Add support for column names with hyphen to SubfieldTokenizer (Masha Basmanova)\n- 3242715959a169dbcdd88946c28488d2365c8886 Respect null-call clause in SQL function execution (Rongrong Zhong)\n- 9aa8f80edf946d26760012d9da4c1b34cff25eb7 Optimize lambda body when possible (Yi He)\n- 83b7d178f0513468593c7639f70d3751a83b8e87 Propagate correct error code in RowExpressionInterpreter (Yi He)\n- bf4f7ec30be0f19e68edf768396a268d4f6cbad3 introduce warnings for nonReserved words moving to reserved (Sujay Jain)\n- 5772869a17e518c48400224667d773cac058a4d8 Materialize stripe stream slice when caching (Shixuan Fan)\n- 98f54e771a617a9a2c5fbd3ce70e65501e6b7964 Fix stripe metadata cache weigher to use retained size (Shixuan Fan)\n- a472c23662f01bcf60607e11bad369a45a0b2d5c Fix filter pushdown for always-true range filter on a boolean column (Bhavani Hari)\n- bc2e8b242da27e9b966035263f2d8516ed4ee731 chunk merge files during cache write to avoid reading excessively large data into memory and other optimizations (Sujay Jain)\n- dcb3e1bb75c5521e9a5310b861209d5ad8e461fe Construct QualifiedFunctionName with CatalogSchemaName and function name (Vic Zhang)\n- 5985800d12ae0f8cbdf08dbba7b54f1e8de67bfd Add array combinations function (Laila A. Wahedi)\n- 36ec9d22557f8873c5b3145745ab63702d07a977 Enable creating table with table_supports_delta_delete property (Ke Wang)\n- d355b8b596d2132ed02b8fb54307ffa7998c187e Add support for filter functions on columns with coercions (Bhavani Hari)\n- 6dc5468bdf2ddb02a077646fb52f9a845b9e17bf Fix SQL function compilation failure (Rongrong Zhong)\n- dedea3b46212ae9e0b00ee0fad446d75df70ace0 Support supplying benchmark queries through MySQL (Nikhil Collooru)\n- 456487eeb5ac5d2b53d9edb81cd2fb25a046c4f2 Minor refactor of ApplyNode constructor (Vic Zhang)\n- 19339ee0410f37bf27af9a4ad55d668024ad30f6 Throw PrestoException for unsupported correlated subquery (Vic Zhang)\n- bd08887707f6201c3ac63876191f60011059558b Use distinct table names in TestHiveRecoverableGroupedExecution (Shixuan Fan)\n- e18bb835ab1ac1f4a67f2fb2acd8bd74138d0d46 Expose H2QueryRunner Handle for Modular Testing (Saksham Sachdev)\n- 0a0a4436a1b0bdb3b3c8daeed366d98744dc3859 Add support for Companion Catalogs in Transactions (Saksham Sachdev)\n- 57f9632a9114bfd75762d8bbcdb2f4e5d203588e Revert \"Optimize PlanFragment serialization\" (Nikhil Collooru)\n- cafa9e28d32740ed9656e40f0984956455b112bd Minor fixes to documents (Wenlei Xie)\n- 4eb854909d8dcd245bd8f4af898c65bd302d6ff8 Rename session variable to be more specific (Bhavani Hari)\n- 042634096c3fe4f660ee9e3ed225e037b4c34bdf Fix size type when calculating retained size. (Ying Su)\n- 48e58fde4298acdb36b862aab21f2b56037bd084 Make restProxyServiceForQuery nullable for the Pinot connector (Devesh Agrawal)\n- fca4353c5b4649cac9d92ad720a249e1e232840f Add all_match, any_match and none_match functions for arrays (Wenlei Xie)\n- 2bee126caf9901eb25c621c5da0b5753f5d880df Extract and rename functional interfaces in ArrayFilterFunction (Wenlei Xie)\n- df7a0e581b27afcef9d9fbe5a1f19d431fcec964 Remove unused FilterVoidLambda interface in ArrayFilterFunction (Wenlei Xie)\n- 7a5cc80d614cd320c8b60ca98eb2ad8d56d169f4 Fix minor typo in CLA instructions (Brian Warner)\n- bba36ec71f455b557579a17448d7330ad4597a79 Update CLA information to reflect EasyCLA (Brian Warner)\n- ef3456a7a5239bc94b0599c03e19bf55be1801fc Introduce caching file system (James Sun)\n- 229b60b9c76630b90b8142e15edb066ec3df7657 Make commitPartition an async operation (Shixuan Fan)\n- 3d2f2a13a4d34e2ecc3e6166bbd088a3e1bd4a87 Add JNI decompressor for zstd files (Gaurav Mittal)\n- 01c66c7959bcc1970d7761f3c54848f64b058605 Fix selective struct reader to make a copy of input positions (Bhavani Hari)\n- 8e5051cd81afed8df74d697d4fc19770ea5952d1 Wrap OrcReader parameters into OrcReaderOptions (Gaurav Mittal)\n- 70308fa48f7dd8a1b9ff1f6b62c51cd0f4d56769 Fix Values are too large error message (Masha Basmanova)\n- cbf1ee762564fb1e5a33e2a978140f28141cd67a Allow range filters on newly added struct fields (Masha Basmanova)\n- 998db977c233803322252035398c4251082a37c3 Add test for range filter on a newly added column (Masha Basmanova)\n- d294ebca931a68155a06b090fddf5011c36c4050 Allow range filters on subfields of structs with coercion (Masha Basmanova)\n- e668b1a526e013f4003c4947d1d79067e52d78e9 Reset allNulls flag in SliceDirectxxx#read (Masha Basmanova)\n- db8067e93766014434ea9d5a8a95562726063150 Reset allNulls flag in SliceDictxxx#read (Masha Basmanova)\n- c0fdc903fee99d2148ccdf417356ccfbfd60c0b8 Skip map entries with null keys (Masha Basmanova)\n- d9e9f391ad06e6a59c9f111d9d0b3bd97eee7236 Run filters without inputs first in OrcSelectiveStreamReader (Sahar Massachi)\n- c0b0edd2c81d1245a9f3000d4a9b382ea77344a8 Evalute predicate on partition columns to prune partitions (Masha Basmanova)\n- dd16d8a486637da64dd8c4ab6df78063636967f3 Add a method to interpret an expression to ExpressionOptimizer (Masha Basmanova)\n- ca4e94cbbf6e1091a119a04523d0eb46a24afe60 Add functionMetadataManager to HiveMetadataFactory (Masha Basmanova)\n- 39a16b360f25e569f5aad49ffc4193408ad66a17 Fix filter pushdown for DELETE queries (Masha Basmanova)\n- 6cced01a5b3e00a02e571f37b8ea5df21486df95 Support reading RC, TEXTFILE fromats with pushdown enabled (Bhavani Hari)\n- d3ee8ec9d5da58eb8f2a24df1ce8955b434da549 Replace Joda-Time libraries with java.time (Ajay George)\n- 315868da110434b23180dcf7fece3ae47589dd7c Add final to histogram class members (Ami Tavory)\n- 9f2359a7b1487b09081045e3582e0e0048071fd2 Fixe minor issues in reservoir sample tests (Ami Tavory)\n- 4b112927ceed96b7e635a30024506f75320edfa3 Move logic for handling different strategy implementations to DifferentialEntropyStateStrategy (Ami Tavory)\n- 62299ab90df05e0ed0ecdfb8b557dcb80cc3a4ac Add cloneEmpty method to differential entropy strategy classes (Ami Tavory)\n- 35308314d18e31a86e06973b1eee123f3eba2879 Add population weight/count methods to reservoir samples (Ami Tavory)\n- 8e15a3afe5e0b7a294a5c251e0d8ac6d583aefee Expand entropy utils class and rename methods (Ami Tavory)\n- 9b0a1a0c2a12588486d1aa15676ad2965ee57176 Update names of variables in code, and function letters in docs. (Ami Tavory)\n- dacfc4519bd14b3319d5b8bb44e8b3f87c981b92 Remove unused types from CostCalculator (James Sun)\n- 6cc6e5b83b97ba276484667a7db27be9869cec2c Fix the build broken by 9633991927 (Ying Su)\n- 906006b09586aede02b33fe4fbc266a53574f2a2 Add memory tracking from StreamReader local buffers (Ying Su)\n- 7be0c9ff4ea8808f7dbb637f55558fe48835b097 Improve BenchmarkBatchStreamReaders (Ying Su)\n- f92fa2570639816e7be432869a6dde750480a487 Improve ORC LongReader (Ying Su)\n- 61408c230dd96db0d40b9c235e28b5766bdbe1ab Improve ORC timestamp reader (Ying Su)\n- 0814f331e7b7eeacf2cb2bc7c23bb319b1f4377d Improve ORC list and map readers (Ying Su)\n- bf96a44cc82024b979c65df44b3312a47b8aebd5 Improve ORC slice direct reader (Ying Su)\n- 9633991927fa30a15a6fc839671d57b196904364 Improve ORC byte reader (Ying Su)\n- 95a57bd66e50253def785a535ca62dc3c9ff899f Improve ORC boolean reader (Ying Su)\n- d45be17905be81c751b41c81865a2398b8031f29 Remove type from OrcRecordReader.readBlock (Ying Su)\n- 96bdef063a49365b56eebb96fe2ffeb380b7df49 Add support for bucket adaptation (Masha Basmanova)\n- 97bd9458f85ba88c0abbbcfc387e32ca6e621f4b Fix getTableStatistics with subfield pruning (Masha Basmanova)\n- fb2079e7851c989b3636a08f488ae49562537f3a Clarify variable names in getTableStatistics (Masha Basmanova)\n- 9805be6eae1ae81e4f947943b58e4e7f93cd5e06 Refactor FilterStatsCalculatorService (Masha Basmanova)\n- c851382753821ec59c338881c17ec2975cd77f0a Fix subfield pruning for legacy unnest (Masha Basmanova)\n- 53a84b0c0197067a4bdc893e0986d046378b94a0 Add test to verify the size of all-null blocks returned (Bhavani Hari)\n- bd306cb9d7d5afc1ec6399cc36efb74a253743a6 Fix the size of all-null blocks returned from Decimal reader (Bhavani Hari)\n- 7ae9f43fa5a32056b47aa3e9697dcdc783b0d12c Fix the size of all-null blocks returned from MapFlat reader (Bhavani Hari)\n- 5b294b0d5a50c092939a5e62f4e08489c9da9296 Fix the size of all-null blocks returned from Timestamp reader (Bhavani Hari)\n- b835ac11bc88feb8062777a62ca956aec5b25d2f Fix the size of all-null blocks returned from List reader (Bhavani Hari)\n- 1b5d86dcaee5aed849fe7efcf0a108b3f70402fa Fix the size of all-null blocks returned from Slice reader (Bhavani Hari)\n- 874255dfa980cb4374a2ecb3d7372c001101436d Fix the size of all-null blocks returned from Long reader (Bhavani Hari)\n- 6d4250a3a373d06607482c5bb091092cadb702ff Fix the size of all-null blocks returned from Double reader (Bhavani Hari)\n- 68ca8a606f81c43325246c6f109d420262e0fb23 Fix the size of all-null blocks returned from Float reader (Bhavani Hari)\n- 300ba88daa74665cf88d373b0d62354443279b97 Fix the size of all-null blocks returned from Byte reader (Bhavani Hari)\n- 1b6d7f97535bee4dd43006961c06b4608015a644 Fix the size of all-null blocks returned from Boolean reader (Bhavani Hari)\n- 91af80f36d3dbc7331e262134c66a57b41b9bd49 Enable testGroupByKeyPredicatePushdown with filter pushdown (Masha Basmanova)\n- 4e71bf3dc95c67396a345f19195d6e3c716ea528 Optimize byte reader when reading contiguous rows with no nulls and no filter (Ying Su)\n- 4e66289e141ba344dcb9a5315e3dabc04ae819ea Test filter-only columns with coercions (Masha Basmanova)\n- 604b7070f255f757c27a51c731f57f35d6cee373 Allow range filters on columns with coercion (Masha Basmanova)\n- f5e14e9e721eedfb191d83d67f310389ef55f7f6 Static import TupleDomain.withColumnDomains (Masha Basmanova)\n- 7e19193a8439f6e508391c54e76a2e798ad568f6 Add coercing tuple domain filters (Masha Basmanova)\n- 5b874b4589bf80c4e9bac1297a1a4737c6a2eeee Introduce HiveCoercer interface (Masha Basmanova)\n- 9c21aaa659f9d4927fe12bec4b8015c8cbf4722e Move out the entire Hive metastore (James Sun)\n- b7dcdb9cf2f2ddcb3fbf86b88770de6da053353c Add requireNonNull checks to ConnectorManager (Sahar Massachi)\n- 8f1b1ae6c2a0ed8819d2f22f6defbc143be85aa4 Incorporate pushed down filter into stats used by CBO (Sahar Massachi)\n- 272a0e97c72a349e6ebce7c4290f116b116cc502 Rename toSymbolStatistics to toVariableStatsEstimate (Sahar Massachi)\n- 154c4465dc1b48a9abcb47d58241fa9459f4c1ef Extract toSymbolStatistics from TableScanStatsRule (Sahar Massachi)\n- 04019d48a374c2c6f4c6d52aef56ec6af1cb865b Add layout handle parameter to ConnectorMetadata.getTableStatistics (Masha Basmanova)\n- abfef5022e864fa91c54e258e896f1cc74aa4fbc Change DomainTranslator.toPredicate to allow any RowExpression (Sahar Massachi)\n- 282a7f17610c7ce34838228d49fce0e65867da9e Add Subfield to RowExpression converter (Sahar Massachi)\n- 35970d38e0079d015b759a67cffb984ca9e88361 Fix filter pushdown for non-partitioned tables (Masha Basmanova)\n- ed163684080a6fdb6c486f85bd8aebbc107c07de Static import Domain.singleValue (Masha Basmanova)\n- 669f9b5558b03705f2ed2ba40bfc6637cd411bbe Move ExtendedHiveMetastore with its deps to new presto-hive-metastore module (Islam Ismailov)\n- 1a18eb9fca7081dbd23d9aa573d746091c801ea3 Do not prune subfields when filter uses entire column (Masha Basmanova)\n- d2a31579e6f68fc2a23c9c6837f075d6b3d4c424 Add basic support for schema evolution to OrcSelectiveRecordReader (Masha Basmanova)\n- 685475f6678a3ccfdb02aebd0dd5a77d5ef6f3f8 Extract coercers from HivePageSource (Masha Basmanova)\n- 7c61a7ee1c468872e7ed44f63052ca5bf916bc0f Add TestHiveClientInMemoryMetastoreWithFilterPushdown (Masha Basmanova)\n- f180de333967f5d92306874f83628936ca732e1c Refactor getTableLayouts logic in AbstractTestHiveClient (Masha Basmanova)\n- 564d288dc98bc02cc41cc45ccac488c4ce5501cf Change testBucketSortedTables to use ORC format (Masha Basmanova)\n- 630a5924dd607ad142286754d69672904f3f7648 Add TestHiveUtils.getDefaultHiveSelectivePageSourceFactories (Masha Basmanova)\n- fadcc9099d9f94bb8bb811f9593718c176c78361 Rename HiveTestUtils.getDefaultHiveDataStreamFactories (Masha Basmanova)\n- 40f9db0f4762f1d623a6f399ccae0244cdefd40e Fix selective readers to make a copy of input positions (Bhavani Hari)\n- b8acb68200b1cd90d809b63ac40871fed1cc08c6 Reset values after getBlock in LongSelectiveStreamReader (Bhavani Hari)\n- 662a00a1780222fcffa2cc4de033bb2fbb82b506 Decode urlencoded property for PRESTO_EXTRA_CREDENTIAL parsing (Yi He)\n- 323bd92e25bb00f467c915f43c2c1e046179269e Add Hive tests with filter pushdown and subfield pruning (Bhavani Hari)\n- dcc5bb23eba4113b8ab9f3911c06aa7e7845bcfc Output verifier events for queries being skipped by pre-filters (Leiqing Cai)\n- cad9d127fd6ce3d46b9bdee1276cd1a5b31f4858 Revert \"Add Subfield to RowExpression converter\" (Sahar Massachi)\n- eac554cf72e9951170366942100f7347c2bd2fc7 Add Subfield to RowExpression converter (Sahar Massachi)\n- 0637dccfadfe53fe8fdca1d21f629476a5825150 Revert \"Add non-linear transformation for structural hash\" (Wenlei Xie)\n- d9c1e2084c70835d5feecf645ed8836064809fc2 fix organizationDiscoveryIntervalMillis bug (Ke Wang)\n- f601571681050c5405227811bd18cbf798287923 Fix double and float NaN comparisons (Bhavani Hari)\n- 22adf2280ad3c5ad1ca7b297a8fc494ac1567b12 Move TestHiveDistributedQueriesWithOptimizedRepartitioning to separate Travis Job (Bhavani Hari)\n- e4769ad84577cb84355c5f1faae2c495d71d024f Add Hive smoke tests with filter pushdown and subfield pruning (Bhavani Hari)\n- 89a41a9dd87baa3bc163da1bdbd4384a74311ee9 Allow creating flat-map reader for non-supported key types (Bhavani Hari)\n- 1a6548649bb2f7dfaddb8e3d24f06dda56164078 Allow reading non-ORC tables when pushdown filter is enabled (Bhavani Hari)\n- ffed56f03f43591a32a5ea77e4189c6cd8bbd1ae Fix GeometryToBingTiles for certain geometries (James A. Gill)\n- 7018ccda951b050935bd9973d5f01b25f2f9084a Reduce the identifier length in generated code for nested columns (Venki Korukanti)\n- 69c6573ba73afb205c84563ced29e9b590dbd841 Add expand_envelope function. (James A. Gill)\n- 593e79048fbe011ed72f76819b09e1f4e4df5483 Allow empty Envelope serialization (James A. Gill)\n- 60aa23d56f9388afed608363d902e22c05d1b8a6 Verify that outputPositions increase monotonically (Bhavani Hari)\n- 2def2e13878b613523c91e60019b08b25fd7b402 Add JSON annotations to SqlFunctionId (Leiqing Cai)", "NaN"], ["13916", "Add sum of cardinality for array column verifier results", "shenyi", "ShenYi", "01/07/20, 12:18:06 AM", "For array column mismatch, this change adds additional information\r\nof the total number of elements across all rows for the same column.\r\n\r\nPart of #13809\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add checks for the cardinalities sum when validating an array column.", "NaN"], ["13917", "Allow hive outstanding splits size limit larger than 2GB", "James Petty", "pettyjamesm", "01/03/20, 06:32:11 PM", "The `hive.max-outstanding-splits-size` configuration passed into `HiveSplitSource` was previously stored as an int, prohibiting values >= 2GB to be used. Switching the representation to long removes the unnecessary restriction.\r\n\r\nCross port of https://github.com/prestosql/presto/pull/2395\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13919", "Skip printing header in GCStatusMonitor if there are no rows ", "Ajay George", "ajaygeorge", "01/03/20, 10:33:58 AM", "At present, GCStatusMonitor will print header even if there are no\r\nactive tasks. This is unnecessary and pollutes the log file\r\n\r\nfixes https://github.com/prestodb/presto/issues/13918\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13922", "Improve error handling for geometry deserialization edge cases", "James Gill", "jagill", "01/10/20, 06:39:41 PM", "Certain geometries could be constructed (via GeometryFromText), but\r\ncould not be deserialized: it raised non-PrestoException exceptions that\r\nbubbled up as non-catchable GenericInternalExceptions.  This is a bad\r\nuser experience!  This change catches those and raise catchable\r\nPrestoExceptions.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve error handling for geometry deserialization edge cases.\r\n```\r", "NaN"], ["13923", "Move OptimizeMixedDistinctAggregations below Logical Optimization", "Saksham", "sachdevs", "01/09/20, 07:06:48 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nThis has no fb production impact so hard to test for general case.", "NaN"], ["13924", "Optimize ByteSelectiveStreamReader for no filter with nulls", "Ying", "yingsu00", "01/04/20, 12:57:46 PM", "Benchmark shows 25% improvements:\r\n\r\nBefore:\r\nBenchmark                             Mode  Cnt  Score   Error  Units\r\nBenchmarkSelectiveStreamReaders.read  avgt   20  0.055 \u00b1 0.002   s/op\r\n\r\nAfter:\r\nBenchmark                             Mode  Cnt  Score   Error  Units\r\nBenchmarkSelectiveStreamReaders.read  avgt   20  0.041 \u00b1 0.002   s/op\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13927", "Optimize NOT IN filter for strings", "Ke", "kewang1024", "01/14/20, 04:33:43 PM", "== NO RELEASE NOTES ==", "NaN"], ["13930", "Presto on Spark [Part 1, Prerequisites]", "Andrii Rosa", "arhimondr", "01/07/20, 06:20:06 PM", "First part of the main PR: https://github.com/prestodb/presto/pull/13760\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13931", "Configure DB Resource Group reload frequency", "Swapnil", "swapsmagic", "01/14/20, 10:44:56 PM", "Making database resource group reload frequency configurable with name `resource-groups.reload-refresh-interval`. This helps reduce some load on the coordinator at the same time give flexibility on changing the frequency given we don't make resource group changes often.\r\nFixing issue: https://github.com/prestodb/presto/issues/13926\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Making database resource group reload frequency configurable\r\n* ...", "NaN"], ["13934", "Update error code when table is dropped during query execution", "Vic Zhang", "viczhang861", "01/23/20, 08:53:34 PM", "Part of #13935\r\n\r\nFor drop table query, error code is changed from HIVE_METASTORE_ERROR\r\nto HIVE_TABLE_DROPPED_DURING_QUERY when table exists after query\r\nstarts execution but dropped before delete operation commits.\r\n\r\nThis PR fixes below scenario\r\n1. query A \"drop table T\" starts running\r\n2. query B \"drop table T\" starts running and passes analyze check (table exists at this moment, otherwise, query will fail with error message \"Table does not exist\")\r\n3. query A tries to commit and succeed\r\n4. query B tries to commit and fail\r\n\r\nTested by setting breakpoint and running above steps\r\n\r\n```\r\n== RELEASE NOTE ==\r\nHive Changes\r\n* When ``DROP TABLE`` query fails because the table has being dropped by other query before this query finishes,  change error code name from ``HIVE_METASTORE_ERROR`` to  ``HIVE_TABLE_DROPPED_DURING_QUERY``\r\n\r\n```\r", "NaN"], ["13938", "Inject cache manager for Raptor HdfsModule with provider", "James Sun", "highker", "01/08/20, 06:31:50 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13940", "Add t-digest functions to Presto", "Timothy Meehan", "tdcmeehan", "01/22/20, 06:27:17 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add tdigest_agg, merge(tdigest), value_at_quantile(tdigest, quantile), values_at_quantiles(tdigest, quantiles), quantile_at_value(tdigest, quantile), quantiles_at_values(tdigest, quantile) for creating, merging, and querying t-digests\r\n\r\nSPI Changes\r\n* isHidden attribute on AggregationFunction and ScalarFunction removed and replaced with visibility\r\n```\r", "NaN"], ["13945", "Fix corrupted verifier executable jar", "Leiqing Cai", "caithagoras", "01/09/20, 10:31:53 PM", "We're using only the `ErrorCode` classes in some submodules of presto and some dependency changes in those modules caused `maven-shade-plugin` to produce a corrupted executable jars.\r\nIdeally, those type of issues can be avoided by moving to `maven-assembly-plugin`.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13946", "Check retryable errors are all recoginzed", "Leiqing Cai", "caithagoras", "01/11/20, 12:47:01 AM", "When creating any retryable errors are not specified in recognized errors, retry will never take place because no Exception will be classified with that error code. This is error-prone and should be checked against.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13947", "Publish tarball for Verifier", "Leiqing Cai", "caithagoras", "01/10/20, 03:40:34 PM", "`maven-shade-plugin` might produce corrupted jar file, and thus we provide `tar.gz` as an alternative in additional to the executable uber jar.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13949", "Enable listing SQL function with session property", "Leiqing Cai", "caithagoras", "01/14/20, 12:55:29 AM", "Resolves https://github.com/prestodb/presto/issues/13864\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Support hiding user-defined SQL functions in ``SHOW FUNCTIONS`` with session property ``list_built_in_functions_only``.\r\n  This can also be achieved by configuration property ``list-built-in-functions-only``, which is repurposed from ``list-non-built-in-functions``.\r\n```\r", "NaN"], ["13953", "Remove totalPartitions from TaskUpdateRequest", "Andrii Rosa", "arhimondr", "01/10/20, 08:21:42 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13954", "prepare for next development iteration - 0.232-SNAPSHOT", null, "bhhari", "01/13/20, 09:32:38 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13955", "Refactor ReorderJoins to use RowExpressions", "Saksham", "sachdevs", "01/15/20, 01:33:08 AM", "TODO:\r\n- ~Fix failing tests~\r\n- ~Run verifier (explain test to verify no-op on query planner)~\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13956", "Fix Pinot segment page source to use ColumnDataType", "Devesh Agrawal", "agrawaldevesh", "01/16/20, 01:46:31 AM", "== NO RELEASE NOTE ==", "NaN"], ["13957", "Use builder to construct RoutineCharacteristics", "Leiqing Cai", "caithagoras", "01/14/20, 06:42:59 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13961", "Increase heap memory to avoid continuous Full GCs", "Ajay George", "ajaygeorge", "01/15/20, 12:14:51 AM", "`test-hive-pushdown-filter-queries-basic` test needs a larger heap to\r\nsmoothly run to completion. With the current default heap size of 2GB\r\nwe consistently see Full GCs. This change will increase it to 4GB.\r\n\r\nfixes https://github.com/prestodb/presto/issues/13965\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["13963", "Update to Airbase 98 and Aiftlift 0.188", "Leiqing Cai", "caithagoras", "01/14/20, 11:34:22 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13966", "Add ability of soft affinity node selection", "Ke", "kewang1024", "02/14/20, 02:26:16 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n* Introduce `NodeSelectionStrategy` with options `NO_PREFERENCE` to indicate data is remotely accessible from workers, `HARD_AFFINITY` to indicate data and workers are colocated, and `SOFT_AFFINITY` to indicate data is remotely accessible but scheduler will make the best effort to fetch the same piece of data from the same worker.\r\n* Replace `isRemoteAccessible()` in `ConnectorSplit` with `getNodeSelectionStrategy()`. `isRemoteAccessible()` is true if and only if `getNodeSelectionStrategy()` returns `HARD_AFFINITY`.\r\n* Replace `getAddresses()` in `ConnectorSplit` with `getPreferredNodes()`. The returned list of addresses hints the scheduler where to schedule splits.\r\n\r\nGeneral Changes\r\n* Add soft affinity scheduling. It makes the best effort to fetch the same piece of data from the same worker. If the preferred workers are too busy to handle more splits, it will fallback to random workers. The option is enabled by connector indicated by `getNodeSelectionStrategy()`\r\n\r\nHive Changes\r\n* Add config `hive.node-selection-strategy` to choose `NodeSelectionStrategy`. When `SOFT_AFFINITY` is selected, scheduler will make the best effort to request the same worker to fetch the same file.\r\n* Deprecate `hive.force-local-scheduling` config. It will be replaced by setting `hive.node-selection-strategy` to `HARD_AFFINITY`.\r\n```\r\n\r", "NaN"], ["13967", "Properly handle optimizable filter expressions", null, "bhhari", "01/22/20, 01:09:41 PM", "A SelectiveStreamReader should be created only if the column is in output or required in a filter function evaluation. In this case we get the columns to be read from unoptimized expression, but column mappings are created from optimized expression which will ignore this column if the expression resolves to \"null\", which will result in creating a SelectiveReader which is not required and hence throws an exception. The fix is to ignore the interim column if the optimized expression discards it.\r\n\r\nFixes #13992\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13971", "Make presto-spark extensible to additional modules", "Wenlei Xie", "wenleix", "01/16/20, 07:24:43 PM", "Please make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nMinor improvement to https://github.com/prestodb/presto/issues/13856", "NaN"], ["13972", "Support MySQL-based FunctionNamespaceManager", "Leiqing Cai", "caithagoras", "02/08/20, 03:47:47 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add a MySQL-based function namespace manager implementation that supports creating, altering, and dropping SQL functions. (:doc:`/admin/function-namespace-managers`)\r\n```", "NaN"], ["13973", "Allocate CPU quanta per query instead of per task", "Andrii Rosa", "arhimondr", "02/06/20, 05:16:15 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13974", "Extend SHOW FUNCTIONS", "Leiqing Cai", "caithagoras", "01/27/20, 10:43:26 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support to show whether functions have variable arity in ``SHOW FUNCTIONS``.\r\n* Add support to show whether functions are built-in in ``SHOW FUNCTIONS``.\r\n```\r", "NaN"], ["13975", "Fix null pointer when creating ConnectorSession", "Ke", "kewang1024", "01/16/20, 10:53:21 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13977", "Use JTS for ST_IsValid and ST_IsSimple", "James Gill", "jagill", "02/25/20, 06:53:12 PM", "JTS isValid and isSimple semantics are closer to the OGC/ISO standards than is Esri -- in fact, Esri gives the same answer for isValid and isSimple.  This means that geometries will need to be checked for both validity and simplicity, and a small set of geometries might have changed their simplicity/validity property.  In addition, the `geometry_invalid_reason` function will return different (but semantically similar) strings.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ST_IsValid and ST_IsSimple adhere to the ISO/OGC standards more closely.  In particular,\r\npreviously ST_IsValid and ST_IsSimple always gave the same answer; now they may be different.\r\nUsers should check both IsValid and IsSimple to be sure their geometries are well-behaved.\r\n* geometry_invalid_reason will return different but semantically similar strings.  It will return the reason for invalidity if it exists; else it will return the reason for non-simplicity if it exists; else it will return null.\r\n```\r", "NaN"], ["13979", "Improve error message for the array_agg function", null, "sujay-jain", "01/27/20, 06:27:03 PM", "\r\n\r\nWhen a single row in ```ArrayAggregationFunction``` exceeds the ```MAX_ARRAY_SIZE``` (aka ```INT_MAX - 8```) we return an internal error to the customer from the Airlift precondition checks. This PR changes the behavior to return a ```USER_ERROR``` when an array's retained size crosses a large enough size (~1G) instead by actively checking for the size of the block being appended to.\r\n\r\nCurrently the limit check comes from here: https://github.com/airlift/slice/blob/master/src/main/java/io/airlift/slice/Slices.java#L40\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13982", "Cache builtin function resolution result", "Rongrong Zhong", "rongrong", "01/21/20, 08:18:54 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve built-in function resolution performance by caching function resolution results.\r\n```", "NaN"], ["13983", "Allow FunctionNamespaceManager in DistributedQueryRunner", "Rongrong Zhong", "rongrong", "01/21/20, 08:19:42 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13984", "Avoid Java stream in NodePartitioningManager#createArbitraryBucketToNode", "Wenlei Xie", "wenleix", "01/18/20, 11:55:41 PM", "We recently observed two issues in production with\r\nNodePartitioningManager#createArbitraryBucketToNode:\r\n\r\n1. Under certain workloads, the Java Stream API shows non-trivial memory\r\nallocation overhead:\r\n```\r\n--- Execution profile ---\r\nTotal samples:         3308476\r\nNon-Java:              18 (0.00%)\r\nSkipped:               1990244 (60.16%)\r\n\r\nFrame buffer usage:    37.1651%\r\n\r\n--- 5030131592 bytes (5.87%), 438 samples\r\n  [ 0] java.util.stream.ReferencePipeline$Head\r\n  [ 1] java.util.stream.StreamSupport.stream\r\n  [ 2] java.util.Collection.stream\r\n  [ 3] com.facebook.presto.sql.planner.NodePartitioningManager$$Lambda$3966.1318525599.apply\r\n  [ 4] java.util.stream.ReferencePipeline$7$1.accept\r\n  [ 5] java.util.stream.StreamSpliterators$InfiniteSupplyingSpliterator$OfRef.tryAdvance\r\n  [ 6] java.util.stream.ReferencePipeline.forEachWithCancel\r\n  [ 7] java.util.stream.AbstractPipeline.copyIntoWithCancel\r\n  [ 8] java.util.stream.AbstractPipeline.copyInto\r\n  [ 9] java.util.stream.AbstractPipeline.wrapAndCopyInto\r\n  [10] java.util.stream.ReduceOps$ReduceOp.evaluateSequential\r\n  [11] java.util.stream.AbstractPipeline.evaluate\r\n  [12] java.util.stream.ReferencePipeline.collect\r\n  [13] com.facebook.presto.sql.planner.NodePartitioningManager.createArbitraryBucketToNode\r\n  [14] com.facebook.presto.sql.planner.NodePartitioningManager.getNodePartitioningMap\r\n  [15] com.facebook.presto.execution.scheduler.SqlQueryScheduler.lambda$null$7\r\n  [16] com.facebook.presto.execution.scheduler.SqlQueryScheduler$$Lambda$3930.304871149.apply\r\n  [17] java.util.HashMap.computeIfAbsent\r\n  [18] com.facebook.presto.execution.scheduler.SqlQueryScheduler.lambda$createStageExecutions$8\r\n  [19] com.facebook.presto.execution.scheduler.SqlQueryScheduler$$Lambda$3921.1181870979.apply\r\n  [20] com.facebook.presto.execution.scheduler.SqlQueryScheduler.getBucketToPartition\r\n  [21] com.facebook.presto.execution.scheduler.SqlQueryScheduler.createStreamingLinkedStageExecutions\r\n  [22] com.facebook.presto.execution.scheduler.SqlQueryScheduler.createStreamingLinkedStageExecutions\r\n  [23] com.facebook.presto.execution.scheduler.SqlQueryScheduler.createStageExecutions\r\n  [24] com.facebook.presto.execution.scheduler.SqlQueryScheduler.<init>\r\n  [25] com.facebook.presto.execution.scheduler.SqlQueryScheduler.createSqlQueryScheduler\r\n  [26] com.facebook.presto.execution.SqlQueryExecution.planDistribution\r\n  [27] com.facebook.presto.execution.SqlQueryExecution.startExecution\r\n  [28] com.facebook.presto.execution.SqlQueryExecution$$Lambda$3179.1952738377.run\r\n  [29] java.util.concurrent.Executors$RunnableAdapter.call\r\n  [30] java.util.concurrent.FutureTask.run\r\n  [31] java.util.concurrent.ThreadPoolExecutor.runWorker\r\n  [32] java.util.concurrent.ThreadPoolExecutor$Worker.run\r\n  [33] java.lang.Thread.run\r\n```\r\n\r\n2. In some rare case on a long running coordinator (e.g. >10 days), we\r\nsee thread may stuck in the stream API call:\r\n```\r\n   java.lang.Thread.State: RUNNABLE\r\n        at java.util.stream.StreamOpFlag.fromCharacteristics(java.base@10/StreamOpFlag.java:733)\r\n        at java.util.stream.StreamSupport.stream(java.base@10/StreamSupport.java:70)\r\n        at java.util.Collection.stream(java.base@10/Collection.java:659)\r\n        at com.facebook.presto.sql.planner.NodePartitioningManager$$Lambda$3966/1318525599.apply(Unknown Source)\r\n        at java.util.stream.ReferencePipeline$7$1.accept(java.base@10/ReferencePipeline.java:271)\r\n        at java.util.stream.StreamSpliterators$InfiniteSupplyingSpliterator$OfRef.tryAdvance(java.base@10/StreamSpliterators.java:1360)\r\n        at java.util.stream.ReferencePipeline.forEachWithCancel(java.base@10/ReferencePipeline.java:127)\r\n        at java.util.stream.AbstractPipeline.copyIntoWithCancel(java.base@10/AbstractPipeline.java:502)\r\n        at java.util.stream.AbstractPipeline.copyInto(java.base@10/AbstractPipeline.java:488)\r\n        at java.util.stream.AbstractPipeline.wrapAndCopyInto(java.base@10/AbstractPipeline.java:474)\r\n        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(java.base@10/ReduceOps.java:913)\r\n        at java.util.stream.AbstractPipeline.evaluate(java.base@10/AbstractPipeline.java:234)\r\n        at java.util.stream.ReferencePipeline.collect(java.base@10/ReferencePipeline.java:578)\r\n        at com.facebook.presto.sql.planner.NodePartitioningManager.createArbitraryBucketToNode(NodePartitioningManager.java:223)\r\n        at com.facebook.presto.sql.planner.NodePartitioningManager.getNodePartitioningMap(NodePartitioningManager.java:128)\r\n        at com.facebook.presto.execution.scheduler.SqlQueryScheduler.lambda$null$7(SqlQueryScheduler.java:355)\r\n        at com.facebook.presto.execution.scheduler.SqlQueryScheduler$$Lambda$3930/304871149.apply(Unknown Source)\r\n        at java.util.HashMap.computeIfAbsent(java.base@10/HashMap.java:1138)\r\n        at com.facebook.presto.execution.scheduler.SqlQueryScheduler.lambda$createStageExecutions$8(SqlQueryScheduler.java:355)\r\n        at com.facebook.presto.execution.scheduler.SqlQueryScheduler$$Lambda$3921/1181870979.apply(Unknown Source)\r\n        at com.facebook.presto.execution.scheduler.SqlQueryScheduler.getBucketToPartition(SqlQueryScheduler.java:618)\r\n```\r\n\r\nThe stream API is introduced for this method in\r\n0195ffd4368b1595952d21477cf83df695719255, when migrating from map to\r\nlist for bucket-to-node. \r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13986", "Add Alluxio Hive metastore", "Haoyuan Li", "haoyuan", "01/24/20, 07:11:50 AM", "```\r\nCo-authored-by: David Zhu <david@alluxio.com>\r\nCo-authored-by: Zac Blanco <zac@alluxio.com>\r\nCo-authored-by: Calvin Jia <calvin@alluxio.com>\r\nCo-authored-by: Bin Fan <binfan@alluxio.com>\r\n\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\nThis change introduces the Alluxio metastore which connects to the [Alluxio catalog service](https://docs.alluxio.io/os/user/2.1/en/core-services/Catalog.html).\r\n```\r", "NaN"], ["13987", "Promote Elasticsearch scan performance with index sorting", "Yunfeng,Wu", "wuyunfeng", "01/22/20, 08:44:45 PM", ">  Scroll requests have optimizations that make them faster when the sort order is _doc. If you want to iterate over all documents regardless of the order, this is the most efficient option\r\n\r\n [https://www.elastic.co/guide/en/elasticsearch/reference/7.5/search-request-body.html#request-body-search-scroll](scroll request)\r\n\r\nWhen Elasticsearch process the scroll request with `_doc` sort,  it will use `MinDocQuery` combined with the user's original query , which show as below:\r\n\r\nhttps://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/search/query/QueryPhase.java#L175-L182\r\n\r\nThis `scan-scroll` request we also used on `Apache - incubator-doris`:\r\n\r\nhttps://github.com/apache/incubator-doris/blob/master/be/src/exec/es/es_scroll_query.cpp#L118-L120\r\n\r\nThis PR would promote about 30%+  scan-filter  performance for presto-elsticsearch-connector \r\n\r", "NaN"], ["13995", "Evaluate filter only once per dictionary value when reading integers", "Maria Basmanova", "mbasmanova", "01/23/20, 02:40:07 AM", "Fixes #13993\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["13996", "Fix flaky TestRowExpressionPredicateExtractor", "Ajay George", "ajaygeorge", "01/22/20, 09:37:39 PM", "```\n== NO RELEASE NOTE ==\n```", "NaN"], ["13997", "Add release notes for 0.231", "Leiqing Cai", "caithagoras", "02/03/20, 09:57:18 PM", "# Missing Release Notes\n## Ariel Weisberg\n- [x] https://github.com/prestodb/presto/pull/13155 Cherry-pick documentation for cost, statistics, CBO (Merged by: Rebecca Schlussel)\n\n## Ke Wang\n- [x] https://github.com/prestodb/presto/pull/13849 Raptor background jobs (Merged by: Jiexi Lin)\n\n## Rohit Jain\n- [x] https://github.com/prestodb/presto/pull/13905 Add max buffer size to Raptor session properties (Merged by: James Sun)\n\n## Saksham Sachdev\n- [x] https://github.com/prestodb/presto/pull/13815 Revert \"Consuming max buffer size from the session properties \" (Merged by: Shixuan Fan)\n\n## ptkool\n- [x] https://github.com/prestodb/presto/pull/13668 Fix IS DISTINCT FROM for decimals with precision > 18 (Merged by: Maria Basmanova)\n- [x] https://github.com/prestodb/presto/pull/10568 Add IGNORE NULLS clause to various Window functions (Merged by: Rongrong Zhong)\n\n# Extracted Release Notes\n- #11262 (Author: Jiexi Lin): Merge partition preference when adding exchange nodes\n  - Add new session property \"aggregation_partitioning_merging_strategy\" to control merging partitioning preference when adding repartition remote exchange around aggregation node. Default option is `LEGACY` and can be overwritten to `TOP_DOWN` or `BOTTOM_UP`.\n- #13604 (Author: James A. Gill): Use JTS instead of ESRI for geometries, Parts 1 and 2\n  - Use JTS instead of Esri for many geometrical operations. + Polygon WKTs must have closed loops.  Previously Esri would close the loops for you. + Certain other invalid geometries will fail to be created from WKTs, such as `LINESTRING(0 0, 0 0, 0 0)`. + Returned WKTs may have a different point order. + Fixes incorrect calculation of extreme points in certain cases.\n- #13729 (Author: Anoop Johnson): Implement Parallel Partition Pruning for Glue Hive Metastore\n  - Add support for parallel partition fetching for the Glue metastore.\n- #13756 (Author: Ke Wang): Raptor read and write with delta delete functionality\n  - Add the ability to read and write with delta deletes.\n- #13758 (Author: Leiqing Cai): Verifier: Improve determinism analysis and checksum query recording\n  - Fix an issue where checksum query text and ID are not recorded if the checksum query fails.\n  - Add new columns ``control_session_properties`` and `test_session_properties` to ``verifier_queries``, and remove column ``session_properties_json``. The value of the removed column can be copied to the two new columns for the schema change.\n  - Add details of determinism analysis runs to the output.\n  - Add configuration property ``max-determinism-analysis-runs`` to control maximum number of determinism analysis runs in case of column mismatch.\n  - Add configuration property ``run-teardown-for-determinism-analysis`` to allow disabling teardown for determinism analysis runs.\n- #13796 (Author: Rongrong Zhong): Make SQL function work for input parameters with lambda\n  - Fix SQL function compilation error when input parameters contain lambda.\n- #13820 (Author: Ariel Weisberg): Sort OOM killer log output\n  - OOM killer log output sorted to put memory heavy nodes and queries first.\n- #13824 (Author: James A. Gill): Use JTS for ST_Buffer\n  - Use more efficient implementation for ST_Buffer.  This produces fewer buffer points on rounded corners, which will produce very similar but different results.  JTS also better handles buffering with small (<1e-9) distances.\n- #13842 (Author: Leiqing Cai): Fix function namespace manager instantiation failure\n  - Fix an issue where server fails to start when two function namespace managers of the same type are specified.\n- #13853 (Author: Leiqing Cai): Skip verification when checksum query fails to compile\n  - Improve Verifier to skip verification when checksum query fails to compile.\n- #13857 (Author: Ariel Weisberg): Fix schema mismatch w/Parquet INT64 & Timestamp\n  - Fix schema mismatch w/Parquet INT64 & Timestamp.\n- #13858 (Author: Anoop Johnson): Get PrestoS3FileSystem to work with the AWS Default Credentials Provider\n  - Get PrestoS3FileSystem to work with the AWS Default Credentials Provider.\n- #13871 (Author: James A. Gill): Add approx_percentile forms with accuracy\n  - Add forms of approx_percentile accepting an accuracy parameter.\n- #13873 (Author: James Petty): Fix raw input stats for ScanFilterAndProject\n  - Fix ScanFilterAndProjectOperator raw input bytes accounting for LazyBlocks.\n- #13874 (Author: Wenlei Xie): Optimize array_join by supporting PROVIDED_BLOCKBUILDER convention\n  - Optimizer performance for array_join.\n- #13878 (Author: Yi He): Add query integrity check in AccessControlManager\n  - Add API to check if the query is unexpectedly modified using the credentials passed in the identity.\n- #13879 (Author: Islam Ismailov): Keep sum stat for string when min/max stat is too long in ORC writer\n  - Output (sum) string stat even if min/max values are too long. This is needed for the read-path to be able to better estimate the size of row.\n- #13898 (Author: Ajay George): Add support for get partition names by filter\n  - Add support for get partition names by filter.\n- #13901 (Author: Rebecca Schlussel): Fix invalid plan for repeated lambdas in order by\n  - Fix failures caused by invalid plans for queries with repeated lambda expressions in the order by clause.\n- #13904 (Author: Rohit Jain): Add caching file system to hive connector\n  - Allow reading data from HDFS while caching the fetched data on local disks. Turn on the feature by specifying the cache directory config `cache.base-directory`.\n- #13916 (Author: Yi Shen): Add sum of cardinality for array column verifier results\n  - Add checks for the cardinalities sum when validating an array column.\n- #13922 (Author: James A. Gill): Improve error handling for geometry deserialization edge cases\n  - Improve error handling for geometry deserialization edge cases.\n\n# All Commits\n- 9008f5af5b219410e8f5baf08de3ff28dfaf9570 Extract Verifier intergation test into abstract test (Leiqing Cai)\n- 9a9ed949fe8e8b1865067087a587b598ed39f309 Check retryable errors are all recoginzed (Leiqing Cai)\n- ca6a2d30da7650c9bfb927a37419de5d61d9c0ff Allow more flexible construction of PrestoExceptionClassifier (Leiqing Cai)\n- a5bb640346201a27d86ace9568fcbc5753c63a74 Add support for get partition names by filter (Ajay George)\n- d22cd7dc5601f389d88bfe2fc1744ec2f7b4e210 Remove totalPartitions from TaskUpdateRequest (Andrii Rosa)\n- 2b7926f6613e221ad3b8906e986dd8554bd97692 Add approx_percentile forms with accuracy (James A. Gill)\n- 262b74ff7b38320e246d2f928927144359ccae19 Refactor approx_percentile functions (James A. Gill)\n- fbb9e987f12a2d3b6ff2c0d041dbd6cf2f6fcca9 Capture geometry deserialization failures (James A. Gill)\n- 910374b4975e62dee41b0d727ba15a10983c9ef2 Use helper method jtsGeometryFromWkt (James A. Gill)\n- 82c9ea00140b38d8e688b73e33cc85f0daefa2f1 Publish tarball for Verifier (Leiqing Cai)\n- 628afede26a354bb5b7bde970d61203f40a97c79 Fix corrupted verifier executable jar (Leiqing Cai)\n- fd5954701491fca86fee90722f62050d2d06bb45 Fix invalid plan for repeated lambdas in order by (Rebecca Schlussel)\n- f94809283ac923eeeb0152f27f64039646d1c49f Move OptimizeMixedDistinctAggregations below Logical Optimization (Saksham Sachdev)\n- 3f8d9dadfb37df0c3da792f9dc851314b99c6ffe Inject cache manager for Raptor HdfsModule with provider (James Sun)\n- 2762513b0d255184b5d6eda1b4c1db48e8f996cc Add caching file system to hive connector (Rohit Jain)\n- 5e574285b11ebf621e1d3e6114336aaeb384338a Add high level CBO documentation (Ariel Weisberg)\n- 73708387222da70c23c1301cdf6873bead405df5 Add documentation for cost and statistics (Lukasz Osipiuk)\n- b21915299b609f673680275204a97e0b15812ef1 Increase visibility of the PrestoSystemRequirements (Andrii Rosa)\n- 48fe432a86f13380899858a8ae8fbb9ec343dbfd Introduce RemoteSourceFactory abstraction (Andrii Rosa)\n- 8a11650a586974374f97ba582388cc3e4affd3a7 Expose connector property for TPCH partitioning (Andrii Rosa)\n- 0f14563c3cc7d067aea366974d1441c267aa7412 Make require Hadoop native configurable (Andrii Rosa)\n- 072f6456ba788379218b2ce3c28b30da616ad91c Refactor TableScanOperator stats collection (James Petty)\n- 0191168f2af77d4051342af3ecf7d8d3a4ce282e Fix raw input stats for ScanFilterAndProject (James Petty)\n- 176e8f8a4db4ce50e02c0fb11da28702e52aeb89 Add checks for the cardinalities sum when validating an array column (Yi Shen)\n- ad998854aff9e021083e244477dbf5f5fcd7a31f Repurpose OrderableArrayColumnValidator to support all array types (Yi Shen)\n- 659c3ee91bbddfb0f61cce4c7bc544c4010f3633 Optimize ByteSelectiveStreamReader for no filter with nulls (Ying Su)\n- bdea351ade9e88a83122733cfe5727f3f3634ef8 Allow hive outstanding splits size limit larger than 2GB (James Petty)\n- 58247b0329002d8e570b02a3cb3e2db1400b977f Skip logging of GCStatusMonitor state if there is no activity (Ajay George)\n- 3a4f2e3e53e741b82297c8f2416c78047f2aa308 Optimize boolean reader when reading contiguous rows with no nulls and no filter (Eugene Kalenkovich)\n- 01852884c5cf834ec02cf5b17fb2fa1182a6e374 Clean up arrays in ORC dictionary after close (James Sun)\n- b2a5e7031212d876518b214617f5e7ff75e5d7d6 Make BenchmarkSelectiveStreamReaders load LazyBlock (Eugene Kalenkovich)\n- 9016a06b51db3d331f48eafe07831d35af393265 This patch updates the description for redis.key-prefix-schema-table to (Joseph Gmitter)\n- 1a3d5ffd845d65f2dc729479a828b07947df37fc Fix IS DISTINCT FROM for long decimals (ptkool)\n- 49ea39b6a98c88b26ff7d68d2c1b4d8007c41080 Add session property to disable adaptive filter reordering (Masha Basmanova)\n- eb207f81fee9f400f7fb6ff0ce8d497d3979160f Cache results of TupleDomainFilterUtils::toFilter (Masha Basmanova)\n- 7360bce7c16d690e21aeacd3caab60c042269f14 Add max buffer size to Raptor session properties (Rohit Jain)\n- 84d9f0a6f1c6b83b5ec9b672251850991264480c Eliminate duplicate table name to avoid conflict in TestRaptorIntegrationSmokeTest (Ke Wang)\n- 241a3f5e020b86e7cfe1088d9162504f2396f349 Keep sum stat for string when min/max stat is too long in ORC writer (Islam Ismailov)\n- 25e1d79d32ae5ae2a115c5ef98057c379d9b7ba0 Add query integrity check in AccessControlManager (Yi He)\n- c8fc21e67bf11f17fd4707654f2c09ac138a9cdc Integrate PrestoS3FileSystem with AWS Default Credentials Provider (Anoop Johnson)\n- f4f888217c887845222e17648fa924678958ce87 Cache optimized RowExpression predicate (Bhavani Hari)\n- 5177bea5654f55ed3b38ac67cb9597e2c5ec5052 Fix requiredField logic in StructSelectiveStreamReader (Bhavani Hari)\n- 5f781d32e1fd1b0057b12fac033975b931a9c21d Add delta in reassign process (Ke Wang)\n- a60985c7cb9c127eb8731ce231c4b62913cd8711 Add priority to Compaction process (Ke Wang)\n- 618b0afe9276e4d6f6ccb052c36db7616f79149b Enable delta compaction and organization (Ke Wang)\n- 145a652917870c84f48b0f24a4402725b24b3e48 Handle failed queries and execute remaining (Nikhil Collooru)\n- 522c91248c7c4f6af5e6b648498df5b618304b18 Create benchmark runner entry point (Nikhil Collooru)\n- 094536133461513a0a6428055887cb519d02310b Add benchmark runner logic (Nikhil Collooru)\n- d58d6877326b61943c26c35efb89610f68536b11 Export results using event clients (Nikhil Collooru)\n- 96d1a0420c46ed6a2442a3598dad5e7c9599e9c1 Support execution of concurrent phases (Nikhil Collooru)\n- eacf13484139a85c53901f2045578c659a65a5b2 Provide retry utilities for benchmark runner (Nikhil Collooru)\n- 2c4b93bee679fc6df38ff68611e13bd716d5bd50 Support querying Presto in benchmark runner (Nikhil Collooru)\n- fdb76117b90544fcaed2d10f8b398bdfdcf333be Optimize array_join by supporting PROVIDED_BLOCKBUILDER convention (Wenlei Xie)\n- ce688136d9a45af6aea8853b0da159198eb3c512 Move ExtractSpatialJoins to after LogicalOptimization (Saksham Sachdev)\n- 82e90bf8ca557e864db4670c1c5349497138e252 Control merging partitioning in aggregation node via session property (Jiexi Lin)\n- 43e8c02eacbe80c29be397f90d15062254cf0839 Force repartition when aggregation node has mixed grouping sets (Jiexi Lin)\n- 8abba9e905c0a8b7225caea6746596507fb1fed6 Use JTS for ST_Buffer (James A. Gill)\n- 584a92f362e4ad9546e0dbf7171ab8f6361ab1bd Add Benchmarks for ST_Buffer (James A. Gill)\n- 7db5fbf09d5899a2fd837f39b72c565461acaa2e Add Geometry construction methods to GeometryBenchmarkUtils (James A. Gill)\n- 7a38da852c2bdb43de3320683dab44d655ec17f8 Fix double and float comparisons for NaNs (Bhavani Hari)\n- 78410289d39d36d07351dad918bd2e9bfbf43994 Refactor OrcPageSource (Ke Wang)\n- c533c6f2d1d21cd14c117f7c1db57c339a80f2a7 Export statistics of delta delete (Ke Wang)\n- e2af4369cfca4040d08079df2a0114823aa24b7f Commit protocol for rewrite delete, delta delete (Ke Wang)\n- 2a97a86f2e46732be7ddb86fb38201504ccf3c89 Add delta delete functionality on worker (Ke Wang)\n- 3749dad43496740b4baeac1520ebac10ed71e85c Enhance RaptorSplit and enable delta read (Ke Wang)\n- e01dda24e674dca2fad109d41c337995a34cc7c2 Increase timeout in TestBackupManager#testCorruption (Leiqing Cai)\n- 58b376713a6eaa49e3b1ffaf1ae441fe37d2609a Remove unused variable from BigintValuesUsingHashTable (Masha Basmanova)\n- c7d8f5534e03271127ffd88f7c0419834b6ccc55 Use bitmask for IN filter when range of values is small (Masha Basmanova)\n- 49a9cd9c5fbfc547e65d9d1c75ceac80475e6827 Rename BigintValues to BigintValuesUsingHashTable (Masha Basmanova)\n- bdd91af8cfac6a2d4a560f4dc92218881738f7a3 Optimize and refactor array_distinct, array_filter and array_sort (Sreeni Viswanadha)\n- a123e5c2a54bfdd7b2ee8860ae62671c146bbdd4 Enable subfield pruning to pass through arbitrary() function (Gautam Parai)\n- 62c4524818a33c4d18472fa25c0be1e28781c647 Modify table definition for new testcases (Gautam Parai)\n- cb4ffb3f4c20260553cb4e9952b83ee8858c2138 Disable TestDistributedSpilledQueries#testLimitWithJoin (Leiqing Cai)\n- 55458740ea5552f70fe637de23a6cb08e5814da3 Skip verification when checksum query fail to compile (Leiqing Cai)\n- bb17af3c68b0f45887f4cf97660e1ec7e892e322 Fix schema mismatch w/Parquet INT64 & Timestamp (Ariel Weisberg)\n- 53bb79fcdb6b901f8c70b579c8287ce94f3f2b74 Fix SQL formatting for CREATE FUNCTION (Leiqing Cai)\n- ab80d8d455e7751fb72a7707fd907f362fb0fde5 Fix function namespace manager instantiation failure (Leiqing Cai)\n- c270e801662b2ab87169ea10c209a5e77bea7c62 Sort OOM killer log output (Ariel Weisberg)\n- 762b183c7c0d2599fd882f6e6fb71cb228e90772 Fix selective stream readers (Masha Basmanova)\n- 469637d75bab64782455d13b3cfe4452cee748f8 Set boolean value to 0 if the position is null (Andrii Rosa)\n- 8c2dac4a038fba75a1857d7715c0436624712098 Check aggregation mask values for null (Andrii Rosa)\n- 378e0631a28fbd826aab2713cf9a7eec5b05f734 Log queries and tasks memory reservation on full GC (Kelvin Fann)\n- 77eb6e00e2120e97d4d70166419e10a5cefb5763 Create zero row files without overwrite flag (Andrii Rosa)\n- efb0c6bac823fd1752f68e18be7a832fee3222e3 Rename PrestoResourceClient to NodeResourceClient (Nikhil Collooru)\n- 692eb2d659b0ecc0ad66c65c7f5e9a936cbe5af7 Fix deserailization error in verifier (Nikhil Collooru)\n- 95eb8a443eda4dd6c8463e221e35430aaabad959 Remove Unused Argument in ParquetPageSource (James Petty)\n- 85fa3e7df41d7acb6c0a7d304eb58441d19449e7 Remove Redundant Parquet Column Index Lookups (James Petty)\n- b6aa6e954680981651a2beb1239d017b56aa5261 Populate basic query info for skipped events in Verifier (Leiqing Cai)\n- 95a6f6df1b7338b6318e11fde5713c80ca1bd479 Optimize zero row files creation for bucketed tables (Andrii Rosa)\n- 0ade47431491c1fd272e4782d6af50cbfaf10104 Optimize loop over filter functions in applyFilterFunctions (Masha Basmanova)\n- e91595d63fb8372f40b0415eab655b9c273e25f4 Reorder filters at runtime (Masha Basmanova)\n- 789364e2d3ce8deb541a857ad4de250037d0e643 Track filter function performance (Masha Basmanova)\n- 48e622014300b9c489081036dbc183eb9965c8f7 Add toString method to TIMESTAMP reader (Masha Basmanova)\n- ac93bd66a88c703dababa68b444cb0d0efb99e7e Evaluate filter function as soon as all inputs have been read (Masha Basmanova)\n- 5589d3dfc420d16591e4fdbd6fd704a81d152492 Read columns of primitive types first (Masha Basmanova)\n- 3938c792e274b543862ec0bc72933a2c56d0d2fa Use a constant for empty page (Masha Basmanova)\n- b526246cdb9d3f38e17d709abf1c4f7a9bd42b52 Fix BooleanInputStream#getSetBits (Masha Basmanova)\n- 98b8c8d7ffdeea319b8e7bdd0577f2e9546868c1 Minor refactor to TableFinishOperator (Wenlei Xie)\n- 46ae1085deb68dad344da9ae4160dbed8749da6d Rename GeometrySerde to EsriGeometrySerde (James A. Gill)\n- ab3297768fdd90ff9a6a4804c23c850e09e48d10 Convert GeoFunctions to JTS: Part 2 (James A. Gill)\n- a5eec6ca631244f9ee397a8a1fb03450a5713536 Convert WKT functions to JTS (James A. Gill)\n- f66cb96ccd155bdfab6d4cf97f488aa54291dea3 Upgrade JTS to 1.16.1 (James Gill)\n- 951eb8df59a3a954b62d00580b57d4939c293535 Reduce Memory used by Finished AsyncQueue Instances (James Petty)\n- 0f7061f9b152998479040c24a0b9740ad7aa2579 Enable bucket pruning for IN predicates (Gautam Parai)\n- 40d35ee4002228084476deddfd78f61ea9578b53 Display bucket pruning info in hive table layout (Gautam Parai)\n- e94f2c19210c056c4613c9fbbbe231914ace42fd Revert \"Consuming max buffer size from the session properties instead of configs in the Raptor page sink provider.\" (Saksham)\n- 1d2be6c855fb62587f0d2943a4371fa01b86c307 Consuming max buffer size from the session properties instead of configs in the Raptor page sink provider. (Rohit Jain)\n- b9780259ee705022cd6bfe9f28de355721963624 Implement Parallel Partition Pruning for Glue Hive Metastore (Anoop Johnson)\n- 6436aa6ca8663cd31e2b1f7ffcd6670ecacef02a Fail creation of table with unsupported bucket count (Vic Zhang)\n- f582c6ff85cb652d62f652e584b1067d66a3fe84 Add a method to compare SqlInvokedFunction (Leiqing Cai)\n- 22f70d4bea0eb842146ee57f53049edc3eedb7af Fix pushdown for filters on multiple subfields of a struct (Masha Basmanova)\n- 1d323240ef5628fb5fc05ebb866bc47bf218e888 Make SQL function work for input parameters with lambda (Rongrong Zhong)\n- 91f3ce774f5ef41b7508dbab8c205de5b79485f8 Make TestUpgradeMetadata single threaded (Leiqing Cai)\n- d838c9baa7e0922d9195abeb93657d5002a51221 Add an option to disable teardown for determinism analysis runs (Leiqing Cai)\n- 2d5ded921fd550feac4176679a1c453cd1a9dbdf Fix variable name in VerifierConfig (Leiqing Cai)\n- 5de945e7e894942e1ad244dd857e6c06bf4f2048 Make determinism analysis run count configurable (Leiqing Cai)\n- bba5aa272fa4397f54d0fbc4689ffd585f565ecf Export determinism analysis details (Leiqing Cai)\n- 7cedd3762cb43483465d3a2a8c41d332ccca55b2 Export checksum query and query id even if the query fails (Leiqing Cai)\n- 2d3a241ea9425b4de5f93ebf7966a63956e96918 Make tableName required in QueryBundle (Leiqing Cai)\n- ed547be3a602b70546b2e173e3c779f3b45e8f0a Move LimitQueryDeterminismAnalyzer.Analysis to top level (Leiqing Cai)\n- e91519a384d18094ce19fee44c8bbfe84c6cfcf1 Use different columns for control and test session properties (Leiqing Cai)\n- f074d639ba4e8186accfada14334051f28baf229 Move verifier_queries table creation SQL to VerifierDao (Leiqing Cai)\n- bc565ebf60f0fc2ee5974df8b84d465d0d4f9ca6 Fix flaky test TestRetryDriver (Leiqing Cai)\n- 70ab7acba57703e7a73019823d68b3240ed7b0d6 Check column exist before alter table for raptor delta delete (Ke Wang)\n- 36e4e8677a073dbdb51bcf10b290e54369d4a56f Update hive.rst with Alluxio docs (Haoyuan Li)\n- 7efeecc58fd612f3e8d1bf00f418dccb84d4d0ae Add support for null treatment clause to various window functions (ptkool)\n- 350f1cde19ba53d2a884642003c8e2aa8377aa8e Create NestedField using lowercase field name (Masha Basmanova)\n- e9c57c81439ebb581c7c942da066934fe1ae6546 Make subfield name matching case insensitive (Masha Basmanova)", "NaN"], ["13998", "Improve correctness check for queries that produce columns of RowType", "Igor Kruk", "kvaturka", "01/30/20, 11:51:37 PM", "Improve struct correctness check output by adding checksum per struct field.\r\n\r\nPart of #13809 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add checks for the individual fields when validating a row column.\r\n\r\n```\r", "NaN"], ["14005", "Move PushdownSubfields below logical optimization", "Saksham", "sachdevs", "01/28/20, 08:24:55 PM", "Ready for review. Looking for ideas on how to test this.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14006", "Support mixed case fields in Elasticsearch", "Zhenxiao Luo", "zhenxiao", "01/25/20, 06:20:49 AM", "Preserve the original (mixed-case) column name for further requests\r\nto Elasticsearch instead of relying on the name from ColumnMetadata,\r\nwhich is lower-cased.\r\n\r\nCherry-pick of https://github.com/prestosql/presto/commit/4217df64c4e1828d90ead0510b4ae70a9c651539\r\n\r\nCo-authored-by: Martin Traverso <mtraverso@gmail.com>\r\n\r\n\r\n== RELEASE NOTES ==\r\n\r\nElasticsearch Changes\r\n* Add support for mixed-case column name for Elasticsearch Connector\r", "NaN"], ["14007", "Support kerberos authentication for Kudu connector", "Yubin Li", "liyubin117", "01/29/20, 05:35:22 PM", "Add the following configuration to etc/catalog/kudu.properties to enable kerberos authentication:\r\nkudu.kerberos-auth.enabled=true\r\nkudu.kerberos-auth.debug.enabled=true\r\nkudu.kerberos-auth.principal=xxx\r\nkudu.kerberos-auth.keytab=xxx.keytab\r\n\r\n== RELEASE NOTES ==\r\n\r\nKudu Changes\r\n\r\nsupport kerberos authentication", "NaN"], ["14008", "Fix EmptyTableLayout predicate to be none", null, "bhhari", "01/24/20, 05:56:57 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14009", "Fix compile error for java 9&11", "Beinan", "beinan", "08/02/20, 02:23:46 PM", "1. Fix  javax.annotations.PreDestroy class not found when compiling with java 11.\r\nThis class is required by presto-cache/src/main/java/com/facebook/presto/cache/LocalRangeCacheManager.java\r\n\r\nBecause javax.annotation had been removed in jdk 11,  we should add this dependency explicitly in the pom.xml of presto-cache\r\n\r\n2. Fix incompatible type error on java 9 & 11\r\n```\r\n[ERROR] COMPILATION ERROR :\r\n[INFO] -------------------------------------------------------------\r\n[ERROR] /Users/beinanw/w/presto/presto-main/src/main/java/com/facebook/presto/transaction/InMemoryTransactionManager.java:[587,46] incompatible types: inference variable T has incompatible bounds\r\n    lower bounds: com.google.common.util.concurrent.ListenableFuture<? extends capture#1 of ? extends java.lang.Object>,java.lang.Object\r\n    lower bounds: com.google.common.util.concurrent.ListenableFuture<? extends java.lang.Object>\r\n```\r\nThis error can be reproduced on 9.181 and 11.0.6 . \r\n\r\nI think it\u2019s caused by an unspecified behavior (or a bug) in some newer version of JDK 9 & 11\r\n\r\nSee the discussion below:\r\n\r\nhttps://bugs.openjdk.java.net/browse/JDK-8206142\r\n\r\nhttps://bugs.openjdk.java.net/browse/JDK-8016196\r\n\r\nSo I add the type parameter explicitly to prevent javac to deduce the type. \r\n\r\nThe two commits below are fixing the unit test failures for the jdk/jre release after Dec 2018\r\n\r\n3. Add the new timezone Asia/Qostanay into zone-index.properties\r\n\r\n4. Upgrade joda time for the newly added timezone\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14010", "Support shuffle on Hive partition columns before write", "Wenlei Xie", "wenleix", "02/05/20, 07:23:33 PM", "Previously, writing worker will receive rows in all partitions,\r\nand thus can write upper to hive.max-partitions-per-writers partitions.\r\n\r\nThis session property allows shuffle on partitioned columns when writing\r\nto partitioned unbucketed Hive tables. As a result, rows in the same\r\npartition will be sent to the same writing worker. This increase the\r\nnumber of maximum partitions written in single query by a factor of\r\nnumber of total writing workers.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Allow shuffle on partitioned columns when writing to partitioned unbucketed Hive tables. This increase the number of maximum partitions written in single query by a factor of number of total writing workers.This behavior has to be explicitly enabled by Connector session property `shuffle_partitioned_columns_for_table_write`. #14010\r\n```\r\n\r", "NaN"], ["14012", "Remote plan execution", "Rongrong Zhong", "rongrong", "09/15/20, 07:38:56 PM", "Resolves #14053 \r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1149", "NaN"], ["14014", "Upgrade slice to 0.38", "James Gill", "jagill", "01/27/20, 08:16:01 PM", "Previous version was 0.36.  There was one minor incompatibility.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Upgrade airlift/slice to 0.38.\r\n```\r", "NaN"], ["14015", "Adding LIKE operator description", "Oskar Austegard", "oaustegard", "02/05/20, 02:29:15 AM", "As discussed in https://github.com/prestodb/presto/issues/1198 adding documentation for LIKE, including use of wildcards and ESCAPE. Verified against test cases in https://github.com/prestodb/presto/blob/9aa8f80edf946d26760012d9da4c1b34cff25eb7/presto-main/src/test/java/com/facebook/presto/sql/TestExpressionInterpreter.java#L1223 and empirical execution of example code\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14016", "Add task memory related session properties ", null, "aweisberg", "01/30/20, 02:48:42 PM", "`query_max_total_memory_per_node ` is required for a specific memory constrained use cases. `query_max_memory_per_node` is added for completeness.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Added query_max_total_memory_per_node and query_max_memory_per_node session properties\r\n```", "NaN"], ["14017", "Fix Elasticsearch connector documentation", "Zhenxiao Luo", "zhenxiao", "01/28/20, 12:26:56 AM", "NaN", "NaN"], ["14018", "Add statistics handling to the Alluxio metastore support", "Haoyuan Li", "haoyuan", "02/02/20, 10:43:27 PM", "Co-authored-by: David Zhu <david@alluxio.com>\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n*  Add statistics handling to the Alluxio metastore support\r\n\r\n```", "NaN"], ["14019", "Cache mvnw", "Timothy Meehan", "tdcmeehan", "01/28/20, 08:22:24 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14020", "Optimize T-Digest structure for small distributions", "Timothy Meehan", "tdcmeehan", "01/29/20, 01:05:09 AM", "Dynamically increase the size of underlying arrays as values are added to digest.\r\n\r\nBenchmark results comparing before/after optimization are displayed below:\r\n\r\n```\r\nBenchmarkTDigest.benchmarkCopyBEFORE\t\tN/A\tavgt\t30\t3328.574 \u00b1  79.584 ns/op\r\nBenchmarkTDigest.benchmarkCopyAFTER\t\tN/A\tavgt\t30\t1969.757 \u00b1  19.348 ns/op\r\nBenchmarkTDigest.benchmarkDeserializeBEFORE\tN/A\tavgt\t30\t1623.594 \u00b1 111.647 ns/op\r\nBenchmarkTDigest.benchmarkDeserializeAFTER\tN/A\tavgt\t30\t 165.251 \u00b1   5.514 ns/op\r\nBenchmarkTDigest.benchmarkInsertsBEFORE\t\tN/A\tavgt\t30\t  67.205 \u00b1   0.796 ns/op\r\nBenchmarkTDigest.benchmarkInsertsAFTER\t\tN/A\tavgt\t30\t  69.787 \u00b1   2.767 ns/op\r\nBenchmarkTDigest.benchmarkMergeBEFORE\t\tN/A\tavgt\t30\t4736.826 \u00b1  24.052 ns/op\r\nBenchmarkTDigest.benchmarkMergeAFTER\t\tN/A\tavgt\t30\t2181.707 \u00b1  49.311 ns/op\r\nBenchmarkTDigest.benchmarkSerializeBEFORE\tN/A\tavgt\t30\t2423.662 \u00b1 138.643 ns/op\r\nBenchmarkTDigest.benchmarkSerializeAFTER\tN/A\tavgt\t30\t 133.829 \u00b1   4.896 ns/op\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\nReplaces #13160", "NaN"], ["14024", "Fix target path typo in HiveWriter", "James Sun", "highker", "01/29/20, 04:30:15 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14032", "Add peak task memory in resource estimates", "Andr\u00e9s Flores", "andres-fm", "02/04/20, 08:14:16 PM", "Peak task total memory information is a critical information that we should provide support into the existing ResourceEstimates class because we can use it to redirect traffic from existing T6 (high memory) cluster to new T1 (low memory) clusters.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14036", "Improve correctness check for RowType columns", "Igor Kruk", "kvaturka", "02/07/20, 07:41:32 PM", "Improve correctness check of row types by applying specific validation to individual fields\r\n\r\nPart of #13809 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add specific validation checks for the individual fields when validating a row column.\r\n```\r", "NaN"], ["14038", "Fix recoverable grouped execution eligibility", "Shixuan Fan", "shixuan-fan", "02/03/20, 05:13:01 PM", "We should only allow recoverable grouped execution when table\r\nwriter merge operator is enabled to avoid enabling for cases\r\nwhere there are parallel table writer operators, otherwise there\r\nwould be multiple \"last page\" from one task, and thus fail the\r\nsanity checkState check.\r\n\r\nThis could be removed after we remove control flag for table writer\r\nmerge operator.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14039", "Account for page header in ORC output buffer", "Islam Ismailov", "islamismailov", "02/04/20, 10:57:11 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add page header size accounting to ORC output buffer to avoid possible overflow.\r\n```", "NaN"], ["14040", "Move Hive filter pushdown logic out of PickTableLayout", "Saksham", "sachdevs", "03/05/20, 08:48:08 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14042", "Presto Druid Connector", "Zhenxiao Luo", "zhenxiao", "02/08/20, 07:03:26 AM", "Co-authored-by: Zhenxiao Luo <zluo@twitter.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nDruid Changes\r\n* Add Druid Connector\r\n```", "NaN"], ["14043", "Expose Hive table properties via system table$properties table", "Zhenxiao Luo", "zhenxiao", "02/02/20, 05:48:21 AM", "Cherry-pick of https://github.com/prestosql/presto/commit/e7478a3200c26f9ca312f9b29c752de4e4843b7e\r\n\r\nCo-authored-by: Zhenxiao Luo <zluo@twitter.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Expose Hive table properties via system table$properties table\r\n```\r", "NaN"], ["14045", "Add Hive directory listing cache", "Zhenxiao Luo", "zhenxiao", "02/12/20, 11:20:26 PM", "Cherry-pick of https://github.com/prestosql/presto/commit/b25ebeb43f06f4c4b86280f1b5f62ad22918c312\r\n\r\nCo-authored-by: Zhenxiao Luo <zluo@twitter.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add directory listing cache for Hive Connector\r\n```\r", "NaN"], ["14046", "Replace Joda-Time libraries with java.time", "Ajay George", "ajaygeorge", "02/03/20, 04:48:37 PM", "Since Java 8 we have java.time packages which are equivalent to Joda and\nthe recommendation from the author of the Joda-Time is to migrate to\njava.time(JSR-310) library.\n\n```\n== NO RELEASE NOTE ==\n```", "NaN"], ["14047", "Update Alluxio version from 2.1.1 to 2.1.2", "Haoyuan Li", "haoyuan", "02/03/20, 08:21:29 PM", "Co-authored-by: David Zhu <david@alluxio.com>\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Update Alluxio version from 2.1.1 to 2.1.2\r\n\r\n```", "NaN"], ["14048", "Update proto utilities for the Database object", "Haoyuan Li", "haoyuan", "02/06/20, 08:07:52 AM", "Co-authored-by: David Zhu <david@alluxio.com>\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n\r\n```", "NaN"], ["14054", "Improve verification for map columns", null, "brendan-driscoll", "02/18/20, 03:09:29 PM", "Part of #13809\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add checks for keys, values, and cardinality sum when validating a map column.\r\n```", "NaN"], ["14057", "Pinot bugs and making it work with the latest trunk pinot", "Devesh Agrawal", "agrawaldevesh", "02/06/20, 09:06:51 PM", "Three changes here: 1 medium and 2 tiny:\r\n* The nameToIndex used for the pinot column ordering map should not assume that the variable reference name is the same as the pinot lowercase name. It should be keyed off the keys in the selections map (in the pinot query generator context) and not the value. It was throwing an error with the query below because the output variables were named like player_initial_33, g_old_51 etc instead of just player_initial, g_old. This is because these columns are referenced twice in the query. \r\nThis again smells like a core bug and it is totally my bad that I screwed this up the last time. \r\n\r\n```\r\nselect * from (select yearid, substr(playerid, 1, 3) as player_initial, sum(homeruns) from pinot.baseballStats.baseballStats group by 1, 2) as m INNER join (select g_old, substr(playername, 1, 3) as player_name_initial, sum(numberofgames) from pinot.baseballStats.baseballStats group by 1, 2) as w ON m.player_initial = w.player_name_initial;\r\n```\r\n\r\n* [tiny] Make this work with the new pinot trunk code which no longer likes \"Content-Type: Application/json\" for just GET requests and non body POSTS :-). As it shouldn't. \r\n* [tiny] renamed the config prefer broker queries to ! forbid broker queries to be true to the name\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n*  Replace config `pinot.prefer-broker-queries` with the inverse config `pinot.forbid-broker-queries`.\r", "NaN"], ["14059", "Update to Joda 2.10.5", "Leiqing Cai", "caithagoras", "02/04/20, 11:11:04 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix an issue where ``DATE_TRUNC`` may produce incorrect results at certain timestamp in ``America/Sao_Paulo``.\r\n```\r", "NaN"], ["14060", "Add release notes for 0.231.1", "Leiqing Cai", "caithagoras", "02/04/20, 11:52:51 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14062", "Display built-in functions first in SHOW FUNCTIONS results", "Leiqing Cai", "caithagoras", "04/24/20, 11:44:50 PM", "In SHOW FUNCTIONS results, list the built-in functions first, and then\r\nthe SQL functions, in alphabetical order of the qualified function\r\nnames.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14063", "Log number of splits received for Presto-on-Spark task", "Wenlei Xie", "wenleix", "02/06/20, 11:46:08 PM", "We have seen very uneven workload assignment in table scan phase when running Presto-on-Spark in production. Log the number of splits on each task to see if it's due to bad split assignment (bad hash function?)\r\n\r\n<img width=\"1532\" alt=\"Screen Shot 2020-02-04 at 10 39 29 PM\" src=\"https://user-images.githubusercontent.com/799346/73817481-dbba1900-479f-11ea-9f65-7ff00f91696f.png\">\r\n\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nMinor improvements to https://github.com/prestodb/presto/issues/13856\r", "NaN"], ["14064", "Add join pushdown predicate at the end of the exisiting predicate", null, "bhhari", "02/06/20, 06:27:16 PM", "Currently, we add the join pushdown predicate at the start of the existing predicate, this will result in the evaluation of that without respecting the order of the existing ones.\r\n\r\nFixes #14065\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14067", "Update Airlift to 0.189", "Andrii Rosa", "arhimondr", "02/06/20, 10:11:52 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14068", "Bump zookeeper from 3.4.13 to 3.4.14", null, "dependabot[bot]", "07/06/20, 01:54:35 PM", "Bumps zookeeper from 3.4.13 to 3.4.14.\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.zookeeper:zookeeper&package-manager=maven&previous-version=3.4.13&new-version=3.4.14)](https://help.github.com/articles/configuring-automated-security-fixes)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/prestodb/presto/network/alerts).\n\n</details>", "NaN"], ["14070", "Revert \"Add release notes for 0.231.1\"", "Leiqing Cai", "caithagoras", "02/07/20, 10:50:08 PM", "The next was not cherry-pick onto the release branch when creating 0.231.1, and thus 0.231.1 is identical to 0.231. The change will be available in 0.232, so thus we should add the release notes there.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14073", "Fix Druid connector pom to new release", "Zhenxiao Luo", "zhenxiao", "02/08/20, 10:09:12 AM", "release upgrade just before merge\r\nfix druid pom", "NaN"], ["14076", "Fix like pattern interpretation", "James Sun", "highker", "02/12/20, 07:59:35 PM", "like pattern constant folding happens only when the optimization level\r\nis above SERIALIZABLE. However, RowExpressionInterpreter::tryHandleLike\r\nhandles like function in a way that it will take the constant folded\r\nlike pattern result together with the original arguments. Under such\r\ncase, tryHandleLike expects the given constant folded like pattern\r\nshould always be a compiled Regex but is not in the above case.\r\n\r\nThe patch relaxes the restriction on the like pattern to be more\r\ngeneric. As long as the return type is Regex, tryHandleLike should be\r\nable to handle it.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix an optimizer failure introduced since 0.229, where a `LIKE` pattern can be deduced into a constant. For example, `col LIKE 'a' and col = 'b'`\r\n```\r", "NaN"], ["14080", "Disable too-many-stages warning for exchange materialization", "Wenlei Xie", "wenleix", "02/11/20, 05:44:47 PM", "With exchange materialization enabled, only a limited number of stages\r\nwill be executed concurrently (controlled by session property\r\nmax_concurrent_materializations).\r\n\r\nToo-many-stages warning is confusing in this case.\r\n\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14081", "Fix hostname related assertion failure in PrestoSparkQueryRunner", "Andrii Rosa", "arhimondr", "02/11/20, 02:55:56 PM", "If the local hostname is not set in the system, the PrestoSparkQueryRunner\r\nfails with the \"java.lang.AssertionError: assertion failed: Expected hostname\".\r\n\r\nFollowing the workaround described in the issue:\r\nhttps://issues.apache.org/jira/browse/SPARK-19394?focusedCommentId=15846109\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nRelated to https://github.com/prestodb/presto/issues/13856\r", "NaN"], ["14082", "Add getPartitionNames support to Alluxio Metastore", "Haoyuan Li", "haoyuan", "02/12/20, 06:05:46 PM", "```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14083", "Balance ArbitraryOutputbuffer distribution over clients", "James Petty", "pettyjamesm", "02/18/20, 02:02:43 AM", "Cross port of https://github.com/prestosql/presto/pull/2788\r\n\r\nPreviously, the order that client buffers were polled was always started with the first ClientBuffer, which could lead to data skew when the master buffer drained before all clients could be polled since the client buffer traversal order was stable (but arbitrary).\r\n\r\nThis change stores the stop index of client buffer iteration to ensure that subsequent polling loops don't overload the first client buffer.\r\n\r\nThis is an alternative, simpler solution to the problem described in https://github.com/prestosql/presto/pull/2225 which does a much better job of documenting the existing issue. Shuffling the order is certainly preferable to skewing data, but the extra allocations and shuffling work shouldn't be necessary to produce a more fair output distribution.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix ArbitraryOutputBuffer to avoid skewing output data distribution\r\n```\r", "NaN"], ["14085", "Strip httpcomponents dependency from Hudi", "James Sun", "highker", "02/11/20, 10:48:48 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14086", "Adding SampleNode stats and cost rule.", null, "ssaumitra", "02/18/20, 03:07:46 PM", "Adding Stats and cost rules for Sample node, so that more efficient plan is generated for the query with sample operator.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14087", "Revert \"Improve error message for the array_agg function\"", null, "sujay-jain", "02/12/20, 06:43:45 AM", "Reverts prestodb/presto#13979\r\n\r\nWe're seeing certain queries failing due to the 1GB restriction. Reverting this PR for now to unblock release.", "NaN"], ["14088", "Strip more dependencies from Hudi", "Bhavani Sudha Saktheeswaran", "bhasudha", "02/12/20, 07:30:05 PM", "Please make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14090", "Add IPPREFIX functions", "Nikhil Collooru", "NikhilCollooru", "02/21/20, 01:19:39 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add :func:`ip_subnet_min`, :func:`ip_subnet_max`, :func:`ip_subnet_range`, \r\n  and :func:`is_subnet_of` functions.\r\n\r\n```\r", "NaN"], ["14091", "Increase Heap size to fix test timeouts", null, "bhhari", "02/13/20, 09:53:07 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14095", "Add soft affinity in scheduling bucketed split for Hive Connector", "Ke", "kewang1024", "02/24/20, 11:39:09 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n* Add parameter `NodeSelectionStrategy nodeSelectionStrategy` in method `createBucketNodeMap` in `ConnectorBucketNodeMap `, indicating which affinity strategy to use when we create bucket nodeMap.\r\n* Add parameter `List<Node> sortedNodes` in method `getBucketNodeMap` in `ConnectorNodePartitioningProvider `, providing a sorted node list for connector to choose from when doing affinity scheduling.\r\n\r\nGeneral Changes\r\n* Add soft affinity scheduling to bucketed split. It makes the best effort to fetch the same bucket data from the same worker. When using dynamic group scheduling, if the preferred workers are unavailable to handle the specific bucket splits, it will fallback to random workers. \r\n\r\n```", "NaN"], ["14096", "Remove unnecessary copies in StreamingAggregationOperator", "James Petty", "pettyjamesm", "02/13/20, 05:52:27 PM", "The previous implementation copied the aggregation group out of the\r\npage for each aggregation operator which is wasteful and extremely\r\nslow when the number of aggregates performed is large.\r\n\r\nPerformance improvement depends on the number of aggregations and\r\nthe number of groups contained in the page, but some workloads can\r\nimprove throughput by more than 100% and generate significantly\r\nless garbage.\r\n\r\nCross port of https://github.com/prestosql/presto/pull/2822\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve performance of StreamingAggregationOperator\r\n```\r", "NaN"], ["14097", "Run presto spark tests in forked VM", "Andrii Rosa", "arhimondr", "02/13/20, 08:10:26 PM", "The tests no longer use stream redirects thus the workaround is\r\nno longer needed.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nRelated to https://github.com/prestodb/presto/issues/13856", "NaN"], ["14098", "Provide view metadata to the Hive Metastore", "Andrew P. Smith", "ucalegon206", "02/26/20, 07:33:50 PM", "Please make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Store column names and types for views in the metastore.  Views in the Hive connector can now only use types supported by Hive. \r\n\r\nSPI Changes\r\n* Change signature for createView in ConnectorMetadata to take a ConnectorTableMetadata instead of a SchemaTableName\r\n```\r\n\r", "NaN"], ["14099", "Implement row base exchange in Presto on Spark", "Andrii Rosa", "arhimondr", "02/25/20, 09:48:14 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14101", "Remove forceLocalScheduling for hive connector", "Ke", "kewang1024", "02/14/20, 10:02:20 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14102", "Add nodeSelectionStrategy to getInfo in HiveSplit", "Ke", "kewang1024", "02/14/20, 08:02:03 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14103", "Allow insert overwrite as default behavior for Hive connector", "Wenlei Xie", "wenleix", "02/17/20, 08:45:38 PM", "Hive connector users often expects INSERT OVERWRITE as the default\r\nINSERT behavior. While there is a session property\r\n`insert_existing_partitions_behavior` to enable this, it has to be\r\nmanually enabled.\r\n\r\nThis commit introduces a new Hive client configuration\r\n`hive.insert-overwrite-immutable-partitions-enabled` to allow admin set\r\ninsert overwrite as the default behavior.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\n\r\nHive Changes\r\n* Introduce  a new Hive client configuration`hive.insert-overwrite-immutable-partitions-enabled` to allow admin set insert overwrite as the default insertion behavior for Hive connector\r\n```\r", "NaN"], ["14104", "Resource Management related cleanup", "Timothy Meehan", "tdcmeehan", "03/11/20, 05:17:11 PM", "Rebase of #12801\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14105", "Allow Raptor create table with customized table properties", "James Sun", "highker", "02/14/20, 10:26:22 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14106", "Add cheap ST_Intersection method for certain cases", "James Gill", "jagill", "02/19/20, 02:51:54 PM", "To find the intersection of an envelope that contains a geometry,\r\nwe can simple return the geometry.  This adds a check for that case,\r\nshort-circuiting the expensive intersection logic.\r\n\r\nIt reduces the benchmark cost for these types of intersections with small\r\npolygons by 200x, and large polygons by 100,000x.\r\n\r\n## Original\r\n```\r\nBenchmark                                                          Mode  Cnt      Score     Error  Units\r\n**BenchmarkSTIntersection.stIntersectionComplexPolygonLargeEnvelope  avgt   15  11572.428 \u00b1 298.935  us/op**\r\nBenchmarkSTIntersection.stIntersectionComplexPolygonSmallEnvelope  avgt   15   1405.820 \u00b1  50.947  us/op\r\nBenchmarkSTIntersection.stIntersectionComplexPolygons              avgt   15  18024.312 \u00b1  93.407  us/op\r\nBenchmarkSTIntersection.stIntersectionSimpleComplexPolygons        avgt   15   3624.582 \u00b1  54.135  us/op\r\n**BenchmarkSTIntersection.stIntersectionSimplePolygonLargeEnvelope   avgt   15     23.695 \u00b1   0.296  us/op**\r\nBenchmarkSTIntersection.stIntersectionSimplePolygonSmallEnvelope   avgt   15     47.711 \u00b1   0.757  us/op\r\nBenchmarkSTIntersection.stIntersectionSimplePolygons               avgt   15     17.121 \u00b1   0.290  us/op\r\n```\r\n## With Envelope Shortcut\r\n```\r\nBenchmark                                                          Mode  Cnt      Score     Error  Units\r\n**BenchmarkSTIntersection.stIntersectionComplexPolygonLargeEnvelope  avgt   15      0.090 \u00b1   0.007  us/op**\r\nBenchmarkSTIntersection.stIntersectionComplexPolygonSmallEnvelope  avgt   15   1426.323 \u00b1  52.648  us/op\r\nBenchmarkSTIntersection.stIntersectionComplexPolygons              avgt   15  18471.559 \u00b1 410.481  us/op\r\nBenchmarkSTIntersection.stIntersectionSimpleComplexPolygons        avgt   15   3722.377 \u00b1  67.931  us/op\r\n**BenchmarkSTIntersection.stIntersectionSimplePolygonLargeEnvelope   avgt   15      0.087 \u00b1   0.001  us/op**\r\nBenchmarkSTIntersection.stIntersectionSimplePolygonSmallEnvelope   avgt   15     47.280 \u00b1   0.524  us/op\r\nBenchmarkSTIntersection.stIntersectionSimplePolygons               avgt   15     16.877 \u00b1   0.364  us/op\r\n```\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* When calculating `ST_Intersection` of a geometry with an enclosing envelope, just return the geometry.  This reduces the cost by up to 10^5x or more for complex polygons when this occurs.\r\n```", "NaN"], ["14107", "Use INFO level log for Presto-on-Spark plan", "Wenlei Xie", "wenleix", "02/17/20, 05:32:12 AM", "Distributed plan is quite important log and should use INFO level\r\ninstead of DEBUG level.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14108", "Pass cache info into connectors", "Ke", "kewang1024", "02/28/20, 08:23:22 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14109", "Add Alluxio Catalog Service doc", "Haoyuan Li", "haoyuan", "02/20/20, 07:05:06 AM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r", "NaN"], ["14110", "Cache PlanFragment serialization", "Timothy Meehan", "tdcmeehan", "02/26/20, 10:20:37 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14113", "Add ZSTD support for writing ORC and DWRF tables", "Rohit Jain", "jainxrohit", "03/14/20, 12:03:51 AM", " Add ZSTD support for writing ORC and DWRF tables\r\nTo enable ZSTD compression, use session property hive.compression_codec='ZSTD'.\r\n\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n*  Add ZSTD support for writing ORC and DWRF tables. To enable ZSTD compression, use session property hive.compression_codec='ZSTD'.", "NaN"], ["14114", "Fix null pointer when creating ConnectorSession for LocalExchange", "Ke", "kewang1024", "02/18/20, 08:39:41 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14115", "Do not run docker based spark integration tests by default", "Andrii Rosa", "arhimondr", "02/18/20, 08:34:13 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nRelated to https://github.com/prestodb/presto/issues/13856\r", "NaN"], ["14116", "Create empty blocks when there is a mismatch in struct schema", null, "bhhari", "02/22/20, 03:10:07 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14119", "Add KHyperLogLog type and UDF", "Matias Correa", "mcorreaiz", "03/09/20, 09:55:01 PM", "Addresses #14035 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add KHyperLogLog Type\r\n* Add MAKE_KHYPERLOGLOG(K, V) -> KHYPERLOGLOG aggregate function\r\n* Add KHyperLogLog related scalar functions\r\n```\r", "NaN"], ["14120", "Fix hash function for soft affnity in Hive", "Ke", "kewang1024", "02/20/20, 03:31:32 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14121", "Add toString() method to BlockEncodingBuffers and ColumnarXXX", "Ying", "yingsu00", "02/27/20, 03:03:02 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14122", "Optimize scan of floats with many nulls", "Maria Basmanova", "mbasmanova", "02/21/20, 01:17:50 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14123", "Optimize aggregation of nulls", "Maria Basmanova", "mbasmanova", "02/23/20, 03:24:31 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14124", "Optimize scan of filter-only no-nulls integers", "Maria Basmanova", "mbasmanova", "02/21/20, 04:20:54 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14125", "Add cast bingtile to/from bigint", "James Gill", "jagill", "03/11/20, 12:32:17 AM", "Externally, tiles are encoded in a string of chars '0' to '3' called a\r\nquadkey.  Internally, Presto encodes a tile in 64 bits, represented by a\r\nBIGINT.  Storing a tile as a bigint is not only more space/cpu efficient\r\nthan storing it as a quadkey, but it also avoids the bucket-skew problem\r\ncaused by the non-uniform distribution of `hash(quadkey) mod 2^k`.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add `cast(tile AS bigint)` and `cast(bigint_value AS bingtile)` to encode/decode Bing tiles to/from bigints.  This is a more efficient storage format that also reduces bucket skew in some cases.\r\n```\r", "NaN"], ["14126", "Update Airlift to 0.190", "Vic Zhang", "viczhang861", "02/20/20, 10:07:53 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14127", "Create presto-hive-base package", "Shixuan Fan", "shixuan-fan", "02/21/20, 02:43:05 AM", "presto-hive-base is intended to be shared among different packages\r\nthat use hive but does not necessarily depend on presto-hive.\r\n\r\nWe consolidated HiveErrorCode in this commit as the initial class.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14128", "Add HadoopDefaultConfigurationUpdater", "Zac Blanco", "ZacBlanco", "02/26/20, 06:21:24 PM", "Without this change, when the `hive.s3-file-system-type` is set to\r\nHADOOP_DEFAULT the server crashes due to a guice injection error\r\n\r\n```\r\n2020-02-20T03:10:22.218Z        ERROR   main    com.facebook.presto.server.PrestoServer Unable to create injector, see the following errors:\r\n1) Explicit bindings are required and com.facebook.presto.hive.s3.S3ConfigurationUpdater is not explicitly bound.\r\n  while locating com.facebook.presto.hive.s3.S3ConfigurationUpdater\r\n    for the 2nd parameter of com.facebook.presto.hive.HdfsConfigurationInitializer.<init>(HdfsConfigurationInitializer.java:71)\r\n  at com.facebook.presto.hive.HiveClientModule.configure(HiveClientModule.java:74)\r\n1 error\r\ncom.google.inject.CreationException: Unable to create injector, see the following errors:\r\n1) Explicit bindings are required and com.facebook.presto.hive.s3.S3ConfigurationUpdater is not explicitly bound.\r\n  while locating com.facebook.presto.hive.s3.S3ConfigurationUpdater\r\n    for the 2nd parameter of com.facebook.presto.hive.HdfsConfigurationInitializer.<init>(HdfsConfigurationInitializer.java:71)\r\n  at com.facebook.presto.hive.HiveClientModule.configure(HiveClientModule.java:74)\r\n1 error\r\n        at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:543)\r\n        at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:159)\r\n        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:106)\r\n        at com.google.inject.Guice.createInjector(Guice.java:87)\r\n        at io.airlift.bootstrap.Bootstrap.initialize(Bootstrap.java:240)\r\n        at com.facebook.presto.hive.HiveConnectorFactory.create(HiveConnectorFactory.java:128)\r\n        at com.facebook.presto.connector.ConnectorManager.createConnector(ConnectorManager.java:358)\r\n        at com.facebook.presto.connector.ConnectorManager.addCatalogConnector(ConnectorManager.java:217)\r\n        at com.facebook.presto.connector.ConnectorManager.createConnection(ConnectorManager.java:209)\r\n        at com.facebook.presto.connector.ConnectorManager.createConnection(ConnectorManager.java:195)\r\n```\r\n\r\nThis change adds a HadoopDefaultConfigurationUpdater class which is\r\nessentially a no-op placeholder to prevent the error above.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Allow server to start when `hive.s3-file-system-type` is set to HADOOP_DEFAULT\r\n```\r", "NaN"], ["14130", "Add release notes for 0.232", "Leiqing Cai", "caithagoras", "02/26/20, 01:32:46 AM", "# Missing Release Notes\n## Andrii Rosa\n- [x] 2b5647665a431daebbf35a80f3e9ed74f95d1296 Run presto spark tests in forked VM\n- [x] e8b6042da7c8cf2fbae68339565e5239f40d4419 Fix hostname related assertion failure in PrestoSparkQueryRunner\n\n## Da Cheng\n- [x] https://github.com/prestodb/presto/pull/13709 Update Kafka connector to 2.3.1 (Merged by: Zhenxiao Luo)\n\n## Haoyuan Li\n- [x] https://github.com/prestodb/presto/pull/13986 Add Alluxio Hive metastore (Merged by: James Sun)\n\n## YuBin Li\n- [x] https://github.com/prestodb/presto/pull/14007 Support kerberos authentication for Kudu connector (Merged by: Wenlei Xie)\n\n## qqibrow\n- [x] https://github.com/prestodb/presto/pull/14017 Fix Elasticsearch connector documentation (Merged by: Wenlei Xie)\n\n## sujay-jain\n- [x] 7700a59080cc24c69aa9a2ca47e87bc0dd0a924b Revert \"Improve error message for the array_agg function\"\n\n## wuyunfeng01\n- [x] https://github.com/prestodb/presto/pull/13987 Promote Elasticsearch scan performance with index sorting (Merged by: Zhenxiao Luo)\n\n# Extracted Release Notes\n- #13354 (Author: Ariel Weisberg): Add use-exact-partitioning-enabled session property\n  - Add use_exact_partitioning session property that forces repartitioning if repartitioning is possible.\n- #13717 (Author: Rebecca Schlussel): Support retries of streaming sections\n  - Add support for retrying failed stages from a materialized point instead of failing the entire query.  The number of retries allowed can be configured using the configuration property max-stage-retries and session property max_stage_retries. The default value is zero.  To take advantage of this feature, exchange_materialization_strategy must be set to 'ALL'.\n  - Add configuration property use-legacy-scheduler and session property use_legacy_scheduler to use a version of the query scheduler from before refactorings to enable full stage retries.  The default value is false. This is a temporary property to provide an easy way to roll back in case of bugs in the new scheduler.  This property will be removed in a couple releases once we have confidence in the stability of the new scheduler.\n- #13799 (Author: Leiqing Cai): Support ALTER FUNCTION\n  - Add support for ``ALTER FUNCTION``.\n- #13894 (Author: James Sun): Support internal communication with thrift\n  - Allow Presto nodes to shuffle data with Thrift protocol. Use config `internal-communication.task-communication-protocol` to control between HTTP and Thrift.\n  - Allow Presto nodes to announce state with Thrift protocol. Use config `internal-communication.server-info-communication-protocol` to control between HTTP and Thrift.\n- #13931 (Author: Swapnil Tailor): Configure DB Resource Group reload frequency\n  - Making database resource group reload frequency configurable.\n  - ...\n- #13934 (Author: Vic Zhang): Update error code when table is dropped during query execution\n  - When ``DROP TABLE`` query fails because the table has being dropped by other query before this query finishes,  change error code name from ``HIVE_METASTORE_ERROR`` to  ``HIVE_TABLE_DROPPED_DURING_QUERY``.\n- #13940 (Author: Timothy Meehan): Add t-digest functions to Presto\n  - Add tdigest_agg, merge(tdigest), value_at_quantile(tdigest, quantile), values_at_quantiles(tdigest, quantiles), quantile_at_value(tdigest, quantile), quantiles_at_values(tdigest, quantile) for creating, merging, and querying t-digests.\n  - IsHidden attribute on AggregationFunction and ScalarFunction removed and replaced with visibility.\n- #13949 (Author: Leiqing Cai): Enable listing SQL function with session property\n  - Support hiding user-defined SQL functions in ``SHOW FUNCTIONS`` with session property ``list_built_in_functions_only``. This can also be achieved by configuration property ``list-built-in-functions-only``, which is repurposed from ``list-non-built-in-functions``.\n- #13972 (Author: Leiqing Cai): Support MySQL-based FunctionNamespaceManager\n  - Add a MySQL-based function namespace manager implementation that supports creating, altering, and dropping SQL functions. (:doc:`/admin/function-namespace-managers`).\n- #13974 (Author: Leiqing Cai): Extend SHOW FUNCTIONS\n  - Add support to show whether functions have variable arity in ``SHOW FUNCTIONS``.\n  - Add support to show whether functions are built-in in ``SHOW FUNCTIONS``.\n- #13982 (Author: Rongrong Zhong): Cache builtin function resolution result\n  - Improve built-in function resolution performance by caching function resolution results.\n- #13998 (Author: Igor Kruk): Improve correctness check for queries that produce columns of RowType\n  - Add checks for the individual fields when validating a row column.\n- #14010 (Author: Wenlei Xie): Support shuffle on Hive partition columns before write\n  - Allow shuffle on partitioned columns when writing to partitioned unbucketed Hive tables. This increase the number of maximum partitions written in single query by a factor of number of total writing workers.This behavior has to be explicitly enabled by Connector session property `shuffle_partitioned_columns_for_table_write`. #14010.\n- #14014 (Author: James Gill): Upgrade slice to 0.38\n  - Upgrade airlift/slice to 0.38.\n- #14016 (Author: Ariel Weisberg): Add task memory related session properties \n  - Added query_max_total_memory_per_node and query_max_memory_per_node session properties.\n- #14018 (Author: Haoyuan Li): Add statistics handling to the Alluxio metastore support\n  - Add statistics handling to the Alluxio metastore support.\n- #14036 (Author: Igor Kruk): Improve correctness check for RowType columns\n  - Add specific validation checks for the individual fields when validating a row column.\n- #14039 (Author: Islam Ismailov): Account for page header in ORC output buffer\n  - Add page header size accounting to ORC output buffer to avoid possible overflow.\n- #14043 (Author: qqibrow): Expose Hive table properties via system table$properties table\n  - Expose Hive table properties via system table$properties table.\n- #14047 (Author: Haoyuan Li): Update Alluxio version from 2.1.1 to 2.1.2\n  - Update Alluxio version from 2.1.1 to 2.1.2.\n- #14057 (Author: Devesh Agrawal): Pinot bugs and making it work with the latest trunk pinot\n  - Replace config `pinot.prefer-broker-queries` with the inverse config `pinot.forbid-broker-queries`.\n- #14059 (Author: Leiqing Cai): Update to Joda 2.10.5\n  - Fix an issue where ``DATE_TRUNC`` may produce incorrect results at certain timestamp in ``America/Sao_Paulo``.\n\n# All Commits\n- 2b5647665a431daebbf35a80f3e9ed74f95d1296 Run presto spark tests in forked VM (Andrii Rosa)\n- 7700a59080cc24c69aa9a2ca47e87bc0dd0a924b Revert \"Improve error message for the array_agg function\" (sujay-jain)\n- e8b6042da7c8cf2fbae68339565e5239f40d4419 Fix hostname related assertion failure in PrestoSparkQueryRunner (Andrii Rosa)\n- 44c044ad827365369f9f255cbb272b3f07c4b30c Add documentation for SQL functions (Leiqing Cai)\n- fbe0b3dca58a1804b20e2aa9e849a92efce854c6 Introduce MySql-based function namespace manager (Leiqing Cai)\n- a59bef803a0f4b2450a6475459e120f6e6c9d7df Introduce db based function namespace manager (Leiqing Cai)\n- c2c90c079c95e54e69eda99eaa052cf661f7c42d Use Return statement as the body for SQL-invoked functions (Leiqing Cai)\n- dc129d8b15e8a262319a7053319b77265e508756 Require function namespace manager to bind with catalog names (Leiqing Cai)\n- 56d063b80c9e6c7a4d672edd5a60f6da9f8e9dd8 Remove dependency of presto-sql-function from presto-main (Leiqing Cai)\n- 2de5d7f6aa365500d6e6be1fb040edf6c7a89713 Improve correctness check for RowType columns (Igor Kruk)\n- 4c2010e59ccf7bdcb1b549e1bfe18675ee6ed560 Add retriedCpuTime to QueryCompletedEvent (Rebecca Schlussel)\n- 6525ea9b1ded94c34f0e93b210afa273f2ce0f4a Naming cleanup in SqlQueryScheduler (Rebecca Schlussel)\n- 615feb51b8e72ac2c266d4685ff36431336b5746 Add support for retrying streaming sections (Rebecca Schlussel)\n- e5eda5733d7ab114107f91b034a5169ebf665bbe Introduce LegacySqlQueryScheduler (Rebecca Schlussel)\n- fe6ebcfea6d70f14264561cb9617c1c72eb2c700 Reorder fields and methods in SqlQueryScheduler (Rebecca Schlussel)\n- 4fd684b3609bdc9fd68e5af6d225aa8d6a62e8bc Make FixedSourcePartitionedScheduler more thread safe (Rebecca Schlussel)\n- f789c543fae3bf2afe7c589877df112c94a24221 Introduce method for empty StageExecutionInfo (Rebecca Schlussel)\n- 2e4d5a3e799110552d84e1706d8b5e247ac47c3a Remove id from StageExecutionInfo (Rebecca Schlussel)\n- b84f177d5fe37017123946cbefb3b5ffd2cf845d Make ExecutionSchedule return StageExecutionAndScheduler (Rebecca Schlussel)\n- 3185bf49ffb2500b7a70f36c411c2353bffce774 Extract SectionExecutionFactory from SqlQueryScheduler (Rebecca Schlussel)\n- 08793ed316806d1c1b2a7933c4b1c7979c7e4aeb Add exponential decay config to failure detector (Andrii Rosa)\n- 3eef1b02da1fd8d09f3992a54f6499695aa6b4cb Enable failure detector in tests (Rebecca Schlussel)\n- b9376252c71985f28638aad3c08a9b9b76c18ae6 Remove unused code from TestStageExecutionStateMachine (Rebecca Schlussel)\n- 85ed09a43001cd0d239d673f5d12ce7818f6ff23 Add configuration property max-stage-retries (Rebecca Schlussel)\n- e650db3092c8ac9da8bdb5210dbdfd4768335ed6 Minor refactor in SqlQueryScheduler (Rebecca Schlussel)\n- 7a4a741e11d3c13bbc6521505ac35244c00beb5a Move opening split source to scheduler creation time (Rebecca Schlussel)\n- 4fd9aafc21639f4d52afbf47f230da1a762a977e Log number of splits received for Presto-on-Spark task (Wenlei Xie)\n- b4909218346dff3896be54750080c7bbc646f4d2 Update Airlift to 0.189 (Andrii Rosa)\n- a3f9aa3566675f4b5fea33a96abc58fddbf56a21 Fail with informative error message if no Pinot expression is selected (Devesh Agrawal)\n- 1701b2267360e52ec1a819abbc494a58a14a9844 Avoid passing Content-Type header when Pinot request body is empty (Devesh Agrawal)\n- 74ad34b2cd27d3ec1d308878d844c807003647c3 Rename preferBrokerQueries to its negate format forbidBrokerQueries (Devesh Agrawal)\n- b05e09ebec004c5ccf7a9f5c05181fcb71d2998f Fix bug with pinot column scan index generation (Devesh Agrawal)\n- 4c7fc034bbcb0a490e8bcaa8842a6d1ea833623a Add join pushdown predicate at the end of the exisiting predicate (Bhavani Hari)\n- f63875571e29aade4663cb7e6447d42126add407 Allocate CPU quanta per query instead of per task (Andrii Rosa)\n- 46e6c9259c7ede6044e6c8f25e9527eb6ace2e15 Update proto utilities for the Database objects (Haoyuan Li)\n- 5833338c127c380f505873614d25862921437e75 Support shuffle on Hive partition columns before write (Wenlei Xie)\n- b4a186ce04eb2c26958ee3a7940c0b259a948648 Rename TableWriterNode#partitioningScheme to tablePartitioningScheme (Wenlei Xie)\n- 5a7074b1165eac3dd202306a44380f793bad885d Adding LIKE operator description (Oskar Austegard)\n- 47afe84967b61483e67fea08a8e1d4e94067f169 Update to Joda 2.10.5 (Leiqing Cai)\n- 10893a67c817ea0f9d169719c7f7e39d16f8a1c8 Account for page header in ORC output buffer (Islam Ismailov)\n- 8ac000dab6366917d3148e99655ffb635dd5c153 Add peak task memory in resource estimates (Andr\u00e9s Flores)\n- 1b45f4958ab600958136f33a3beade7209919973 Update alluxio version (Haoyuan Li)\n- 2dd6798d277269dce0a5558cefc0c8d9a2d29bd8 Fix recoverable grouped execution eligibility (Shixuan Fan)\n- 4f1380006f660e214ce55bff107a8a9c629a5156 Replace Joda-Time libraries with java.time (Ajay George)\n- 05310b74f5f7a86130f142da5700ca66c08b1b78 Add statistics handling to the Alluxio metastore support (Haoyuan Li)\n- fc8cb11bafb067d8be2c7e19a11c6be59298102e Expose Hive table properties via system table$properties table (qqibrow)\n- 0c5aa2dc3728c53e6c2658cdf222fff04a398a8c Improve correctness check for queries that produce columns of RowType (Igor Kruk)\n- fc3d1ee6580ea2cffc495de0fdc357e966564001 Add task memory related session properties (Ariel Weisberg)\n- 8eb01928c1e6e24ba3a6261c97255fd6351cbbc6 Upgrade Kafka Connector to 2.3.1 (Da Cheng)\n- 1ec81384fc9b1d06c71f325ef0d2b34f8993c762 Support kerberos authentication for Kudu connector (YuBin Li)\n- 66fb17c2b1dfd876912a15969864bafa801bb5e0 Fix target path typo in HiveWriter (James Sun)\n- a26f440944689cb907c77d272e180b570cd2d39c Optimize T-Digest structure for small distributions (Timothy Meehan)\n- 8296be0c8610858ac9bdca31477be46fa900ec5c Move PushdownSubfields below logical optimization (Saksham Sachdev)\n- 30f1db0aa8c444c1ed750e1bda95891b04a79808 Cache mvnw (Tim Meehan)\n- dcbbe44166fcbca84b4219d97b5c6d738486f132 Fix Elasticsearch connector documentation (Zhenxiao Luo)\n- 8a1bb1bab7b143108df10cd1ac909d809d33fd20 Extend SHOW FUNCTIONS to display whether functions are built-in (Leiqing Cai)\n- c3b04323a4565c65805ec1c0e391cfb663498506 Extend SHOW FUNCTIONS to display whether functions have variable arity (Leiqing Cai)\n- 283e30075c7c9b1b3391a1747af39b0e51e48aa5 Fix testShowFunctions (Leiqing Cai)\n- b1cda9788996b492ae3fe2b1b72df227ad89d02f Upgrade slice to 0.38 (James Gill)\n- c4878275a2bead060da509a183de46b310c99e08 Improve error message for the array_agg function (Sujay Jain)\n- 6eed9b9b5eec8b134ed4868e82e226a1fce90396 Support mixed case fields in Elasticsearch (Zhenxiao Luo)\n- a9bbc6e38e5cfbbcbf63b766798ad41e80e096ad Set predicate in EMPTY_TABLE_LAYOUT to none (Bhavani Hari)\n- 5106dc1440b1ec4a660b752425ff5a44d9b15428 Add Alluxio Hive metastore (haoyuan)\n- ce5eeb77387c7ea9e76473d1e7d93ad06a95e2c0 Update error code when table is dropped during query execution (Vic Zhang)\n- 823da6f3e68121d8bbcbf310dffd8fa164e58cf8 Evaluate filter only once per dictionary value (Masha Basmanova)\n- d5ee715bb6b165fb6567b663c74ca5ecf21cc6ab Separate filter and no-filter code paths (Masha Basmanova)\n- ae5e74cf7ea9bef904b35bb17bb7afa82e003b7a Fix flaky TestRowExpressionPredicateExtractor (Ajay George)\n- cd41fa58c8975550329bf901378aa2d59079e813 Format comment and modify import style (wuyunfeng01)\n- ab55def6dfa025299e5e7cce6df04f1f7343de09 Promote Elasticsearch scan performance with index sorting (wuyunfeng01)\n- b34278e2c47569bdb0dafa7c41732bf27438df22 Properly handle optimizable filter expressions (Bhavani Hari)\n- fb37c6e90c132fe85e92d155797e34a73cc59af5 Add t-digest functions to Presto (Tim Meehan)\n- d09e4df79f5b7c122c70dca8d080f5be423ee59d Migrate function isHidden to new visibility enum (Tim Meehan)\n- 63cf099c741aff1ac4da41bda8e75b5f1d5adecd Allow FunctionNamespaceManager in DistributedQueryRunner (Rongrong Zhong)\n- 683474aaad754219fe59d85b81322d2fc54d72be Cache builtin function resolution result (Rongrong Zhong)\n- 2e96b922e26444027ca63ee6f2e16737517bfab7 Update access modifiers in BenchmarkSelectiveStreamReaders (Ying Su)\n- f555aa69f51685a737b66c76c798d93f097901b7 Avoid Java stream in NodePartitioningManager#createArbitraryBucketToNode (Wenlei Xie)\n- 20c7af79dc4dcb4e9b0dc7cc167a3506692c9bc3 Make acknowledgeResults to return future (James Sun)\n- 9d47a24d17a8078893e2531066906665053c0ce8 Add server info client side thrift implementation (James Sun)\n- 8ffd650b58d233d67fcff5948d5d3aca5b77039c Abstract RemoteNodeState HTTP implementation to HttpRemoteNodeState (James Sun)\n- 7416a8c0bf4f9ffb0bf09fc82a8b04427c59fd88 Add server info server side thrift implementation (James Sun)\n- 9d030b5a18304c84b459674d9c8732e58723fd93 Update NodeState with enum values (James Sun)\n- 8e44e9b83a87d98fa3d34eddbb20b7ee72cc4092 Add TestHiveDistributedQueriesWithThriftRpc (James Sun)\n- be031b148c7a32c19dca93dc0ac87eb8771648f5 Rewrite page too large exception based on network protocols (James Sun)\n- 785eb79149186413e0ea5da1d260ff8a1c821acc Add thrift support for v1/task/results (James Sun)\n- a2e20c7afa4a44ed0cd1401c631be3913895fa79 Add thrift communication protocol config (James Sun)\n- 6c9afc4ff3851d914a7b3bc4df9203d270e5e53c Add ThriftRpcShuffleClient implementation (James Sun)\n- 2f18683f4aab3051e47123a158f95b299bd859a1 Abstract exchange HTTP client to RpcShuffleClient (James Sun)\n- 5e8cae87adf74e8668b2ff0b793a5e9b3cdc2cf8 Rename HttpPageBufferClient to PageBufferClient (James Sun)\n- f3443e717e0fd4d6b7c54aefbc3faf242d800773 Add thrift support for exchange server (James Sun)\n- 125d77a9a0d3acc97e26568b0889b4eb88ecf1f5 Announce thrift port for PrestoServer (James Sun)\n- 1f28b21d7d66e963fabcaa2b2b49cbf45695b76b Initial support with Thrift RPC (Wenlei Xie)\n- 4b944705a8babd518e67b23507e74e85e3974341 Strip netty dependency from presto-cassandra (James Sun)\n- a49a812ac05cc77f8ec0360a8c79e55f8a75b505 Remove netty dependency from presto-mongodb (James Sun)\n- 0c28697354545613c1f3fb6babf1be5d886caea6 Strip netty dependency from presto-elasticsearch (James Sun)\n- 53e3648980aedbd9e5686b1fd79f283414c1d221 Fix null pointer when using ConnectorSession (Ke Wang)\n- c0fe70217928dd65930c5bfd15e6c785eacecc98 Make presto-spark extensible to additional modules (Wenlei Xie)\n- e64c4e26df7691fa819a804a90b52ff992bdff46 Propagate legacyMapSubscript to selective map reader (Masha Basmanova)\n- b7a74a2a9b68b9078724b3c73e70c4c3a2723d9b Add legacyMapSubscript to SqlFunctionProperties (Masha Basmanova)\n- 500c6baf578148ec102f7d8c1bae3656f0a8eb7f Add legacy_map_subscript hidden session property (Masha Basmanova)\n- dcf7f66a823cd0fb85c7193749bce63e65302524 Add comment to ActualProperties.Global.nullsAndAnyReplicated (Ariel Weisberg)\n- 9218834e5f06ed251282ac4628a3ef849460c38c Make PreferredProperties.mergeWithParent methods private (Ariel Weisberg)\n- 9a68dfeeeffc7e4a8f4abc4f1e62dc354ef2ae7f Add partitioning_precision_strategy session property (Ariel Weisberg)\n- 652ca6aff79600a3a3c8c380855b296eb961f8d6 Fix Pinot segment page source to use ColumnDataType (Devesh Agrawal)\n- b6325d975cc2bcb387406fac9960334f27902fe8 Refactor ReorderJoins to use RowExpressions (Saksham Sachdev)\n- eabf46febfd10628c71a093d9818518f10594f31 Increase heap memory to avoid continuous Full GCs (Ajay George)\n- ff6dd730616962021be095fc0e9bde038f67fd81 Update to Airbase 98 and Aiftlift 0.188 (Leiqing Cai)\n- 32ca53947691b80ac3c5112862b5a1a66ad09e51 Configure DB Resource Group reload frequency (Swapnil Tailor)\n- 386164e888fdbf69cb735b10c942e23a3b2afca6 Support ALTER FUNCTION (Leiqing Cai)\n- 48b2fee2253ee5cdb7c7c55070f00307755e64e1 Add syntax support for ALTER FUNCTION (Leiqing Cai)\n- 68f2d6fb87b6be178a6c9e3c8301bd697c3cec25 Format parameter types in one line in DROP FUNCTION (Leiqing Cai)\n- 0278615238cc12b9e3119c96ade4fa0774ff8c0f Use builder to construct RoutineCharacteristics (Leiqing Cai)\n- 76ee53ce94713710e1a907443f28e6d8295ab205 Add end-to-end test for Presto on Spark using Docker (Andrii Rosa)\n- a7aa3425871eb771fc29a6d0a0f57169ca2c21dd Add Spark launcher (Andrii Rosa)\n- f0741135cf18bf235683b56f4937bef9fd0b0f9d Generate Presto on Spark tar.gz package (Andrii Rosa)\n- cb5a00d1e311536f864db0a6eeffc1d03dfee793 Presto on Spark initial commit (Andrii Rosa)\n- cc398f55f1b2f05ce84f0cdca20f549e134a4d3a Add benchmark for BytesValues (Ke Wang)\n- fc695e0c85ab532e9abe0c673d15fbe66d00bfe0 Optimize NOT IN filter for strings (Ke Wang)\n- 422d7a9223c0ebfd40188cd5724d80e6529b57fb Rename isNanAllowed function and update comment (Ke Wang)\n- 479e6552eff9f6e1776e3f40913d053e934241c5 Enable listing SQL functions with session property (Leiqing Cai)", "NaN"], ["14131", "Use concurrent scheduler for timeout executor", "Vic Zhang", "viczhang861", "02/25/20, 07:30:59 PM", "To avoid contention during task scheduling.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n*  Add configuration ``task.http-timeout-concurrency`` to improve performance of task timeout executors\r", "NaN"], ["14133", "Disable hanging test in TestDistributedSpilledQueries", "Leiqing Cai", "caithagoras", "02/21/20, 11:32:43 PM", "```\r\n2020-02-21T00:23:33.405-0600 WARNING No test started or completed in 8.00m. Running tests:\r\ncom.facebook.presto.tests.TestDistributedSpilledQueries::testJoinDoubleClauseWithRightOverlap running for 40.42m\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14134", "Fix checksum queries for columns of RowType", "Leiqing Cai", "caithagoras", "02/21/20, 07:31:24 PM", "Checksum query would fail with `SYNTAX_ERROR` for certain queries generating RowType columns.\r\n```\r\nCaused by: com.facebook.presto.sql.parser.ParsingException:\r\nline 475:61: mismatched input 'group'. Expecting: <identifier>\r\n```\r\n\r\nIf a field name of a row coincide with a reserved words (e.g. `r.group`), the generate checksum query would contain a deference expression `\"r\".group`, which is invalid. Instead `\"r\".\"group\"` should be used.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Fix an issue where invalid checksum queries can be generated for certain queries containing columns of ``RowType``.\r\n```\r", "NaN"], ["14137", "Enable spatial join for spherical geography ST_Distance", "James Gill", "jagill", "03/25/20, 02:52:03 PM", "Previously, we did not extract a spatial join for any spherical\r\ncalculations.  This enables joins for `ST_Distance(geog1, geog2) < r`\r\nfor spherical geographies `geog1` and `geog2` for inner joins.\r\n\r\nCurrently `ST_Distance` for geographies only takes points.  Later\r\nwork can extend to other geographic objects, to outer-type joins,\r\nor to other functions (like `ST_Contains`).\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Enable spatial joins for `ST_Distance(p1, p2) < r` for spherical geography points `p1` and `p2`.\r\n```\r", "NaN"], ["14139", "Adding fnv1 and fnv1a hash functions", "Widagdo Setiawan", "widagdos", "02/28/20, 11:27:40 PM", "Fixes #12500.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Added fnv1_32, fnv1_64, fnv1a_32, fnv1a_64,\r\n\r\n```", "NaN"], ["14141", "Use HiveFileContext for FileOpener and FileDescriptorSource", "James Sun", "highker", "02/24/20, 06:25:11 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14142", "Fail task with large update size", "Nikhil Collooru", "NikhilCollooru", "02/23/20, 12:21:03 AM", "Fail the query when the task update size\r\nexceeds the limit set by the parameter\r\nexperimental.internal-communication.max-task-update-size.\r\n\r\nThis fixes issue #14129 \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14143", "Add stats and cost rule for IntersectNode", null, "ssaumitra", "05/01/20, 08:43:13 PM", "Adding stats and cost rule for IntersectNode, so that more efficient plan can be generated for the query containing intersect operator\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14144", "Provide additional details docs for Alluxio Connector", "Haoyuan Li", "haoyuan", "02/23/20, 02:44:01 AM", "```\r\n== NO RELEASE NOTE ==\r\n\r\n```\r", "NaN"], ["14146", "A quick and dirty fix of CTAS failures: unable to rename from viewfs", "Beinan", "beinan", "08/25/20, 10:20:08 PM", "Seeing the failure below when run CTAS on viewfs\r\n\r\n`Unable to rename from viewfs://hadoop-dw2-nn.smf1.twitter.com/user/beinanw/warehouse/aaa/.hive-staging/538ee740-6cce-46b7-96a7-ca0fbfc52148 to viewfs://hadoop-dw2-nn.smf1.twitter.com/user/beinanw/warehouse/aaa: target directory already exists`\r\n\r\nIt seems that presto creates the temporary folder - \".hive-staging/{id}\" just under the target folder, which would also create the target folder in advance if the target folder did not exist.  But for the new tables (such as the table crated by CTAS), the target folder should not be created until the prepareAddTable():SemiTransactionalHiveMetastore got called, otherwise we will see the failure above.\r\n\r\nTo fix this failure,  I make it check the existence of the target folder first.  If the target folder is not there, the temporary folder is created as {target_folder}/../.hive-staging/{id} rather than {target_folder}/.hive-staging/{id}.   In this case, the temporary folder will be created in the sibling folder of the target folder.  So the target folder won't be created in advance.\r\n\r\n\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14147", "Use absolute error when a floating point column mean is close to 0", "Leiqing Cai", "caithagoras", "02/25/20, 05:19:21 AM", "- Instead of requiring either the control mean or the test mean to be exactly 0, use the absolute error rule when at least one of them is close to 0.\r\n- When checking a floating point column using the absolute error rule, match if both are control mean and test mean are within the absolute margin, mismatch otherwise.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Improve correctness check for floating point columns whose mean values of either the control query or the test query is closed to 0.\r\n```\r", "NaN"], ["14148", "Failure resolver improvements", "Leiqing Cai", "caithagoras", "03/01/20, 04:26:38 AM", "Most of this PR is refactoring, with a few functional changes:\r\n- Each failures resolvers can be enabled and disabled individually.\r\n- Verification was skipped when a checksum query fails with `COMPILER_ERROR`. It is now marked as `FAILED_RESOLVED`.\r\n- Only control checksum query failed with `COMPILER_ERROR` will now gets resolved; test and determinism analysis checksum query will not.\r\n- Resolve message updated.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to disable individual failure resolvers (:pr:`14148`).\r\n* Add support to auto-resolve control checksum query failures with ``COMPILER_ERROR`,\r\n  instead of skipping the verification.\r\n```\r", "NaN"], ["14149", "Fix checksum column generation for map keys and map values", "Leiqing Cai", "caithagoras", "02/25/20, 12:08:02 AM", "Map keys and map values are both arrays. When checksuming an array,\r\nwe need to handle the case in which the array element is orderable                                                                                                                                                                                                       \r\nbut the sorting fails.\r\n\r\nThe sort of an array would fail in certain cases when the array\r\nelements are array with nulls or row with nulls. As an example,\r\narray_sort(ARRAY[ARRAY[null], ARRAY[1]]) would fail.\r\n\r\nThis has been properly handled in ArrayColumnValidator, so we should\r\napply the same logic to the keys checksum and the values checksum\r\nfor an map column.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix an issue where checksum query would fail for queries containing map columns whose key or value types are arrays or rows.\r\n```\r", "NaN"], ["14150", "Treat queriers referencing certain catalogs as non-deterministic", "Leiqing Cai", "caithagoras", "02/25/20, 05:36:36 AM", "We're seeing verification failure due to query reading from MySQL tables. We need a way to allow Verifier to mark them as non-deterministic if rerunning the control query does not identify the query as non-deterministic.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support for specifying non-deterministic catalogs by configuration property ``determinism.non-determinism-catalogs``.\r\nQueries explicitly referencing tables from those catalogs are treated as non-deterministic.\r\n```\r", "NaN"], ["14151", "Rename presto-hive-base to presto-hive-common", "Wenlei Xie", "wenleix", "02/25/20, 05:02:16 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14152", "Optimize configuration distribution in Presto on Spark", "Andrii Rosa", "arhimondr", "02/26/20, 02:58:21 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nRelated to https://github.com/prestodb/presto/issues/13856\r", "NaN"], ["14153", "Update Verifier documentation", "Leiqing Cai", "caithagoras", "02/28/20, 12:13:11 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14154", "Set airlift version to 0.190", "Trong Nguyen", "TrongDucNguyen", "02/27/20, 05:29:31 AM", "Set airlift version to 0.190", "NaN"], ["14155", "Predicate Pushdown for Druid connector", "Zhenxiao Luo", "zhenxiao", "02/26/20, 09:54:19 PM", "\r\n```\r\n== RELEASE NOTES ==\r\n\r\nDruid Changes\r\n* Add filter pushdown for Druid connector to improve query performance.\r\n```\r", "NaN"], ["14158", "Increase test runner heap size", "Andrii Rosa", "arhimondr", "02/25/20, 06:41:25 PM", "Lately we've been observing high GC pressure when running unit and\r\nintegration tests. Increasing heap size is supposed to mitigate this issue.\r\nTravis machines are equipped with 7.5GB of memory, thus the increase\r\nfrom 2GB to 4GB should be fine.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14159", "Minor refactor of array_position", "Rongrong Zhong", "rongrong", "02/26/20, 02:12:15 PM", "* Remove redundant logic to check null on equal method invocation\r\n* Clarify error message\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14160", "Use HiveFileContext for OrcFileTailSource", "Ke", "kewang1024", "02/26/20, 09:30:30 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14161", "Add classification_fallout function to FunctionManager", null, "bhhari", "02/26/20, 02:10:21 AM", "Before this change the function is not registered.\r\n\r\n```\r\n== RELEASE NOTE ==\r\nGeneral Changes\r\nFix function registration for :func:`classification_fallout function`\r\n\r\n```\r", "NaN"], ["14164", "Presto spark packaging improvements", "Andrii Rosa", "arhimondr", "02/26/20, 07:52:52 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\n\r\nRelated to https://github.com/prestodb/presto/issues/13856", "NaN"], ["14165", "Remove presto-thrift-connector-api's dependency on presto-main", "Rongrong Zhong", "rongrong", "02/26/20, 10:15:53 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nSPI changes\r\n* Move `JsonType` to SPI\r\n```", "NaN"], ["14167", "Optimize scan of integers with many nulls", "Maria Basmanova", "mbasmanova", "02/27/20, 02:59:53 PM", "Same optimization as #14122 applied to integers.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14169", "Streamline identity projections", "Maria Basmanova", "mbasmanova", "02/27/20, 03:02:06 PM", "Remove unnecessary copy and redirection.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14171", "Druid connector fix", "Zhenxiao Luo", "zhenxiao", "02/27/20, 07:13:47 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14172", "Use HiveFileContext for StripeMetadataSource", "Ke", "kewang1024", "02/28/20, 12:52:11 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14173", "Optimize initial batch size for scan", "Maria Basmanova", "mbasmanova", "03/11/20, 01:07:28 PM", "When all columns are of fixed width types, we can accurately estimate the\r\nsize of each row and calculate maximum number of rows that fit under\r\nhive.orc.max-read-block-size. This allows scan to produce full size pages\r\nfrom the start and avoids inefficiencies associated with processing small\r\npages.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14174", "Disable Cassandra and Mongo integration tests by default", "Leiqing Cai", "caithagoras", "02/28/20, 07:24:54 PM", "- Move some tests in `presto-cassandra` and `presto-mongodb` into a separate profile so that they don't gets run by default.\r\n- Enable those tests on Travis.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14175", "Disable failure detector for TestMemoryWorkerCrash", "Rebecca Schlussel", "rschlussel", "02/28/20, 02:55:56 PM", "It causes the test to be flaky\r\n\r\nFixes #14170 \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14176", "Improve determinism analysis of queries with top-level LIMIT clause", "Leiqing Cai", "caithagoras", "03/01/20, 04:26:21 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Fix incorrect decision for determinism analysis of queries with top-level ``LIMIT`` clause. (:pr:`14176`)\r\n* Improve query performance during determinism analysis of queries with top-level ``LIMIT`` clause.\r\n```\r", "NaN"], ["14177", "A new end point to get resource group stats", "cem cayiroglu", "cemcayiroglu", "03/26/20, 05:13:33 PM", "A load balancer can use resource group level stats to do better\r\nload balancing. The new endpoint returns a ResourceGroupInfo.\r\nResourceGroupInfo contains a list of ResourceGroupInfo's as its subtrees.\r\nResourceGroupInfo consists of basic stats (running and queued query counts)\r\nand actual queries as strings. This method leaves the queries empty\r\nsince it is not needed. The end method does not return the dynamically\r\ngenerated the resource groups.", "NaN"], ["14178", "Classify Invalid ORC file as an External Error", null, "bhhari", "03/05/20, 12:47:02 AM", "Before this change, if there is a mismatch in the outputType and streamDescriptor type, the resulting exception is classified as an Internal Error. This change updates it to external error with descriptive error message.\r\n\r\nFixes #14179\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14181", "Support determinism analysis for simple queries with top-level ORDER BY LIMIT clause", "Leiqing Cai", "caithagoras", "03/04/20, 02:36:39 AM", "Support determinism analysis for simple queries with top-level ORDER\r\nBY LIMIT clause. Select, Insert, and CreateTableAsSelect are supported,\r\nbut only when the query part is a Select query, not a SetOperation(\r\ni.e., Union, Intersect, and Except).\r\n\r\n`INSERT INTO ...SELECT ... ORDER BY ... LIMIT ...` is supported.\r\n`INSERT INTO ... SELECT ... UNION ALL ... SELECT ... ORDER BY ...                                                                                                                                                                           \r\nLIMIT ...` is not supported.\r\n\r\nTo check for determinism of ORDER BY LIMIT queries, we run the Select\r\nquery with limit N + 1, project all the necessary ORDER BY columns,\r\nand check whether there is a tie on the ORDER BY columns for the\r\nn-th row and the (n+1)-th row.\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Support determinism analysis for simple queries with top-level ``ORDER BY LIMIT`` clause. (:pr:`14181`)\r\n```\r", "NaN"], ["14182", "Refactor cacheable for BucketNodeMap", "Ke", "kewang1024", "03/02/20, 07:20:52 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14184", "Fix BenchmarkPartitionedOutputOperator", "Ying", "yingsu00", "03/06/20, 03:46:53 PM", "OptimizedPartitionedOutputFactory#createOutputOperator() now takes a new parameter `Optional<OutputPartitioning> outputPartitioning` and require it to be present. The change came from the \"Refactor LocalExecutionPlanner\" commit in https://github.com/prestodb/presto/pull/14099. However the call site in  BenchmarkPartitionedOutputOperator was passing in an empty `outputPartitioning`, which broke the benchmark with the following call stack:\r\n```\r\njava.lang.IllegalArgumentException: outputPartitioning is not present\r\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:141)\r\n\tat com.facebook.presto.operator.repartition.OptimizedPartitionedOutputOperator$OptimizedPartitionedOutputFactory.createOutputOperator(OptimizedPartitionedOutputOperator.java:286)\r\n\tat com.facebook.presto.operator.repartition.BenchmarkPartitionedOutputOperator$BenchmarkData.createOptimizedPartitionedOutputOperator(BenchmarkPartitionedOutputOperator.java:290)\r\n\tat com.facebook.presto.operator.repartition.BenchmarkPartitionedOutputOperator$BenchmarkData.access$000(BenchmarkPartitionedOutputOperator.java:127)\r\n\tat com.facebook.presto.operator.repartition.BenchmarkPartitionedOutputOperator.optimizedAddPage(BenchmarkPartitionedOutputOperator.java:119)\r\n\tat com.facebook.presto.operator.repartition.generated.BenchmarkPartitionedOutputOperator_optimizedAddPage_jmhTest.optimizedAddPage_avgt_jmhStub(BenchmarkPartitionedOutputOperator_optimizedAddPage_jmhTest.java:191)\r\n\tat com.facebook.presto.operator.repartition.generated.BenchmarkPartitionedOutputOperator_optimizedAddPage_jmhTest.optimizedAddPage_AverageTime(BenchmarkPartitionedOutputOperator_optimizedAddPage_jmhTest.java:154)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)\r\n\tat org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\nThis PR is to fix the problem.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14187", "Break presto-tests Travis jobs to reduce test times", null, "bhhari", "03/04/20, 07:13:15 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14189", "Disable Redis integration test by default", null, "sujay-jain", "03/03/20, 01:04:26 AM", "Move some tests in presto-redis into a separate profile so that they don't get run by default.\r\nEnable those tests on Travis.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14190", "Support ifNotExists when copying TPCH tables", "Leiqing Cai", "caithagoras", "03/02/20, 10:54:38 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14191", "Add tracking of memory allocation per operator", "Maria Basmanova", "mbasmanova", "03/05/20, 06:50:50 PM", "Here is a snippet of the query JSON fetched from the coordinator featuring new fields: addInput/getOutput/finishAllocation\r\n\r\n```\r\n  }, {\r\n      \"stageId\" : 1,\r\n      \"stageExecutionId\" : 0,\r\n      \"pipelineId\" : 1,\r\n      \"operatorId\" : 1,\r\n      \"planNodeId\" : \"4\",\r\n      \"operatorType\" : \"HashAggregationOperator\",\r\n      \"totalDrivers\" : 408,\r\n      \"addInputCalls\" : 1,\r\n      \"addInputWall\" : \"23.92ms\",\r\n      \"addInputCpu\" : \"22.66ms\",\r\n      \"addInputAllocation\" : \"3.64MB\",\r\n      \"rawInputDataSize\" : \"0B\",\r\n      \"rawInputPositions\" : 0,\r\n      \"inputDataSize\" : \"115B\",\r\n      \"inputPositions\" : 1,\r\n      \"sumSquaredInputPositions\" : 1.0,\r\n      \"getOutputCalls\" : 820,\r\n      \"getOutputWall\" : \"33.97ms\",\r\n      \"getOutputCpu\" : \"31.83ms\",\r\n      \"getOutputAllocation\" : \"376.24kB\",\r\n      \"outputDataSize\" : \"115B\",\r\n      \"outputPositions\" : 1,\r\n      \"physicalWrittenDataSize\" : \"0B\",\r\n      \"blockedWall\" : \"0.00ns\",\r\n      \"finishCalls\" : 411,\r\n      \"finishWall\" : \"16.55ms\",\r\n      \"finishCpu\" : \"15.69ms\",\r\n      \"finishAllocation\" : \"94.47kB\",\r\n      \"userMemoryReservation\" : \"0B\",\r\n      \"revocableMemoryReservation\" : \"0B\",\r\n      \"systemMemoryReservation\" : \"0B\",\r\n      \"peakUserMemoryReservation\" : \"387.73kB\",\r\n      \"peakSystemMemoryReservation\" : \"0B\",\r\n      \"peakTotalMemoryReservation\" : \"387.73kB\",\r\n      \"spilledDataSize\" : \"0B\"\r\n    }, {\r\n```\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14192", "Allow custom hive session in `AbstractTestHiveClient`", null, "mayankgarg1990", "03/03/20, 01:05:27 AM", "We need this hook to allow for an internal implementation of a connector to override the session properties.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14193", "Improve scale writer creation based on producer buffer", "Wenlei Xie", "wenleix", "03/06/20, 05:51:14 AM", "We observe two cases in production cause current scale writer heuristics\r\nnot able to scale:\r\n\r\n1. When there is skew on the producer side and more than half of the\r\nproducer buffer is not overutilized.\r\n2. When grouped execution is enabled and each bucket doesn't make the\r\nbuffer to be overutilized.\r\n\r\nThis commit tries to improve the situation by considering overall\r\nproducer buffer utilization when deciding scale the writers.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve the scale writer heuristics by considering overall producer buffer utilization. This can be enabled by using the session property `optimized_scale_writer_producer_buffer` and the configuration property `optimized-scale-writer-producer-buffer`.\r\n```\r", "NaN"], ["14194", "Keep queries in the queue when task count is high", "Vic Zhang", "viczhang861", "03/12/20, 12:19:16 AM", "Use total running task count as a proxy for cluster load and keep \r\nqueries in the queue if load is too high.\r\n\r\n- Attached figure shows the result from `getTaskLimitExceeded()` in a real cluster,  correlation of this chart with queue size makes debugging easier.\r\n\r\n![Screen Shot 2020-03-04 at 7 41 38 PM](https://user-images.githubusercontent.com/6372365/75936476-39e62480-5e50-11ea-9a70-95078c7289ef.png)\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for stopping new query execution when total number of tasks exceeds a set limit to help with reliability. The limit can be set by configuration property ``experimental.max-total-running-task-count-to-not-execute-new-query``.\r\n```\r\n\r", "NaN"], ["14195", "Wait for final task info on abort", "Andrii Rosa", "arhimondr", "03/04/20, 03:24:55 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14197", "Fix finishStatisticsCollection to have correct ConnectorSession", null, "mayankgarg1990", "03/04/20, 05:29:06 PM", "Without this fix, the ConnectionSession was created without any session properties and hence the implementation which tried to use session properties failed.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14198", "Materialize LazyBlock in scan", "Ying", "yingsu00", "03/04/20, 07:13:54 PM", "Fix the problem downstream operators might receive LazyBlock after scan\r\nintroduced by https://github.com/prestodb/presto/pull/14169\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14200", "Limit pushdown for Druid connector", "Zhenxiao Luo", "zhenxiao", "03/04/20, 10:57:32 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nDruid Changes\r\n* Add LIMIT evaluation pushdown to Druid connector\r\n```", "NaN"], ["14201", "Always use JSON for PlanFragment serialization", "Timothy Meehan", "tdcmeehan", "03/04/20, 05:02:16 AM", "SMILE support has flaky edge cases, and we cache the\r\nserialization now which already has reduced the cost.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14203", "Allow column pruning in SHOW STATS", "Maria Basmanova", "mbasmanova", "03/04/20, 10:05:32 PM", "Fixes #14202\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14204", "Compare type by (name,type) pair rather than (index,type) pair during Parquet's schema mismatch checking", "Beinan", "beinan", "04/21/20, 11:10:12 PM", "Regarding parquet's schema mismatch checking, the existing implementation is selecting each sub-field by its index, which might cause fake alert of mismatching. e.g.\r\nstruct<a bigint, b string>. might mismatch parquet's group type {b binary, a int64}, but actually they are the same type.\r\n\r\nSo we add a map of <name, type> to compare each sub-field by name->type pair rather than index->type pair.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14205", "Remove unused HiveFileContext in OrcReader", "James Sun", "highker", "03/05/20, 12:36:39 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14209", "Add nodeSelectionStats to monitor affinity scheduler", "Ke", "kewang1024", "03/07/20, 02:03:10 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14210", "Separate some modules to their own Travis jobs to reduce test times", null, "sujay-jain", "03/05/20, 01:53:11 AM", "```TEST_OTHER_MODULES``` job in travis is one of the longest running and often times out.\r\n\r\nThis PR separates ```presto-elasticsearch```, ```presto-orc``` and ```presto-thrift-connector``` into their own travis jobs to reduce the amount of time ```TEST_OTHER_MODULES``` job takes. \r\n\r\nOn test runs this reduced the run time of ```TEST_OTHER_MODULES``` from about 60 minutes to 25 minutes\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14212", "Remove version tags for presto projects from sub-module pom files", "Leiqing Cai", "caithagoras", "03/05/20, 03:06:02 PM", "We're seeing Nexus flakiness:\r\n```\r\norg.eclipse.aether.transfer.ArtifactNotFoundException: Could not find artifact\r\ncom.facebook.presto:presto-server:tar.gz:0.234-20200305.020634-32 in nexus\r\n\r\norg.eclipse.aether.transfer.ArtifactNotFoundException: Could not find artifact\r\ncom.facebook.presto:presto-spark-package:tar.gz:0.234-20200304.195247-22\r\n```\r\n\r\nIt's unclear to me why Maven are not useing locally built `tar.gz`, but is trying to download from Nexus instead. So this PR is only an attempt. Also, use `jar` instead of `tar.gz` for `presto-spark-package` in\r\n`presto-spark-testing` for the same purpose.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14213", "Split test-hive-materialized on Travis", "Leiqing Cai", "caithagoras", "03/05/20, 04:15:53 PM", "The Travis job used to take about 50 minutes.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14214", "Verifier fixes and improvements", "Leiqing Cai", "caithagoras", "03/31/20, 07:19:11 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Fix an issue where resubmitted queries always fail.\r\n* Add support to output verification results for failures due to Verifier internal errors.\r\n* Add support to skip teardown queries in case control and test queries succeeds but\r\nverification fails. This can be enabled by configuration property ``smart-teardown``, which replaces ``run-teardown-on-result-mismatch``.\r\n```", "NaN"], ["14215", "Add release notes for 0.233", "Leiqing Cai", "caithagoras", "03/16/20, 10:49:35 PM", "# Missing Release Notes\n## Bhavani Hari\n- [x] https://github.com/prestodb/presto/pull/14161 Add classification_fallout function to FunctionManager (Merged by: Rongrong Zhong)\n\n## Masha Basmanova\n- [x] https://github.com/jinyangzhen/presto/pull/1 latest presto changes (Merged by: \u661f\u79fb)\n\n## Sujay Jain\n- [x] https://github.com/prestodb/presto/pull/14087 Revert \"Improve error message for the array_agg function\" (Merged by: Wenlei Xie)\n\n## Tim Meehan\n- [x] 0208f7304805d5bb12062afc5e7e6b2d0036a11a Always use JSON for PlanFragment serialization\n\n## Ying Su\n- [x] 9c1a7fecb7a584fbc07b0bb2f7040d915a662ae9 Materialize LazyBlock in scan\n\n## Zhenxiao Luo\n- [x] https://github.com/prestodb/presto/pull/14073 Fix Druid connector pom to new release (Merged by: Zhenxiao Luo)\n\n# Extracted Release Notes\n- #13823 (Author: Yifeng Jiang): Extend ST_Points to support major Well-Known spatial objects\n  - Improve ST_Points to add support for major Well-Known spatial objects. ST_Points now supports POINT, LINESTRING, POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON and GEOMETRYCOLLECTION.\n- #13907 (Author: Vic Zhang): Support non-blocking IO for page transport\n  - Add configuration property `exchange.async-page-transport-enabled` to turn on non-blocking IO for page transport.\n  - Add configuration property `exchange.async-page-transport-timeout` for server side timeout.\n  - Use URL prefix '/v1/task/async' for async page transport request.\n- #13966 (Author: Ke Wang): Add ability of soft affinity node selection\n  - Introduce `NodeSelectionStrategy` with options `NO_PREFERENCE` to indicate data is remotely accessible from workers, `HARD_AFFINITY` to indicate data and workers are colocated, and `SOFT_AFFINITY` to indicate data is remotely accessible but scheduler will make the best effort to fetch the same piece of data from the same worker.\n  - Replace `isRemoteAccessible()` in `ConnectorSplit` with `getNodeSelectionStrategy()`. `isRemoteAccessible()` is true if and only if `getNodeSelectionStrategy()` returns `HARD_AFFINITY`.\n  - Replace `getAddresses()` in `ConnectorSplit` with `getPreferredNodes()`. The returned list of addresses hints the scheduler where to schedule splits.\n  - Add soft affinity scheduling. It makes the best effort to fetch the same piece of data from the same worker. If the preferred workers are too busy to handle more splits, it will fallback to random workers. The option is enabled by connector indicated by `getNodeSelectionStrategy()`.\n  - Add config `hive.node-selection-strategy` to choose `NodeSelectionStrategy`. When `SOFT_AFFINITY` is selected, scheduler will make the best effort to request the same worker to fetch the same file.\n  - Deprecate `hive.force-local-scheduling` config. It will be replaced by setting `hive.node-selection-strategy` to `HARD_AFFINITY`.\n- #13977 (Author: James Gill): Use JTS for ST_IsValid and ST_IsSimple\n  - ST_IsValid and ST_IsSimple adhere to the ISO/OGC standards more closely.  In particular, previously ST_IsValid and ST_IsSimple always gave the same answer; now they may be different. Users should check both IsValid and IsSimple to be sure their geometries are well-behaved.\n  - Geometry_invalid_reason will return different but semantically similar strings.  It will return the reason for invalidity if it exists; else it will return the reason for non-simplicity if it exists; else it will return null.\n- #14042 (Author: Zhenxiao Luo): Presto Druid Connector\n  - Add Druid Connector.\n- #14045 (Author: Zhenxiao Luo): Add Hive directory listing cache\n  - Add directory listing cache for Hive Connector.\n- #14054 (Author: Brendan Driscoll): Improve verification for map columns\n  - Add checks for keys, values, and cardinality sum when validating a map column.\n- #14076 (Author: James Sun): Fix like pattern interpretation\n  - Fix an optimizer failure introduced since 0.229, where a `LIKE` pattern can be deduced into a constant. For example, `col LIKE 'a' and col = 'b'`.\n- #14083 (Author: James Petty): Balance ArbitraryOutputbuffer distribution over clients\n  - Fix ArbitraryOutputBuffer to avoid skewing output data distribution.\n- #14090 (Author: Nikhil Collooru): Add IPPREFIX functions\n  - Add :func:`ip_subnet_min`, :func:`ip_subnet_max`, :func:`ip_subnet_range`, and :func:`is_subnet_of` functions.\n- #14095 (Author: Ke Wang): Add soft affinity in scheduling bucketed split for Hive Connector\n  - Add parameter `NodeSelectionStrategy nodeSelectionStrategy` in method `createBucketNodeMap` in `ConnectorBucketNodeMap `, indicating which affinity strategy to use when we create bucket nodeMap.\n  - Add parameter `List<Node> sortedNodes` in method `getBucketNodeMap` in `ConnectorNodePartitioningProvider `, providing a sorted node list for connector to choose from when doing affinity scheduling.\n  - Add soft affinity scheduling to bucketed split. It makes the best effort to fetch the same bucket data from the same worker. When using dynamic group scheduling, if the preferred workers are unavailable to handle the specific bucket splits, it will fallback to random workers.\n- #14096 (Author: James Petty): Remove unnecessary copies in StreamingAggregationOperator\n  - Improve performance of StreamingAggregationOperator.\n- #14098 (Author: Andrew Smith): Provide view metadata to the Hive Metastore\n  - Store column names and types for views in the metastore.  Views in the Hive connector can now only use types supported by Hive.\n  - Change signature for createView in ConnectorMetadata to take a ConnectorTableMetadata instead of a SchemaTableName.\n- #14103 (Author: Wenlei Xie): Allow insert overwrite as default behavior for Hive connector\n  - Introduce  a new Hive client configuration`hive.insert-overwrite-immutable-partitions-enabled` to allow admin set insert overwrite as the default insertion behavior for Hive connector.\n- #14106 (Author: James Gill): Add cheap ST_Intersection method for certain cases\n  - When calculating `ST_Intersection` of a geometry with an enclosing envelope, just return the geometry.  This reduces the cost by up to 10^5x or more for complex polygons when this occurs.\n- #14128 (Author: Zac Blanco): Add HadoopDefaultConfigurationUpdater\n  - Allow server to start when `hive.s3-file-system-type` is set to HADOOP_DEFAULT.\n- #14131 (Author: Vic Zhang): Use concurrent scheduler for timeout executor\n  - Add configuration ``task.http-timeout-concurrency`` to improve performance of task timeout executors.\n- #14134 (Author: Leiqing Cai): Fix checksum queries for columns of RowType\n  - Fix an issue where invalid checksum queries can be generated for certain queries containing columns of ``RowType``.\n- #14139 (Author: Widagdo Setiawan): Adding fnv1 and fnv1a hash functions\n  - Added fnv1_32, fnv1_64, fnv1a_32, fnv1a_64,.\n- #14147 (Author: Leiqing Cai): Use absolute error when a floating point column mean is close to 0\n  - Improve correctness check for floating point columns whose mean values of either the control query or the test query is closed to 0.\n- #14148 (Author: Leiqing Cai): Failure resolver improvements\n  - Add support to disable individual failure resolvers (:pr:`14148`).\n  - Add support to auto-resolve control checksum query failures with ``COMPILER_ERROR`, instead of skipping the verification.\n- #14149 (Author: Leiqing Cai): Fix checksum column generation for map keys and map values\n  - Fix an issue where checksum query would fail for queries containing map columns whose key or value types are arrays or rows.\n- #14150 (Author: Leiqing Cai): Treat queriers referencing certain catalogs as non-deterministic\n  - Add support for specifying non-deterministic catalogs by configuration property ``determinism.non-determinism-catalogs``. Queries explicitly referencing tables from those catalogs are treated as non-deterministic.\n- #14155 (Author: Zhenxiao Luo): Predicate Pushdown for Druid connector\n  - Add filter pushdown for Druid connector to improve query performance.\n- #14165 (Author: Rongrong Zhong): Remove presto-thrift-connector-api's dependency on presto-main\n  - Move `JsonType` to SPI.\n- #14176 (Author: Leiqing Cai): Improve determinism analysis of queries with top-level LIMIT clause\n  - Fix incorrect decision for determinism analysis of queries with top-level ``LIMIT`` clause. (:pr:`14176`).\n  - Improve query performance during determinism analysis of queries with top-level ``LIMIT`` clause.\n\n# All Commits\n- 9c1a7fecb7a584fbc07b0bb2f7040d915a662ae9 Materialize LazyBlock in scan (Ying Su)\n- 0208f7304805d5bb12062afc5e7e6b2d0036a11a Always use JSON for PlanFragment serialization (Tim Meehan)\n- f39f9ab2dda1c1671e26cd93ca4da4896e6c35c9 Add configuration for async page transport timeout (Vic Zhang)\n- 2eee499a2b13d92ce606aa4640628e2917771fd5 Create new URL for async page transport (Vic Zhang)\n- 8b359693130dba3672bbeb895ecd4aeb4bd49689 Support non-blocking IO for page transport (Vic Zhang)\n- 3952dce0a018fc1ffadea34a7a0f476068c8c999 Allow custom hive session in `AbstractTestHiveClient` (Mayank Garg)\n- 6642993bfeba63b6a4278e9789a5f9c471285a80 Disable Redis integration test by default (Sujay Jain)\n- efa663226fea916904b189ed045417e2cdaf1e70 Support ifNotExists when copying TPCH tables (Leiqing Cai)\n- d52db96c0526f1dab2d0e7a03ba9f465763975ed Refactor cacheable for BucketNodeMap (Ke Wang)\n- 66e4101f3bc88a92dd78827148828d5f51ac5c1f Resolve checksum query compiler error instead of skipping it (Leiqing Cai)\n- c5e0c03fc747996d83db0386023d67f51be36a00 Distinguish between control, test, and determinism analysis checksum (Leiqing Cai)\n- 03ff7c5b5924c22914cfb359bb886b17317585cc Allow failure resolvers to be disabled individually (Leiqing Cai)\n- e56754e134fdeb188231609389e5f2207017fa66 Remove AbstractPrestoQueryFailureResolver (Leiqing Cai)\n- 7c36df95506c191c4f7a0ac7dce36387a71c6d69 Separate QueryException into two classes (Leiqing Cai)\n- 3f980812cc0096f3bfce55de7c1656f4044ec3f3 Fix and improve performance for limit query determinism analysis (Leiqing Cai)\n- f083d2ab4f3d58ae6e5dcae4cded95c1c063132a Restructure LimitQueryDeterminismAnalyzer (Leiqing Cai)\n- 778e885e0818ac1a857071277142611173e637ba Adding fnv1 and fnv1a hash functions (Widagdo Setiawan)\n- d8eb92bdfac5f592e7fde20e527921a8fe764fb8 Assign HiveFileContext values from splitContext (Ke Wang)\n- 8d4a18b5f8975d8817ebc09c0fba36eb0be74659 Fix SimpleNodeSelector node selection for HARD AFFINITY (Ke Wang)\n- 58df37881b5e3fac8305093df1c355de49a7f6eb Introduce cacheable for bucketed splits (Ke Wang)\n- e1e16b88a54459928a557e8abe523790c671b63a Fix generated nodeList when using DynamicBucketNodeMap (Ke Wang)\n- 342afc8ef0cba8ed2c95f00bd8e1ce952e8ab484 Decide cacheable when scheduling unbucketed splits (Ke Wang)\n- ae2af70730697a1e80d7c31dcc3c0ad1288b9549 Add SplitContext in SPI ConnectorPageSourceProvider (Ke Wang)\n- 0cc2047c94062466cd37008735522b03e896f0eb Introduce SPI SplitContext in Split (Ke Wang)\n- 070670eed066c0d0fe300ddd63d8d57300586a26 Disable Cassandra and Mongo integration tests (Leiqing Cai)\n- e6b43bf7aa6b3ec4257a3d9a0a20a19d6c0b3f08 Disable failure detector for TestMemoryWorkerCrash (Rebecca Schlussel)\n- 7809294e4799cc7419826712e24ce8c1214e1299 Use HiveFileContext for StripeMetadataSource (Ke Wang)\n- d9eb1930ac964579ed173e370eacc71c4f3afc0a Update Verifier documentation (Leiqing Cai)\n- ca8d9c84873245aae674dd0c20a35910f6fa4868 Add toString() to BlockEncodingBuffers and DecodedBlockNode (Ying Su)\n- 9c842b7a15dfef356b0d128aadb1b90a30333c81 Add toString() to ColumnarXXX (Ying Su)\n- 7218b40c97a87ae8263d4b0e732634ffc7f4054a Improve toString() for DictionaryBlock and RunLengthEncodedBlock (Ying Su)\n- 16d300893cddcf97697d44712cd1baa9a37c18fc Streamline identity projections (Masha Basmanova)\n- e18b45a8a1059b87917f0699a317ff4508d4d358 Optimize scan of integers with many nulls (Masha Basmanova)\n- 74c874ffc6464e35c9404defe9d80bfa81043afc Extract readNoFilter and readWithFilter methods (Masha Basmanova)\n- 816efa9cec39c7f8ca68808dc23f1bef3a289193 Fix predicate pushdown condition for Druid connector (Zhenxiao Luo)\n- fcbfc61dd0535437717daef06e265c691dc220ba Fix Druid DeterminismEvaluator injector (Zhenxiao Luo)\n- 5f7e1871b9bc45e5570d4ccc3c11ea3e1f600d7a Cache PlanFragment serialization (Tim Meehan)\n- fedef207e2e66645858728ff5228ad1b7a2b6324 Add fromBytes to Codec (Tim Meehan)\n- 50ddaa4d7c0b1c01c8dfb7a7683c752062c09e86 Remove presto-thrift-connector-spi to presto-main (Rongrong Zhong)\n- f5e4998454cae78565ae58d9e488e7b5e4491e0a Move JsonType to presto-spi (Rongrong Zhong)\n- d2cb512894abe9f6f47da5469e1bc1458e0836f8 Code cleanup for Druid predicate pushdown (Zhenxiao Luo)\n- 63c5777de36b33ea4ec3af4d8f97164451e683ff Get rid of presto util dependency in Druid connector (Zhenxiao Luo)\n- 88cdbdfb7c73bcbc74f6299180814ffb32bcb75a Add Testcase for Druid connector (Zhenxiao Luo)\n- 72ca98f4d0c287a6a14a2181dfcf5a1e07672cd9 Predicate Pushdown for Druid connector (Zhenxiao Luo)\n- bbd61d87823c4bea8c27ca848dfa279fcc963be1 Druid Connector PlanOptimizer (Zhenxiao Luo)\n- 9be680beedfa4a169c31512f7dd14e2a934f9b24 Remove unnecessary maven-shade-plugin run from presto-spark-base (Andrii Rosa)\n- 93dfdf6b0cd5394e35767363ecc1ff995dc2cc20 Reduce presto-spark-classloader-interface scope in presto-spark-base (Andrii Rosa)\n- 5c305b4783f01a6a9e225eff233931d4a234dcd5 Remove dependencies from the shaded presto-launcher pom (Andrii Rosa)\n- a0be44c38a9767a426aad781c5cea926ef67ef5b Provide view metadata to the Hive Metastore (Andrew Smith)\n- bce54d8ce587e2286e09cf9164414f595c18a3f8 Add HadoopDefaultConfigurationUpdater (Zac Blanco)\n- 7b8c2dabf723afba6309e5eb189b088e16b5b2c9 Make sure spark service is created only once (Andrii Rosa)\n- 0f1ad662329cd49a7c92f165f19c1644c410284d Optimize presto on spark configuration deployment (Andrii Rosa)\n- 8cfcd15b29db39dcbc8d2cb207d56a0a6c6ecc2d Minor refactor of array_position (Rongrong Zhong)\n- e20ca08b3d136093e0eca2de1368326c3767a84c Use HiveFileContext for OrcFileTailSource (Ke Wang)\n- 7ece9f70876a1fadd709212c2cd0ab412aea4f73 Add classification_fallout function to FunctionManager (Bhavani Hari)\n- 6eb0293532a3151eb4818efdfec8c4b264a7a538 Implement row base exchange in Presto on Spark (Andrii Rosa)\n- 2fd3fe8b20200317abf7e5fd28a8121314d209cc Refactor RemoteSourceFactory (Andrii Rosa)\n- ad6a5b6a937ce81301ad30b1efc3bfd537dd5476 Refactor LocalExecutionPlanner (Andrii Rosa)\n- 97817e0da533b286dbc4786290a1f6597dcc6160 Implement serialize/deserializePosition in Block/BlockBuilder (Andrii Rosa)\n- f1e0a8033002c03c36d963f1c499e25b8128c154 Create normal block with all null positions in createAllNullsBlock (Andrii Rosa)\n- 20aa3228be1317c74659be6a57003b79e738ae55 Use concurrent scheduler for timeout executor (Vic Zhang)\n- f4bc471446a8b52d694b033011bbc057719f1c6c Use JTS for ST_IsValid, ST_IsSimple, and geometry_invalid_reason (James Gill)\n- f6534c749ee6325464fc73eb0e7ec5a156eea927 Separate testing ST_IsValid from geometry_invalid_reason (James Gill)\n- 4e81ed139235cf0599b9d17565524b3c1b24ca7e Increase test runner heap size (Andrii Rosa)\n- 5455da8fdbeaf85430cb7a87430f76872550de40 Treat queriers referencing certain catalogs as non-deterministic (Leiqing Cai)\n- d5dfeba5d754ddf4390e0405a3e232feaa6069ed Extract determinism analyzer to a separate class (Leiqing Cai)\n- fb05233667d336bc31ab6a65601e9eb2d0fde5b2 Use absolute error when a floating point column mean is close to 0 (Leiqing Cai)\n- 0d66630de79c723625628c4441a1ddd3ba58f550 Rename presto-hive-base to presto-hive-common (Wenlei Xie)\n- 4236e477c986a3c9137098c2940758d24fb4dfcd Remove redundant default constructors in column validators (Leiqing Cai)\n- 0edd3613133ec6acdb230bcf78c47347d59d60cf Fix checksum column generation for map keys and map values (Leiqing Cai)\n- 2fb54ed0dc380791876810237c116f587efc2470 Support soft affinity in DynamicLifespanScheduler (Ke Wang)\n- df528a013e5b622157c89435896adac045dc17fa Support soft affinity for bucketed splits (Ke Wang)\n- 951c5653da4d3b7fb9c925f123d264a93daaba9d Use HiveFileContext for FileOpener and FileDescriptorSource (James Sun)\n- 1e7780e7a151f43b3107267b0bdedde0686ce511 Optimize aggregation of nulls (Masha Basmanova)\n- c756603f0768c807c4cac83cee009853f00136a4 Add position to 'position is not valid' error message (Masha Basmanova)\n- 0ed6c7d1a22db120d340a5479bba8321fff7467c Provide additional details and docs for Alluxio (Haoyuan Li)\n- 697eaea344c736efc9ced695635c5f2b598554e9 Fail task with large update size (Nikhil Collooru)\n- 3994a9de7c5bc4300fd3efe89517cc69cb242a06 Create empty blocks when there is a mismatch in struct schema (Bhavani Hari)\n- fec2fc15767c58d5c537679ed4bacd8c8cc8965d Disable hanging test in TestDistributedSpilledQueries (Leiqing Cai)\n- adfbe159f104d8010986f5cbeb308a2c20b3b491 Fix checksum queries for columns of RowType (Leiqing Cai)\n- 63f524003aedf6dd866cda90582b677b958197ef Use consistent naming convension for checksum column aliases (Leiqing Cai)\n- a553d1cfb5a7df64e761f50b5e995e6152589607 Optimize scan of filter-only no-nulls integers (Masha Basmanova)\n- 1b846503529114663fe5de5d0291bbf3f6c671df Optimize scan of floats with many nulls (Masha Basmanova)\n- 6647e13f64883f7cfa89221d91b981bcc3a57618 Create presto-hive-base package (Shixuan Fan)\n- e0d58cdb37afaddcc09f1201dcd0f7cdc89fb7d0 Add IPPREFIX functions (Nikhil Collooru)\n- 99b0ac3ee5beadd958a8bab1e216361d928f4709 Update Airlift to 0.190 (Vic Zhang)\n- 768b4a0fe9e9e37373a2b122d38d1c4ace967a99 Fix hash function for soft affnity in Hive (Ke Wang)\n- 8e07e49f17ffcdbb84ad4f283ed332029ce4e4d4 Add Alluxio Catalog Service doc (Haoyuan Li)\n- bfb18d6c21999f7fe9cb6fb9822af3ef1b6003a1 Add cheap ST_Intersection method for certain cases (James A. Gill)\n- 3c674caef99f98a54a9acff406209505111bbfc6 Add Benchmark tests for ST_Intersection (James A. Gill)\n- c49add003960896d25186f1fd0af9f13e9db15b3 Fix null pointer when creating ConnectorSession for LocalExchange (Ke Wang)\n- 0529b4075be3e9e675719951c5b967fe95a9f3d8 Do not run docker based spark integration tests by default (Andrii Rosa)\n- 6e24976c079481a4dd7d0220c1d2c6f3603b9daf Improve verification for map columns (Brendan Driscoll)\n- 685303098bd3933e1669ca6ea045d92671df9a00 Add SampleNode stats and cost rule (Saumitra Shahapure)\n- 4a914eb7512ee48699a2493c6604f232ce40384d Balance ArbitraryOutputbuffer distribution over clients (James Petty)\n- a5195e88e0ee1fda7a47da977c0d81661e81e178 Allow insert overwrite as default behavior for Hive connector (Wenlei Xie)\n- 5e4bfdf00a98046306a7d7057a352986ff290c2b Use INFO level log for Presto-on-Spark plan (Wenlei Xie)\n- 68170388b2a40bd67d2a62029f756250941bfea1 Allow create table with customized table properties (James Sun)\n- 22bcbbccb5fc1888dbde5d90dffc3c946a2d92cc Move Raptor metadata related singletons to metastore module (James Sun)\n- ccbb3bf45b2c27ef75e470a211ad1a9c11c15d7e Remove forceLocalScheduling for hive connector (Ke Wang)\n- 047b4c921bd5e352f0ed83aef540d16e38ba787e Add nodeSelectionStrategy to getInfo in HiveSplit (Ke Wang)\n- da1161dcdd4e407636951b275aa4117fb485f5b7 Add simple soft affinity support for HiveSplit (Ke Wang)\n- 55c5add33b008701f3511f74a3cba0bbbe81a0e1 Add SoftAffinityNodeSelection ability for unbucketed splits (Ke Wang)\n- 76b4087fd9decdd32c79c79f7677d3048c1de38b Extract RandomNodeSelection logic from SimpleNodeSelector (Ke Wang)\n- 7fd2c9a1c9d4255982fdb7724d6953ddc9a3f0b3 Refactor getAddresses in SPI ConnectorSplit (Ke Wang)\n- a7c78f272b7ecb8f1f67679cbfd7806ac8ed0b3f Refactor isRemotelyAccessible in SPI ConnectorSplit (Ke Wang)\n- a2d55c1deb74f2ababde29da804d235d44dccda8 Add nodeSelectionStrategy in Hive ClientConfig and SessionProperties (Ke Wang)\n- e53af1ad6029c15c24ba2583569cda23625233d5 Move node select related class to nodeSelection package (Ke Wang)\n- 85b45fb9a0ee676f8ae6434eb8efaf78f35d8bbe Extend ST_Points to support major Well-Known spatial objects (Yifeng Jiang)\n- 980a75c937978951ddfa0c304d3cf1eab200cdaf Increase Heap size to fix test timeouts (Bhavani Hari)\n- 26be3fde2dfa3042fd50389f1f4aee7bba2724d5 Run presto spark tests in forked VM (Andrii Rosa)\n- 4ae5d1ecaf56ad7ef3d07bd86090eec806a9cf23 Remove unnecessary copies in StreamingAggregationOperator (James Petty)\n- ceb5784d26c2128944c617fb4a24b01539f42d02 Implement CachingDirectoryListor with delegation (Zhenxiao Luo)\n- f0b57fe7a09ce2b5bccfc1af99246ab26afbd8d3 Make file status cache disabled by default (Zhenxiao Luo)\n- 954f1ce55d1941c63ac3a9a30cd215db4c91da5a Add Hive directory listing cache (qqibrow)\n- ecb9167d40630f173625082a14d3a0b694c3ce4a Fix like pattern interpretation (James Sun)\n- 52786cffbe0dba09a1d0b5c99e4a643147a4d533 Strip more dependencies from Hudi (Bhavani Sudha Saktheeswaran)\n- d0973099fdeb38310f934beee96bc7383e4bd5aa Add getPartitionNames support to Alluxio Metastore (Haoyuan Li)\n- 4ce85cf09d662b059ec3666ce6614b31d18d11ee Revert \"Improve error message for the array_agg function\" (sujay-jain)\n- e06784343cecc5ba65dd381df4a58b6b857c3a21 Strip httpcomponents dependency from Hudi (James Sun)\n- 419ba19044446634a168259ab6fe625fcf09126a Disable too-many-stages warning for exchange materialization (Wenlei Xie)\n- 9fd2459d98efd0809023b175ba53775466b74cc6 Support PathFilter when fetching splits for Hudi tables (Bhavani Sudha Saktheeswaran)\n- a5a93473d534ae89d55a802844aecd27d2bb00fe Fix hostname related assertion failure in PrestoSparkQueryRunner (Andrii Rosa)\n- 303e640f5da27a8c3785aae2f4129fa2293d062f Fix Druid connector pom to new release (Zhenxiao Luo)\n- 687f0b4f53c4ee37efe033374a25eb86f3876018 Fix Druid ErrorCode (Zhenxiao Luo)\n- 905778ab6c4972c310d9ca7ceac1466a67004ce5 Remove unused files and comments from druid connector (Zhenxiao Luo)\n- f0145ee9333397dce631559374858ad0eb9ccd4e Druid connector schema configurable (Zhenxiao Luo)\n- 8c8c2aa197e19c0968679a457b044f7671b2a85c Add Druid connector documentation (Zhenxiao Luo)\n- bf91489756cd4040138f26a6c8a5bc73e703b2c0 Fix getReadTimeNanos for DruidSegmentPageSource (Zhenxiao Luo)\n- 92b1f31928f09cc06336a77e2044f24333515e2a Fix Druid 0.17.0 index generation (Zhenxiao Luo)\n- a53178d7041549e9528ffcf5b41bd2c0b86653e8 Fix Druid metadata Json format (Zhenxiao Luo)\n- ae7489eb2ada392f500a0a525efbef77ef55d99f Add DruidQueryRunner (Zhenxiao Luo)\n- 9fcd940706624de7f4936b7f198062f0af0c6eb1 Remove unused variable and methods from druid connector (Zhenxiao Luo)\n- e745e1074580d9e1a760cea673f20e8d3e891c3c Upgrade to druid 0.17.0 (Zhenxiao Luo)\n- 895650fd26e7b16ef0f0b4050a58931461abed0b Fix coding style for Druid connector (Zhenxiao Luo)\n- d03ffc9bcae50c4a5d94b87c27784acf7cb2d679 Presto Druid Connector (Hao Luo)", "NaN"], ["14216", "Allow forcing streaming exchange for Mark Distinct", null, "aweisberg", "03/11/20, 10:02:31 PM", "Mark distinct is enabled by default and introduces a stage per distinct column in a query in order to redistribute the tracking of distinct values across the entire cluster. With materialized exchange the cost of these additional exchanges is higher. Enabling streaming exchange if there is sufficient memory available to perform the distincting will reduce the cost of these additional stages while still reducing the memory used by other by non-Mark Distinct stages.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Allow forcing streaming exchange for Mark Distinct using 'query.use-streaming-exchange-for-mark-distinct'\r\n```", "NaN"], ["14217", "Fix candiateNodes selection for Soft Affinity", "Ke", "kewang1024", "03/05/20, 11:30:28 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14219", "Add syntax support for external function", "Rongrong Zhong", "rongrong", "06/25/20, 01:08:19 AM", "#resolves #14218\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support to create external function (this does not include external function execution).\r\n```\r", "NaN"], ["14221", "Add @JsonProperty tag to ignoreNulls flag so it works in distributed \u2026", "Sreeni Viswanadha", "kaikalur", "03/12/20, 11:32:51 PM", "Add jsonproperty tag to ignoreNulls so it works in distributed execution as well.\r\n\r\n==  RELEASE NOTES ==\r\n\r\nFixes a bug in window functions with IGNORE NULLS not working properly in a distributed environment as the flag is not serialized propery.\r\n\r", "NaN"], ["14222", "Reduce positions array size in PartitionBuffer", "Ying", "yingsu00", "03/16/20, 06:28:56 PM", "Partially resolves https://github.com/prestodb/presto/issues/14162\r\n\r\nIn OptimizedPartitionedOutputOperator, the PagePartioner has \r\n`partitionCount` number of PartitionBuffer, and each PartitionBuffer \r\nhas a positions array to record the positions to be appended to this \r\nbuffer. Previously this positions array was initialized to be the \r\nincoming page's `positionCount` size. This could waste lots of memory\r\nsince each partition may only get a small portion of rows. This commit\r\nreduces the initial positions array size from `positionCount` to\r\n`min(positionCount, (positionCount / partitionCount + 1) * 2)` and\r\ngrow it on the fly.\r\n\r\nJMH benchmark shows about 8-10% gain in retained size (partitionCount = 256):\r\n\r\n\u00a0 | addPage | optimizedAddPage | optimizedAddPage_reducePositions | Gain%\r\n-- | -- | -- | -- | --\r\nBIGINT | 335,715,874 | 424,471,312 | 390,497,040 | 8%\r\nARRAY(BIGINT) | 327,680,906 | 294,847,046 | 270,506,390 | 8%\r\nROW(BIGINT,BIGINT) | 315,490,496 | 261,529,566 | 236,571,362 | 10%\r\n\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14223", "Harden blockEncodingBuffers in OptimizedPartitionedOutputOperator", "Ying", "yingsu00", "03/11/20, 01:01:26 PM", "blockEncodingBuffers in OptimizedPartitionedOutputOperator#partitionBuffer is not supposed to be accessed by multiple threads, but it might be helpful to harden initializeBlockEncodingBuffers to make sure it never leaves blockEncodingBuffers partially initialized. This PR is to harden the blockEncodingBuffers when initializing them.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14224", "Aggregation Pushdown for Druid connector", "Zhenxiao Luo", "zhenxiao", "03/17/20, 07:24:48 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nDruid Changes\r\n* Aggregation Pushdown for Druid connector\r\n```\r", "NaN"], ["14225", "Optimize socket pooling", "Andrii Rosa", "arhimondr", "03/17/20, 12:43:02 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Optimize connection pooling to avoid running out of sockets in certain cases\r\n```\r", "NaN"], ["14227", "Remove workaround for ConcurrentMap.compute", "Rebecca Schlussel", "rschlussel", "07/01/20, 05:43:27 PM", "We have been using java 8 features for a long time, so we can remove this\r\nworkaround from PipelineContext.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14228", "Rename DruidConnectorPlanOptimizer to DruidPlanOptimizer", "James Sun", "highker", "03/10/20, 02:39:40 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14229", "Fix typo in NodeSelectionStats", "Shixuan Fan", "shixuan-fan", "03/10/20, 12:42:48 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14230", "Auto resolve time limit exceeded on checksum queries", "Leiqing Cai", "caithagoras", "03/10/20, 07:06:37 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to auto-resolve control check query failures due to ``EXCEEDED_TIME_LIMIT``.\r\n```\r", "NaN"], ["14233", "Fix reading a map column having negative integers as keys", null, "bhhari", "03/10/20, 02:07:05 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14234", "Reuse buffers in OptimizedPartitionedOutputOperator - part 1 serializedRowSizes, offsetsCopy, positions, mappedPositions and offsets", "Ying", "yingsu00", "03/23/20, 10:34:27 PM", "Partially resolves #14162\r\n\r\nOptimizedPartitionedOutputOperator persists a set of local buffers. For example, each PartitionBuffer has a serializedRowSizes array to hold the estimated sizes for each row. Each BlockEncodingBuffer has a offsetsCopy that holds the adjusted offsets for the whole block. These buffers can be reused under some circumstances. This PR reuses serializedRowSizes between different PartitionBuffers, reuse offsetsCopy between different BlockEncodingBuffers, reuse positions, mappedPositions and offsets between BlockEncodingBuffers. For pages with nested type columns like array, map and row with high cardinality, or the block is DictionaryBlock, these optimizations can save over 30% memory.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14237", "Fix ArrayBlockEncodingBuffer#getRetainedSizeInBytes()", "Ying", "yingsu00", "03/13/20, 12:13:33 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14240", "Connector document change", "Ke", "kewang1024", "03/17/20, 10:32:12 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14242", "Support other security mechanism in Presto-on-Spark", "Wenlei Xie", "wenleix", "03/24/20, 09:51:12 PM", "\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14244", "Add ExtendedFileSystem", "Rohit Jain", "jainxrohit", "04/07/20, 09:55:20 PM", "ExtendedFileSystem is an extension of the FileSystem, which supports additional APIs earlier supported by FileOpener and DirectoryLister.\r\nAll internal Filesystem used in the Presto is extended from the ExtendedFileSystem. \r\nFileOpener classes to support FileSystem operations have been removed.\r\nA necessary refactoring was required hence few classes have been moved to presto-hive-common.\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/906", "NaN"], ["14245", "Fix BlockFlattener when DictionaryBlock's idsOffset is nonzero", "Ying", "yingsu00", "03/15/20, 02:21:48 AM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14246", "Refactor PrestoSparkLauncher", "Andrii Rosa", "arhimondr", "03/16/20, 09:55:31 PM", "Add the PrestoSparkDistributionFactory interface that allows to\r\noverride package provider, spark context and configuration properties.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nRelated to https://github.com/prestodb/presto/issues/13856\r", "NaN"], ["14247", "Add toString() to SimpleArrayAllocator", "Ying", "yingsu00", "03/18/20, 02:54:01 PM", "Adding toString() to SimpleArrayAllocator to make debugging easier.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14248", "Add AccessControlContext to store client information for security purposes", "Nikhil Collooru", "NikhilCollooru", "03/26/20, 06:32:48 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n* All the methods in ``SystemAccessControl`` now take additional \r\n  parameter ``AccessControlContext context``\r\n\r\n```\r\n\r", "NaN"], ["14249", "Fix memory tracking for PartitionedOutputOperator", "Ying", "yingsu00", "03/16/20, 02:46:18 PM", "Previously the PartitionedOutputOperator tracks memory after finishing\r\nprocessing the page. This could cause severe under counting when the\r\npages are very large and flush every time because flush() resets the\r\npageBuilders. This commit reports the memory before flush() which is\r\nmore accurate. It also changes the counting from getSizeInBytes() to\r\ngetRetainedSizeInBytes() because the first was under counting for 20%.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14252", "Fix subscript expression optimization during subfield pushdown", null, "bhhari", "03/17/20, 09:49:05 PM", "The latest code in the PushdownSubFields optimizes the subscript expression before creating the subfields. Therefore there are some cases where the optimization results in null in case of maps.\r\nSo when we try to extract the subfield with null it results in an NPE. Adding a `null` check to the index optimized expression resolves the issue.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r", "NaN"], ["14255", "Relax length limit of parameter type list and return type", "Leiqing Cai", "caithagoras", "04/08/20, 07:34:30 AM", "Relax the length of the parameter type list and the return type of\r\na CREATE FUNCTION statement to 30k to support functions with\r\ncomplex row types as the input parameters and the return value.\r\n\r\nTo support this:\r\n- function_id may have a length over 30k characters, and must be\r\n  stored as text, and can no longer be part of an index. Instead,\r\n  store its hash value in the MySQL table as well and use the hash\r\n  in the index.\r\n- parameter is the JSON representation of the input parameters, which may\r\n  have a length over 2^16, and must be stored as mediumtext.\r\n- return_type may have a length over 30k characters, and must be\r\n  stored as text.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve ``CREATE FUNCTION`` to allow parameter type list and return type to have a length\r\n  up to 30k characters.\r\n```", "NaN"], ["14259", "Add JMH based benchmarks for Parquet reader", "Venki Korukanti", "vkorukanti", "03/18/20, 01:14:37 AM", "Please make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14260", "Disable Presto-on-Spark query stats collection", "Wenlei Xie", "wenleix", "03/17/20, 08:27:22 PM", "It is not working as expected at this moment. \r\n- Task stats collection shows as unreadable bytes on Spark UI\r\n- Query info collection might cause excessive memory usage on Driver. \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\n\r\nMinor improvement to https://github.com/prestodb/presto/issues/13856", "NaN"], ["14261", "Add JSON format for EXPLAIN with type LOGICAL and DISTRIBUTED", "Chyi-Kwei Yau", "chyikwei", "06/03/20, 01:03:04 PM", "Add Json format for `EXPLAIN` with type `LOGICAL` and `DISTRIBUTED`. (Issue #11189)\r\n\r\n- for both Logical and distributed type, I use `JsonRenderedNode` in  JsonRenderer ([link](https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/sql/planner/planPrinter/JsonRenderer.java#L57-L111))\r\n\r\n- Distributed plan is using sorted map base on [QueryMonitor](https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/event/QueryMonitor.java#L328). (Also moved `JsonPlanFragment` from QueryMonitor to JsonRenderer.)\r\n\r\n- sample output in [this gist](https://gist.github.com/chyikwei/10fda9fe1bbb31f16ecfac5ff9b59938)\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add JSON format for `EXPLAIN` with type `LOGICAL` and `DISTRIBUTED`.\r", "NaN"], ["14262", "Avoid Exceptions when closing queryRunner", "Leiqing Cai", "caithagoras", "03/18/20, 07:32:50 PM", "This is to deflake `TestQueuesDb` and `TestQueues`.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14264", "Set use-legacy-scheduler true by default", "Rebecca Schlussel", "rschlussel", "03/19/20, 02:18:02 AM", "There was a regression with the refactored scheduler, that it could\r\ncause high cpu usage on the coordinator.  Use legacy scheduler by\r\ndefault until that's fixed.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Change the default for configuration property ``use-legacy-scheduler`` to ``true`` in order to mitigate a regression in the new scheduler.\r\n\r\n```\r\n\r", "NaN"], ["14266", "Support verifying SELECT queries with DATE or UNKNOWN columns", "Leiqing Cai", "caithagoras", "03/20/20, 07:47:42 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support for verifying ``SELECT`` queries that produce ``DATE`` or ``UNKNOWN`` (null) columns.\r\n```\r", "NaN"], ["14267", "Handle coalesce partition handle when bucket count does not match but compatible", "Rongrong Zhong", "rongrong", "03/24/20, 08:53:50 PM", "Should we have release note for this? It's technically a fix. Also do we have release note for default session value changes?\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Fix query failures with incompatible partition handle when session property ``partial_merge_pushdown_strategy`` is set to ``PUSH_THROUGH_LOW_MEMORY_OPERATORS`` and ``optimize_full_outer_join_with_coalesce`` is set to ``true`` and query has mismatched partition and uses ``FULL OUTER JOIN`` with ``COALESCE``.\r\n```\r", "NaN"], ["14268", "Optimize parquet gzip decompression", "James Petty", "pettyjamesm", "03/20/20, 03:55:07 PM", "Cross contribution of https://github.com/prestosql/presto/pull/3175\r\n\r\nAvoids creating an intermediate buffer (with the full `uncompressedSize` capacity) and copy from buffer to `DynamicSliceOutput` in parquet gzip decompression. Also avoids allocating the full 8k gzip input buffer size when the slice input is smaller. Finally, a validation check is added to verify the `uncompressedSize` was correct whereas previously the resulting slice would contain garbage data at the end (likely zeroed memory).\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14273", "Support verifying structured types containing DATE and UNKNOWN", "Leiqing Cai", "caithagoras", "03/23/20, 09:34:09 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support for verifying ``SELECT`` queries that produce structured types containing ``DATE`` or ``UNKNOWN`` (null).\r\n```\r", "NaN"], ["14275", "Upgrade presto pinot connector to be compatible with new Broker routing and time boundary APIs in Pinot 0.3.0 release", "Xiang Fu", "xiangfu0", "04/01/20, 07:19:18 AM", "Recently Pinot upgraded the Broker side routing table API and changed the behavior of timeBoundary query API. This PR aims to be compatible with new changes.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n* Adding support for new Pinot Routing Table APIs.\r\n```", "NaN"], ["14276", "Fix bucket pruning when value is null and the predicate is 'IS NULL'.", "Sreeni Viswanadha", "kaikalur", "03/24/20, 12:56:21 AM", "When filtering in buckets with matching values, handle NULL properly.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nFixed bucket pruning when the predicate is IS NULL and the table actually has rows with that value which we were previously not considering.\r\n```\r", "NaN"], ["14281", "Remove usage of String.format in HTTP client hot path", "James Petty", "pettyjamesm", "03/23/20, 06:40:15 PM", "Avoids the use of String.format from the trace token request filter to avoid unnecessary allocations and worse performance in the http client hot path. Allocation profiles showed this as representing one percent of all worker allocations (by number of allocations, not proportional to size) while queries were running. After this change, it represents ~0.2%.\r\n\r\nCross contribution of https://github.com/prestosql/presto/pull/3215, see that PR for allocation profile flame graphs.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14282", "Add release notes for 0.233.1", "Leiqing Cai", "caithagoras", "03/24/20, 02:41:18 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14283", "Create ThriftBufferResult and ThriftSerializedPage", "Vic Zhang", "viczhang861", "03/25/20, 02:58:22 AM", "Make SerializedPage independent of thrift serialization.\r\n\r\nThe goal is to move SerializedPage and related classes to spi module. This PR makes migration easier.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14286", "Reuse buffers in OptimizedPartitionedOutputOperator - part 2 : byte[] buffers in BlockEncodingBuffer", "Ying", "yingsu00", "03/27/20, 12:38:10 PM", "Partially resolves #14162\r\n\r\nThe buffers e.g. valuesBuffers in BlockEncodingBuffer can be reused\r\nafter the current partition get flushed and when there is no more\r\nbatches in the current page. This can save a lot of memory\r\nwhen the page is very large and can fill the buffers every time.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14289", "Remove unused ConnectorIdentity#{equals, hashCode}", "Wenlei Xie", "wenleix", "03/25/20, 06:13:50 PM", "Cherry-pick of https://github.com/prestosql/presto/commit/f7a012eabbd42fd3febbcdbb6880cd6312ecb0bb\r\n\r\nCo-authored-by: Dain Sundstrom <dain@iq80.com>\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14290", "Move RemoteSplit URI construction from coordinator to worker", "Shixuan Fan", "shixuan-fan", "03/25/20, 08:29:20 PM", "Coordinator could spend ~200ms to construct the URIs in a single\r\nthread, which does not seem necessary and could be done in workers.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14292", "Move SerializedPage to SPI", "Vic Zhang", "viczhang861", "03/26/20, 06:31:44 PM", "This PR is part of effort to migrate SerializedPage and related utilities to SPI\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14295", "Enforce buffer size limits for BlockEncodingBuffer", "Ying", "yingsu00", "04/13/20, 03:40:35 PM", "Partially resolves #14162\r\nDepend on https://github.com/prestodb/presto/pull/14286\r\n\r\nThe byte[] buffers in BlockEncodingBuffer used to be grown by a factor\r\nof 2. This could cause the buffers larger than necessary. Reducing\r\nthe expansionFactor has negative impact on performance. Therefore we\r\nwant to enforce stricter limit when growing these buffers. This PR\r\nuses incoming blocks' retained sizes as an estimation for how large the\r\nbuffers can grow. It shows 30% to 40% memory usage reduction for fixed\r\nwidth types and 10% reduction for variable width types for the\r\nOptimizedPartitionedOutputOperator.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14296", "Fix getRetainedSizeInBytes for ColumnarXXX", "Ying", "yingsu00", "04/01/20, 01:59:01 PM", "The ColumnarXXX#getRetainedSizeInBytes() used to add up nullCheckBlock\r\n, nested blocks and offsets's sizes. But nullCheckBlock already covers\r\nthe nested blocks and offsets. The nested blocks, e.g. elementsBlock\r\nin ColumnarArray, is just a wrapper of nullCheckBlock's values block.\r\nTherefore it is sufficient and close enough to just count nullCheckBlock's\r\nsize.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14297", "Fix getRetainedSizeInBytes for DecodedBlockNode", "Ying", "yingsu00", "03/26/20, 01:45:54 PM", "The DecodedBlockNode#getRetainedSizeInBytes() used to add up the current\r\nnode and all of its childrens' sizes. But the decodedBlock in current\r\nnode already covers all children's decodedBlocks. This commit removes\r\nthe double counting by removing the children's retained sizes from the\r\nsum.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14299", "Clean up queries in DispatchManager", "Timothy Meehan", "tdcmeehan", "03/26/20, 09:23:05 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14300", "Move PagesSerde to SPI", "Vic Zhang", "viczhang861", "03/31/20, 06:05:39 PM", "To use PagesSerde in connector in the future,  need to add BlockEncodingSerde into ConnectorContext\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14302", "Add ST_Centroid for points, multi-points on a sphere", "David Taieb", "DTAIEB", "04/13/20, 04:56:38 PM", "```\r\n== RELEASE NOTES ==\r\nGeospatial computation changes\r\n* Add support for :func:`ST_AsText` to accept Spherical Geographies.\r\n* Add support for :func:`ST_Centroid` to accept Spherical Geography Points and MultiPoints.\r\n\r\n```\r", "NaN"], ["14303", "Extract common subexpression in page projection at codegen", "Rongrong Zhong", "rongrong", "04/30/20, 10:15:43 PM", "Testing with Facebook workload shows positive CPU improvements overall. There are known corner cases that might cause regression, namely, very long list of projects with common sub expressions. In this case, we will compile all projections with cse into a single PageProjection, this would result in large bytecode size, and for some reason, a very wide projection has performance regression (We see similar behavior for wide ROW). These will be addressed in a follow up PR. The optimization can be turned off if regression is observed.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add optimization for page projection by extract and compute common subexpressions among all projections first. This optimization can be turned off by session property ``optimize_common_sub_expressions``.\r\n```", "NaN"], ["14305", "Add release notes for 0.234", "Leiqing Cai", "caithagoras", "04/16/20, 01:44:04 AM", "# Missing Release Notes\n## Cem Cayiroglu\n- [x] https://github.com/prestodb/presto/pull/14177 A new end point to get resource group stats (Merged by: Andrii Rosa)\n\n## Jonas Bauer\n- [x] 54dd54cb681af311e9b6fb6910ae004dfa71988b Minor exception message fix for ArrayBlockBuilder\n\n## Ke\n- [x] 0834046fc180310fa1b33f7f150ca15f201ad5d1 Update connectors.rst\n- [x] 8f1c09343bddd0c852d5fbacf43750543d34bdae Update connectors.rst\n- [x] 446e7f57aa0114882569723efdcd035d15c5cd5d Update connectors.rst\n- [x] 82f8a4be45ff1f09f69381a27ae9641622eaadae Update connectors.rst\n\n## Rohit Jain\n- [x] https://github.com/prestodb/presto/pull/14113 Add ZSTD support for writing ORC and DWRF tables (Merged by: Maria Basmanova)\n\n## Sreeni Viswanadha\n- [x] https://github.com/prestodb/presto/pull/14276 Fix bucket pruning when value is null and the predicate is 'IS NULL'. (Merged by: Rongrong Zhong)\n- [x] https://github.com/prestodb/presto/pull/14221 Add @JsonProperty tag to ignoreNulls flag so it works in distributed \u2026 (Merged by: Rongrong Zhong)\n\n# Extracted Release Notes\n- #14119 (Author: Mat\u00edas Correa): Add KHyperLogLog type and UDF\n  - Add KHyperLogLog Type.\n  - Add MAKE_KHYPERLOGLOG(K, V) -> KHYPERLOGLOG aggregate function.\n  - Add KHyperLogLog related scalar functions.\n- #14125 (Author: James A. Gill): Add cast bingtile to/from bigint\n  - Add `cast(tile AS bigint)` and `cast(bigint_value AS bingtile)` to encode/decode Bing tiles to/from bigints.  This is a more efficient storage format that also reduces bucket skew in some cases.\n- #14137 (Author: James A. Gill): Enable spatial join for spherical geography ST_Distance\n  - Enable spatial joins for `ST_Distance(p1, p2) < r` for spherical geography points `p1` and `p2`.\n- #14181 (Author: Leiqing Cai): Support determinism analysis for simple queries with top-level ORDER BY LIMIT clause\n  - Support determinism analysis for simple queries with top-level ``ORDER BY LIMIT`` clause. (:pr:`14181`).\n- #14193 (Author: Wenlei Xie): Improve scale writer creation based on producer buffer\n  - Improve the scale writer heuristics by considering overall producer buffer utilization. This can be enabled by using the session property `optimized_scale_writer_producer_buffer` and the configuration property `optimized-scale-writer-producer-buffer`.\n- #14194 (Author: Vic Zhang): Keep queries in the queue when task count is high\n  - Add support for stopping new query execution when total number of tasks exceeds a set limit to help with reliability. The limit can be set by the cluster property by ``experimental.max-total-running-task-count-to-halt-scheduling``.\n- #14200 (Author: Zhenxiao Luo): Limit pushdown for Druid connector\n  - Add LIMIT evaluation pushdown to Druid connector.\n- #14216 (Author: Ariel Weisberg): Allow forcing streaming exchange for Mark Distinct\n  - Allow forcing streaming exchange for Mark Distinct using 'query.use-streaming-exchange-for-mark-distinct'.\n- #14224 (Author: Zhenxiao Luo): Aggregation Pushdown for Druid connector\n  - Aggregation Pushdown for Druid connector.\n- #14225 (Author: Andrii Rosa): Optimize socket pooling\n  - Optimize connection pooling to avoid running out of sockets in certain cases.\n- #14230 (Author: Leiqing Cai): Auto resolve time limit exceeded on checksum queries\n  - Add support to auto-resolve control check query failures due to ``EXCEEDED_TIME_LIMIT``.\n- #14248 (Author: Nikhil Collooru): Add AccessControlContext to store client information for security purposes\n  - All the methods in ``SystemAccessControl`` now take additional parameter ``AccessControlContext context``.\n- #14264 (Author: Rebecca Schlussel): Set use-legacy-scheduler true by default\n  - Change the default for configuration property ``use-legacy-scheduler`` to ``true`` in order to mitigate a regression in the new scheduler.\n- #14266 (Author: Leiqing Cai): Support verifying SELECT queries with DATE or UNKNOWN columns\n  - Add support for verifying ``SELECT`` queries that produce ``DATE`` or ``UNKNOWN`` (null) columns.\n- #14267 (Author: Rongrong Zhong): Handle coalesce partition handle when bucket count does not match but compatible\n  - Fix query failures with incompatible partition handle when session property ``partial_merge_pushdown_strategy`` is set to ``PUSH_THROUGH_LOW_MEMORY_OPERATORS`` and ``optimize_full_outer_join_with_coalesce`` is set to ``true`` and query has mismatched partition and uses ``FULL OUTER JOIN`` with ``COALESCE``.\n- #14273 (Author: Leiqing Cai): Support verifying structured types containing DATE and UNKNOWN\n  - Add support for verifying ``SELECT`` queries that produce structured types containing ``DATE`` or ``UNKNOWN`` (null).\n\n# All Commits\n- 0f2b203ee43142117e22809f4c1b728dbfd2f66b Reuse buffers in BlockEncodingBuffer (Ying Su)\n- 5b8b26e15bc3f958b0dede3beb2d6a02f1ff6352 Add byte[] support to ArrayAllocator (Ying Su)\n- 3c2e9eecd150c2e2381467893017d43ca92452a0 Return arrays in reverse order of borrowing (Ying Su)\n- ec684fb0e5d353caa7a14c6fcd02ee67ae51113c Clean up queries in DispatchManager (Tim Meehan)\n- efd979afa5efcf616ac1ae0b806fcad3a0cf1ab0 Add AccessControlContext to store client information for security purposes (Nikhil Collooru)\n- 61e3c8805bf8cf45d871f6cfbce06c82cb5c1880 Move SerializedPage to SPI (Vic Zhang)\n- 4e5dd5f990f8e27f83498a63a71d5aea94c27e33 Remove guava dependency from SerializedPage (Vic Zhang)\n- 96409c39eba60b09fe4eb1a973a18bfbae5593f7 Move PageCodecMarker to SPI (Vic Zhang)\n- 151f57bb6778016b11e51fa6c26b21f09af43765 Move BlockSerdeUtil to SPI (Vic Zhang)\n- acb8af46e374402c1d4482e20b6cadcf1915d30c An end point to get resource group level stats (Cem Cayiroglu)\n- e9b4d1fb4d5be5ec8fc9fe6b6fb1c17cf0382f0a Fix getRetainedSizeInBytes for DecodedBlockNode (Ying Su)\n- 410dd0900390bf08f16a1a5030483c7d77cf1b0e Move RemoteSplit URI construction from coordinator to worker (Shixuan Fan)\n- 59ddb0b2a5e827f4ea908cd2637b53441f2ec07c Remove unused ConnectorIdentity#{equals, hashCode} (Wenlei Xie)\n- 7af7880282938066fe28eb0f29b31110948c0355 Add partitioned spherical spatial joins (James A. Gill)\n- 0638aa062782a1cbd698c0490474863e96d910cf Extract spherical geography functions to SphericalGeoFunctions (James A. Gill)\n- 9bebca50fc11732c884ce20fd38745530bc83ac0 Enable spatial join for spherical geography ST_Distance (James A. Gill)\n- 8bc8c94825747e98e179da1d36593f64607f6149 Extract some spherical geometry functions to helper class (James A. Gill)\n- 6028f4dd8c25ef475943c84a6236948fb2aabf2f Create ThriftBufferResult and ThriftSerializedPage (Vic Zhang)\n- 5f21344035932afdbddd037ed9d5cea63025a880 Add PrestoSparkAuthenticatorProvider (Wenlei Xie)\n- 9c32d6b57d033dd5226e83cd7c731b73f6db249c Introduce TokenAuthenticator (Wenlei Xie)\n- 68e4367dc3abcb21dec09fd552212e130cd1871d Handle coalesce partition handle with mismatched bucket count (Rongrong Zhong)\n- dd38222aa43968aabcdc61aeb2a9971f14a767ed Change default value for colocated_join to true (Rongrong Zhong)\n- 6613a6a1be3f4c03e4bb77382cb71a7a0fbd42e4 Fix bucket pruning when value is null and the predicate is 'IS NULL'. (Sreeni Viswanadha)\n- e90fd6baf9b47701ab8346df054ebf58878a43c2 Reuse positions, mappedPositions and offsets in BlockEncodingBuffer (Ying Su)\n- acac2e4741016d9fece82832fd1db496302103c6 Add isNested flag to BlockEncodingBuffer (Ying Su)\n- ed23d49411f66551bf6cc2f8cfcfe847c7ea34af Reuse offsetsCopy in BlockEncodingBuffer (Ying Su)\n- df3b10a7630dc3a8d0a1aa2e46e7d46e99eb4d36 Add ArrayAllocator to BlockEncodingBuffer (Ying Su)\n- d3f9cfc25faf3791d9c634cf2bd0c4e4317814fd Reuse serializedRowSizes buffer between different PartitionBuffers (Ying Su)\n- 1bbd1310d3526cae9c7d82566d6dcefe3ec147e3 Support verifying structured types containing DATE and UNKNOWN (Leiqing Cai)\n- ed7a6ef6cdd9d9326e1d0e38d41660a99c8b24e4 Use Type instead of TypeSignature in QueryRewriter (Leiqing Cai)\n- 613e19e16d0d7c897adb2a6988e40aa32c216fb3 Remove usage of String.format in HTTP client hot path (James Petty)\n- 5078fd67464fd2a6f5d82e46a12cf9ffd756fd34 Support verifying SELECT queries with UNKNOWN columns (Leiqing Cai)\n- 65a29b3755421a8ffc21a8980402674125cec68c Support verifying SELECT queries with DATE columns (Leiqing Cai)\n- ae3cafe490bc1db1e96fd5e4a0391971ff0f7bbd Improve JdbcPrestoAction (Leiqing Cai)\n- 7dc045335195503c1f6c586c414a52a485d3deb7 Optimize parquet gzip decompression (James Petty)\n- 075b70e46a69da2cc53bbb37c79939ead4b8ab69 Set use-legacy-scheduler true by default (Rebecca Schlussel)\n- a3df4d1e2856e234c1d792ff8ee3b5ce04bdc00e Avoid Exceptions when closing queryRunner (Leiqing Cai)\n- 0bfc8cc59e39684e4db31ca307b4da6e7993403d Add toString() to SimpleArrayAllocator (Ying Su)\n- ac183423a7f2c07607eea80911859e415a0307d2 Add benchmarks for Parquet reader (Venki Korukanti)\n- 32011d8849a44955eb60cf64cae720c400a22d3a Make PageReader class public (Venki Korukanti)\n- a99684e12a569e2e315c959b0cca083140d6327c Removed unused method (Venki Korukanti)\n- 528bd5c7d62e66f617637495a9958c2da81b5c4a Move ColumnIOConverter to presto-parquet package (Venki Korukanti)\n- 5bad5a33bd0e2196f032f5d52b490f9c51f4ecc3 Add FileParquetDataSource for testing purpose (Venki Korukanti)\n- fd5a7ddb40773c15163576e4e6632a67fbbc9483 Removed unused ParquetDataSource#size (Venki Korukanti)\n- 11ae1f277400e157912296f20c3d6dd4f6b33480 Revert \"Update connectors.rst\" (Ke Wang)\n- 0491783e950e2812b1c4b0a37b16553c050577f5 Fix subscript expression optimization during subfield pushdown (Bhavani Hari)\n- caff58e35ec3fc73a01929c3d48fbf51bc638ce8 Disable Presto-on-Spark query stats collection (Wenlei Xie)\n- ad1be427c1e48423c556a87d4b7227b90918a4e8 Aggregation Pushdown for Druid connector (Zhenxiao Luo)\n- c61194f4cbe704f3c410b3d0526e7a9007b32201 Do not cancel running HTTP requests (Andrii Rosa)\n- aa77278fd535551e46a80182143685af696b22e9 Update Airlift to 0.191 (Andrii Rosa)\n- 7c7fa10c91271cc5f7b1abc5b30261ce253fc20b Remove overrides for idle connection timeout (Andrii Rosa)\n- 7dda502dee9608a818219aa63f4f7158e6c2bf87 Refactor PrestoSparkLauncher (Andrii Rosa)\n- 000fa29dcbf3958e988a20f92febe058baba9e6d Reduce positions array size in PartitionBuffer (Ying Su)\n- dd86faafcdefe98ed6ba037018ab281aa589a5f9 Fix memory tracking for PartitionedOutputOperator (Ying Su)\n- 2b4315460a22e4a1abe0abe48159f872c93331ec Add support for testing DictionaryBlock with non-zero offset (Ying Su)\n- 660f0a715f565438de9f9b1723ae74ab2978380c Fix BlockFlattener when DictionaryBlock's idsOffset is nonzero (Ying Su)\n- f88d6528e0050fda436264b4057c5cfea8efa199 Refactor TestBlockFlattenner (Ying Su)\n- 5d1797c745fa406d4d5ab96423b1ca5f40a00157 Add ZSTD support for writing ORC and DWRF tables (Rohit Jain)\n- b0814187bebbb25b28f768c0dc77dc901822116f Fix ArrayBlockEncodingBuffer#getRetainedSizeInBytes() (Ying Su)\n- 39c43b66e1838090e278fd68e0eecc836c230bd3 Add @JsonProperty tag to ignoreNulls flag so it works in distributed execution as well. (Sreeni Viswanadha)\n- f35c6608120572f8d81fe2beb0bc374021a90510 Keep queries in the queue when task count is high (Vic Zhang)\n- f6fd9ef756eaa053cd2db702ddb3be4e6335ca77 Refactor TestQueryTaskLimit (Vic Zhang)\n- f13cee1ccf62e2c3218c1ae2593ed30ddbd90b67 Add configuration property to control query queuing (Vic Zhang)\n- 66e94ef1ea7debada9e44c918f2b79f27d07c5d0 Rename configuration properties for task count limit (Vic Zhang)\n- f9bf064946accbf7c66f9cba592c84af76a90b95 Allow forcing streaming exchange for Mark Distinct (Ariel Weisberg)\n- 4d5bcffdc471a8e887ac84c6f84be26ae7d070c6 Use static imports in TestAddExchangePlans (Ariel Weisberg)\n- 7bfc20373981d931b6d18ca570915d4da518b1ae Add purger to ExecutingStatementResource (Tim Meehan)\n- 3af06bd62c959414488ec14e6ee548f837fc98f7 Simplify token management in protocol Query (Tim Meehan)\n- 916cc6ce61b3e6989d2b5973fbfe9714a16147cb Fix result caching in protocol Query (Tim Meehan)\n- 4a992da4c7d2eaeb49765e1489f8836449cedb97 Catch errors from LocalDispatchQuery querySubmitter (Tim Meehan)\n- 58c0b7ed8aed38b7eb6f3e310c46171e11ea8574 Change local dispatch to finish immediately after query submission (Tim Meehan)\n- 87b87293db3a978bda076741703c9398d43b1fc0 Cleanup dispatcher executor management (Tim Meehan)\n- c372011e7db53a6ffcbc23382751d6851001942c Fix handling of failures during query creation (Tim Meehan)\n- 42b40c7e9aa54d4a54a17f13d3ab5f61f1b3d893 Simplify query manager stats tracking (Tim Meehan)\n- 9c17fc1fae790148bc8e977558c4da4762e5c9b8 Rename SqlQueryManagerStats to QueryManagerStats (Tim Meehan)\n- b93f31e0cf5d4cdbfa9551029910ca5123705095 Simplify DispatchInfo construction (Tim Meehan)\n- 39d639fc53cd299d596fb8781dd6540c909ef58a Remove Optional from QueryStateMachine resourceGroup (Tim Meehan)\n- 7999f849f62e072ebe3097a9a85f4044cd954f87 Improve query event stats for immediately failed queries (Tim Meehan)\n- e23009a1c9ac3766b6d973ac72c6e7114e1aa183 Add LocalCoordinatorLocation (Tim Meehan)\n- 2240313e603aa06ebd27d87c60a2dfcb1fcfb58d Add peak tasks to BasicQueryStats (Tim Meehan)\n- a92f1616cf509d48133e171ee21ad982782ed521 Split out queued phase from QueryManager (Tim Meehan)\n- f72d51120bcb9dd4e5571e5833085103fc80a61e Add DISPATCHING query states (Tim Meehan)\n- 8b7c734ffd9112d8e166f2c1f9e5fe98f21a6fa5 Remove system startup minimum worker requirement (Tim Meehan)\n- 153761bf66a10a7350e0e73456e74fe6b8e3ca76 Add query id to NoSuchElementException (Tim Meehan)\n- 8d5283d7ab6e4ed0cc5bd303fc1497bfcface81c Optimize initial batch size for scan (Masha Basmanova)\n- a902a34b9f066111deaa420c94d79f1f10d944ee Harden blockEncodingBuffers in OptimizedPartitionedOutputOperator (Ying Su)\n- 0834046fc180310fa1b33f7f150ca15f201ad5d1 Update connectors.rst (Ke)\n- 8f1c09343bddd0c852d5fbacf43750543d34bdae Update connectors.rst (Ke)\n- 446e7f57aa0114882569723efdcd035d15c5cd5d Update connectors.rst (Ke)\n- 82f8a4be45ff1f09f69381a27ae9641622eaadae Update connectors.rst (Ke)\n- 3977cbf887bef782d2aaaecffb76fa206e107c65 Improve entropy for bing tile bigint encoding (James A. Gill)\n- e167e4604d57a14712aa41bcbce1e625433c2994 Add BingTile cast to/from bigint (James A. Gill)\n- 59f137a8dab4a380a0d97c914e06cd203f4924a5 Auto-resolve EXCEEDED_TIME_LIMIT on control checksum queries (Leiqing Cai)\n- a021f7f9e50cad23854104ed2fbd371fb275a41c Fix reading a map column having negative integers as keys (Bhavani Hari)\n- 0b7ab4b1a928b240d306af7d7c586e5c419e3f73 Rename PinotConnectorPlanOptimizer to PinotPlanOptimizer (James Sun)\n- b03e989a9437624065eb9e92b16d707300371cb5 Rename DruidConnectorPlanOptimizer to DruidPlanOptimizer (James Sun)\n- e57af67a9db67335552a0eaf4c5425851049d683 Fix typo in NodeSelectionStats (Shixuan Fan)\n- f4b78693370a970d576f680b24877fd4e9a1852e Create KHyperLogLog type (Mat\u00edas Correa)\n- 411b5c64aa00f655a50fe3d819082a024e7c5f99 Add nodeSelectionStats to monitor affinity scheduler (Ke Wang)\n- fd0ec6b42c549d7f3dc5650453ca73300a6a0af7 Add test methods to BenchmarkPartitionedOutputOperator (Ying Su)\n- 65606a1d6db77a6f534c431076ccb69e8c4d4d15 Fix BenchmarkPartitionedOutputOperator (Ying Su)\n- f5d93fbf3aacc60f672581fc2c1e1180c2b1f704 Minor fix to BenchmarkPartitionedOutputOperator (Ying Su)\n- 2ab25438a2507b1438d60e7992714a261f9c9308 Improve scale writer creation based on producer buffer (Wenlei Xie)\n- a4608e1d8c1beabb87aa8ca15968464e3e03d233 Fix candiateNodes selection for Soft Affinity (Ke Wang)\n- 54dd54cb681af311e9b6fb6910ae004dfa71988b Minor exception message fix for ArrayBlockBuilder (Jonas Bauer)\n- 0e144a6d592e564eaf054f5252d13eb6b1f44a98 Add allocation info to Task/Stage/QueryStats (Masha Basmanova)\n- 76e311d337cccc146ed2718dd7490ea7691ff41a Add allocation info to PipelineStats (Masha Basmanova)\n- ec3dd652afc36e671a131a91d2afbfde943ec8d4 Add allocation into to DriverStats (Masha Basmanova)\n- e10de40d13894fcaab5d61268bf1f8d6fc6d8ac9 Refactor TestQueryStats (Masha Basmanova)\n- 419d15563dd8a32ed9e13ae80dbf7066ad3bc0a3 Add allocation info to OperatorStats (Masha Basmanova)\n- 5fe659f6f52964ac6b11a916d18b81557b35fb55 Track allocations per operator (Masha Basmanova)\n- a9d1d337b0ba94a26f33082e8fb41715e968d776 Propagate flags to track allocation to OperationTimer (Masha Basmanova)\n- a64e5d876f4d87df63f33e7a39665834cc6c67d3 Add config properties to enable tracking memory allocations (Masha Basmanova)\n- 569be238fc441078cddf81d540c890489a8c977d Split test-hive-materialized on Travis (Leiqing Cai)\n- 757fd25aee3604141df01fe6c7a0fd4b233db2b8 Remove version tags for presto projects from sub-module pom files (Leiqing Cai)\n- b1c5015c19910808e599c49a54325556373f8016 Move Hive filter pushdown logic out of PickTableLayout (Saksham Sachdev)\n- 9fa0caeeaa9d6df3368a78ce57b006b8cad16d1d Separate some modules into their own travis jobs to reduce test times (Sujay Jain)\n- 5dadf5d9efbfc83628722da55a9392903f7d3551 Throw OrcCorruptionException for column type mismatch (Bhavani Hari)\n- 05407a75d93d6c0b173d6f30d36a491636043b47 Remove unused HiveFileContext in OrcReader (James Sun)\n- f8ee9fd00c75ec2c6a9a0cb1b59c99cf86c7d851 Add LIMIT evaluation pushdown to Druid connector (Zhenxiao Luo)\n- d46146586c09e6478b27d5f9c4c60ace783ffc4b Allow column pruning in SHOW STATS (Masha Basmanova)\n- d8d3939f3348451cdc9a2d322e9901856a488a7f Materialize LazyBlock in scan (Ying Su)\n- b36f46321b4e83f2fcd32ca897f34cb489d7630b Break presto-tests Travis jobs to reduce test times (Bhavani Hari)\n- 6d662bd06f953baa4a0278313941195e2489edb8 Fix finishStatisticsCollection to have correct ConnectorSession (Mayank Garg)\n- b429ddb4eabd15ab1f7c06242b0ef39ab5bf0534 Wait for final task info on abort (Andrii Rosa)\n- 1f774667198e72e29a124e8a39dc88b7a2e609d8 Always use JSON for PlanFragment serialization (Tim Meehan)\n- 7aed00893642e967c0cc9d310ab31e83a6d0efa4 Support determinism analysis for simple ORDER BY LIMIT queries (Leiqing Cai)\n- 1e070b2e42ff0c3a370bf4f410299786544a26c0 Improve Presto results conversion (Leiqing Cai)", "NaN"], ["14310", "Fix memory allocation for nestedPositions in the map reader", "Maria Basmanova", "mbasmanova", "03/31/20, 10:13:38 PM", "CC: @bhhari @yingsu00 @oerling \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14312", "Add documentation for KHyperLogLog", "Timothy Meehan", "tdcmeehan", "04/06/20, 08:07:24 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14313", "Add Caching Documentation", "Haoyuan Li", "haoyuan", "04/03/20, 06:36:29 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14314", "Misc optimizations", "James Petty", "pettyjamesm", "03/31/20, 05:52:34 PM", "Cross contribution of https://github.com/prestosql/presto/pull/3292 with some additional changes to the parquet reader that were unnecessary the other PR.\r\n\r\nSplit into individual commits to make this easier to review. The general theme is replacing usages of `Slice#getBytes()` when a suitable alternative exists.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14315", "Reduce memory allocations in SliceDirectSelectiveStreamReader", "Maria Basmanova", "mbasmanova", "03/31/20, 10:10:30 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14316", "Reduce memory allocations in OrcInputStream", "Maria Basmanova", "mbasmanova", "03/31/20, 10:11:10 PM", "Reuse byte[] array holding compressed data.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14317", "Add SqlBaseParser initializer hook and refreshable ATN caches", "James Petty", "pettyjamesm", "03/31/20, 08:51:50 PM", "Cross contribution of https://github.com/prestosql/presto/pull/3186\r\n\r\nAdds utility classes that enable explicit initialization and management of antlr parser and lexer ATN caches. Without them, these fields are static constants that can grow to retain multiple GB of heap space depending on input query strings.\r\n\r\nThese utilities are currently not wired into anything but can be used to periodically refresh the parser cache if desired.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14320", "Add max buffer count config property for optimized repartitioning", "Ying", "yingsu00", "04/07/20, 09:30:30 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add config property driver.max-page-partitioning-buffer-count, to limit the total number of buffers per optimized repartitioning operator. The default value is 1,000,000.\r\n```", "NaN"], ["14321", "Optimize metastore calls", "Andrii Rosa", "arhimondr", "04/08/20, 03:44:09 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14322", "Adding more support for  Pinot types to Presto type conversion", "Xiang Fu", "xiangfu0", "04/03/20, 02:49:06 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nPinot Connector Changes\r\n* Support mapping Pinot `BYTES` data type to Presto `VARBINARY` type\r\n* Support mapping Pinot time fields with days since epoch value to Presto `DATE` type\r\n* Support mapping Pinot time fields with milliseconds since epoch value to Presto `TIMESTAMP` type\r\n* Put Pinot Field type in to column comment field shown as DIMENSTION, METRIC, TIME, DATETIME, to provide more information.\r\n* Adding configs 'pinot.infer-date-type-in-schema' and 'pinot.infer-timestamp-type-in-schema' to switch on/off the type conversion. Default is OFF.\r\n```", "NaN"], ["14324", "Add peak [task] total memory to returned stats ", null, "aweisberg", "04/07/20, 09:15:01 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add peak [task] total memory to returned stats \r\n```", "NaN"], ["14325", "Fix bugs in Druid connector aggregation pushdown", "Zhenxiao Luo", "zhenxiao", "04/02/20, 03:28:05 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14326", "Support caching all tables in file status cache", "Shixuan Fan", "shixuan-fan", "04/03/20, 04:41:34 PM", "```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Added support in file status cache to cache all tables. This could be enabled by setting ``hive.file-status-cache-tables`` to ``*``.\r\n```", "NaN"], ["14327", "Move MarkDistinctNode to SPI", "Xiang Fu", "xiangfu0", "04/03/20, 08:34:05 PM", "```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14328", "Upgrade aircompressor to 0.15", "Zhenxiao Luo", "zhenxiao", "04/03/20, 05:20:28 PM", "need ZstdCompressor\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14329", "Simple PageFile format", "Vic Zhang", "viczhang861", "04/13/20, 07:43:54 PM", "There is no split support yet. All pages are written continuously to a single file.\r\n\r\nExisting tests in `HiveIntegrationSmokeTes` and `TestHivePageSink` cover new formats.\r\n\r\nAttached benchmark result shows much faster read/write compared with ORC format. Reading from PageFile format is 4X faster; Writing PageFile format is 40X faster.\r\n\r\n![91361137_564002691179682_3632670565724061696_n](https://user-images.githubusercontent.com/6372365/78816759-2a528200-79a0-11ea-83da-295952a6bd3a.png)\r", "NaN"], ["14330", "Report failure to compile pushed down filter as COMPILE_ERROR", null, "sujay-jain", "04/07/20, 02:39:34 PM", "Fixing the incorrect error categorization by returning the ```PrestoException``` wrapped inside the ```UncheckedExecutionException```.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14331", "EXPLAIN (TYPE IO) support for Aria", "john roll", "jbroll", "04/16/20, 04:03:24 PM", "Allow Connectors to control column constraints in IOPlanPrinter\r\n\r\n* Add a new metadata method . toExplainIOConstraints to the Metadata and ConnectorMetadata interfaces to optionally return only Constraint for use in IOPlanPrinter.\r\n* Interface default method returns the current constraints\r\n* Implement HiveConnector.toExplainIOConstraints() to filter out non-partition key columns.\r\n\r\nAre there additional data types that Aria can return in Constraints that can also be partition columns?\r\n* Supported:\r\n   * VarcharType\r\n   * TinyintType, SmallintType, IntegerType, BigintType\r\n   * BooleanType\r\n\r\nI need some pointer on how to test this.\r\n```\r\n\r\n```\r", "NaN"], ["14332", "Make table properties a config property", "Leiqing Cai", "caithagoras", "04/05/20, 01:16:43 AM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/936\r\n\r\nInstead of harding coding table properties, making it a configuration property.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support for specifying table properties override for temporary Verifier tables, through configuration property ``control.table-properties`` and ``test.table-properties``.\r\n```\r", "NaN"], ["14334", "Fix named query output does not match column list", "Rongrong Zhong", "rongrong", "04/30/20, 09:38:11 PM", "Resolves #14333 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix VerifyException when named query is a table reference (:issue:`14333`).\r\n```\r", "NaN"], ["14335", "Add native parquet writer", "Zhenxiao Luo", "zhenxiao", "04/07/20, 12:29:32 AM", "Cherry-pick of https://github.com/prestosql/presto/commit/70b12894bc867c29491cf1eb6ac3da4935f91980\r\n\r\nCo-authored-by: Zhenxiao Luo <zluo@twitter.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Native Parquet Writer for Presto\r\n```\r", "NaN"], ["14336", "Drop presto-orc's dependency on presto-hive-common", "Nikhil Collooru", "NikhilCollooru", "04/04/20, 05:57:00 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14340", "Fix NullPointerExpression in ExpressionInterpreter::visitBindExpression", "Rongrong Zhong", "rongrong", "04/05/20, 10:47:36 PM", "ImmutableList::toImmutableList does not allow null input. However, null\r\nis generally a valid return value when process Expression in ExpressionInterpreter.\r\nUse Collector.toList() instead.\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix potential `NullPointerException` in `ExpressionInterpreter::visitBindExpression` when processed values contain `null`.\r\n```\r", "NaN"], ["14341", "Best effort push down distinct count to pinot", "Xiang Fu", "xiangfu0", "04/09/20, 09:10:57 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n* Add capability to push down distinct count query to Pinot with best effort.\r\n```", "NaN"], ["14345", " Drop presto-orc's dependency on presto-memory-context", "Nikhil Collooru", "NikhilCollooru", "04/08/20, 02:24:31 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14347", "Handle empty and arbitrary slices in Parquet reader", "James Petty", "pettyjamesm", "04/08/20, 07:07:34 PM", "Port fix for recent parquet reader optimization that would fail to handle empty `Slice` arguments from https://github.com/prestosql/presto/pull/3339\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14349", "Fixing date type handling in Presto-Pinot Connector", "Xiang Fu", "xiangfu0", "04/09/20, 09:07:59 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14350", "Fix minor typo in error message", "Shixuan Fan", "shixuan-fan", "04/07/20, 05:15:14 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14351", "Reduce instance size and block copies in Page class", "James Petty", "pettyjamesm", "04/07/20, 02:38:12 PM", "Refactors Page class to avoid extra allocations and copies. Fields didn't require `AtomicLong` semantics and simple volatile fields are now used instead. Additionally, trusted methods can avoid copying `Block[]` in the page constructor by using a new static helper method. Finally, no valid reason should exist for `Page` subclassing, so the class is now final.\r\n\r\nCross contribution of https://github.com/prestosql/presto/pull/2975\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14352", "Add Internationalization (i18n) plugin and Myanmar UDFs", "Nick LaGrow", "nlagrow", "04/16/20, 06:55:51 PM", "## RELEASE NOTES\r\n\r\n### Context \r\nText entered in Myanmar is often entered in non-Unicode-compliant Zawgyi. These are rendered differently based on the font encoding of devices in Myanmar. This means if someone entered Burmese-script text on a Zawgyi device, someone on a Unicode device could not read it, and vice versa.\r\n\r\nExample:\r\n* Unicode: \u1014\u1004\u103a \u1021\u1001\u102f\u1018\u101a\u103a\u1019\u103e\u102c\u101c\u1032? (readable)\r\n* Zawgyi: \u1014\u1004\u1039 \u1021\u1001\u102f\u1018\u101a\u1039\u1019\u103d\u102c\u101c\u1032? (unreadable)\r\n\r\nYou can learn more about the overall issue at http://www.unicode.org/faq/myanmar.html\r\n\r\nSo if you query a search term in a Myanmar script in Presto, you'll only match text entered with the same font encoding.\r\n\r\nThese changes propose to functions to help resolve this:\r\n\r\n* `MYANMAR_FONT_ENCODING(text)` which simply returns whether the encoding is Zawgyi or Unicode\r\n* `MYANMAR_NORMALIZE_UNICODE(text)` which leaves Unicode as is and converts Zawgyi to Unicode using Google's Myanmar Tools algorithm: https://github.com/google/myanmar-tools\r\n\r\nThis change also adds an `Internationalization` or `i18n` plugin to cover other cases unique to certain languages", "NaN"], ["14354", "Support verifying SELECT queries with certain non-storable columns", "Leiqing Cai", "caithagoras", "04/08/20, 07:34:01 AM", "Support verifying SELECT queries with certain non-storable columns\r\n    \r\nWhen verifying SELECT queries\r\n- Cast Time columns to Timestamp\r\n- Cast Timestamp With Time Zone columns to Varchar\r\n- Cast Decimal columns to Double\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support for verifying ``SELECT`` queries that produce columns of ``TIME``,\r\n  ``TIMESTAMP WITH TIME ZONE``, or ``DECIMAL`` types, or columns of structured types\r\n  with those types.\r\n```\r", "NaN"], ["14355", "Pass the correct value for fileSize in Hive internal split factory", "Venki Korukanti", "vkorukanti", "04/07/20, 06:34:21 AM", "We are passing the split length as `fileSize` to `InternalSplit`\r\nwhich was used by Parquet reader when trying to find the last few bytes\r\nthat contain the Parquet file magic code to verify the file is a Parquet\r\nfile. PR #12780 accidentally modified this. It looks like Parquet is the\r\nonly reader which is using the filed `fileSize and this reproes only\r\nif `InputFormat.getSplits()` is used which is not the most common.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Fix a bug in Hive split calculation which affects Parquet reader in few corner cases\r\n```", "NaN"], ["14356", "Revert #13699", "Andrii Rosa", "arhimondr", "04/07/20, 03:37:57 AM", "Due to https://github.com/prestodb/presto/pull/13699#pullrequestreview-388727912\r", "NaN"], ["14359", "Fix NPE when reading structs with a newly added subfield", null, "bhhari", "04/09/20, 08:49:27 PM", "If a nestedStream is missing for a field in the schema create a MissingFieldStreamReader\r\nFixes #14358 \r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14360", "Fix AddExchanges filter predicate when filter pushhdown is enabled", "Saksham", "sachdevs", "04/07/20, 09:53:44 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14361", "Add Alluxio Catalog in Cache documentation", "Haoyuan Li", "haoyuan", "04/08/20, 05:46:30 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14365", "Disallow invoking SQL functions in SQL function body", "Leiqing Cai", "caithagoras", "04/14/20, 10:23:13 PM", "Current, executing a SQL function that calls another SQL function\r\nthrows internal error with \"compiler failed\" error message.\r\n\r\nIf the fix the compiling issue, we'll then have to properly handle\r\npossible recursion.\r\n\r\nTherefore, we should disallow such function body meanwhile.\r\n\r\nDepended by https://github.com/facebookexternal/presto-facebook/pull/940\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add check to disallow invoking SQL functions in SQL function body.\r\n```\r", "NaN"], ["14366", "Support SHOW CREATE FUNCTION", "Leiqing Cai", "caithagoras", "04/30/20, 11:54:55 PM", "Resolves https://github.com/prestodb/presto/issues/13976\r\n\r\nSupport syntax, semantic analysis, and execution for SHOW CREATE\r\nFUNCTION. An optional list of parameter types may be specified.\r\n\r\nFor each matching signature, returns the canonical CREATE FUNCTION\r\nthat can be used to create the exact same function, along with\r\nthe parameter type list of that function signature so that user\r\ncan differentiate among the functions.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for SHOW CREATE FUNCTION.\r\n```", "NaN"], ["14370", "Eagerly dereference resources", null, "bhhari", "04/14/20, 04:30:43 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\nThese changes will speed up the GC process for these variables.\r", "NaN"], ["14372", "count distinct pushdown for druid connector", "Zhenxiao Luo", "zhenxiao", "04/11/20, 04:17:08 AM", "fix https://github.com/prestodb/presto/issues/14357\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nDruid Changes\r\n* count distinct pushdown for druid connector\r\n* fix druid connector segment scan\r\n```", "NaN"], ["14374", "Fix section labeling for KHyperLogLog documentation", "Leiqing Cai", "caithagoras", "04/11/20, 12:50:20 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14375", "Drop presto-orc's dependency on ConnectorSession", "Nikhil Collooru", "NikhilCollooru", "04/17/20, 04:52:50 AM", "depended by https://github.com/facebookexternal/presto-facebook/pull/955\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14376", "Rename ORC related classes", "Vic Zhang", "viczhang861", "04/13/20, 02:36:03 PM", "These classes have no dependency on ORC format, rename to make them reusable.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14383", "Add preferred ordering columns for unbucketed table", "Shixuan Fan", "shixuan-fan", "04/17/20, 10:00:36 PM", "Currently sorted table is only a property of bucketed table. However,\r\nwe do see sorted file has several benefits, including better\r\ncompression for storage and faster scanning because of OrcReader\r\nskipping stripes. Introducing preferred ordering for unbucketed\r\ntable could bridge the gap. This property only means this table\r\nwould prefer to have files sorted by specified columns but does\r\nnot enforce it.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Added table property preferred_ordering_columns to support writing sorted files for unbucketed table. The list of ordering columns could be specified using the preferred_ordering_columns table property.\r\n```", "NaN"], ["14384", "Speed up GC of MapFlatSelectiveReader by clearing member variables in\u2026", null, "bhhari", "04/14/20, 12:09:38 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14385", "Add no-execution-node Exception for affinity scheduler", "Ke", "kewang1024", "04/16/20, 03:44:52 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14386", "Add soft memory limit configuration properties", null, "aweisberg", "04/22/20, 05:44:50 PM", "Soft memory limits are default memory limits given to each query that can be overridden using session properties up to the hard limit set by the existing configuration properties.\r\n\r\nHaving soft limits makes it easier to migrate a workload to lower memory limits by allowing only the queries that require higher limits to specify them while defaulting other queries to lower limits.\r\n\r\nAdds:\r\n\r\nquery.soft-max-memory-per-node\r\nquery.soft-max-total-memory-per-node\r\nquery.soft-max-total-memory\r\nquery.soft-max-memory\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add soft memory limit configuration properties.\r\nSoft memory limits are default memory limits given to each query that can be overridden using session properties up to the hard limit set by the existing configuration properties.\r\nAvailable soft memory limit configuration properties are ``query.soft-max-memory-per-node``, ``query.soft-max-total-memory-per-node``, ``query.soft-max-total-memory``, and ``query.soft-max-memory``.\r\n\r\n\r\n```", "NaN"], ["14387", "Add subfield pruning to StripeReader", null, "bhhari", "04/15/20, 01:16:10 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\nWhile processing a struct column, we used to read all the nested streams even though a single nested field was required or accessed.\r\nEg. consider a column `col row(a bigint, b bigint, c row(...))`\r\nFor sql query` SELECT col.a` we will end up reading metadata for columns b, c,  all nested columns for c.  This increases the InputBytes processed, processing time, which can be improved by pruning the read for non-required subfields.", "NaN"], ["14388", "Reduce LazyBlock copy / wrapping in ScanFilterAndProjectOperator", "James Petty", "pettyjamesm", "04/14/20, 10:33:24 PM", "Previously, LazyBlock instances that were already loaded would still be wrapped in a new LazyBlock that reported statistics on load. Now, only not-yet-loaded LazyBlock instances will require that wrapping. Additionally, when no changes to the Page blocks are required, the original page can be reused when recording input\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14389", "Add status endpoint for HTTP HEAD requests", "James Petty", "pettyjamesm", "04/15/20, 12:59:09 AM", "Previously, node pings in `HeartbeatFailureDetector` would go to the root path and return 404 from workers because no endpoint was mapped to handle them. Now the failure detector points to /v1/status and receives empty HTTP OK responses instead.\r\n\r\nCross contribution of https://github.com/prestosql/presto/pull/3428\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14390", "Refactor flattenCollection to GeometryUtils", "James Gill", "jagill", "04/23/20, 05:14:10 AM", "We will need for this function in several places, and it is purely geometric.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14391", "Fix ClusterStatsResource", "Timothy Meehan", "tdcmeehan", "04/16/20, 12:53:33 AM", "Use DispatchManager for accurate queueing stats.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14392", "Support built-in SQL scalar functions", "Leiqing Cai", "caithagoras", "06/12/20, 12:37:18 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for defining SQL-invoked functions in plugins.\r\n* Add functions :func:`array_sum` and :func:`map_normalize`.\r\n```\r", "NaN"], ["14393", "Refactor AWS SDK client metrics collection", "James Petty", "pettyjamesm", "04/23/20, 08:34:30 PM", "Adds a parent abstract class to `PrestoS3FileSystemMetricsCollector` so that other SDK clients can share the metrics collector support.\r\n\r\nAdds reporting for client retry pause time indicating how long the thread was asleep between request retries in the client itself.\r\n\r\nFixes the reporting client timings. Previously, when the client retried a request only the first request timings would be recorded in the stats. Now, all request timings are reported individually.\r\n\r\nFixes `PrestoS3SelectClient` stats reporting. Previously, stats were collected in a `PrestoS3FileSystemStats` instance that wasn't actually registered with JMX.\r\n\r\nCross contribution of https://github.com/prestosql/presto/pull/3429\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Add AWS client retry pause time metrics to PrestoS3FileSystemStats\r\n* Fix AWS client metric reporting when using S3 select\r\n```\r", "NaN"], ["14394", "Fix failing SQL functions test", "Leiqing Cai", "caithagoras", "04/15/20, 09:59:19 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14395", "Add test coverage for MySqlConnectionModule", "Leiqing Cai", "caithagoras", "04/16/20, 01:44:12 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14396", "Remove compression for creating zero row file for missing buckets", "Rohit Jain", "jainxrohit", "04/16/20, 11:36:38 PM", "Using compression for zero row files for missing buckets was running into issues with ZSTD compression as it was not supported by downstream dependencies.\r\nNot using compression while creating zero row files is also better from efficiency and compression perspective. In my local testing I observed that for any unused bucket, it was generating 34 bytes with ZLIB, compared to 27 bytes with no compression.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\nRemove compression for creating zero row file for missing buckets\r\n```\r", "NaN"], ["14398", "Make PageFile format splittable", "Vic Zhang", "viczhang861", "04/28/20, 10:43:00 AM", "A pagefile footer has two parts\r\n\r\n- Offset of each stripe,  the first stripe always starts at offset 0\r\n- Size of entire footer in Integer\r\n\r\nFor example, for a file with 3 stripes (100 bytes each),  footer will be\r\n- Long integer array [0, 100, 200] \r\n- size = SIZE_OF_LONG * 3 + SIZE_OF_INT = 28 bytes\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14399", "Fix bug in IS_SUBNET_OF function", "Nikhil Collooru", "NikhilCollooru", "04/17/20, 03:48:20 AM", "    is_subnet_of (IPPREFIX '2804:431:b000::/37', IPPREFIX '2804:431:b000::/38') -- True\r\n    is_subnet_of (IPPREFIX '2804:431:b000::/38', IPPREFIX '2804:431:b000::/37') -- False\r\n\r\nWe need to check that both MAX and MIN of ipprefix2 lies in the subnet of ipprefix1. Otherwise both the above statements will return True. \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14402", "Update discovery-server to 1.31.", null, "sujay-jain", "04/17/20, 03:32:24 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14403", "Fix WarningsCollector", "Timothy Meehan", "tdcmeehan", "04/17/20, 09:58:55 PM", "When this was introduced in DispatchManager, it was unintentionally made\r\nto be stateful.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14404", "Remove deprecated functions from ConnectorSession", "Nikhil Collooru", "NikhilCollooru", "04/17/20, 05:43:32 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/958", "NaN"], ["14407", "Add config for orc compression codec", "Rohit Jain", "jainxrohit", "04/23/20, 02:30:27 AM", "Adding a configuration to handle compression codec for handling orc and dwrf storage format. Use hive.orc_compression_codec to override the generic compression codec for orc and dwrf storage format. The reason to add an extra configuration was the unavailability of uniform support of all compression codec across all storage formats. The ZSTD compression codec is only available for orc and dwrf storage format.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n--------------\r\nAdd configuration property `hive.orc-compression-codec` to override `hive.compression-codec` for ORC and DWRF formats. If specified, ORC and DWRF files are compressed using this codec. RC, Parquet, and other files are compressed using `hive.compression-codec`.\r\n```\r", "NaN"], ["14409", "Fix QueryResource for queued queries", "Timothy Meehan", "tdcmeehan", "04/24/20, 11:46:14 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14411", "Add Native Parquet Writer in Hive Module and Misc fixes", "qqibrow", "qqibrow", "05/26/20, 06:17:40 PM", "Please make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\nN/A\r\n\r\nHive Changes\r\nAdd native parquet writer, can be turned on by \"parquet_optimized_writer_enabled\" session property.\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14416", "Fix estimatedSerializedSizeInBytes calculation in ColumnarXXX", "Ying", "yingsu00", "04/21/20, 06:18:20 AM", "Fix TestUnnestOperator broken by https://github.com/prestodb/presto/pull/13746\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14418", "Drop presto-orc's dependency on presto-array", "Nikhil Collooru", "NikhilCollooru", "04/21/20, 07:10:10 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14420", "Fix writing tables with preferred ordering using temp path", "Shixuan Fan", "shixuan-fan", "04/21/20, 08:37:29 PM", "When SORTED_WRITE_TO_TEMP_PATH_ENABLED is true, we would require\r\na temporary path for sorted writes.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14421", "Remove unused code from TestRowBasedSerialization#testRandomBlocks", "Ying", "yingsu00", "04/23/20, 03:33:44 PM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14422", " Clean up PlanOptimizerProvider while dropping connector", "Yubin Li", "liyubin117", "04/22/20, 05:39:37 AM", "Previously, ConnectorPlanOptimizerManager.addPlanOptimizerProvider() is invoked while adding connector, But PlanOptimizerProvider is not cleaned up while dropping connector, Dropping and adding a connector with the same name will trigger an exception like \u201cConnectorPlanOptimizerProvider for connector '%s' is already registered\u201d. \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14428", "Remove positionCount check in createRandomLongDecimalsBlock", "Ying", "yingsu00", "04/28/20, 12:57:21 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14429", "Add flatten_geometry_collections function", "James Gill", "jagill", "06/03/20, 09:11:49 PM", "GeometryCollections are often problematic for geometry computations.\r\nFirst, some standard functions perform non-intuitively for them, or may\r\nthrow an exception.  Second, they can be arbitrarily nested, so ensuring\r\nthat you've flattened a geometry to contain no GeometryCollections is\r\nchallenging in SQL.  A common request is to recursively flatten any\r\nencountered GeometryCollections, which is what this function does.\r\n\r\nNon-GeometryCollections transform to a singleton array:\r\n`MULTIPOINT(0 0, 1 1)` goes to `[MULTIPOINT(0 0, 1 1)]`.\r\n\r\nGeometryCollections are recursively flattened to an array of their\r\n(leaf) constituents:\r\n`GEOMETRYCOLLECTION(POINT(0 0), GEOMETRYCOLLECTION(POINT(1 1)))`\r\ngoes to `[POINT(0 0), POINT(1 1)]`.  The order of the array is\r\nimplementation-dependent and not guaranteed.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeoSpatial Changes\r\n* Add `flatten_geometry_collections` function to recursively flatten `GeometryCollection`s.\r\n```\r", "NaN"], ["14431", "Fix query client timeout", "Timothy Meehan", "tdcmeehan", "04/27/20, 06:54:58 PM", "The `queryPurger` in `QueuedStatementResource` was inadvertently triggering a heartbeat when checking if the query was expired, preventing expiration.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14435", "Add session property for druid connector", "Zhenxiao Luo", "zhenxiao", "04/26/20, 11:30:25 PM", "\r\n```\r\n== RELEASE NOTES ==\r\n\r\nDruid Changes\r\n* session property, druid.pushdown, it added, to control whether to pushdown all query processing to druid\r\n```\r", "NaN"], ["14438", "presto-druid changed", "miller", "mqiang", "04/25/20, 06:39:39 PM", "\r\n```\r\n1.add presto-druid connector to server tarball\r\n2.Fix float value for druid connector\r\n3.escape keyword for druid column name\r\n```\r", "NaN"], ["14439", "Pre-calculate nested positions array size in BlockEncodingBuffer", "Ying", "yingsu00", "05/01/20, 02:13:01 PM", "Resolves https://github.com/prestodb/presto/issues/14433\r\n\r\nThe positions array for nested BlockEncodingBuffer used to be grown on\r\nthe fly. This uses extra CPU and makes the positions array larger than\r\nrequired because the growth factor was 2.0. This commit pre-calculates\r\nthe nested positions array size, allocate the memory in one shot, then\r\npopulate the positions.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14443", "Add function scale_qdigest", "Timothy Meehan", "tdcmeehan", "04/30/20, 04:16:31 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add ``scale_qdigest`` function to scale a ``qdigest`` to a new weight\r\n```\r\n\r\nReplaces #14072", "NaN"], ["14446", "Add release notes for 0.234.1 and 0.234.2", "Leiqing Cai", "caithagoras", "04/27/20, 08:55:01 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14449", "Cse filter", "Rongrong Zhong", "rongrong", "04/30/20, 11:56:15 PM", "Depends on #14303\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Enable `optimize_common_sub_expressions` for filter\r\n```\r", "NaN"], ["14451", "Fix interruption handling in Driver", "Andrii Rosa", "arhimondr", "04/29/20, 05:13:01 PM", "The issue was discovered when testing Presto on Spark.\r\n\r\nSpark emits the `interrupt` signal to cancel the task.\r\n\r\nIf the interrupt signal is emitted when the Driver is about to finish (before\r\nthe `isFinished` method is called) the execution never finishes.\r\n\r\nThe `finish` method tries to ackquire the lock that alwayws returns `false`\r\nwhen the interrupted flag is set. Since the `false` is interpreted as a failure\r\nto aquire the lock, the finish is called again and again as it never succeeds.\r\n\r\nSince the timeout for aquiring the lock is tiny (<=100ms) it is fine to try\r\nto aquire the lock in a non interruptible manner to avoid taking care of non\r\ntrivial semantics of interrupting.\r", "NaN"], ["14452", "Refactor presto spark launcher", "Andrii Rosa", "arhimondr", "04/28/20, 07:23:54 PM", "Extract Presto Spark bootstraping code into the PrestoSparkRunner.\r\nHaving a runner that can be initialized without providing the ClientOptions\r\nis more flexible, as it is not strongly coupled with the CLI arguments.", "NaN"], ["14455", "Fix duplicated table name in TestHiveIntegrationSmokeTest", "Vic Zhang", "viczhang861", "04/28/20, 06:44:57 PM", "Update test table name to avoid interleaving\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14456", "Handle pruned columns in a pushed down pinot table scan", "Devesh Agrawal", "agrawaldevesh", "05/01/20, 08:59:05 PM", "## Problem: _PruneUnreferencedOptimizer_ can prune columns after pushdown has been done.\r\n\r\n_PruneUnreferencedOptimizer_ runs after the connector pushdown optimizer and prunes away columns it thinks are not used in the table scan node. This creates a mismatch between the generated PQL and the columns expected during actual scan. This can happen in both broker and segment pinot scan codepaths.\r\n\r\nAs an example, consider this query:\r\n```\r\nselect count(*) from baseballstats group by teamid limit 10\r\n```\r\n\r\nIn this query, the pinot connector sees the plan: \r\n```\r\nProject (count) -> Limit 10 -> Aggregation (teamid, count(*)) -> Scan baseballstats.\r\n```\r\n\r\nSince projections cannot currently be pushed on top of aggregations, so only the plan until the limit is pushed down. The resulting plan looks like:\r\n```\r\nProject(count) -> Pushed table scan node with PQL: \"select count(*) from baseballstats group by teamid\".\r\n```\r\nNote that the pushed table scan will emit \"teamid\" and \"count(*)\". \r\n\r\nPruneUnreferencedOptimizer runs next and removes the \"teamid\" from the scan node. This creates a problem: we are emitting an extra column teamid in the PinotBrokerPageSource which the scan does not need. \r\n\r\n## Solution: Remember the original column handles that corresponded to the generated PQL\r\n\r\nThis PR attempts to handle this scenario: both in the broker and segment codepaths. It does so with a lot of plumbing: It remembers the original column handles when the PQL was generated in the _PinotTableHandle_. These are passed eventually to the page sources via the _PinotSplit_. \r\n\r\nThe page source then checks which columns can be ignored or have been re-ordered. The broker page source already had some support for this (necessitated by the _hiddenColumnSet_ for handling group-by without aggregations) and I have merely generalized this support. It was also trivial to add this support to the segment page source.\r\n\r\nAdded unit tests for these conditions in the pinot broker and segment page sources.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14457", "Refactor and create new presto-common module", "Nikhil Collooru", "NikhilCollooru", "05/06/20, 02:01:33 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/973", "NaN"], ["14458", "read druid query response stream by page", "miller", "mqiang", "04/29/20, 04:37:15 PM", "\r\n```\r\nread druid query response stream by page\r\nissues: https://github.com/prestodb/presto/issues/14440\r\n```\r", "NaN"], ["14460", "Implement JDBC PreparedStatement.getMetaData", "Adam J. Shook", "adamjshook", "05/15/20, 02:00:07 PM", "Addresses #14029 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nJDBC Changes\r\n* Adds PreparedStatement support for getMetaData\r\n```\r", "NaN"], ["14461", "Support compression for PageFile", "Vic Zhang", "viczhang861", "05/05/20, 06:32:24 PM", "A pagefile footer has three parts\r\n\r\n1. Compression\r\n\r\n- compression string length and compression name\r\n\r\n2. StripOffsets\r\n- Count of stripes and offset of each stripe\r\n3. Size of entire footer length in integer\r\n\r\n\r\n![pagefile_compression_test](https://user-images.githubusercontent.com/6372365/80649178-86f50a00-8a3f-11ea-8120-0fa4f511b7ff.png)\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14463", "Error on excessively large code generation", "Sreeni Viswanadha", "kaikalur", "05/01/20, 05:08:17 PM", "ASM ClassWriter has two modes - COMPUTE_MAXS, COMPUTE_FRAMES. The first one is really fast but the second is super expensive and can end up using a lot of memory for situations where it was going to fail anyway. So we first do a call with COMPUTE_MAXS to get any errors out quickly and this will help in not causing the GC storms and worker crahses.\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Call the ASM's simple ClassWriter with COMPUTE_MAXS flag to flush out things like code size too big before we do the expensive COMPUTE_FRAMES call which can result in worker GC storms.\r\n\r\n```\r", "NaN"], ["14465", "Partition projections by common subexpressions", "Rongrong Zhong", "rongrong", "05/08/20, 11:05:22 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14466", "Fix getBlockView to use positionCount instead of outputPositionCount", null, "bhhari", "05/05/20, 01:39:18 PM", "Fix the block view to return right number of positions incase of all Nulls using the positionCount\r\nFixes #14470 \r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14468", "Fix integer overflow when creating a BigIntValues Filter", null, "bhhari", "05/05/20, 07:11:58 PM", "Fix Int overflow exception which is caused when calculating (max - min + 1 ) because of long wrap around. \r\n\r\nFixes #14469 \r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14471", "Simplify searched case translation", "Sreeni Viswanadha", "kaikalur", "05/19/20, 09:44:25 PM", "Currently, SearchedCase is translated as nested IF expressions and even with 500 cases, we generate a complicated expression potentially causing stack overflows. So the fix is to translate searched case as:\r\n\r\nCASE TRUE WHEN p1 THEN r1 WHEN p2 THEN r2.. ELSE r_else END\r\n\r\nwhich will result in SWITCH operator which is flatter.\r\n\r\nAlso, fixed RowExpressionInterpreter to not call then part and else part unconditionally in interpreting IF.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fixed translation of searched case expressions to be flat\r\n\r\n```\r", "NaN"], ["14472", "Lower startup-grace-period for TestRaptorIntegrationSmokeTestMySql", "James Sun", "highker", "05/04/20, 07:37:21 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14473", "Add release notes collection script", "Leiqing Cai", "caithagoras", "05/07/20, 03:54:45 AM", "Add the shell script in presto so that it's easier for people to use.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14475", "FastutilSetHelper ObjectStrategy#equals result can be null.", null, "sujay-jain", "05/07/20, 06:06:14 PM", "We're seeing some queries fail due to this [verify](https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/util/FastutilSetHelper.java#L208) check. The assumption here seems to be that ```SET_CONTAINS``` would be used only for non-complex types, but it reality can also be used for complex types - eg: the test case included in [TestHivePushdownFilterQueries](presto-hive/src/test/java/com/facebook/presto/hive/TestHivePushdownFilterQueries.java)  \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14476", "Add release notes for 0.235", "Leiqing Cai", "caithagoras", "05/16/20, 12:17:43 AM", "# Missing Release Notes\n## Andrii Rosa\n- [x] https://github.com/prestodb/presto/pull/14451 Fix interruption handling in Driver (Merged by: Andrii Rosa)\n- [x] https://github.com/prestodb/presto/pull/14452 Refactor presto spark launcher (Merged by: Andrii Rosa)\n- [x] https://github.com/prestodb/presto/pull/14356 Revert #13699 (Merged by: Andrii Rosa)\n\n## John Roll\n- [x] https://github.com/prestodb/presto/pull/14331 EXPLAIN (TYPE IO) support for Aria (Merged by: Maria Basmanova)\n\n## Nick LaGrow\n- [x] https://github.com/prestodb/presto/pull/14352 Add Internationalization (i18n) plugin and Myanmar UDFs (Merged by: Rongrong Zhong)\n\n## Rohit Jain\n- [x] https://github.com/prestodb/presto/pull/14407 Add config for orc compression codec (Merged by: Maria Basmanova)\n- [x] https://github.com/prestodb/presto/pull/14396 Remove compression for creating zero row file for missing buckets (Merged by: Maria Basmanova)\n- [x] https://github.com/prestodb/presto/pull/14244 Add ExtendedFileSystem (Merged by: James Sun)\n\n## Tim Meehan\n- [x] https://github.com/prestodb/presto/pull/14443 Add function scale_qdigest (Merged by: Leiqing Cai)\n\n## Vic Zhang\n- [x] https://github.com/prestodb/presto/pull/14329 Simple PageFile format (Merged by: Andrii Rosa)\n\n## Wenlei Xie\n- [x] https://github.com/prestodb/presto/pull/13854 Support table write commit in Presto on Spark (Merged by: Andrii Rosa)\n\n## miller\n- [x] https://github.com/prestodb/presto/pull/14458 read druid query response stream by page (Merged by: Zhenxiao Luo)\n- [x] https://github.com/prestodb/presto/pull/14438 presto-druid changed (Merged by: Zhenxiao Luo)\n\n# Extracted Release Notes\n- #13699 (Author: Curt): HMS Impersonation Access and breakdown metrics by hosts\n  - Impersonation Access by using HMS delegation token.\n  - Enable multi HMS instances load balancing and breakdown metrics by HMS hosts.\n- #13746 (Author: Ying Su): Optimize UNNEST\n  - Improve performance of UnnestOperator.\n- #14214 (Author: Leiqing Cai): Verifier fixes and improvements\n  - Fix an issue where resubmitted queries always fail.\n  - Add support to output verification results for failures due to Verifier internal errors.\n  - Add support to skip teardown queries in case control and test queries succeeds but verification fails. This can be enabled by configuration property ``smart-teardown``, which replaces ``run-teardown-on-result-mismatch``.\n- #14255 (Author: Leiqing Cai): Relax length limit of parameter type list and return type\n  - Improve ``CREATE FUNCTION`` to allow parameter type list and return type to have a length up to 30k characters.\n- #14275 (Author: Xiang Fu): Upgrade presto pinot connector to be compatible with new Broker routing and time boundary APIs in Pinot 0.3.0 release\n  - Adding support for new Pinot Routing Table APIs.\n- #14302 (Author: David Taieb): Add ST_Centroid for points, multi-points on a sphere\n  - Add support for :func:`ST_AsText` to accept Spherical Geographies.\n  - Add support for :func:`ST_Centroid` to accept Spherical Geography Points and MultiPoints.\n- #14303 (Author: Rongrong Zhong): Extract common subexpression in page projection at codegen\n  - Add optimization for page projection by extract and compute common subexpressions among all projections first. This optimization can be turned off by session property ``optimize_common_sub_expressions``.\n- #14320 (Author: Ying Su): Add max buffer count config property for optimized repartitioning\n  - Add config property driver.max-page-partitioning-buffer-count, to limit the total number of buffers per optimized repartitioning operator. The default value is 1,000,000.\n- #14322 (Author: Xiang Fu): Adding more support for  Pinot types to Presto type conversion\n  - Support mapping Pinot `BYTES` data type to Presto `VARBINARY` type.\n  - Support mapping Pinot time fields with days since epoch value to Presto `DATE` type.\n  - Support mapping Pinot time fields with milliseconds since epoch value to Presto `TIMESTAMP` type.\n  - Put Pinot Field type in to column comment field shown as DIMENSTION, METRIC, TIME, DATETIME, to provide more information.\n  - Adding configs 'pinot.infer-date-type-in-schema' and 'pinot.infer-timestamp-type-in-schema' to switch on/off the type conversion. Default is OFF.\n- #14324 (Author: Ariel Weisberg): Add peak [task] total memory to returned stats \n  - Add peak [task] total memory to returned stats.\n- #14326 (Author: Shixuan Fan): Support caching all tables in file status cache\n  - Added support in file status cache to cache all tables. This could be enabled by setting ``hive.file-status-cache-tables`` to ``*``.\n- #14332 (Author: Leiqing Cai): Make table properties a config property\n  - Add support for specifying table properties override for temporary Verifier tables, through configuration property ``control.table-properties`` and ``test.table-properties``.\n- #14334 (Author: Rongrong Zhong): Fix named query output does not match column list\n  - Fix VerifyException when named query is a table reference (:issue:`14333`).\n- #14335 (Author: Zhenxiao Luo): Add native parquet writer\n  - Native Parquet Writer for Presto.\n- #14340 (Author: Rongrong Zhong): Fix NullPointerExpression in ExpressionInterpreter::visitBindExpression\n  - Fix potential `NullPointerException` in `ExpressionInterpreter::visitBindExpression` when processed values contain `null`.\n- #14341 (Author: Xiang Fu): Best effort push down distinct count to pinot\n  - Add capability to push down distinct count query to Pinot with best effort.\n- #14354 (Author: Leiqing Cai): Support verifying SELECT queries with certain non-storable columns\n  - Add support for verifying ``SELECT`` queries that produce columns of ``TIME``, ``TIMESTAMP WITH TIME ZONE``, or ``DECIMAL`` types, or columns of structured types with those types.\n- #14355 (Author: Venki Korukanti): Pass the correct value for fileSize in Hive internal split factory\n  - Fix a bug in Hive split calculation which affects Parquet reader in few corner cases.\n- #14365 (Author: Leiqing Cai): Disallow invoking SQL functions in SQL function body\n  - Add check to disallow invoking SQL functions in SQL function body.\n- #14366 (Author: Leiqing Cai): Support SHOW CREATE FUNCTION\n  - Add support for SHOW CREATE FUNCTION.\n- #14372 (Author: Zhenxiao Luo): count distinct pushdown for druid connector\n  - Count distinct pushdown for druid connector.\n  - Fix druid connector segment scan.\n- #14383 (Author: Shixuan Fan): Add preferred ordering columns for unbucketed table\n  - Added table property preferred_ordering_columns to support writing sorted files for unbucketed table. The list of ordering columns could be specified using the preferred_ordering_columns table property.\n- #14386 (Author: Ariel Weisberg): Add soft memory limit configuration properties\n  - Add soft memory limit configuration properties. Soft memory limits are default memory limits given to each query that can be overridden using session properties up to the hard limit set by the existing configuration properties. Available soft memory limit configuration properties are ``query.soft-max-memory-per-node``, ``query.soft-max-total-memory-per-node``, ``query.soft-max-total-memory``, and ``query.soft-max-memory``.\n- #14393 (Author: James Petty): Refactor AWS SDK client metrics collection\n  - Add AWS client retry pause time metrics to PrestoS3FileSystemStats.\n  - Fix AWS client metric reporting when using S3 select.\n- #14435 (Author: Zhenxiao Luo): Add session property for druid connector\n  - Session property, druid.pushdown, it added, to control whether to pushdown all query processing to druid.\n- #14449 (Author: Rongrong Zhong): Cse filter\n  - Enable `optimize_common_sub_expressions` for filter.\n- #14463 (Author: Sreeni Viswanadha): Error on excessively large code generation\n  - Call the ASM's simple ClassWriter with COMPUTE_MAXS flag to flush out things like code size too big before we do the expensive COMPUTE_FRAMES call which can result in worker GC storms.\n\n# All Commits\n- c609bf9cf4c497eb11f2f933269063238df52899 Prevent ASM library causing worker GC storms. (Sreeni Viswanadha)\n- b13f60bd4d27395d059756dce944b86b6b419694 Pre-calculate nested positions array size in BlockEncodingBuffer (Ying Su)\n- a75114b306232a7565075da4f53cb83a9157ca62 Enable common sub expression optimization in PageFunctionCompiler filter (Rongrong Zhong)\n- f9f9d83c84dbb35fc49e5b4c70326024d8ff3d1e Support SHOW CREATE FUNCTION (Leiqing Cai)\n- ecfe63ece55cf63e49cd9da00bdd55c691faf77b Unify QualifiedName to QualifiedFunctionName conversion (Leiqing Cai)\n- b6659dac0d67ff8dc80561b97fb0794a9a7eb0f8 Performance benchmark for common sub expression projection (Rongrong Zhong)\n- c096a8e9a913939e632bf86bc1c3fb34fd8d9e02 Generate projection bytecode based on common sub expression (Rongrong Zhong)\n- bc8e55724705f209977ac368ab9815a4da7e0a44 Change PageFunctionCompiler to compile a list of projections (Rongrong Zhong)\n- a95a5bf3eb305992e6cd45ffd1cea83549e6e465 Remove PageProjection::getType (Rongrong Zhong)\n- ee1f00442f315024c1d1e8703f7c66a0587050b2 Add API in PageFunctionCompiler to compile a list of projections (Rongrong Zhong)\n- 555df6a06ceda5a8bc43928486bd12820eb71b72 Change PageProjection to produce List<Block> (Rongrong Zhong)\n- 1361d6df2052191e7997f2104f59f11c4e59456d Fix named query output does not match column list (Rongrong Zhong)\n- a5a04929e51756cac167c14cf73399a0df74f4c5 Fix invalid quantile to be a user error (Tim Meehan)\n- 34500b106c5309ac5c6e2b9af71e114da60fd838 Add function scale_qdigest (Tim Meehan)\n- 52eaac51fbc9a3e15857266ca06b4ec5db64ded4 Fix interruption handling in Driver (Andrii Rosa)\n- 84451771c18eed5c81a6c8a4e077c8f87de9a1d2 read druid query response stream by page (miller)\n- c9f3fb634808508c3245ef88fe54745bc76f358b Refactor presto spark launcher (Andrii Rosa)\n- 22d97dde8c213eeb345ca3a9ac62aac6ce567bbb Fix duplicated table name in TestHiveIntegrationSmokeTest (Vic Zhang)\n- aaeaa5ba217dc5af865e624b388823be86d8f03b Remove positionCount check in createRandomLongDecimalsBlock (Ying Su)\n- ca3f93f2597303657dd9a07a5affd6547d6a171c Fix \"bound must be positive\" exception when creating DictionaryBlock (Ying Su)\n- 0a0c6ff04654b61140ae9a0aef2a04ac735c6a2a Support splittable read of PageFile (Vic Zhang)\n- f8fac992cdcb0c1a74f660e30f7c422a1f00491a Create zero row file for PageFile format (Vic Zhang)\n- 1cc59eb4d01692f8ec43810c46b59a58820b29d8 Add footer to PageFile format (Vic Zhang)\n- 2bfddd03187f5e81da333fe31fbf5ece21607d68 Support table write commit in Presto on Spark (Wenlei Xie)\n- c93c8ce8dcd3763b79eeb3f947f17058345ce54f Introduce PageSinkCommitStrategy (Wenlei Xie)\n- 8df52544dbda9aa23d0379c39ca84df3673c565e Rename partition / lifespan commit into page sink commit (Andrii Rosa)\n- 2c9385a1f03e75d30e9cd5070138740d5ec1f10d Rename PageSinkProperties#isPartitionCommitRequired (Wenlei Xie)\n- b4f826e1f6a7f1ce33d4d4a3da68826e79a099cc Refactor HiveWriterFactory (Wenlei Xie)\n- 0e6c5ffd1df775e5da562fa1f0a809ccdbf94939 Fix query client timeout (Tim Meehan)\n- 34b3f6a75b333d5c072bd81e0638ee7711bb68a1 Add session property for druid connector (Zhenxiao Luo)\n- a20247b1e77890381faa068b5564af8066031feb escape keyword for druid column name (miller)\n- 7a988d5657d537516317c362be2d015dd5ff8417 Fix float value for druid connector (miller)\n- 353853bde9f528441c8fa4f70f1d9a5d31acc497 add presto-druid connector to server tarball (miller)\n- 4405e0cbc7af572d4457b5d6331fa59912a68af4 Allow queued queries to be preempted and canceled (Tim Meehan)\n- 25fe380c3305b0d744abe252dbebaa6e5405382e Fix QueryResource for queued queries (Tim Meehan)\n- 90e74057d960665351529af8e2bc582947f2884e Display built-in functions first in SHOW FUNCTIONS (Leiqing Cai)\n- b61e0b698a0722a312e36eedba6a120c3c5d6e1f Use shared metrics collector in PestoS3ClientFactory (James Petty)\n- ab241b8443c7ba6102f93de836fde9c43724f37d Refactor AWS SDK Client Metrics Collection (James Petty)\n- 22a421bb426ca9fc848c8b37ecc37bdfe8872ca0 Remove unused code from TestRowBasedSerialization#testRandomBlocks (Ying Su)\n- fc81c05a26cec5d8f532c613161d6709bd478db7 Refactor flattenCollection to GeometryUtils (James A. Gill)\n- 8f7edbd6b2d491b6c124cbdd0e27c3015a98f728 Add config for orc compression codec (Rohit Jain)\n- f220d7f47f0fa0f8f9dc5bc446903f0cbffe170b Add soft memory limit configuration properties (Ariel Weisberg)\n- 6aac57fee4b1f177330e4c164b703209da5934ee Clean up PlanOptimizerProvider while dropping connector (Yubin Li)\n- 7a01f2be75286cc3ba2cfe15939f5d5773d95822 Make parquet's schema check more tolerant for adding and removing sub-field in struct (Beinan Wang)\n- 746c290a0374c987fc8935ff7e3eb1df34a46d9b add unit test for parquet schema mismatch checker and fix code review issues (Beinan Wang)\n- 5368a56d78ed72ad5e1ccf2d025083ec98f5935c Use field name for Parquet schema mismatch checking (Beinan Wang)\n- e1ddc0e2cdfd3ed5089e55b7c1c20d84bc491381 Fix writing tables with preferred ordering using temp path (Shixuan Fan)\n- 957ee09d43ca50bd1902f019a82cb251824420bc Drop presto-orc's dependency on presto-array (Nikhil Collooru)\n- 8f12b44d803a5fd1d47fff358d6cd69eaeb6d161 Fix estimatedSerializedSizeInBytes calculation in ColumnarXXX (Ying Su)\n- 96c6da60bbe9df7031d75ab88e82b5a31dcbdc61 Improve BenchmarkUnnestOperator (Ying Su)\n- 6e09add9d0ad7968c7644065cef15efc7f259845 Optimize Unnest (Ying Su)\n- aa240aa73d114081d254abf7e86a225b46cec7ef Refactor Unnester (Ying Su)\n- 36df11f11116cc0a5f66ca0d9d49bd76aa7b3968 Refactor UnnestOperator (Ying Su)\n- 78225ade5d59640fff8ca3362bf77c56a4d6315a Add appendNull() to Block interface (Ying Su)\n- e64bb52f1f4585979c3b00d521ab11dae46644ee Add support for generating blocks and pages with specified percentage of nulls (Ying Su)\n- 01d9ef470f3ef81c444fcf3a66affbd19a70551b Add benchmark for copying blocks (Ying Su)\n- 3e4f18232ef791188f3388377fc0889922f23333 Add preferred ordering columns for unbucketed table (Shixuan Fan)\n- 77530ba5863c1ff3952555e51844c1ae6802058d Move sorting column string conversion methods into SortingColumn (Shixuan Fan)\n- bf4485f24780320a2194e7bb2c5676f2bd85b39c Replace comma string with static constant COMMA (Shixuan Fan)\n- c8dbd9a149229d1912f0280fbcd340beee7ea7f1 Fix WarningsCollector (Tim Meehan)\n- dd0a82f449b5d5088a443c5284c1fd7f140cb65f Remove deprecated functions from ConnectorSession (Nikhil Collooru)\n- 0e916c680aae4339ed8ec788184183866251d283 Update discovery-server to 1.31. (Sujay Jain)\n- 6bab32ac4639256314ead93d8d970cb056f1c1f2 Drop presto-orc dependency on ConnectorSession (Nikhil Collooru)\n- a2a4c46aaf5163fdd07960e764a3610619be82be Send SqlFunctionProperties to operators (Nikhil Collooru)\n- 42d5f118fdda6ded6b637c3d53b2a93f7624e3ff Add sessionStartTime,sessionLocale,sessionUser to SqlFunctionProperties (Nikhil Collooru)\n- 8c469bda0657f41c146796ea2ccaaea6dfa552e6 Fix bug in IS_SUBNET_OF function (Nikhil Collooru)\n- 3384c20a24c7118cde3f6eefd4d7c4fa7e3058e7 Remove compression for creating zero row file (Rohit Jain)\n- 669cf6ea45945d61d90784dfc65be2804a5f81b2 Add Internationalization (i18n) plugin and Myanmar UDFs (Nick LaGrow)\n- 60fbdb065db0d7f4e0d34a7da638f812842629e2 Fix EXPLAIN (TYPE IO) when Aria scan is ON (John Roll)\n- 24be9ab8acc63ee0a3c188a2d7d53b40bf3250d0 Use NO_NODES_AVAILABLE Exception for affinity scheduler (Ke Wang)\n- 29afd77004af88c4f13702893ff75e90ccec854f Add test coverage for MySqlConnectionModule (Leiqing Cai)\n- d8c6b8f62eaa6720e6e3f0582f3d8a31c9a8763c Fix ClusterStatsResource (Tim Meehan)\n- 319aff3694d9c22ab3f625951a28599e92e5924e Fix failing SQL functions test (Leiqing Cai)\n- f62b73b33bb670db98b25bc7f318d1c25b9f21c0 Add subfield pruning to StripeReader (Bhavani Hari)\n- eb60ad9301dee48da99d1f3d9ed83bbcdf0bfba4 Add status endpoint for HTTP HEAD requests (James Petty)\n- a48650226a9d917a4ea47ff7ae9cda000da4105c Reduce LazyBlock copy / wrapping in ScanFilterAndProjectOperator (James Petty)\n- bcabbdf933076c5a82428d15a9792f10fcf337de Disallow invoking SQL functions in SQL function body (Leiqing Cai)\n- 80b03de610e306851e64a55ffde7e0740549d729 Support function namespace in integration tests (Leiqing Cai)\n- 0d3235a41d1b9c7ce57ed96deb36b45444e10d6c Update h2 to 1.4.199 (Leiqing Cai)\n- 9e5eb477b8b5741bb2e482d1beb7d20751324ecd Speed up GC of MapFlatSelectiveReader by clearing member variables in close() (Bhavani Hari)\n- ab52a1a58763575a713b240a407a3d7b52a3b944 Speed up GC of DecimalStreamReader by clearing member variables in close() (Bhavani Hari)\n- 34841afb54a2d5bea0f3731b790592d080a0caa7 Speed up GC of StructStreamReader by clearing member variables in close() (Bhavani Hari)\n- 25faa639cbda56ae53d5326d8dd2a8ecf27b2fec Speed up GC of ListStreamReader by clearing member variables in close() (Bhavani Hari)\n- dc60bb22f5d640289c062750b65a1afbcbef4525 Speed up GC of TimestampSelectiveReader by clearing member variables in close() (Bhavani Hari)\n- 615879039567b1afc9dfe8b5f67a626c052052a0 Speed up GC of DoubleSelectiveReader by clearing member variables in close() (Bhavani Hari)\n- 3a78b32bdc0d835807020b57c01a42e3e03c2b5a Speed up GC of FloatSelectiveReader by clearing member variables in close() (Bhavani Hari)\n- 13738ff96fe65a30252bd1870a099ad5daa7b26a Speed up GC of BooleanSelectiveReader by clearing member variables in close() (Bhavani Hari)\n- 3d66fde80b47676618ab030aa3debc557b31f184 Speed up GC of ByteSelectiveReader by clearing member variables in close() (Bhavani Hari)\n- c5c58158a6973ea36a62d107922d6fb334b1b808 Speed up GC of OrcReader by clearing member variables in close() (Bhavani Hari)\n- 3559e8823df01e47af51ff67b40a3aec7764d245 Speed up GC of MapSelectiveReader by clearing member variables in close() (Bhavani Hari)\n- 924c584f737c445009c8d58047fbf31d7bbc056c Speed up GC of LongSelectiveReaders by clearing member variables in close() (Bhavani Hari)\n- 4ac5783f84360c25615723b14afa55398fe7e70a Speed up GC of SliceSelectiveReader by clearing member variables in close() (Bhavani Hari)\n- d9d839837afa086c600b5493f23544e70085cc6d Add PageFile format in HiveFileFormatBenchmark (Vic Zhang)\n- 777ff920abaa6f26a036e4668100903e1abe3b72 Disable unnecessary test for PageFile format (Vic Zhang)\n- 7f329aab533b1af8602b2e10f188fad82204953f Add control property for PageFile stripe max size (Vic Zhang)\n- efdeb1274a1907782bcc54d78ae4608315facdfb Create PageFilePageSource to read PageFile (Vic Zhang)\n- 7f1138dfbf7c36014f6eed7eff50221fd9e33827 Support write serialized pages to PageFile format (Vic Zhang)\n- 191979c7b75d7483cfad14d2afca85ad84e561fc Add BlockEncodingSerde to ConnectorContext (Vic Zhang)\n- cf42f75855174b26fd5199b683de3ddee1ad59f0 Add ST_Centroid for points and multipoints on a sphere (David Taieb)\n- 5a969b2739dc41aabb2a1d67ac99ad74c83ebc94 Enforce buffer size limits for BlockEncodingBuffer (Ying Su)\n- ef718277394f712ad07c8387a1f565aef4cab8b1 Introduce estimatedSerializedSizeInBytes in DecodedBlockNode (Ying Su)\n- d1f21b5d17cdfe755b4fd45eed7dc7f9bbaef67f Introduce estimatedSerializedSizeInBytes in ColumnarXXX (Ying Su)\n- e83844e06373c6ebfd921b816dadcdcb4b3627c3 Rename pageSize to partitionBufferCapacity (Ying Su)\n- 6ec48b2a2b50442640d0b095f3d72397ada11aae Refactor getRetainedSizeInBytes in DecodedBlockNode (Ying Su)\n- 61679f4f0778d700402001226240c46543c87fde Change nested buffers to AbstractBlockEncodingBuffer (Ying Su)\n- d90e533ff9e9ba34c50286e00e63bb983522172a Rename OrcDataSink to DataSink (Vic Zhang)\n- 8c65a959e526b95d61e6a6350987e4ab09a23abb Rename OutputStreamOrcDataSink to OutputStreamDataSink (Vic Zhang)\n- eaa0c57c7a5e00b83f2bc8c74a4e81d77c6300c2 Rename OrcDataOutput to DataOutput (Vic Zhang)\n- ad004b74fd3412f4c9fb94d820e8a012b6450c14 Remove unused methods in TestDruidQueryBase (Zhenxiao Luo)\n- 5f89b532e12ef653ad2a5acc577bd5acdf5f8eb6 Fix druid connector segment scan (Zhenxiao Luo)\n- 1e51f2f2d52b1f7ce0719330eea597bb274d077b count distinct pushdown for druid connector (Zhenxiao Luo)\n- b7a0941b8cf6211c4d0ad1246fcdc0e7c2d48435 Fix section labeling for KHyperLogLog documentation (Leiqing Cai)\n- e3386889222438453aac2bf448777c943a507321 Fix NPE when reading structs with a newly added subfield (Bhavani Hari)\n- 61681cfe20c37d0a2252d9417ae9a67032af79f7 Best effort to push down distinct count function to pinot (Xiang Fu)\n- 99653c6e1989b2bb8cd5e20c59027950482a577d fixing date type in pinot-connector (Xiang Fu)\n- 3c2d9d80e4aba2c7fcba2a8b1df28c4c66875221 Handle empty and arbitrary slices in Parquet reader (James Petty)\n- fdfd5dfdc8090c56eac750f02e914596b8f8e1e0 Optimize metastore communication when writing to partitioned table (Andrii Rosa)\n- d699f4687bec616344054c3a27806e23a2e4c863 Add smoke test for insert into immutable partition (Andrii Rosa)\n- 0366472937d460808550fa883c6da9e4e7bc2104 Refactor TestHiveIntegrationSmokeTest (Andrii Rosa)\n- f7ecf0e720914f7a53d73789a08de0eb14526a17 Refactor HiveWriterFactory#getWriterParameters (Andrii Rosa)\n- 701c24aefddbd1dbb45fc6428c1c9ad53dc6771d Refactor HiveWriterFactory#createWriter (Andrii Rosa)\n- 4c4307526af2185a8b494d29d59f654d48d51773 Make timestamp columns nullable (Leiqing Cai)\n- 030052bd17d519cc0b8a8662704f646ef905b066 Relax length limit of parameter type list and return type (Leiqing Cai)\n- a8de10f13138f4d9ee3746db83f503eabd09cd6e Improve TestMySqlFunctionNamespaceManager (Leiqing Cai)\n- d4fc345d1bd1cfe5403e6fc332da29a27f13f456 Support verifying SELECT queries with certain non-storable columns (Leiqing Cai)\n- 10c3bd02db21e19d89ed01986c8a22adabaf82d0 Add Alluxio Catalog in Cache documentation (Haoyuan Li)\n- 95bcb3947cad1570e19a0adaebc58009aa362ada Drop presto-orc's dependency on presto-memory-context (Nikhil Collooru)\n- d9fc4620e554cb97b4c6fffd538b40c3f790b3e1 Add ExtendedFileSystem (Rohit Jain)\n- 2d9e768a03b5c9d804825c9a489383465f373801 Fix bug AddExchanges dropping filter when pushdown is enabled (Saksham Sachdev)\n- 726e18ae85e583b8493c39f97761e9f2976e2298 Add max buffer count config property for optimized repartitioning (Ying Su)\n- 36141ed8ea368117d30a3f0abb9f3d2ebd452dd1 Add peak [task] total memory to returned stats (Ariel Weisberg)\n- 5c296b93e07cb923ff8fcf3a31a85379d0fc155b Fix minor typo in error message (Shixuan Fan)\n- 11b8d41c866eebd8c46e8a7d691c0979aa27cee8 Report failure to compile pushed down filter as COMPILE_ERROR (Sujay Jain)\n- 30c0c9a3c33b764a1163b60e554d5707dc974b22 Reduce instance size and block copies in Page class (James Petty)\n- 5812d786c14d6d194d82ac6a5c549d27907e906e Pass the correct value for fileSize in Hive internal split factory (Venki Korukanti)\n- 824f282594be1b8135938e46d3a1d201793e96d0 Revert \"Hive Meta Store impersonation access\" (Andrii Rosa)\n- fa769f73f0fce1d6cfce52b04a3eaa6382b038bb Revert \"HMS multiple instance metrics\" (Andrii Rosa)\n- 88f304cca162c8d8e1e95b7401ca0a997f2ec253 Revert \"refactoring to use HMS Authentication Module\" (Andrii Rosa)\n- 4b2e750abdaee7ced9b0ab555065bbd52fa1b7a4 Revert \"add Config for multiple hms instances\" (Andrii Rosa)\n- 837f6a0afee5f7a474cca323927084b47f88c0a7 Revert \"Update HMS memory settings\" (Andrii Rosa)\n- a82d63caf226f293c3e1d69fbe294f2f9b2615f1 Revert \"address review comments\" (Andrii Rosa)\n- c615632b9d2fe0332097a4c0a043fdd68fee6e23 Use full name for Parquet classes (Zhenxiao Luo)\n- d041bdeca2867551b6a3ba2b5e29bbee247ddb04 Add native parquet writer (qqibrow)\n- 824fbc2f5f30cfdb04bb0484269378803873334c address review comments (Curt)\n- 358f72724121a32bfce58d902e3359c54de87bda Update HMS memory settings (Curt)\n- ff9ae918ecf9f0bec7aa9f9f401d70160e231bea add Config for multiple hms instances (Curt)\n- a5321125a6ce49f4b9bd22bb8435b436c49b3551 refactoring to use HMS Authentication Module (Curt)\n- 4d144f0474854377c9acf430f7926280c6da9739 HMS multiple instance metrics (Curt)\n- bb21a83c200ed6a3dc336587c29623693a785069 Hive Meta Store impersonation access (Zhongting Hu)\n- 719823f92aea236bd7f7627014e5c384ef969e22 Add documentation for KHyperLogLog (Tim Meehan)\n- 05e03901374b809347d79b2aeb486dcb1429adc6 Fix NullPointerExpression in ExpressionInterpreter::visitBindExpression (Rongrong Zhong)\n- b9eab4443caa0d2a5a1f09083f7cd4173e786862 Make table properties a config property instead of hard-coded (Leiqing Cai)\n- 0bed384af9ae769b0d5096a75ca6bcc833ed52b0 Drop presto-orc's dependency on presto-hive-common (Nikhil Collooru)\n- abf9a5bf37b70a4c05f3dfa8b4528bd310074837 Move MarkDistinctNode to SPI (Xiang Fu)\n- c3334ebe99ec8aa9931de0934c20736bd967f2a3 Add a page on caching (Bin Fan)\n- fcb7e4180ec4b816c6d89f68ddf7a7d5008ed05f Upgrade aircompressor to 0.15 (Zhenxiao Luo)\n- bf9b5261805a8c654c9e630fa18ac8e13a8cba11 Support caching all tables in file status cache (Shixuan Fan)\n- a1d0999c83b193f305a15cb5a5127dd17496c197 Update Presto schema example in pinot connector doc (Xiang Fu)\n- 30584d20ed02832e1f0087b5a23264f186e0c655 Address comments (Xiang Fu)\n- c1412c7636f13572d7de0cf62efa617e730ac7d0 Adding config to swtich on/off date type and timestamp type inferral (Xiang Fu)\n- cd439c500db4e13513263c9b69b550d39e52d6f9 Adding support for Date and Timestamp data type support for Pinot (Xiang Fu)\n- 68a0632bed80a896df3b41fa74080c83c963ccd4 Fix bugs in Druid connector aggregation pushdown (Zhenxiao Luo)\n- 3facc7374a17755cc979235ee242a13265cc5a7c Fix getRetainedSizeInBytes for ColumnarXXX (Ying Su)\n- 3f03c2c31986da23ec2b91772da2d6dc961c5246 change variable errMsgSplits to errorMessageSplits (Xiang Fu)\n- 175cddb830f87836a3b01fcea6dc2075318d01eb Address comments (Xiang Fu)\n- 0ebc22050b6e93e2f1dd0d3e5fdc2dbe3c01333a Upgrade presto pinot connector to be compatible with new Pinot Broker routingTable and time boundary APIs in Pinot 0.3.0 release. (Xiang Fu)\n- 3e64b8a863757727eca224645e523284da4f5803 Fix memory allocation for nestedPositions in the map reader (Masha Basmanova)\n- 489c233a86eb89eb3ae0410b6e8c83892e558333 Reduce memory allocations in OrcInputStream (Masha Basmanova)\n- 37a9750fa82507f2ac980f1c723915faf5dfd851 Reduce memory allocations in SliceDirectSelectiveStreamReader (Masha Basmanova)\n- 7eb85a22e35f782387d0cdcb78abe79eee4cd645 Add SqlBaseParser initializer hook and refreshable ATN caches (James Petty)\n- da2cc5620075d12abb19f3860a5459615f515cb4 Restart verification if checksum queries fail with table already exists (Leiqing Cai)\n- b65fc7c4cdb2aac7de3dc84e7834957da63a03bb Improve teardown skipping (Leiqing Cai)\n- 2df407bd7b64d95ea8450e7f6a116de75818497e Fix AbstractTestVerifierIntegrationSmokeTest (Leiqing Cai)\n- 7f833d0b27ac96b5d8f8d7c76e5c426992409dfe Expose unexpected exceptions in Verifier (Leiqing Cai)\n- 7e74c269a29598c66c6434db6f455a2523f19109 Improve readability in AbstractVerification (Leiqing Cai)\n- aa4c4ff1879f23308b5920bd65d98f1e3483f256 Fix verification resubmission (Leiqing Cai)\n- cfe1d3dc79c5f6cf028202cf1a8701c0f49dae1d Improve VerificationContext (Leiqing Cai)\n- 3ffdf85861030f520da7e35c6c99e0f5d8dccdde Move PagesSerde and PagesSerdeUtil to SPI (Vic Zhang)\n- 0afbdb36b6b2f1d4e9fb0169061c5b2f1a067b35 Add PageCompressor in SPI (Vic Zhang)\n- f98d7adc65e356c610fe1f0050e939ef4f8d3403 Move SpillCipher to SPI (Vic Zhang)\n- f98e644d18937f2eb38e55b0ff16c26e153d985e Remove guava dependency from PagesSerdeUtil (Vic Zhang)\n- 2bba1ccf3dfe2a83c59402fbddd387b4d74def51 Remove data copy from InMemoryRecordSet (James Petty)\n- 5c52eeaebee5ec61479e585dbf27dcc09fcf2155 Optimize Slice functions to avoid Slice.getBytes() (James Petty)\n- af1f416328bac2f130130bf473be667349209e48 Use JSON generator Base64 conversion in BlockJsonSerde (James Petty)\n- 778b5a8bd3cc190373716656bf605f7b6f449b95 Avoid Slice copies in parquet reader (James Petty)", "NaN"], ["14477", "Optimize createPredicate in getTableLayout", "Shixuan Fan", "shixuan-fan", "05/05/20, 08:21:52 PM", "Since hive partition columns have exact values, we could deduplicate\r\nbefore creating SortedRangeSet, whose sorting could be very expensive\r\nwhen there are multiple partition columns and large number of\r\npartitions, and this sorting would happen multiple times during\r\nthe query planning stage due to table layout related optimizations\r\nlike filter pushdown.\r\n\r\nDepended by https://github.com/facebookexternal/presto-facebook/pull/975\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Improve planning time for queries that scan large number of partitions across multiple partition columns\r\n```", "NaN"], ["14482", "Disable memory allocation tracking by default", "Maria Basmanova", "mbasmanova", "05/05/20, 07:19:46 PM", "Fixes #14478\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14485", "Fix partitioned spatial joins with small spatial index", "James Gill", "jagill", "10/07/20, 02:38:19 PM", "In a partitioned spatial join, rows with geometries are assigned 0, 1, or\r\nmore partition indexes via the KdbTree spatial partition.  The geometry\r\nis assigned one index for each leaf node rectangle it intersects.\r\n\r\nCurrently, if a geometry is outside the bounding box of a KdbTree, it is\r\ndropped: it's assigned an empty partition index array, which is\r\nunnested, resulting in the row being dropped.  This can be an efficiency\r\nmeasure: if one side of the join is much smaller than the other, then\r\nthe bounds will drop many rows before they are sent to the join worker.\r\n\r\nHowever, if the bounds are less than both the build- and probe-side of\r\nthe join, then rows that would have matched in a non-partitioned join\r\nwill be dropped when you partition the join.  This makes the correctness\r\nof the partitioned join dependent on the partition chosen, which can\r\nlead to some surprising output changes that could be reasonably viewed\r\nas data loss.\r\n\r\nThis change makes the KdbTree \"open\": its outer boundaries extend to\r\ninfinity.  This means all points in the plane belong to exactly one leaf node\r\nof the KdbTree.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeospatial Changes\r\n* Allow geometries outside of the spatial partitioning to match in a partitioned spatial join.\r\n```\r", "NaN"], ["14490", "Handle Exceptions from presto-common", "Nikhil Collooru", "NikhilCollooru", "05/06/20, 09:29:28 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14491", "Change common sub-expression functions to return check flags and return result", "Rongrong Zhong", "rongrong", "05/08/20, 11:05:59 PM", "The way we do common sub-expressions is to create 2 variables for each cse, 1 boolean flag to indicate whether the cse has been computed, another to store the computed results. So the logic for using cse is `if(!computed) compute; return computed_result;`. There are two ways to generate these:\r\n1. wrap cse computation in a function, and when cse is used, check the flag, if flag is false, invoke function, otherwise load the result\r\n2. wrap the flag check and computation in a function, which will return the result, so when cse is used, invoke the function.\r\n\r\nWe did some benchmarking with Facebook workload and found that approach 2 gives better performance. This is probably due to jit being able to better inline and predict the branch. Currently the implementation is approach 1, switching to approach 2 with this PR.", "NaN"], ["14493", "Do not prune partition columns which fail expression evaluation", null, "bhhari", "05/08/20, 06:48:14 PM", "During the Query Plan phase, we optimize the partition expression and check if it can be pruned.\r\nIn case the partition expression throws an exception during evaluation we need to include it in the list of partitions to be scanned instead of pruning, as existing filters might evaluate to `false` which makes this expression not evaluate.\r\n\r\nCurrently with the pushdown filter path we have a bug where when the partition expression eval fails, we are pruning the partition.\r\n\r\n```\r\npresto:tpch> show create table test_prune_failure;\r\n                Create Table\r\n---------------------------------------------\r\n CREATE TABLE hive.tpch.test_prune_failure (\r\n    x integer,\r\n    p varchar(3)\r\n )\r\n WITH (\r\n    format = 'ORC',\r\n    partitioned_by = ARRAY['p']\r\n )\r\n(1 row)\r\n\r\n\r\n\r\npresto:tpch> select * from test_prune_failure ;\r\n  x  |  p\r\n-----+-----\r\n 123 | abc\r\n\r\npresto:tpch> SELECT * FROM test_prune_failure WHERE x>0 and cast(p AS int) > 0;\r\n x | p\r\n---+---\r\n(0 rows)\r\n```\r\nWithout this fix the query returns 0 rows, but it should fail with. `failed: Cannot cast 'abc' to INT`.\r\n\r\nFixes #14492 \r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14495", "Initial implementation of Broadcast join for Presto on Spark", "Andrii Rosa", "arhimondr", "05/12/20, 10:38:57 AM", "Required by https://github.com/prestodb/presto/issues/13856\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r", "NaN"], ["14496", "Presto on Spark small refactorings and bug fixes", "Andrii Rosa", "arhimondr", "05/11/20, 09:21:41 PM", "Required to prepare Presto on Spark for running AbstractTestQueries\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14498", "Drop presto-orc's dependency on presto-spi", "Nikhil Collooru", "NikhilCollooru", "05/08/20, 09:19:25 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14499", "Add Alluxio based data caching", "Rohit Jain", "jainxrohit", "05/09/20, 05:08:56 AM", "This change enables Alluxio client-side local data cache on the worker.\r\nThis change also does a bit of refactoring of caching related pieces.\r\nTo enable Alluxio local cache, use following configuration\r\ncache.enabled=true\r\ncache.type=ALLUXIO\r\n\r\nCo-authored-by: Bin Fan <fanbin103@gmail.com>\r\nCo-authored-by: Bin Feng <binfeng@alluxio.com>\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/984\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add local data caching support with `Alluxio <https://www.alluxio.io/>`_ cache library. To enable it, set config `cache.enabled=true` and `cache.type=ALLUXIO`\r\n```", "NaN"], ["14500", "Improve presto-druid escaping special chars in column and table name ", "Beinan", "beinan", "05/12/20, 05:14:55 PM", "1 escaping special characters in the column name\r\n2 escaping special characters in the table name\r\n3 escaping special characters for the json request to druid server\r\n4 fix bugs for extra escaping when scan hdfs file. \r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14501", "Beinan/druid hdfs bug fixing", "Beinan", "beinan", "05/12/20, 10:24:55 PM", "1 Add configuration for hdfs config resources.  \r\n2 Add dependency of fastutil for Druid HDFS file parsing\r\n3 Support query columns with 'Other' type\r\n4 Add the problematical file path to the presto exception message\r\n5 Add presto-druid into presto-product-test config properties\r\n6 Exclude transitive dependency to fix build error\r\n\r\nTested on viewfs.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14504", "Upgrade Drift to version 1.23", "Ajay George", "ajaygeorge", "05/09/20, 12:50:32 AM", "Upgrade Drift to version 1.23\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/985\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14505", "Support cachable flag in CachingDirectoryLister", "Shixuan Fan", "shixuan-fan", "05/12/20, 05:10:28 PM", "This could provide more flexibility on whether a query should use\r\ncache or not. For example, we might not want ingestion query to\r\npollute cache, and only want cache to serve latency sensitive\r\nqueries.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nDepended by https://github.com/facebookexternal/presto-facebook/pull/986", "NaN"], ["14506", "Auto-resolve certain result mismatches", "Leiqing Cai", "caithagoras", "05/19/20, 01:55:01 PM", "The PR extends `FailureResolver` to not only be able to resolve query failures, but also result mismatches. Added 2 types of result mismatch resolvers.\r\n\r\n## StructuredColumnMismatchResolver\r\nIf array element or map key/value contains floating point types,\r\ncolumn checksum is unlikely to match. Hence, we can allow Verifier\r\nto auto resolve those cases.\r\n* For an array column, resolve if the element type contains floating\r\n  point types and the cardinality matches.\r\n* For a map column, resolve if either key or value contains floating\r\n  point types, the cardinality sum matches, and the checksum of the\r\n  key or value that does not contains floating point type matches.\r\n* Resolve a test case only when all columns are resolved.\r\n\r\n## IgnoredFunctionsMismatchResolver\r\nIn the case of a results mismatch, if the query uses a function in a\r\nspecified list, the test case is marked as resolved.\r\nThis is useful to mark certain FB functions that is known to produce\r\ndifferent results on Verifier.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to auto-resolve result mismatch for structured-type columns.\r\n* Add support to auto-resolve result mismatch in case the query uses functions to be ignored.\r\n```\r", "NaN"], ["14507", "Fix timeout of testQueuingWhenTaskLimitExceeds", "Vic Zhang", "viczhang861", "05/12/20, 09:42:18 AM", " When `InternalResourceGroupManager::taskLimitExceeded` is updated by testing method `setTaskLimitExceeded`, there is a delay in updating root resource group's state (each resource group could have its own limit and this is updated asynchronously).\r\n\r\nThis flaw exists only in testing method.\r\n\r\nBefore this fix:\r\n One failure out of 30 running of test `testQueuingWhenTaskLimitExceeds` on average.\r\nAfter the fix:\r\n No failure after 100 runs.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14514", "Fix Presto Proxy for Java 10", "Timothy Meehan", "tdcmeehan", "05/13/20, 12:18:43 AM", "Starting from JDK9+, a Jaxb implementation is not provided by default by the JDK.  This provides the necessary dependency.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14515", "Run AbstractTestQueries suite with Presto on Spark", "Andrii Rosa", "arhimondr", "05/22/20, 02:02:28 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14516", "Create DataSink using DataSinkFactory", "Vic Zhang", "viczhang861", "05/15/20, 11:19:47 PM", "depended by facebookexternal/presto-facebook#989\r\n\r\n- Use factory class to create DataSink\r\n  \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14518", "Fix null config issue for caching filesystem", "Rohit Jain", "jainxrohit", "05/14/20, 04:42:02 AM", "When using caching filesystems, in few cases, configuration was found to be null. Which was resulting into null pointer exception.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14522", "Integrate Presto on Spark with the TaskExecutor", "Andrii Rosa", "arhimondr", "06/02/20, 03:33:38 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14524", "canonicalize body of LambdaDefinitionExpression", "Rongrong Zhong", "rongrong", "06/15/20, 10:01:49 PM", "For two LambdaDefinitionExpressions if their input argument types are the same and how arguments are used are the same, they should be considered the same and can use the same generated bytecode.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14526", "Allow cached reads on the non-offset version of readFully", "Ryan Rupp", "ryanrupp", "05/18/20, 10:36:53 PM", "Missing override caused reads the don't use the full overloaded version of the method to not be cached such as Parquet metadata reads.\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\nAllow Hive file caching to cache Parquet metadata reads\r\n```\r", "NaN"], ["14527", "Aggregation ORDER BY & DISTINCT spilling", "Saksham", "sachdevs", "07/02/20, 09:12:37 PM", "This PR implements ORDER BY and DISTINCT spilling for use in aggregation functions. Will be publishing docs on implementation details and updating this PR in the future.\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add local disk spilling support for aggregation functions with `ORDER BY` or `DISTINCT` syntax.\r\n```\r", "NaN"], ["14531", "Fix DruidRequestBody JSON serialization error and add unit test case", "Beinan", "beinan", "05/15/20, 03:54:02 AM", "The static inner class DruidRequestBody has to be public, otherwise we will get a json serialization exception when sending the request to druid.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14532", "Analyze table fails for tables with struct columns", null, "sujay-jain", "05/19/20, 06:04:45 PM", "This PR filters out non-primitive column types when creating metastore column statistics. This fixes the issue of ```ANALYZE``` failing on tables due to the check [here](https://github.com/prestodb/presto/blob/master/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftMetastoreUtil.java#L694)\r\n\r\nFixes #14494\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Fix  ANALYZE table_name failure for tables with map, list or struct columns (:issue:`14494`).\r\n```", "NaN"], ["14533", "Expose ErrorType in VerifierQueryEvent", "Leiqing Cai", "caithagoras", "05/15/20, 11:32:17 PM", "This will help us categorize high vs low signals from verification failure.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add ``ErrorType`` of query failures to verification outputs.\r\n```\r", "NaN"], ["14535", "Remove workers from active nodes based on status", "Swapnil", "swapsmagic", "05/27/20, 07:14:05 PM", "Introducing StatusDetector that can detect if any worker is marked to be not used even if they are up (i.e. maintenannce mode)\r\nDefault implementation of StatusDetector is NoOp which does nothing. And can be overriden by providing specific implementation.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Introducing NodeStatusService interface to find out if a worker is in maintenance mode or ready to take tasks. Default behavior is no op. But can be implemented and override the behavior.  Config controlling the behavior `node-status-service.enabled` \r\n```", "NaN"], ["14537", "Upgrade Drift to version 1.24", "Nikhil Collooru", "NikhilCollooru", "05/15/20, 11:16:13 PM", "Upgrade Drift to version 1.24\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/995\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14538", "Warn when using expensive UNION DISTINCT queries", "Greg Nazario", "gregnazario", "05/19/20, 05:59:31 PM", "UNION DISTINCT can be expensive, when there is a lot of data involved.\r\nWe are sending a warning when there there are many visible fields, and\r\nsome may be expensive to process. The current criteria are, that there\r\nare greater than 3 visible fields, and that one is a complex type.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve experience by warning user of poorly performant UNION DISTINCT usage.\r\n```", "NaN"], ["14541", "Add support for SHOW FUNCTIONS LIKE pattern", null, "prithvip", "06/04/20, 12:29:33 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Support listing functions whose names match a specified pattern using the\r\n  SHOW FUNCTION LIKE syntax.\r\n```\r\n\r", "NaN"], ["14542", "Add documentation for driver.max-page-partitioning-buffer-count", "Ying", "yingsu00", "07/08/20, 10:23:28 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14543", "Improve BenchmarkSelectiveStreamReaders", "Ying", "yingsu00", "09/08/20, 10:31:21 AM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14545", "Spatial partition refactor", "James Gill", "jagill", "06/02/20, 03:21:02 PM", "This is two refactor commits from #14485, with reviewer comments applied.  I'm hoping to land these to reduce the commit stack on the PR: the later commits are still in flux.\r\n\r\nThere is basically no user-facing change in this PR.  The order of the returned `spatial_partitions` array is reversed, but this function is undocumented and used primarily in a partitioned spatial joins, which then unnests the array (destroying the order).\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14548", "Fix Parquet schema mismatch for type upgrade (int32 -> int64)", "Venki Korukanti", "vkorukanti", "05/18/20, 11:44:44 PM", "\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Fix Parquet schema mismatch when type is upgraded (int32 -> int64)\r\n```", "NaN"], ["14551", "Improve the web UI by providing a production mode for build", "Chunxu Tang", "ChunxuTang", "05/22/20, 09:52:15 PM", "This PR provides an option to build the web UI dist with a production mode, concentrating on smaller JS bundles.\r\n\r\nThe size of the original web UI built package is **~33 MB**. After enabling the production mode, the size will be reduced to **~5.9 MB**. This can improve the page loading time of Presto Web UI.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14552", "Update compiledLambdaMap in RowExpressionCompiler for SQL functions", "Rongrong Zhong", "rongrong", "06/01/20, 09:38:19 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14553", "Make TaskStatus Leaner", "cem cayiroglu", "cemcayiroglu", "06/03/20, 03:46:27 AM", "The coordinator to worker transportation is known to be inefficient, which takes roughly 60% or more CPU time on coordinators. We are planning revamp the coordinator - worker communication.\r\n\r\nThis PR is part of the general effort. TaskStatus is transported heavily from workers to coordinator. Decoding of  TaskStatus  contributes to 9-10% of the coordinator CPU.\r\n\r\nTaskStatus JSON decoding improved about 15% by making it leaner. \r\n\r\nBefore:\r\n![Screen Shot 2020-05-19 at 6 13 10 PM](https://user-images.githubusercontent.com/16330476/82394062-d4d8ae80-99fc-11ea-9268-4cabe1796b63.png)\r\n\r\nAfter:\r\n![Screen Shot 2020-05-19 at 6 06 08 PM](https://user-images.githubusercontent.com/16330476/82394068-d73b0880-99fc-11ea-9733-fa8cb07a4947.png)\r", "NaN"], ["14554", "Add additional logging to resource group scheduler", null, "mayankgarg1990", "05/20/20, 09:35:10 PM", "This is needed for an ongoing debugging where in we are seeing queries\r\ngetting stuck and not being moved from the queued state to the running\r\nstate\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r", "NaN"], ["14557", "Introduce table cache quota to HiveConnector", "Ke", "kewang1024", "06/04/20, 12:45:11 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Introduced cache quota with respect to a scope. A scope could be at global, schema, table, or partition level. Cache quota prevents queries scanning too much data to disrupt cache locality. Such queries can only use the cache within their own scopes. Cache quota now only works with `FILE_MERGE` cache. Turn it on with config `cache.cache-quota-scope` and `cache.default-cache-quota`.\r\n```\r\n\r\nFor further potential steps (as following PR)\r\n1. Read the table quota and whitelist from FB internal configuration system\r\n2. Break the dependency between presto-hive and presto-cache\r\n3. Add support for CATALOG level quota if needed\r\n\r\n\r\nProject doc: https://docs.google.com/document/d/1FDcPG3okhs4tfQ3ABF-5yAJSkV4PaSmIHytfn1JNft0/edit?usp=sharing\r", "NaN"], ["14558", "Set an upper limit on number of projections per PageProjection", "Rongrong Zhong", "rongrong", "05/27/20, 08:04:39 PM", "This is to address performance regressions when too many projections are compiled into one class.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14559", "Implement bucketed tables for Presto on Spark", "Andrii Rosa", "arhimondr", "06/15/20, 07:51:04 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14561", "Check for all Throwables while get query results", null, "mayankgarg1990", "05/20/20, 05:43:59 PM", "There are situations where in `AssertionError`s are thrown in the code and that cannot be caught with `Exception`. Changing this to catch `Throwable`\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14562", "Streamlined implementation of core control flow and logical operators", "Sreeni Viswanadha", "kaikalur", "06/02/20, 09:16:39 PM", "Streamlined and simplified the control flow and logical operators to be flat if/goto/end pattern. In addition, flattened logical operators so the code generated is simpler and can be optimized better. Also for switch, we don't generate nested IF anymore. This also sets up for better/more efficient null-handling going forward.\r\n\r\n```\r\n== NO RELEASE NOTES ==\r\n\r\n```\r\n\r", "NaN"], ["14563", "Skip escaping columnnames for table-scan on hdfs", "Beinan", "beinan", "05/21/20, 08:56:18 PM", "Remove the escaping logic in getAssignments, because during the scanning on hdfs files, we don't need escape column names.  \r\nThis change won't break the current dql generation,  we have unit tests cover that.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14565", "Add release notes for 0.235.1", "Timothy Meehan", "tdcmeehan", "05/20/20, 07:43:38 PM", "\r", "NaN"], ["14566", "Add release notes for 0.234.3", "Timothy Meehan", "tdcmeehan", "05/20/20, 11:03:38 PM", "NaN", "NaN"], ["14570", "Fix NPE in CommonSubExpressionRewriter triggered by CASE-WHEN expression", "Rongrong Zhong", "rongrong", "05/26/20, 07:00:44 AM", "Fixes #14567\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix NPE in common sub-expression optimization triggered by CASE-WHEN expression\r\n```\r", "NaN"], ["14573", "Presto on Spark small fixes", "Andrii Rosa", "arhimondr", "05/22/20, 07:44:34 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14574", "Fix #14529 by removing the Java package string from each node type in the web UI", "Chunxu Tang", "ChunxuTang", "05/27/20, 04:49:29 PM", "The root cause of the Issue #14529, mentioned by @cymvp, is that the web UI obtains the full names of node types with Java package strings, but tries to process them as short names.\r\n\r\nIn this PR, I removed the Java package string from each node type in the front-end.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14578", "Filtering nulls from joins when possible", null, "ssaumitra", "06/23/20, 01:26:35 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add optimization to push null filters to the INNER side of equijoins. The optimization can be enabled with `optimize-nulls-in-joins`.\r\n```\r", "NaN"], ["14581", "Support all types for temporary hive table", "Vic Zhang", "viczhang861", "06/02/20, 08:02:10 PM", "- Unit test will fail before the code change.\r\n- Unit test will fail if `temporary_table_storage_format `is not `PAGEFILE`\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14583", "Add release notes for 0.236", "Leiqing Cai", "caithagoras", "06/05/20, 08:43:55 PM", "# Missing Release Notes\n## Rongrong Zhong\n- [x] https://github.com/prestodb/presto/pull/14491 Change common sub-expression functions to return check flags and return result (Merged by: Rongrong Zhong)\n- [x] 46993651d8c2e19c07c11598c5dc41575f7ccaeb Fix NPE in CommonSubExpressionRewriter triggered by CASE-WHEN expression\n\n## Ryan Rupp\n- [x] https://github.com/prestodb/presto/pull/14526 Allow cached reads on the non-offset version of readFully (Merged by: Zhenxiao Luo)\n\n# Extracted Release Notes\n- #14460 (Author: Adam J. Shook): Implement JDBC PreparedStatement.getMetaData\n  - Adds PreparedStatement support for getMetaData.\n- #14471 (Author: Sreeni Viswanadha): Simplify searched case translation\n  - Fixed translation of searched case expressions to be flat.\n- #14477 (Author: Shixuan Fan): Optimize createPredicate in getTableLayout\n  - Improve planning time for queries that scan large number of partitions across multiple partition columns.\n- #14499 (Author: Rohit Jain): Add Alluxio based data caching\n  - Add local data caching support with `Alluxio <https://www.alluxio.io/>`_ cache library. To enable it, set config `cache.enabled=true` and `cache.type=ALLUXIO`.\n- #14506 (Author: Leiqing Cai): Auto-resolve certain result mismatches\n  - Add support to auto-resolve result mismatch for structured-type columns.\n  - Add support to auto-resolve result mismatch in case the query uses functions to be ignored.\n- #14532 (Author: Sujay Jain): Analyze table fails for tables with struct columns\n  - Fix  ANALYZE table_name failure for tables with map, list or struct columns (:issue:`14494`).\n- #14533 (Author: Leiqing Cai): Expose ErrorType in VerifierQueryEvent\n  - Add ``ErrorType`` of query failures to verification outputs.\n- #14538 (Author: Gregory Nazario): Warn when using expensive UNION DISTINCT queries\n  - Improve experience by warning user of poorly performant UNION DISTINCT usage.\n- #14548 (Author: Venki Korukanti): Fix Parquet schema mismatch for type upgrade (int32 -> int64)\n  - Fix Parquet schema mismatch when type is upgraded (int32 -> int64).\n\n# All Commits\n- 46993651d8c2e19c07c11598c5dc41575f7ccaeb Fix NPE in CommonSubExpressionRewriter triggered by CASE-WHEN expression (Rongrong Zhong)\n- 081de9638fb5db5f04151d03d7586187cea75a7f Add additional logging to resource group scheduler (Mayank Garg)\n- de43c1522fe4ef4aa81d7a0a962bafdc99e57375 Check for all Throwables while get query results (Mayank Garg)\n- b90ad7766311006042b2551ca8ff5d36d67f1d6d Simplify searched case translation. (Sreeni Viswanadha)\n- c89a8e3579689fa3ad15cbed245702cc917a169c Filter out non primitive columns when creating metastore column statictics (Sujay Jain)\n- 1229e1fd5f3f1b3733d7a7678afbc086b59b2206 Warn when using expensive UNION DISTINCT queries (Gregory Nazario)\n- b6985d3a5da63710bf95732ffa526232f223d02d Bind classes as singleton in FailureResolverModule (Leiqing Cai)\n- a10adac975888c5465926f5b369a29caa18808a9 Auto-resolve queries referencing certain functions (Leiqing Cai)\n- eeab40c3fcc7b5999b3b1f95322e355e17dcebac Auto-resolve mismatched structured-type columns (Leiqing Cai)\n- ba67a8d536c3215927e3416ca202b912cf197976 Compute cardinality checksum for array and map columns (Leiqing Cai)\n- bf0db246ce4c3966ad866d1081d35edfef803b98 Convert MatchResult.mismatchedColumns to a list (Leiqing Cai)\n- 0f2bf8e1269f01c337efa17baa6d72e1efbc76dc Return checksum objects in column validator (Leiqing Cai)\n- cf64512600a19242dcc3ec8638be990aa5e0a021 Remove unused class in VerifierLimitationFailureResolver (Leiqing Cai)\n- f43e4441f46e56e6816bca0cfc8b66473a9c3b21 Parquet schema mismatch for type upgrade (int32 -> int64) (Venki Korukanti)\n- e58c3511c692698160968affe5f95922f462da37 Allow cached reads on the non-offset version of readFully (Ryan Rupp)\n- bb2e6160dbb3730cf733fa3b09e7fc1b2e9a2f78 Expose ErrorType in VerifierQueryEvent (Leiqing Cai)\n- 0221e24b54bf710997d9f421834b6af087ef6859 Add OutputStreamDataSinkFactory (Vic Zhang)\n- 1d9d967f1af238d31aa9af84f768b9fc230d2509 Upgrade Drift to version 1.24 (Nikhil Collooru)\n- 561e977f3b39c056ef76ea00ff826f0148039b93 Implement JDBC PreparedStatement getMetaData (Adam J. Shook)\n- e2a5010472caec495a893185f7ab6fdf653068c2 Fix DruidRequestBody JSON serialization error and add unit test cases (Beinan Wang)\n- b9865182d9b6c58853c43bf00fb62f739b5f9087 Fix null config issue for caching filesystem (Rohit Jain)\n- 00dd95161d528e148765efc4e4d72a33aede190f Fix Presto Proxy for Java 10 (Tim Meehan)\n- e5226228051da204ac5d8b62f11317d47dc898c8 Refactoring and fixing code review issues (Beinan Wang)\n- 4803f79484efb4baa51272327971405e2814ffae Add unit test case for druid.hadoop.config.resources (Beinan Wang)\n- 5b2397565265ec08b00e2e39e9550c409e2faff8 Exclude transitive dependency to fix build error (Beinan Wang)\n- 448ca8b70e8c98851e9e8891e4dcbdbb2391136e Add presto-druid into presto-product-test config properties (Beinan Wang)\n- b7acf134f314e94604fb9ade07ee777380db3767 Add the problematical file path to the presto exception message (Beinan Wang)\n- eeb990760b5ae37319cba936d699a3f9ede517a0 Support query columns with 'Other' type (Beinan Wang)\n- 52a87c70c561eec4663530042e4f2c5f53a8bcc8 Add dependency of fastutil for Druid HDFS file parsing (Beinan Wang)\n- e628b4d792719301b901f19bcf75035012ac7aab Add configuration for hdfs config resources (Beinan Wang)\n- 86a6e586bdeff8f5723851814a7ca9389d855756 Refactoring and fix code reivew issue (Beinan Wang)\n- 028ba0083f1ae100b62418e0781a3d36c995a1cd Escaping variable name and the column name inside distinct count. (Beinan Wang)\n- 4d3e19d3eb1257f089deb357c7fb6731e80102b5 Improve JSON serialization for the http requests to Druid (Beinan Wang)\n- 9898dba7ced3e4411d21fc5f53e5d2a7baee81a8 Fix the exception of 'Internal file \\\"column_name\\\" doesn't exist' (Beinan Wang)\n- 880928aaee68faca26ff333c13e54e96c216ab1c Escaping special characters in druid table name and column name. (Beinan Wang)\n- 5166a2ebf8811fc9169111eb277884e022f5ed17 Fix json type deserialization issue on DruidSegmentInfo (Beinan Wang)\n- 37aa1b1465d968e6c6650440a108dfa30055208b Support cachable flag in CachingDirectoryLister (Shixuan Fan)\n- adb411680d35c776761c90d15ac1e439235f8508 Use functional style for Optional handling (Shixuan Fan)\n- a1367571e8aad24a1f2f604c2e980daf57030937 Remove redundant requireNonNull (Shixuan Fan)\n- 69eacc0513d5f35c76e2fb1bdb6b89a6bb03865a Use java util Supplier in BackgroundHiveSplitLoader (Shixuan Fan)\n- fac749a71afe65cbf07e9e3e73145b7171ab93ec Implement broadcast join (Andrii Rosa)\n- 973e7de41a79a203a8d266f39d63025ed3777d13 Increase visibility of BlockUtil (Andrii Rosa)\n- f3735f9d738dbc3e43ac85f68958122ae000766b Refactor PrestoSparkTaskInputs (Andrii Rosa)\n- d7cecad12faa8761eaccf3f3943438f898e83056 Refactor PrestoSparkRemoteSourceOperator (Andrii Rosa)\n- 01590935deefddbce0c4dc2a418324c1a9c091b8 Improve rollback handling in PrestoSparkQueryExecutionFactory (Andrii Rosa)\n- 71d6280c3ba422f4f5cb7143bd7f22e636cbedcc Refactor PrestoSparkRddFactory (Andrii Rosa)\n- a6ea2487ad2aaf3006caa037017402053ac4f092 Update task limit state without delay for testing (Vic Zhang)\n- 83882e0c29fb1316a020ad01464927174680fff6 Ensure only single SparkContext exist per jvm (Andrii Rosa)\n- e565e3d0bc3744e95c9800ecdda002973b70399a Run presto-spark-base tests in a separate travis job (Andrii Rosa)\n- 19d6f0a2f56dc09803517317f99eca2c2c826a48 Decrease logging in PrestoSparkQueryRunner (Andrii Rosa)\n- 510a1e84b8ad86334f3280d96228a50b66eb4241 Use Hive connector in PrestoSparkQueryRunner (Andrii Rosa)\n- bcab682907a6884e959de7adf3738489926a573b Improve PrestoSparkQueryRunner (Andrii Rosa)\n- 40a3bac47f8e57841b67b1b9742d56026f24571c Register system catalog for Presto on Spark (Andrii Rosa)\n- 75187c6979c7a93ca7d66a24f61636c45e439e72 Fix PrestoSparkOutputOperator (Andrii Rosa)\n- 472538a02722c5409649533f9a602b5b76d9dd2d Add Alluxio based data caching (Rohit Jain)\n- 5699056e4c0ca44f4d98e2e0dc4bebc872ce28fb Upgrade Drift to version 1.23 (Ajay George)\n- 2e38dd471b5b48fe9d08740a4ddc266cef412e8f Change common sub-expression methods to return result (Rongrong Zhong)\n- 5618ebd4f2d8579f8d31a0e13c20b70a1241981c Partition projections into subsets that depend on same common sub-expressions (Rongrong Zhong)\n- 10a8cb56c5a93ae8a8497ffc7dda9ecb83a07faa Add extracted common sub-expression to debug level logging (Rongrong Zhong)\n- cc312861c9df16c95a693949f710712c36ed606b Drop presto-orc usage of NOT_SUPPORTED PrestoException (Nikhil Collooru)\n- 20b4db7136187d959ee12ea233412b6391ef6369 Drop presto-orc usage of GENERIC_INTERNAL_ERROR PrestoException (Nikhil Collooru)\n- 04daeb45f6d2ba2abd9e3cf59a8a755256b4da1a Drop presto-orc usage of INVALID_FUNCTION_ARGUMENT PrestoException (Nikhil Collooru)\n- 007891b015623d378fd56b05e11917a6120b5986 Do not prune partitions with recoverable failures in filter evaluation (Bhavani Hari)\n- e4fbf228981ab834f065a8d70ea6a98b0dffae1c FastutilSetHelper ObjectStrategy equals result can be null. (Sujay Jain)\n- b70fcf3e077916b3c975cb4af0f465ee01a1796b Add release notes collection script (Leiqing Cai)\n- 9057aea31af2d9f01b439580da22b415793e6c5d Handle NotSupportedException from RowType, MapType, ArrayType (Nikhil Collooru)\n- e7a2cb0ed589a852d743cf9780dd9fc39ef8d490 Handle NotSupportedException from SingleMapBlock, TimeZoneKey (Nikhil Collooru)\n- 76d18ba99ad036a891ebcdf7df744c952ed6f84b Catch InvalidFunctionArgumentException and rethrow as PrestoException (Nikhil Collooru)\n- efe29ef9d4ac84ca31675b97caa7603bb3ee8950 Remove unused OutOfRangeException (Nikhil Collooru)\n- 29acdbaff128fff2de185c28fb26031be1d88019 Refactor predicate package into presto-common (Nikhil Collooru)\n- 04138bc26a4fc801630fe936463d89d071d00c0a Move Type,Block,SqlFunctionProperties to presto-common (Nikhil Collooru)\n- e5710cddd85d70f739c4877f692b73cae1a61aa0 Move Subfield to presto-common (Nikhil Collooru)\n- b5f7eea32cfcf5a490458baae789c76f211d7255 Refactor function package of presto-spi into presto-common (Nikhil Collooru)\n- c4635ef9bd65a5a8be3817737379039021c5fac8 Add types of Exceptions to presto-common (Nikhil Collooru)\n- 1a15abad6eee705bdd3da7a15f6946883fc6727d Add new presto-common module (Nikhil Collooru)\n- 56117346d6b2f2154e4d37af02efa7a25b90e5a5 Optimize createPredicate in getTableLayout (Shixuan Fan)\n- 8886f79944f347ed58108f01ed402d2e31f89cd0 Remove unused field in HiveMetadata (Shixuan Fan)\n- 46d18342378d93283f592127def1d39ee27e2c31 Disable memory allocation tracking by default (Masha Basmanova)\n- 5bd40c56a06b8bc48d7d0a361c506f456dc68e86 Fix integer overflow when creating a BigIntValues Filter (Bhavani Hari)\n- 516aba5b59ad138c72666ddad42d988a379ea5d6 Add PageFile compression in HiveFileFormatBenchmark (Vic Zhang)\n- cd2a34bd2320872814698ed2e228c6ea1c0edf37 Support reading compressed PageFile format (Vic Zhang)\n- f297b39d95a67c7501026ed3eb9ad9ce66bdedc4 Add compression information in PageFile footer (Vic Zhang)\n- f7cfca440852e00deebd16349a18d53993adebb5 Implement Decompressor using zlib (Vic Zhang)\n- f2cef22ed2c03d5dbe3bb30c77e87a6d23fa0d57 Add LZ4 compression in HiveCompressionCodec (Vic Zhang)\n- e03790e14dd0b71903440e421b4283de22642791 Support page compression in PageFileWriter (Vic Zhang)\n- 491bab7752f5c697669cb25d14c18cf0d0463abc Add adapters for PageCompressor and PageDecompressor (Vic Zhang)\n- 3dd9704fc8bc8be4014876631aaaed55ad3c7cc5 Fix getBlockView to use positionCount instead of outputPositionCount (Bhavani Hari)\n- 659c0c2ca62d780bc638045b9eed2a53102d7378 Lower startup-grace-period for TestRaptorIntegrationSmokeTestMySql (James Sun)\n- dbf3a80c504efc569fb59e969ca60ac8739a8560 Handle pruned columns in a pushed down pinot table scan (Devesh Agrawal)\n- 6eb0d35a718c8695f292d3f0991b1c680ca6bf3c Add stats and cost rule for IntersectNode (Saumitra Shahapure)", "NaN"], ["14585", "Use local private credentials (json key file) to refresh GCS access token", "Beinan", "beinan", "08/02/20, 07:10:39 PM", "Generating and refresh GCS access token by the local private credentials (json key file)\r\nSupport the credentials of either \"service_account\" or \"authorized_user\" type\r\nAdded to both presto-cli and presto-jdbc\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Use local private credentials (json key file) to refresh GCS access token\r\npresto-cli --extra-credential hive.gcs.credentials.path=\"${PRIVATE_KEY_JSON_PATH}\"\r\n```\r\n\r\n\r", "NaN"], ["14586", "Code refactor for new parquet batch reader", "Zhenxiao Luo", "zhenxiao", "05/29/20, 12:49:51 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14588", "Add support for Druid's basic and kerberos authentication", "Beinan", "beinan", "05/31/20, 11:25:57 PM", "Basic authentication is supported by airlift's BasicAuthRequestFilter\r\nKerberos authentication is supported by the embedded SpnegoAuthentication in airlift's http-client\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for Druid's basic and kerberos authentication\r\n```\r\n\r", "NaN"], ["14589", "Do not return MapBlock's hashtables in Optional", "Ying", "yingsu00", "06/03/20, 01:29:45 PM", "In recent production workload we observed large amount of memory\r\nallocation on the Optional objects when getting a MapBlock's hashtables,\r\nand it resulted in high GC activities that could lead to reliability\r\nissues. This commit directly returns the hashtables in raw int array\r\nwithout wrapping it up with Optional.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14590", "Fix auto-resolution of checksum query failure ", "Leiqing Cai", "caithagoras", "06/05/20, 04:31:51 AM", "Checksum queries used to fail with COMPILER_ERROR if the generated\r\nbytecode is too large. This has been changed recently with a\r\ndedicated error code GENERATED_BYTECODE_TOO_LARGE.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Fix auto-resolution of checksum query failure due to query complexity.\r\n```\r", "NaN"], ["14593", "Fix NegativeArraySizeException in REPLACE", "Maria Basmanova", "mbasmanova", "06/01/20, 06:29:28 PM", "Update \"replace\" function to check for integer overflow and throw INVALID_FUNCTION_ARGUMENT with explanation instead of java.lang.NegativeArraySizeException.\r\n\r\nFixes #14592\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14594", "Making Node Status Service Optional", "Swapnil", "swapsmagic", "06/10/20, 11:48:14 PM", "Making default NodeStatuService binding as optional. So if no binding is provided DiscoveryNodeService won't check for NodeStatus but if an implementation is provided for the NodeStatusService, nodes status is being checked using isActive method and filter out nodes which are up and connected to Discovery Service but are not in active state (i.e. in Maintenance mode but still up and running).\r\n\r\nReason: previous binding was not allowing it to be overridden with alternative implementation and it was always bind with AllowAllStatusService which leads to allow all hosts all the time.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Optional NodeStatusService to check Nodes status (i.e. Available/In Maintenance) through some external source and mark them Inactive if not available to process queries.\r\n```", "NaN"], ["14595", "Fix Verifier failure when session properties contains execution time", "Leiqing Cai", "caithagoras", "06/02/20, 11:10:16 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Fix an internal error when session properties of a control or a test query contains `query_max_execution_time`.\r\n```\r", "NaN"], ["14597", "Do not create Slice objects when seeking map keys", "Ying", "yingsu00", "06/10/20, 05:59:34 PM", "When seeking a key represented in Slice format, the comparison of the\r\nkey value and the value in the SingleMapBlock at the position of the\r\nhashCode was done through keyBlockNativeEquals.invokeExact(), which\r\ncreates a new Slice object. When there are many map subscript operations\r\nin a query, many small Slice objects are created and this causes high\r\nGC overhead which may lead to reliability problems. This commit uses\r\nthe newly introduced Block.equals(int, Slice) to do the comparison and\r\nit does not create the Slice objects.\r\n\r\nIt also improves the map subscript operation on varchars by up to 30%.\r\n\r\nBefore:\r\n```\r\nBenchmark                           (mapSize)      (name)  Mode  Cnt     Score     Error  Units\r\nBenchmarkMapSubscript.mapSubscript          1   fix-width  avgt   20    43.658 \u00b1   2.070  ns/op\r\nBenchmarkMapSubscript.mapSubscript          1   var-width  avgt   20    76.798 \u00b1   1.174  ns/op\r\nBenchmarkMapSubscript.mapSubscript          1  dictionary  avgt   20    71.693 \u00b1   1.728  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13   fix-width  avgt   20   673.659 \u00b1  35.320  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13   var-width  avgt   20  1549.805 \u00b1 114.924  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13  dictionary  avgt   20  1505.389 \u00b1  55.895  ns/op\r\n```\r\nAfter:\r\n\r\n```\r\nBenchmark                           (mapSize)      (name)  Mode  Cnt     Score    Error  Units\r\nBenchmarkMapSubscript.mapSubscript          1   fix-width  avgt   20    31.709 \u00b1  1.050  ns/op\r\nBenchmarkMapSubscript.mapSubscript          1   var-width  avgt   20    56.716 \u00b1  1.767  ns/op\r\nBenchmarkMapSubscript.mapSubscript          1  dictionary  avgt   20    66.426 \u00b1  2.030  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13   fix-width  avgt   20   569.891 \u00b1 21.386  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13   var-width  avgt   20  1061.052 \u00b1 49.004  ns/op\r\nBenchmarkMapSubscript.mapSubscript         13  dictionary  avgt   20  1118.587 \u00b1 71.122  ns/op\r\n```\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14601", "Adding Task State to RunningSplitInfo", "cem cayiroglu", "cemcayiroglu", "06/05/20, 06:03:32 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nIt was observed that there was a long running split\r\nafter the query was abandoned/cancelled. Adding the\r\ntask state info to do better troubleshooting.\r\n\r\n\r\nThe new output of \"/v1/maxActiveSplits\" looks like:\r\n\r\n6 splits have been continuously active for more than 600.00s seconds\r\n\r\n\"20200603_191419_00288_79sup.3.0.220-18\" tid=103 **status=running**\r\n  at java.base@10/java.net.SocketInputStream.socketRead0(Native Method)\r\n  at java.base@10/java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\r\n  at java.base@10/java.net.SocketInputStream.read(SocketInputStream.java:171)\r\n  at java.base@10/java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n  at java.base@10/java.io.DataInputStream.readFully(DataInputStream.java:200)\r\n  at java.base@10/java.io.DataInputStream.readFully(DataInputStream.java:170)\r", "NaN"], ["14602", "Bump up Cassandra driver version to 3.6.0-1", "James Sun", "highker", "06/04/20, 12:47:14 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nCassandra Changes\r\n* Fix missing Netty library introduced in version 0.229\r\n```\r", "NaN"], ["14603", "Retry and resubmit certain query failures", "Leiqing Cai", "caithagoras", "06/10/20, 09:37:16 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1015\r\n\r\nRetry:\r\n- Retry `DESCRIBE` query failure due to `TIME_LIMIT_EXCEEDED`.\r\n\r\nResubmit:\r\n- `CLUSTER_OUT_OF_MEMORY` on test cluster may happen when the concurrency\r\non the test cluster is too high given the size of the cluster.\r\n- `ADMINISTRATIVELY_PREEMPTED` should also be resubmitted at a later time.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to retry ``DESCRIBE`` queries failed with ``TIME_LIMIT_EXCEEDED``.\r\n* Add support to resubmit verification for ``CLUSTER_OUT_OF_MEMORY`` and \r\n  `ADMINISTRATIVELY_PREEMPTED` errors.\r\n```", "NaN"], ["14604", "Optimize ArrayAllocator CPU usage in OptimizedPartitionedOutputOperator", "Ying", "yingsu00", "06/18/20, 02:09:55 PM", "The SimpleArrayAllocator takes a substantial CPU amount in OptimizedPartitionedOutputOperator. This PR introduces UncheckedStackArrayAllocator which is 7x faster than SimpleArrayAllocator. The new UncheckedStackArrayAllocator does not keep track of borrowed arrays, therefore the user shall make sure there are no duplicate returnArray calls. \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Remove max buffer count config property `driver.max-page-partitioning-buffer-count` for optimized repartitioning\r\n\r\n```\r", "NaN"], ["14605", "Export additional fields in VerifierQueryEvents", "Leiqing Cai", "caithagoras", "06/05/20, 08:02:41 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add control query IDs, test query IDs, and peak total memory to verification outputs.\r\n```\r", "NaN"], ["14606", "Create warning for COUNT(DISTINCT) to use approx_distinct", "Matej Briskar", "mbriskar", "06/09/20, 06:25:49 PM", "COUNT(DISTINCT xxx) can be a very expensive operation when the cardinality is high for xxx. In most scenarios, approx_distinct would be enough. This commit produces a warning if COUNT(DISTINCT) is detected.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* COUNT(DISTINCT) suggests using approx_distinct instead\r\n```\r\n\r", "NaN"], ["14608", "Create quantile_at_value function for QDigest type", "Peizhen Guo", "pguofb", "06/09/20, 02:23:51 PM", "This commit fixes #14423. It creates a quantile_at_value function for QDigest type. When given an input value X, `quantile_at_value(qdigest(T), X)` will return an approximate quantile of X.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14609", "Bind SparkProcessType in Presto on Spark context", "Andrii Rosa", "arhimondr", "06/04/20, 07:14:23 PM", "In some situations it is required to know where the code is running (executor / driver).\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14611", "Fixing broken Test due to Reliable Resource Group changes", "Swapnil", "swapsmagic", "06/04/20, 10:06:36 PM", "Fixing broken test where we look for failed query in queryRunner.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14612", "Parquet batch reader refactor", "Zhenxiao Luo", "zhenxiao", "06/05/20, 01:47:05 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14613", "Register kryo classes", "Andrii Rosa", "arhimondr", "06/05/20, 03:19:35 PM", "When classes are not registered, Kryo falls back to inlining fully specified\r\nclass name as part of the serialization algorithm\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14614", "Add session property to disable warnings, or treat warnings as errors", null, "prithvip", "06/09/20, 05:57:08 PM", "== RELEASE NOTES ==\r\n```\r\nGeneral Changes\r\n* Add new session property ``warning_handling `` to control how warnings are handled. The options are ``SUPPRESS``, ``NORMAL`` and ``AS_ERROR``. The default value is ``NORMAL``.\r\n```\r", "NaN"], ["14615", "Update default storage directory in /etc/raptor.properties to support\u2026", null, "prithvip", "06/09/20, 07:31:48 PM", "\u2026 OSX development\r\n\r\nPrestoServer attempts to create directories in \"storage.data-directory\" for\r\nRaptor. This is configured in etc/raptor.properties by default as \"/var/data\".\r\nIn OSX, this is not a writable directory, so PrestoServer fails to start. This\r\nchange fixes the issue by setting storage.data-directory to /tmp/raptor.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14616", "Reduce the number of Callable for testCachingDirectoryLister", "Shixuan Fan", "shixuan-fan", "06/08/20, 07:09:37 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14617", "Include fields in delta delete to table index", "Ke", "kewang1024", "06/09/20, 05:40:14 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14618", "Collect file stats", "Nikhil Collooru", "NikhilCollooru", "07/24/20, 05:37:05 AM", "Collect the Partition - file stats (file name, size) from the HiveWriter to be stored in a TBD location and then later use it during scheduling to avoid the directory Listing call. \r\nIn this PR we are collecting the stats, tracking the blob size and then throwing it away. \r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1084\r", "NaN"], ["14621", "Add reader and writer support for DWRF encryption", "Rebecca Schlussel", "rschlussel", "06/26/20, 11:31:35 PM", "Adds support to the ORC reader and writer for dwrf encrypted columns\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14622", "Skip BIND in common sub-expression extraction", "Rongrong Zhong", "rongrong", "06/08/20, 09:37:46 PM", "BIND returns a function type rather than a value type so it's not\r\nsuitable for common sub-expression optimization.\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix compiler failure with function type when common sub-expression optimization is enabled.\r\n```\r", "NaN"], ["14624", "Add SET_AGG as an efficient replacement for ARRAY_AGG(DISTINCT)", "Sreeni Viswanadha", "kaikalur", "06/10/20, 05:26:42 PM", "Currently ARRAY_AGG DISTINCT is treated as a DISTINCT aggregation (as it should be). But that creates plans that are not efficient in some cases, especially like generating MarkDistinct operator etc. \r\n\r\nSo we added a new aggregation function SET_AGG which is not treated as a DISTINCT agg, but keeps only distinct values at partial aggregation level. The implementation has been carved out of MAP_AGG. Some of the big jobs have seen latency improvements of 10-15x - especially if there are few distinct values.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Added a new aggregation function SET_AGG<T> that returns ARRAY<T> - equivalent to ARRATY_AGG(DISTINCT). \r\n```", "NaN"], ["14625", "Enforce memory limits on broadcasted tables for lookup join", "Peizhen Guo", "pguofb", "06/15/20, 02:20:42 PM", "**Background**: we want to prevent people from exceeding the lower local memory limits supported by certain types of hardware even if they are running on hardware that is configured with a higher local memory limit while doing broadcast join.\r\n\r\nIf the build side of the broadcast join exceeds the memory limits it will generate an `ExceededMemoryLimitException` that terminates the query.\r\n\r\n\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* a new session property `query_max_broadcast_memory` that allows for specifying the maximum amount of memory a query can use for broadcast join.\r\n\r", "NaN"], ["14626", "Add support for column names with ampersand or pipe to SubfieldTokenizer", "Venki Korukanti", "vkorukanti", "06/09/20, 02:56:02 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14627", "Fix missing nested streams due to column name mismatch", null, "bhhari", "06/10/20, 06:55:06 PM", "In StripeReader we read only required nested streams\r\nby matching the requested nested field name to the name persisted.\r\nWhen written by other compute engines the name persisted can be\r\nuppercase which causes mismatch. Case conversion of persisted name fixes this.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14628", "Elasticsearch connector Improvements", "Zhenxiao Luo", "zhenxiao", "06/10/20, 04:11:19 PM", "\r\n```\r\n== RELEASE NOTES ==\r\n\r\nElasticsearch Changes\r\n* Use shard primary host in Elasticsearch connector to save extra hop in Elasticsearch\r\n* Update Elasticsearch connector to use Elasticsearch Http client\r\n* Load tables dynamically in Elasticsearch\r\n* Add support for Elasticsearch query string syntax\r\n* Refresh Elasticsearch nodes periodically\r\n* Add system.nodes table to Elasticsearch\r\n* Add support for AWS IAM authorization to Elasticsearch connector\r\n* Handle mixed-case columns in Elasticsearch\r\n* Fix predicate pushdown for Elasticsearch\r\n* Add support for querying Elasticsearch aliases\r\n* Add Elasticsearch array support using definitions in the _meta field\r\n* Add support for nested types in Elasticsearch\r\n```", "NaN"], ["14631", "Support all bucketing types for hive temporary table", "Vic Zhang", "viczhang861", "06/19/20, 06:19:04 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14632", "Add allowed roles for HTTP endpoints", "Zac", "zacw7", "07/22/20, 10:20:08 PM", "Partially implements #14639 \r\n\r\nDepends on: prestodb/airlift [#17](https://github.com/prestodb/airlift/pull/17)\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Specify allowed roles for HTTP endpoints\r\n```\r", "NaN"], ["14633", "Add alluxio config validation flag", "Rohit Jain", "jainxrohit", "06/11/20, 05:34:29 AM", "Adding alluxio config validation flag, it can be used to disable the cache configuration validation. Cache configuration validation is a costly operation.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14634", "Optimize shuffle efficiency for Presto on Spark", "Andrii Rosa", "arhimondr", "06/18/20, 02:07:33 PM", "If release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14636", "Make TaskStatus Leaner", "Ajay George", "ajaygeorge", "06/18/20, 02:12:17 PM", "taskInstanceId field is currently represented as a UUID string which\r\ntakes up 72 bytes whereas representing them as two longs takes only\r\n16 bytes which is a 78% improvement.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14638", "Fix session properties for query execution", "Leiqing Cai", "caithagoras", "06/14/20, 06:13:13 AM", "We're seeing checksum query timeout after 200 seconds, even though the timeout was configured at 20 minutes. In those cases, `query_max_run_time` session property was set on the control/test query, and the session properties was applied to all queries issued by the Verifier including the checksum queries.\r\n\r\n- Apply session properties only to control and test main queries.\r\n- Ignore query max run time session property.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Fix an issue where session properties of control and test queries also affects checksum queries.\r\n```\r", "NaN"], ["14640", "Add Presto JDBC URL flag to add session property overrides", "Timothy Meehan", "tdcmeehan", "06/25/20, 05:36:16 AM", "Session property overrides can be specified by the sessionProperty flag,\r\nwith each session property separated by `;`, and each key value pair\r\nseparated by `:`.\r\n\r\nFor example:\r\n\r\n`jdbc:presto://localhost:8080?sessionProperties=prop1:value1;prop2:value2`\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add sessionProperty override to Presto JDBC URI\r\n```", "NaN"], ["14641", "Add session property for targetResultSize", "Timothy Meehan", "tdcmeehan", "06/25/20, 07:38:37 PM", "Setting the `target_result_size` session property will add the specified\r\n`targetResultSize` as a parameter to the nextUri in a statement.\r\n\r\nThe default value for `targetResultSize` is 1MB.  It is useful to override this value in certain cases; for example, for scenarios which have limited bandwidth available, setting a larger value would mean larger payloads are streamed directly from the coordinator to the client before further requests for data are made, reducing wall time.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add ``target_result_size`` session property to customize data batch sizes being streamed from coordinator\r\n```", "NaN"], ["14644", "Fix SingleMapBlock.seekKeyExact bug", "Ying", "yingsu00", "06/15/20, 02:46:00 PM", "When seeking map key of Slice type, we need to compare the bytes value\r\nin the key block to the key value. When the current position in the\r\nkey block is shorter than the key's length, it could return wrong\r\nresults or IndexOutOfBoundsException. This commit fixes this issue.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14645", "Add data validation to Alluxio data caching", null, "zhiyua-git", "06/16/20, 03:40:08 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14646", "Adding time type support for Pinot predicate pushdown", "Xiang Fu", "xiangfu0", "06/23/20, 10:11:00 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n* Support predicate pushdown for literals of type `DATE`, `TIMESTAMP`, or `TIMESTAMP_WITH_TIME_ZONE`\r\n\r\n```\r", "NaN"], ["14649", "Adding a config to retry on pinot exceptions", "Xiang Fu", "xiangfu0", "06/19/20, 05:16:48 AM", "The purpose of this PR is to allow Presto retry on Pinot side data fetch exceptions, e.g. some servers not responding/timeout/etc.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n* Adding config: `pinot.mark-data-fetch-exceptions-as-retriable` to let presto retry on pinot data fetcher related exceptions.\r\n```\r", "NaN"], ["14650", "Fix header handling in Presto Proxy", "Timothy Meehan", "tdcmeehan", "06/25/20, 03:33:39 AM", "Currently, Presto headers returned from the coordinator are corrupted by the Proxy, which is surrounding the values in `[...]`.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14653", "Add release notes for 0.237", "Leiqing Cai", "caithagoras", "06/19/20, 08:42:43 PM", "# Missing Release Notes\n## Cem Cayiroglu\n- [x] https://github.com/prestodb/presto/pull/14553 Make TaskStatus Leaner (Merged by: Andrii Rosa)\n\n## Ke Wang\n- [x] https://github.com/prestodb/presto/pull/14557 Introduce table cache quota to HiveConnector (Merged by: James Sun)\n\n## Swapnil Tailor\n- [x] https://github.com/prestodb/presto/pull/13595 Reliable Resource Group With Versioning (Merged by: Rongrong Zhong)\n\n## Ying Su\n- [x] a51dfffa472fd79cc4230e5cbbae8c935235456e Fix SingleMapBlock.seekKeyExact bug\n- [x] f64829df71cadeef70a9dab4a771436f38f0cebb Change BlockBuilder's expectedEntries to be the number of values\n\n# Extracted Release Notes\n- #14261 (Author: Chyi-Kwei Yau): Add JSON format for EXPLAIN with type LOGICAL and DISTRIBUTED\n  - Add JSON format for `EXPLAIN` with type `LOGICAL` and `DISTRIBUTED`.\n- #14392 (Author: Leiqing Cai): Support built-in SQL scalar functions\n  - Add support for defining SQL-invoked functions in plugins.\n  - Add functions :func:`array_sum` and :func:`map_normalize`.\n- #14429 (Author: James A. Gill): Add flatten_geometry_collections function\n  - Add `flatten_geometry_collections` function to recursively flatten `GeometryCollection`s.\n- #14535 (Author: Swapnil Tailor): Remove workers from active nodes based on status\n  - Introducing NodeStatusService interface to find out if a worker is in maintenance mode or ready to take tasks. Default behavior is no op. But can be implemented and override the behavior.  Config controlling the behavior `node-status-service.enabled`.\n- #14541 (Author: prithvip): Add support for SHOW FUNCTIONS LIKE pattern\n  - Support listing functions whose names match a specified pattern using the SHOW FUNCTION LIKE syntax.\n- #14570 (Author: Rongrong Zhong): Fix NPE in CommonSubExpressionRewriter triggered by CASE-WHEN expression\n  - Fix NPE in common sub-expression optimization triggered by CASE-WHEN expression.\n- #14588 (Author: Beinan Wang): Add support for Druid's basic and kerberos authentication\n  - Add support for Druid's basic and kerberos authentication.\n- #14590 (Author: Leiqing Cai): Fix auto-resolution of checksum query failure \n  - Fix auto-resolution of checksum query failure due to query complexity.\n- #14594 (Author: Swapnil Tailor): Making Node Status Service Optional\n  - Optional NodeStatusService to check Nodes status (i.e. Available/In Maintenance) through some external source and mark them Inactive if not available to process queries.\n- #14595 (Author: Leiqing Cai): Fix Verifier failure when session properties contains execution time\n  - Fix an internal error when session properties of a control or a test query contains `query_max_execution_time`.\n- #14602 (Author: James Sun): Bump up Cassandra driver version to 3.6.0-1\n  - Fix missing Netty library introduced in version 0.229.\n- #14603 (Author: Leiqing Cai): Retry and resubmit certain query failures\n  - Add support to retry ``DESCRIBE`` queries failed with ``TIME_LIMIT_EXCEEDED``.\n  - Add support to resubmit verification for ``CLUSTER_OUT_OF_MEMORY`` and `ADMINISTRATIVELY_PREEMPTED` errors.\n- #14605 (Author: Leiqing Cai): Export additional fields in VerifierQueryEvents\n  - Add control query IDs, test query IDs, and peak total memory to verification outputs.\n- #14606 (Author: Matej Bri\u0161k\u00e1r): Create warning for COUNT(DISTINCT) to use approx_distinct\n  - COUNT(DISTINCT) suggests using approx_distinct instead.\n- #14614 (Author: prithvip): Add session property to disable warnings, or treat warnings as errors\n  - Add new session property ``warning_handling `` to control how warnings are handled. The options are ``SUPPRESS``, ``NORMAL`` and ``AS_ERROR``. The default value is ``NORMAL``.\n- #14622 (Author: Rongrong Zhong): Skip BIND in common sub-expression extraction\n  - Fix compiler failure with function type when common sub-expression optimization is enabled.\n- #14624 (Author: Sreeni Viswanadha): Add SET_AGG as an efficient replacement for ARRAY_AGG(DISTINCT)\n  - Added a new aggregation function SET_AGG<T> that returns ARRAY<T> - equivalent to ARRATY_AGG(DISTINCT).\n- #14628 (Author: Zhenxiao Luo): Elasticsearch connector Improvements\n  - Use shard primary host in Elasticsearch connector to save extra hop in Elasticsearch.\n  - Update Elasticsearch connector to use Elasticsearch Http client.\n  - Load tables dynamically in Elasticsearch.\n  - Add support for Elasticsearch query string syntax.\n  - Refresh Elasticsearch nodes periodically.\n  - Add system.nodes table to Elasticsearch.\n  - Add support for AWS IAM authorization to Elasticsearch connector.\n  - Handle mixed-case columns in Elasticsearch.\n  - Fix predicate pushdown for Elasticsearch.\n  - Add support for querying Elasticsearch aliases.\n  - Add Elasticsearch array support using definitions in the _meta field.\n  - Add support for nested types in Elasticsearch.\n\n# All Commits\n- a51dfffa472fd79cc4230e5cbbae8c935235456e Fix SingleMapBlock.seekKeyExact bug (Ying Su)\n- f64829df71cadeef70a9dab4a771436f38f0cebb Change BlockBuilder's expectedEntries to be the number of values (Ying Su)\n- dbadef16fea9259171def38b47950056aaf0e84c Support built-in SQL-invoked functions (Leiqing Cai)\n- 946a4f4805866594c10803e5395e3038bb7d09d3 Combine FunctionMetadata constructors (Leiqing Cai)\n- 480ec9ba976e06e24324e2c6e5f4451f8e560335 Rename SqlParameter to Parameter (Leiqing Cai)\n- 362cecf95fd8ffd3a26113913c7ceb3a4212f31f Store SqlFunction in built-in Function Namespace Manager (Leiqing Cai)\n- d6b5e51880efe9fe2d2ef6dcca38e922cddaa485 Add alluxio config validation flag (Rohit Jain)\n- 3692a11a7bbab443dfe8d731ec1347a06d6d09b3 Making Node Status Service Optional (Swapnil Tailor)\n- 1f7a05b599eb0649712d03d8dbb179d689930c71 Resubmit query for CLUSTER_OUT_OF_MEMORY and ADMINISTRATIVELY_PREEMPTED (Leiqing Cai)\n- 1bd3681117bccd44d63713aeed9fb63fd3d4a88e Make PrestoExceptionClassifier.shouldResubmit an instance method (Leiqing Cai)\n- e4baa7267b805be5a0b1058fbd74f32000212923 Retry DESCRIBE query failure due to TIME_LIMIT_EXCEEDED (Leiqing Cai)\n- 28ac3cbb768a6fd7cec3716f13a897c64a9302fc Use Builder to construct PrestoExceptionClassifier (Leiqing Cai)\n- aed423eb884f0010b0ac18e063aca527a1c63b1d Fix missing nested streams due to column name mismatch (Bhavani Hari)\n- fa655f6cc8a3d92634acd26d2359ceab4abebc0b Do not create Slice while comparing keys in SingleMapBlock (Ying Su)\n- d44786a4b7d5633c6a0944f1599e3d167216e8be Implement Slice operations for Int128ArrayBlock(Builder) (Ying Su)\n- 52bdb60927189cf6ab41b9bc98dfffac4b66297c Implement SET_AGG as an efficient replacement for ARRAY_AGG(DISTINCT). (Sreeni Viswanadha)\n- 90f72c3873999b6c417f5ed9841978e068f1725b Report field name when decoding Elasticsearch value fails (Zhenxiao Luo)\n- 74f944da5a987a32d1b599d0640f05c7a4169c4d Add support for nested types in Elasticsearch (Zhenxiao Luo)\n- 1c076d48a4fb5c8ea00de7dbeb307a5111fd6988 Fix error message typo in Elasticsearch decoders (Zhenxiao Luo)\n- f6f7d735d41d759160897ccea44e1e7ca9f8ca41 Handle empty object field in Elasticsearch (Zhenxiao Luo)\n- 4e4d67c04802b59a73dc5f7306897c2cdd255763 Add Elasticsearch array support using definitions in the _meta field (Zhenxiao Luo)\n- d7e1df7166f633e5ba09178dd42fd88580d57893 Add option to ignore Elasticsearch publish_address (Zhenxiao Luo)\n- 4d436fd6b946fc948ef873f92cb8e4c36067202d Iterate over Elasticsearch documents in index order (Zhenxiao Luo)\n- 4c7acd9c099d39ce8e5019827fdc729c2541034e Refresh Elasticsearch index before test queries (Zhenxiao Luo)\n- bbdc93dcfbd0538fa9bf039c3f1285dd77636537 Add support for querying Elasticsearch aliases (Zhenxiao Luo)\n- e169f88cf2d0f77880decad192bdedad58794bb6 Use filter clause for Elasticsearch queries (Zhenxiao Luo)\n- 566e79c49fa96fdd43061112ee1afc3adfdda299 Fix predicate pushdown for Elasticsearch (Zhenxiao Luo)\n- ae19772f7e89d7fa14e1ae2293199f7fd6f47e45 Handle mixed-case columns in Elasticsearch (Zhenxiao Luo)\n- 97f32b6bd6367859bf2b9c9a34c4b728e71985cf Add support for datetimes exposed as numbers in Elasticsearch (Zhenxiao Luo)\n- 1a1c3d617f17affcc0077daafef04aabe37f7000 Add support for AWS IAM authorization to Elasticsearch connector (Zhenxiao Luo)\n- ab543066b82c53b550597c939f30630d157c26e7 Move ElasticsearchClient to client package (Zhenxiao Luo)\n- 512a22cb4b337ad2893d9acdf325474d859efed9 Support nodes with no public http address (Zhenxiao Luo)\n- d4f21f3a75aed42fa78afd45ed40e81f79d33b02 Use absolute paths for Elasticsearch requests (Zhenxiao Luo)\n- c316e181e9d8feb63e5610b9d47f5667ae07fd7b Add system.nodes table to Elasticsearch (Zhenxiao Luo)\n- eba938f593336df9c9488714ad2dc39224085ba7 Move IndexMetadata, NodesResponse, SearchShardsResponse, Shard, ElasticsearchNode into Elasticsearch client directory (Zhenxiao Luo)\n- 31d772d7461f845ca4fe90e299b015299b48a165 Refresh Elasticsearch nodes periodically (Zhenxiao Luo)\n- a94d5fb3d9c5a9287c8b442b8c63687689e317d7 Make Elasticsearch retry timeout configurable (Zhenxiao Luo)\n- cd91f76414afac736e65fd5174369f60c5774398 Add support for Elasticsearch query string syntax (Zhenxiao Luo)\n- c295c03f2da2129ca6b891fe2aa3329a0da349db Adjust Elasticsearch timeout defaults (Zhenxiao Luo)\n- a9136a1ba7f06e4777b590745677bde67169c5d5 Load tables dynamically in Elasticsearch (Zhenxiao Luo)\n- 56a8b5945aaed8104071b08502332f20dbe24ef9 bulk load elasticsearch test data (Zhenxiao Luo)\n- 8bc19dbc5ccda961a79c46206d680e4905d94ca6 Update Elasticsearch connector to use Elasticsearch Http client (Zhenxiao Luo)\n- 335af1ab16c366afffa97234fced91d8bd6ed467 Move metadata specific methods to ElasticsearchMetadata (Zhenxiao Luo)\n- 0d9c6b43fa6e5ae5a4a4ffbb5750b161d91ad52b Embed index and type in Elasticsearch connector handle (Zhenxiao Luo)\n- 405efd8984b42b74e2190e07c7b22fd3b77abf44 Use shard primary host in Elasticsearch connector to save extra hop in Elasticsearch (Zhenxiao Luo)\n- 86340f303274f346de0bc1c9523090434b88b00f Encapsulate logic for fetching shards (Zhenxiao Luo)\n- b89611d01be3856c91ddb01eb7c92fb7ef61f77f inline method for Elasticsearch getSearchShards() (Zhenxiao Luo)\n- 8e413ed0b1d4e596986c6dc98b2814125fad0ea8 Remove indexExactMatch from Elasticsearch connector (Zhenxiao Luo)\n- d098fe73c0130883089e044c4c778e8b0029a348 Simplify logic for Elasticsearch connector getSearchShards (Zhenxiao Luo)\n- 3ebaed0f7b8194e032c8ada057a1ec134ebbb8ca Remove unused parameter in Elasticsearch getSearchShards() (Zhenxiao Luo)\n- 8a53371e3d0e0f7ad4379238cd5ae1fd9c1c1696 clear Elasticsearch scroll when connector query close (Zhenxiao Luo)\n- 7211d9bf87b176b9854e373c5df3375194f11eca Make ElasticsearchQueryBuilder stateless (Zhenxiao Luo)\n- 9c65e1d2999f9f39c363ce232dc446f749308db5 Cache Elasticsearch clients (Zhenxiao Luo)\n- 8733d082c1f255f371ca4bfc299c62c4b7eb93c5 Make Elasticsearch config catalog wide (Zhenxiao Luo)\n- 457321020258b4308f79543a829e4d87fbfe7573 Rename ElasticSearchConnectorConfig to ElasticSearchConfig (Zhenxiao Luo)\n- d0ed535113fec5f4cb29322dea5c5d77f2a37a43 Fix ObjectMapper typo in ElasticsearchClient (Zhenxiao Luo)\n- 67659458f086abab9016b942f550bc2b1b6130c5 Update default storage directory in /etc/raptor.properties (prithvip)\n- 793ef467c5f9fc24c9eba456cafe8a0f696585b2 Create warning for COUNT(DISTINCT) to use approx_distinct for performance benefits (Matej Bri\u0161k\u00e1r)\n- 2cd47ca3fd47d526c572cfd301f41cfbab2c91a5 Add session property to disable warnings, or treat warnings as errors (prithvip)\n- 3121083e9ce9239b5882541f6c4141d0cd3c280f Add quantile_at_value function (Peizhen Guo)\n- e86c3216498748880bafd72b0028d0af4f7e63d5 Include fields in delta delete to table index (Ke Wang)\n- d296acbcfd686f084d5dd7e8f228f5f98c0be736 Add support for column names with ampersand or pipe to SubfieldTokenizer (Venki Korukanti)\n- 97310190550f224bdc6a94023d4108288775f336 Skip BIND in common sub-expression extraction (Rongrong Zhong)\n- 62a53a2bfe4e9e25b11b79070465230f4744fed2 Reduce the number of Callable for testCachingDirectoryLister (Shixuan Fan)\n- a060f533e98ba12a11606d8881172e0a5dce5152 Refactor testNoHangIfPartitionIsOffline (Shixuan Fan)\n- 4289044f3549569df42300fe5995501895e44d6e Export peakTotalMemoryBytes in VerifierQueryEvent (Leiqing Cai)\n- c14078b55c670ec1625092d114df31d34a8d1f64 Collect and export setup and teardown query IDs in VerifierQueryEvent (Leiqing Cai)\n- 3781361c1e0c6e9ab4d8930b6e2af2e95781c619 Register Kryo classes for more efficient serialization (Andrii Rosa)\n- dffb88e7aca36ea91ea3619d5883098178c23c12 Update spark-core to 2.0.2-6 (Andrii Rosa)\n- 6ad381e321ef5cb44c366ca91b7be63a0b9af566 Adding Task State to RunningSplitInfo (Cem Cayiroglu)\n- d0e669f78e2bca1bccf7eb58ae6eb307593e4163 Fix auto-resolution of checksum query failure (Leiqing Cai)\n- 71d18a3772e261c1e39bd58a8f022e32a551c711 refactor Parquet batch reader definition && repetition level context (Zhenxiao Luo)\n- e3b71c3fae3e04dc52c229671813022af8591670 refactor Parquet batch reader definitionLevelDecoder (Zhenxiao Luo)\n- 8f3efadf4941e230a2b6bb9285caeeeab4d968a1 refactor Parquet batch reader RLEBitPackedDecoder and ValuesDecoder (Zhenxiao Luo)\n- 042c9ab2e0d9fd6bfb1dd23ffddcd24b0c7277a5 Simplify logic for Parquet batch reader seek (Zhenxiao Luo)\n- 41f9b3553142f86d84e4f67740f0f5ed2217cdf8 Fixing broken Test due to Reliable Resource Group changes (Swapnil Tailor)\n- a9dd203122a3169ee2debbf6e6a47d3704b3f657 Bind SparkProcessType in Presto on Spark context (Andrii Rosa)\n- 130bb67d80ffe3e1d1089e31cc84b55cda789f6b Bump up Cassandra driver version to 3.6.0-1 (James Sun)\n- 4db99a82ae8499a79615dc59fb7d730a40e72e59 Add cache quota support for hive connector (Ke Wang)\n- 3e40faab0b4743e9426feefd19c406c7a957ede6 Add support for SHOW FUNCTIONS LIKE pattern (prithvip)\n- 72d91f1b3ce05dade201b5da6346316796f82f1a Add flatten_geometry_colletions function (James A. Gill)\n- 88633eae7da9e69da3a2cbf94eccf5c223f5aa71 Reliable Resource Groups with versioning (Swapnil Tailor)\n- 78922f8a4a8cd797e1f7bbff8177bd7d98bc6126 Do not return MapBlock's hashtables in Optional (Ying Su)\n- 20c5d563a51eda554c615cdefb7c277d7678f90f use static import (Chyi-Kwei Yau)\n- 91e91f8654f8ab72806649d36fe89a229ddc7977 use sorted map for distributed plan (Chyi-Kwei Yau)\n- 67114fb606ce535151c492b05a4bdda91f3cd4be Make TaskStatus Leaner (Cem Cayiroglu)\n- 91f6d2d8087de230e800f4705e6687b00b8fdd1b Fix Verifier failure when session properties contains execution time (Leiqing Cai)\n- 6fea1e47c3971f525b4b87d6dfcdc4613c9b9bc2 Streamlined implementation of core control operators (Sreeni Viswanadha)\n- c877d105629a52f76c00f5b30d2f79692fec7915 Support all types for temporary hive table (Vic Zhang)\n- 9ce5647c07a8d0f96d1c0aad5f94adbd64b26f48 Decrease lock contention in PrestoSparkRowBuffer (Andrii Rosa)\n- 38219b6f7ddd245f432432c3a829614922b9297c Implement presto like threading model for Presto on Spark (Andrii Rosa)\n- dfaad1a7df01d6169f766e8201ca6f96e9e31609 Replace recursion with a stack in KdbTree (James A. Gill)\n- 753efa3c4744603eb6713fff74e23ac351423f20 Refactor ExtractSpatialJoins (James A. Gill)\n- 35ea4f32f0ef98ad7a60d68fce9f37b4640cb749 Update compiledLambdaMap in RowExpressionCompiler for SQL functions (Rongrong Zhong)\n- b9b4546bf3e0ef4d2454e419b2d98a9b29874ef0 Fix NegativeArraySizeException in REPLACE (Masha Basmanova)\n- 89feb611e434d259c6f7881083a5cc40e85e9865 Code refactoring and fix code review issue (Beinan Wang)\n- dc986a5fec45549fc25138db22d6d0ac868870a0 Implement druid kerberos authentication (Beinan Wang)\n- 7a6d65927e04e37993ea9701bed7bdffe2522cc7 Implement druid http basic authentication (Beinan Wang)\n- 99a34e67229304cbcf73706ba840c0bf7c4bd377 Add druid authentication type to DruidConfig (Beinan Wang)\n- abb828f248ce9c5e3f50502e8fd95a4ec08e99e3 Code refactor for new parquet batch reader (Zhenxiao Luo)\n- 87769c32dd9c82ba39fd61840f2cd5765208855e Fix type mismatch issues caused by the new parquet reader (Venki Korukanti)\n- 7db308543ec7a9bcea0782066cb64afc5dcd47c2 Add an option to  verify new column reader results (Venki Korukanti)\n- e85f86baa5522e36e7855c514d5a756506b13c8b Add Parquet nested column batch readers (Venki Korukanti)\n- 1bd1b4e2514ec112b43d1e3ceb457605109e6f33 Parquet int64/timestamp/boolean column batch readers (Venki Korukanti)\n- 7ee360ec2e2a20890bcee2dafb71c6f18d43baba Create a freemarker based Parquet batch ColumnReader generator from template (Venki Korukanti)\n- e268544ddd100bfd21bd01c43e8924474817361d New Parquet varbinary column batch reader (Venki Korukanti)\n- 3637ede47cf3b23c717b153eca88a6ef1e648de1 New Parquet Int32 column batch reader (Venki Korukanti)\n- 52f7c3a5f72396ef38823eb4885aa955507d4311 Add new Parquet session option for enabling batch reads in parquet (Venki Korukanti)\n- 51a59610d1a347336ff472e3186b590855137811 ColumnReaderFactory to create ColumnReaders (Venki Korukanti)\n- 807552c3db3bb6178b8c18ff57cce631fc19ff34 Rename PrimitiveColumnReader to AbstractColumnReader (Venki Korukanti)\n- 45aeeb1149c0a73da651b1d982db436e2c457cd3 Introduce ColumnReader interface (Venki Korukanti)\n- fda3ae821059bbc763a670a50fe1865da9b17f3b Set an upper limit on number of projections per PageProjection (Rongrong Zhong)\n- 37377fdfa6f208809b77185abca3b1d0bdcb2f92 Fixing a bug in common sub expression partitioning (Rongrong Zhong)\n- e85ce87f6958d9908cc62be0193e82b7218b0d88 Blacklist worker based on SMC status (Swapnil Tailor)\n- 1aa031208925d75cc118844ab7034f0b4c07eddc Format the single quotes to double quotes in the getChildren() method (ChunxuTang)\n- 21cede7221ce89c770c8a9ab14adfe777eecdf3f Remove the java package from each node type in the web UI (ChunxuTang)\n- 0552cdeb5c19b9ce7adcc01c771d8c47ac4caa6b Set ParquetWriterOptions based on session parameters (qqibrow)\n- 46fe2941c64e33942eeb049ab33f94514ea5ba86 Set parquet writer page size and row group size in ParquetTester (qqibrow)\n- 09920a8a2648857e1e15e32d57296ca363eda253 Support setting page size and row group size in parquet writer (qqibrow)\n- d65d9d9092a039ebd94a231f1b9c4c0ab4c15cd8 Add encoding should be called after getBytes() and before reset() (qqibrow)\n- 7fb7e098e3e5c1e589f9601065846b4002908fbd resetDictionary right after get dictionary page (qqibrow)\n- 124d22ff4235a473ad1bbda01c10b8b9b38ae2b4 Set statistics in RowGroup metadata (qqibrow)\n- 10ddb7bf764bfd131bc368b1aa0971e1b88601b3 Add compression in ParquetFileWriterFactory (qqibrow)\n- 5428bbc11d22c2618f4c65cd5982d977ac0c5d9c Add test for optimized parquet writer in hive module (qqibrow)\n- 375d38116db4d27a468b0c2ec07de77557abbbb4 Add parquet writer in hive module (qqibrow)\n- fc207eddce500b89de8bb12a3434f0a37f4ad2cc Expose written bytes and buffered bytes in ParquetWriter (qqibrow)\n- 258b79781a7cf2f4f80a338f866f0a19dbbb456d Fix NPE in CommonSubExpressionRewriter triggered by CASE-WHEN expression (Rongrong Zhong)\n- a2b5f6e0d8781b04bb10fa46b0b85c43a24c5ff6 Provide a production mode to build web dist (ChunxuTang)\n- 1ca5cf32f0d36346b5b5714e78a657968af7c4db Format the quotes of mode to single quotes (ChunxuTang)\n- 1bceed18118d8af29a405f4f230aeac90c0f2133 Refactor strings to template literals in Presto UI (ChunxuTang)\n- 954e4714ac1aac177bbf00d5641088cc7eb15289 Fix error message in IntegerIdentityPartitioner (Andrii Rosa)\n- 51df86ebc46b0b76dba403089958ede65ab88acc Disable force single node output for Presto on Spark (Andrii Rosa)\n- 533aed3ed8b6ef4a0a5d60634543450672da7aca Reduce spark integration tests memory footprint (Andrii Rosa)\n- 2236f9959419f408d6ba15ed91bc397408227945 Invoke spark-submit directly (Andrii Rosa)\n- 4f47c5280da90a1c2684188fc397c1601efeafb1 Fix PrestoSparkRowBuffer synchronization (Andrii Rosa)\n- e906ea6582570d134634397b978d8a4839acfa6e Do not close testing SparkContext that is still in use (Andrii Rosa)\n- 3644a873d6b9474a0a55f3eff5f2a343ddea5a35 Collect input rdds in parallel (Andrii Rosa)\n- 5d5d24020f07851f4b135da171a39731716f964f Run AbstractTestQueries suite with Presto on Spark (Andrii Rosa)\n- e9727e8f8b5b4a17fedc1fd9ff0860b263bb7f18 Propagate exceptions from executor to driver (Andrii Rosa)\n- a693597da1c89c993bedba00871c732fe5f32b7d Support nulls in results for Presto on Spark (Andrii Rosa)\n- 8b610afa6de6671366a4206db77fede3b5a068a2 Support broadcast union (Andrii Rosa)\n- c5ddfd4208751f3532e76a5cb97bbc26561ecff4 Add PrestoSparkSettingsRequirements (Andrii Rosa)\n- 4e6b976f0cf8a334db68346025cf65985558df09 Implement N-Way join and UNION ALL (Andrii Rosa)\n- 8e74d6443930ebc6095967abb3b0434a4c5ea16d Implement values in Presto on Spark (Andrii Rosa)\n- 1b994063c773903c07f1197b19480c0ff99cf3d5 Skip escaping columnnames for table-scan on hdfs (Beinan Wang)", "NaN"], ["14654", "Upgrade drift to 1.25", null, "mayankgarg1990", "06/16/20, 05:27:56 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14656", "Make Proxy timeout configurable", "Timothy Meehan", "tdcmeehan", "06/24/20, 05:00:54 PM", "It may be desirable to configure this value in certain circumstances, such as for large batch sizes and in scenarios where there is limited bandwidth available.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14657", "Reduce redundant calls to setNull in DoubleSumAggregation", "Sreeni Viswanadha", "kaikalur", "06/18/20, 08:22:30 PM", "Minor performance fix to avoid calling setNull on the state multiple times. I have seen CPU time for just the agg part go down from 6.1hrs to 5.8hrs for some queries - indicating a ~5% improvement.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14659", "Add peakTaskTotalMemoryBytes to Verifier output", "Leiqing Cai", "caithagoras", "06/17/20, 06:48:47 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to report peak task memory usage for control and test queries.\r\n```\r", "NaN"], ["14662", "Print more information for BlockEncodingBuffer.toString()", "Ying", "yingsu00", "06/18/20, 02:38:02 AM", "Printing more information for BlockEncodingBuffer.toString() makes the debugging easier.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14663", "Pre-calculate totalSliceLength before ensureCapacity", "Ying", "yingsu00", "06/18/20, 02:09:41 PM", "ensureCapacity is quite expensive operation, and we want to minimize the\r\ncallsites of it. This commit pre-calculate the totalSliceLength before\r\nallocating the sliceBuffer in ensureCapacity, so that only one\r\nensureCapacity is called per batch.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14667", "Fix flaky TestFileSingleStreamSpiller", "Andrii Rosa", "arhimondr", "06/18/20, 02:46:39 AM", "The test was sporadically failing at\r\n\r\n> assertEquals(listFiles(spillPath.toPath()).size(), 0);\r\n\r\nThe assertion verifies that the spiller does the cleanup on close.\r\n\r\nSince the test is not single threaded, test methods are allowed to\r\nrun in parallel. Thus it is not correct to write to a single shared\r\nfolder. Instead each test method has to write to it's own folder to\r\nguarantee that the folder will be empty after the cleanup.\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14668", "Fix for tasks not showing in Query details", "cem cayiroglu", "cemcayiroglu", "06/18/20, 07:14:46 PM", "There's a JS error: \r\n\r\nreact-dom.development.js:16543 Uncaught TypeError: Cannot read property 'slice' of undefined\r\n    at getTaskIdSuffix (utils.js:314)\r\n    at getTaskNumber (utils.js:318)\r\n    at eval (QueryDetail.jsx:392)\r\n    at Array.sort (<anonymous>)\r\n    at StageSummary.componentDidUpdate (QueryDetail.jsx:391)\r\n\r\n\r\n\r\nOpening the UI in a debugger, we see it's failing here:\r\n\r\n            stage.latestAttemptExecutionInfo.tasks.sort(function (taskA, taskB) {\r\n                return (0, _utils.getTaskNumber)(taskA.taskStatus.taskId) - (0, _utils.getTaskNumber)(taskB.taskStatus.taskId);\r\n            });\r\n\r\nThis was removed recently: https://github.com/prestodb/presto/pull/14553/files#diff-188532f86e998c7d6e1c22f37c7db165L128", "NaN"], ["14669", "Treat SQL function parameter case-insensitive", "Rongrong Zhong", "rongrong", "06/25/20, 03:23:36 AM", "Function parameters should be case-insensitive.\r\nAlso fixing:\r\n* Store the parameter names as originally provided by users\r\n* Use identifier default setting for delimited\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix case sensitivity issue in SQL function parameters. Function parameters should be treated case-insensitive.", "NaN"], ["14670", "BenchmarkRunner fixes and make it extensible", "Leiqing Cai", "caithagoras", "07/22/20, 10:31:47 PM", "```\r\n== NO RELEASE NOTES ==\r\n```\r", "NaN"], ["14674", "Avoid wrapping iterators", "Andrii Rosa", "arhimondr", "06/19/20, 04:46:41 PM", "Wrapping iterators results is large stacks when all method invocations are virtual potentially degrading the performance when the average row size is tiny.\r\n\r\nAdditionally it makes much harder to \"downcast\" the iterator provided by Spark if some further optimizations are needed. \r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14675", "Initial Support of Adaptive Optimization with Presto Unlimited", "Peizhen Guo", "pguofb", "07/15/20, 08:14:45 PM", "**Background**: Presto Unlimited will materialize exchange outputs into temporary tables (see https://github.com/prestodb/presto/issues/12387) , which creates an opportunity to invoke CBO during runtime on later stages based on temporary table statistics generated by previous stages. This will result in more reliable optimizations for complex queries whose later stages often have less accurate _estimated_ statistics.\r\n\r\nSpecifically, this PR achieves the following goals.\r\n1. Enable table and column statistics to be collected for temporary tables, and enable the statistics to be correctly fetched and processed by statsCalculators.\r\n2. Create an iterative optimization rule that can leverage temporary table statistics to calculate statistics for the probe and build side of a Join node and swap them when the build side is larger.\r\n3. Make CBO invokable at LegacySqlQueryScheduler and optimize the plan right before scheduling and actual execution.\r\n\r\nBasic testing:\r\n\r\n- In TestHiveIntegrationSmokeTest::[testMaterializedPartitioning](https://github.com/prestodb/presto/blob/master/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java#L3037) test suite, the rule correctly captures two queries [1](https://github.com/prestodb/presto/blob/master/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java#L3132) and [2](https://github.com/prestodb/presto/blob/master/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java#L3160) where build side is larger than probe side, and invoking CBO did not affect query result correctness.\r\n- Will add more unit tests.\r\n\r\n== NO RELEASE NOTE ==\r\n\r", "NaN"], ["14676", "Fix session properties for determinism analysis main queries", "Leiqing Cai", "caithagoras", "06/25/20, 08:40:24 PM", "We should apply the same session properties as the control main query\r\nwhen running the main query during determinism analysis.\r\n\r\nRelease notes is covered in https://github.com/prestodb/presto/pull/14638 so don't need any release notes here.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14678", "Parquet: Handle nested column schema changes when subfield pruning is enabled", "Venki Korukanti", "vkorukanti", "06/20/20, 12:12:28 AM", "Summary:\r\nAfter schema is changed (new field at depth > 1) and when trying to read files with old schema\r\nwe end up with a situation where the type of the newly added field type is derived incorrectly.\r\nWhen this new field type is merged with type of other subfields within the same column, we get\r\nthe following error.\r\n\r\nError opening Hive split file:/var/folders/kg/8w0vwj4n26db9qwybcwmpml80000gn/T/PrestoTest3674412409435240427/hive_data/tpch/test_subfield_multilevel_pruning/20200619_021654_00016_hyx9i_753399fd-0201-4017-82bf-43c808e41bba (offset=0, length=624): repetition constraint is more restrictive: can not merge type optional group shipdate {\r\n  optional int32 ship_day;\r\n  optional int32 ship_month;\r\n} into repeated group shipdate {\r\n  optional int32 ship_day;\r\n  optional int32 ship_month;\r\n}\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Fix a bug in Parquet reader which manifests when there are nested column schema changes\r\n```", "NaN"], ["14680", "Replace FileSystemContext in presto-raptor by HdfsContext", "Felipe Vieira C\u00f4rtes", "fvcortes", "06/25/20, 06:28:17 PM", "This PR is intended to fix [ #14011](https://github.com/prestodb/presto/issues/14011) by replacing the use of FileSystemContext in presto-raptor by HdfsContext since presto-hive-metastore has been separated into a standalone module.\r\n\r\n== NO RELEASE NOTE ==\r", "NaN"], ["14681", "Add metadata support for reading hive encrypted data", null, "mayankgarg1990", "06/25/20, 02:25:56 PM", "This PR adds metadata support #14621. Adds the constructs to pass encryption information to `HiveSplit` and also the table properties needed to define encryption settings for a table.\r\n\r\nEncryption metadata support for writing data to Hive will add in an upcoming PR.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add support for reading Hive tables with DWRF encryption at rest.\r\n```\r\n\r\ndepended by facebookexternal/presto-facebook#1040", "NaN"], ["14684", "Add an optimizer rule to remove redundant assignments in ProjectNode", "Rongrong Zhong", "rongrong", "06/23/20, 09:25:28 PM", "Fixes #14683\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14685", "added additional connectors for presto-spark-package", "Ravion", "Ravion", "06/22/20, 09:57:55 PM", "Please make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14688", "Fix estimated serialized size for BlockEncodingBuffers", "Ying", "yingsu00", "07/06/20, 12:51:44 PM", "We used Blocks' sizeInBytes or logicalSizeInBytes to estimate the max capacity of the BlockEncodingBuffers. However, there were some error in calculating the max capacity from the decodedBlock.estimatedSerializedSizeInBytes such that the exclusive portion(exclusive of children BlockEncodingBuffers) of the current BlockEncodingBuffer was mistakenly passed to the children BlockEncodingBuffers as inclusive portion. Also, the max capacity for the nested blocks was incorrectly calculated if they are RLE or Dictionary Blocks . This PR fixes these two problems. With these fixes, the CPU time for the reported regressed query in T67972617 was reduced  from 100s to 20s.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14689", "    Fix integer overflow error in Bing Tiles", "James Gill", "jagill", "06/22/20, 04:52:57 PM", "Since Java only has signed ints, zoom 23 was causing int overflows in certain cases. In particular, if you are look at pixels at the eastern/southern edges of the world, `256 << 23 == 2 ** 31`, while\r\nmax int is `2 ** 32 - 1`.\r\n\r\nWe fix this by doing these calculations in tiles, for which the corresponding number of 1 << 23, which is within an int's bounds.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeospatial Changes\r\n* Fix integer overflow in certain cases with Bing Tiles.\r\n```\r", "NaN"], ["14694", "Use alluxio version 2.2.2", "Rohit Jain", "jainxrohit", "06/22/20, 05:56:28 PM", "Alluxio 2.2.2 carries various critical fixes for data caching. These fixes are required to improve presto data caching reliability and efficiency. \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14696", "Support common sub-expression optimization in CursorProcessorCompiler", null, "frankobe", "06/30/20, 11:10:09 PM", "Reuse common subexpression optimization in https://github.com/prestodb/presto/pull/14303 to avoid generating cse methods for projection and filter expressions in CursorProcessorCompiler\r\n\r\nInitial benchmark from `CommonSubExpressionBenchmark`\r\n```\r\nBenchmark                                      (dictionaryBlocks)  (functionType)  (optimizeCommonSubExpression)  Mode  Cnt        Score       Error  Units\r\nCommonSubExpressionBenchmark.ComputeRecordSet                true            json                           true  avgt   20  1254079.283 \u00b1 14513.663  ns/op\r\nCommonSubExpressionBenchmark.ComputeRecordSet                true            json                          false  avgt   20  1705059.614 \u00b1 33912.064  ns/op\r\nCommonSubExpressionBenchmark.ComputeRecordSet                true          bigint                           true  avgt   20    17707.901 \u00b1  3226.856  ns/op\r\nCommonSubExpressionBenchmark.ComputeRecordSet                true          bigint                          false  avgt   20    22631.380 \u00b1  1418.222  ns/op\r\nCommonSubExpressionBenchmark.ComputeRecordSet                true         varchar                           true  avgt   20   134511.019 \u00b1  2928.476  ns/op\r\nCommonSubExpressionBenchmark.ComputeRecordSet                true         varchar                          false  avgt   20   162009.251 \u00b1  2437.937  ns/op\r\nCommonSubExpressionBenchmark.ComputeRecordSet               false            json                           true  avgt   20  1278711.407 \u00b1 24349.271  ns/op\r\nCommonSubExpressionBenchmark.ComputeRecordSet               false            json                          false  avgt   20  1695407.459 \u00b1 18522.358  ns/op\r\nCommonSubExpressionBenchmark.ComputeRecordSet               false          bigint                           true  avgt   20    16169.200 \u00b1   706.694  ns/op\r\nCommonSubExpressionBenchmark.ComputeRecordSet               false          bigint                          false  avgt   20    16731.742 \u00b1   167.647  ns/op\r\nCommonSubExpressionBenchmark.ComputeRecordSet               false         varchar                           true  avgt   20   126666.129 \u00b1  1589.824  ns/op\r\nCommonSubExpressionBenchmark.ComputeRecordSet               false         varchar                          false  avgt   20   147536.758 \u00b1  1496.214  ns/op\r\n```\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n*  Add optimization for cursor projection & filter by extract and compute common subexpressions among all projections & filter first. This optimization can be turned off by session property ``optimize_common_sub_expressions``.\r\n\r\n```\r\n\r", "NaN"], ["14697", "Use alluxio version 2.2.2", "Rohit Jain", "jainxrohit", "06/22/20, 08:34:26 PM", "Alluxio 2.2.2 carries various critical fixes for data caching. These fixes are required to improve presto data caching reliability and efficiency.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14699", "Close PrestoSparkService on the Driver", "Andrii Rosa", "arhimondr", "06/23/20, 03:06:16 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14700", "Include table name and parquet file name in error message on schema mismatch", "Vivek", "ClarenceThreepwood", "06/25/20, 09:31:07 PM", "\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14704", "Adding Pinot SQL endpoint support", "Xiang Fu", "xiangfu0", "07/17/20, 10:00:08 AM", "The motivation of this PR to add Pinot SQL endpoint support is that Pinot Community is moving to SQL syntax/endpoint on query side to support more features and also plan to deprecate PQL queries.\r\n\r\nThis PR:\r\n* Support Pinot new SQL endpoint for broker queries.\r\n* Enable this feature by setting`pinot.use-pinot-sql-for-broker-queries=true`.\r\n* Support aggregation/group by/oder by  pushdown to query Pinot broker.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n* Add Pinot SQL endpoint support.\r\n```\r", "NaN"], ["14710", "Refactor presto-common predicate tests", "Lucas Damo", "lucasdamo", "06/24/20, 04:05:54 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14712", "Upgrade ZSTD version", "Rohit Jain", "jainxrohit", "06/24/20, 08:30:20 AM", "The newer ZSTD version outperforms the previous version in both\r\ncompression and decompression speed.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14713", "Add session property to set bucket function type", "Vic Zhang", "viczhang861", "06/24/20, 08:55:17 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n*  Add support for exchange materialization of table bucketed by non-hive types. This can be enabled using value ``PRESTO_NATIVE`` for the ``bucket_function_type_for_exchange`` session property  or the ``hive.bucket-function-type-for-exchange `` configuration property.\r\n```\r\n\r", "NaN"], ["14715", "Add session property to ignore table bucketing", "Vic Zhang", "viczhang861", "06/25/20, 10:04:14 PM", " - When table bucket count is not large enough, using such table count\r\n   won't achieve expected partitioning performance.\r\n - Ignoring table bucketing makes it possible to use a larger bucket\r\n   count when needed, e.g., exchange materialization.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14716", "Fix artificially high queued query metrics", "Jeremy DeGroot", "jeremy-degroot", "06/30/20, 10:37:49 PM", "Fixes #14705\r\n\r\nFixed a bug where queries that users cancelled early in their life-cycle would not decrement the queued queries counter, causing this metric to be artificially high.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14718", "Remote function planning", "Rongrong Zhong", "rongrong", "07/16/20, 06:16:11 PM", "This PR introduced the concept of \"remote function\" into planner and generates query plan that's aware of the remote functions. Depends on #14219. Partially addresses #14053.\r\n\r\nRan verifier explain tests across all dcs and all tests passed.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14721", "Fix logger crash for Kafka connector", "wade_liu", "liusuquan", "06/26/20, 04:16:26 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14722", "Add documentation for a deployment example", "Adam J. Shook", "adamjshook", "06/29/20, 05:00:44 PM", "Open to reworking or moving this documentation elsewhere.\r\n\r\nRelates to https://github.com/prestodb/prestodb.github.io/pull/86\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14723", "Write support for DWRF encryption at rest", null, "mayankgarg1990", "07/06/20, 09:05:19 PM", "Flow `EncryptionInformation` to `HiveOutputTableHandle` and `HiveInsertTableHandle` through `HiveWritableTableHandle`. Extended the `HiveEncryptionInformationProvider` to also provide the `EncryptionInformation` necessary for when we are writing data.\r\n\r\nIn addition to this, did some refactor in `HiveMetadata` to better re-use code for translating settings between table properties and hive properties for both read and write cases.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nDepended by https://github.com/facebookexternal/presto-facebook/pull/1053\r", "NaN"], ["14724", "Enable statistics collection for temporary tables", "Peizhen Guo", "pguofb", "06/26/20, 03:31:47 PM", "Enable table and column level statistics collection when writing intermediate data\r\nto temporary table for materialized exchanges. Collected statistics will be used to\r\nadaptively change join order in runtime.\r\n\r\nStatistics aggregation for column statistics are disabled by default due to its cost,\r\ncontrolled by Feature config experimental.enable-stats-collection-for-temporary-table.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14725", "Fix NPE in geometry_invalid_reason", "James Gill", "jagill", "06/26/20, 03:15:47 AM", "Embarassingly, this method threw an NPE on valid geometries, which it\r\nwas not supposed to.  This commit uses Optional and adds a guard.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeospatial Changes\r\n* Fix NPE in :func:`geometry_invalid_reason`.\r\n```\r", "NaN"], ["14726", "Update documentation for SHOW FUNCTIONS and SHOW CREATE FUNCTION", "Leiqing Cai", "caithagoras", "06/26/20, 03:35:01 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14727", "Add back address forwarding for Proxy", "Timothy Meehan", "tdcmeehan", "06/26/20, 04:22:00 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14728", "Enums support #1: base types and operators", "Daniel Ohayon", "daniel-ohayon", "08/31/20, 05:56:07 PM", "This PR is the first in a series that aims to introduce support for user-defined types, and specifically user-defined enums, into Presto (see #14691 for an overview of the work).\r\n\r\nThis one focuses specifically on introducing the base types and generic operators that apply to enums. \r\nWe support\r\n* enum literals, eg `Mood.HAPPY`\r\n* cast to and from base types\r\n* `=`, `!=` and `IS_DISTINCT`\r\n* hash operator used for `IN (...)` and `APPROX_DISTINCT()`\r\n* cast to and from JSON\r\n* comparisons and ordering (using the rules of the underlying types)\r\n\r\n\r\n## Implementation notes\r\n* Enum literals like `Mood.HAPPY` are parsed as Dereferences in the AST (like `my_table.my_col`) and they are then rewritten as EnumLiterals in the TranslationMap's rewriter. \r\n* I added a new type bound constraint to express that we want a given type to be a long enum or varchar enum, so that we can have enum operator signatures like `<T extends LongEnumType> equals(T, T): bool` . This constraint is similar to the `orderable` and `comparable` constraint we use to define the type signature of functions like the equal operator on arrays.\r\n\r\nIn future PRs, I will introduce \r\n* the ability to register enum types from plugins\r\n* the ability to serialize enum data to the client\r\n\r\n\r\n\r\n\r", "NaN"], ["14729", "Disable encryption tests temporarily", null, "mayankgarg1990", "06/26/20, 03:08:14 PM", "The current tests provide an invalid encryption metadata which is causing failures\r\nas the actual reader code is being landed. Disable the tests for now to unbreak trunk\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14734", "Add support for column names with spaces to SubfieldTokenizer", "Maria Basmanova", "mbasmanova", "07/07/20, 11:08:39 AM", "Fixes #14400 \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14736", "Make TestRaptorIntegrationSmokeTestMySql single threaded", "James Sun", "highker", "06/29/20, 05:10:29 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14739", "Add a limit on total number of bytes read from storage in table scan", "countryman4687", "fgwang7w", "07/31/20, 04:39:36 PM", "Fixes #14701 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add `query.max-scan-physical-bytes` configuration and `query_max_scan_physical_bytes` session properties to limit total number of bytes read from storage during table scan. The default limit is 1PB.\r\n```", "NaN"], ["14740", "Fix confusion matrix computation for classification function", null, "fornaix", "06/30/20, 05:55:43 PM", "Fixes #14731 and fixes #14750\r\n1. fix document mistake for `classification_fall_out`\r\n2. fix confusion matrix computation.\r\n\r\nDefinition: https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context)\r\n\r\n|  | predicated positive | predicated negative |\r\n| --- | --- | --- |\r\n| condition positive | True Positive(TP) | False Negative(FN) |\r\n| condition negative | False Positive(FP) | True Negative(TN) |\r\n\r\nthere are some variables in code.\r\n```\r\ntotalTrueWeight = {sum of condition positive}\r\ntotalFalseWeight = {sum of condition negative}\r\nrunningTrueWeight = {sum of predicated negative in {condition positive}}\r\nrunningFalseWeight = {sum of predicated negative in {condition negative}}\r\n```\r\n\r\nSo we got\r\n\r\n|  | predicated positive | predicated negative | |\r\n| --- | --- | --- | --- |\r\n| condition positive | totalTrueWeight - runningTrueWeight | runningTrueWeight | totalTrueWeight |\r\n| condition negative | totalFalseWeight - runningFalseWeight | runningFalseWeight | totalFalseWeight |\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix  :func:`classification_miss_rate` and :func:`classification_fall_out` functions (:pr:`14740`)\r", "NaN"], ["14742", "Elk msg fix", "Zhenxiao Luo", "zhenxiao", "06/29/20, 01:46:53 AM", "Fix elasticsearch connector commit message\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14743", "Add invoker security mode for views", "Sanket Dige", "sansanketdg", "07/20/20, 06:14:54 PM", "Cherry-pick of https://github.com/prestodb/presto/commit/a2c3a1f1d80a21c35cfb36ecdb41c3ad01415ced and https://github.com/prestosql/presto/commit/c408a78244ac0f9c1360b872b9569dc29c71b62f\r\n\r\nCo-authored-by: David Phillips <david@acz.org>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for 2 security modes for views. The default `DEFINER` security mode is the same as the previous behavior. \r\nTables referenced in the view are accessed using the permissions of the view owner (the **creator** or\r\n**definer** of the view) rather than the user executing the query. In the `INVOKER` security mode, tables referenced in the\r\nview are accessed using the permissions of the query user (the **invoker** of the view).\r\n```\r", "NaN"], ["14747", "Turn off failure detector by default in tests", "Rebecca Schlussel", "rschlussel", "06/30/20, 10:17:02 PM", "The node failure detector can make tests flakey by marking active nodes\r\noffline.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14748", "Introduce CacheQuotaRequirementProvider", "Ke", "kewang1024", "07/15/20, 04:43:17 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nDepended by facebookexternal/presto-facebook#1047\r\nDepends on https://github.com/facebookexternal/presto-facebook/pull/1047", "NaN"], ["14751", "Parquet: Handle missing struct columns as part of the nested column pruning", "Venki Korukanti", "vkorukanti", "07/01/20, 07:07:19 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14752", "Revert \"Reliable Resource Groups with versioning\"", "Swapnil", "swapsmagic", "06/30/20, 11:06:06 PM", "This reverts commit 88633eae7da9e69da3a2cbf94eccf5c223f5aa71.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Reverting Reliable Resource Group Versioning", "NaN"], ["14753", "Implement PrestoDatabaseMetaData getClientInfoProperties", "Adam J. Shook", "adamjshook", "07/01/20, 06:44:58 PM", "Fixes #14686 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nJDBC Changes\r\n* Implemented DatabaseMetaData.getClientInfoProperties API\r\n```", "NaN"], ["14754", "Fix thread listing under heavy workloads (thread churn)", "Partha Kanuparthy", "kvbp2kfb", "07/07/20, 12:11:00 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix NPE in `/v1/thread` end point.\r\n```", "NaN"], ["14755", "Revert \"Reliable Resource Groups with versioning\"", "Swapnil", "swapsmagic", "06/30/20, 11:38:00 PM", "This reverts commit 88633eae7da9e69da3a2cbf94eccf5c223f5aa71.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Reverting Reliable Resource Group changes", "NaN"], ["14756", "Ensure exception thrown by scheduled tasks are logged", "Andrii Rosa", "arhimondr", "08/10/20, 05:19:22 PM", "Regularly scheduled tasks are dying as long as at least single run fails.\r\nCatching and logging the failures will help to debug problems in scheduled tasks.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14758", "Fixes for DWRF encryption", "Rebecca Schlussel", "rschlussel", "07/01/20, 04:10:19 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14760", "Fix flaky testAndInFilter test", "Rebecca Schlussel", "rschlussel", "07/07/20, 08:06:06 PM", "random(10) was returning 0 sometimes causing tests to be flaky\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14762", "Add support for DWRF encryption without compression", "Rebecca Schlussel", "rschlussel", "07/06/20, 10:52:18 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14766", "Fix stats for dwrf encryption", "Rebecca Schlussel", "rschlussel", "07/06/20, 01:52:41 PM", "Unencrypted stats need dummy entries for encrypted columns to ensure\r\nbackwards compatibility.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14767", "Modify regex for compatibility with both mysql, mariadb", "Amit Sadaphule", "amitsadaphule", "08/14/20, 04:11:40 PM", "The regular expressions used to match the expected exception message\r\nfor testAlterFunctionAmbiguous and testDropFunctionAmbiguous tests\r\nin presto-function-namespace-managers was hard bound to the message\r\nreturned by mysql. Modified that to match MariaDB's message too\r\nwhen used as a replacement for mysql.\r\n\r\nFixes this issue: https://github.com/prestodb/testing-mysql-server/issues/12#issuecomment-650223303\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14770", "Add release notes for 0.238", "Swapnil", "swapsmagic", "07/16/20, 09:55:13 PM", "# Missing Release Notes\n## Cem Cayiroglu\n- [x] https://github.com/prestodb/presto/pull/14668 Fix for tasks not showing in Query details (Merged by: Leiqing Cai)\n\n## Mayank Garg\n- [x] https://github.com/prestodb/presto/pull/14681 Add metadata support for reading hive encrypted data (Merged by: Rebecca Schlussel)\n\n## Peizhen Guo\n- [ ] https://github.com/prestodb/presto/pull/14625 Enforce memory limits on broadcasted tables for lookup join (Merged by: Rebecca Schlussel)\n\n## Swapnil Tailor\n- [x] c7eecd9b972097653a01c2f1188aca8fb6ef9f6e Revert \"Reliable Resource Groups with versioning\"\n\n# Extracted Release Notes\n- #14219 (Author: Rongrong Zhong): Add syntax support for external function\n  - Add support to create external function (this does not include external function execution).\n- #14578 (Author: Saumitra Shahapure): Filtering nulls from left and right in inner join\n  - Add optimization to push null filters to the INNER side of equijoins. The optimization can be enabled with `optimize-nulls-in-joins`.\n- #14604 (Author: Ying Su): Optimize ArrayAllocator CPU usage in OptimizedPartitionedOutputOperator\n  - Remove max buffer count config property `driver.max-page-partitioning-buffer-count` for optimized repartitioning.\n- #14638 (Author: Leiqing Cai): Fix session properties for query execution\n  - Fix an issue where session properties of control and test queries also affects checksum queries.\n- #14640 (Author: Tim Meehan): Add Presto JDBC URL flag to add session property overrides\n  - Add sessionProperty override to Presto JDBC URI.\n- #14641 (Author: Tim Meehan): Add session property for targetResultSize\n  - Add ``target_result_size`` session property to customize data batch sizes being streamed from coordinator.\n- #14646 (Author: Xiang Fu): Adding time type support for Pinot predicate pushdown\n  - Support predicate pushdown for literals of type `DATE`, `TIMESTAMP`, or `TIMESTAMP_WITH_TIME_ZONE`.\n- #14649 (Author: Xiang Fu): Adding a config to retry on pinot exceptions\n  - Adding config: `pinot.mark-data-fetch-exceptions-as-retriable` to let presto retry on pinot data fetcher related exceptions.\n- #14659 (Author: Leiqing Cai): Add peakTaskTotalMemoryBytes to Verifier output\n  - Add support to report peak task memory usage for control and test queries.\n- #14669 (Author: Rongrong Zhong): Treat SQL function parameter case-insensitive\n  - Fix case sensitivity issue in SQL function parameters. Function parameters should be treated case-insensitive.\n- #14678 (Author: Venki Korukanti): Parquet: Handle nested column schema changes when subfield pruning is enabled\n  - Fix a bug in Parquet reader which manifests when there are nested column schema changes.\n- #14689 (Author: James Gill):     Fix integer overflow error in Bing Tiles\n  - Fix integer overflow in certain cases with Bing Tiles.\n- #14713 (Author: Vic Zhang): Add session property to set bucket function type\n  - Add support for exchange materialization of table bucketed by non-hive types. This can be enabled using value ``PRESTO_NATIVE`` for the ``bucket_function_type_for_exchange`` session property  or the ``hive.bucket-function-type-for-exchange `` configuration property.\n- #14725 (Author: James Gill): Fix NPE in geometry_invalid_reason\n  - Fix NPE in :func:`geometry_invalid_reason`.\n\n# All Commits\n- c7eecd9b972097653a01c2f1188aca8fb6ef9f6e Revert \"Reliable Resource Groups with versioning\" (Swapnil Tailor)\n- fc1f2bde5eb4d4eb2d16f31de49fd27f0e852d54 Add back address forwarding for Proxy (Tim Meehan)\n- 9c17deb5be22cdbd87f16fa4dc5af21282e5b969 Fix logger crash for Kafka connector (liuququan)\n- 1952222f409531e9e72d4f73f2f56f30ef668b62 Update documentation for SHOW FUNCTIONS and SHOW CREATE FUNCTION (Leiqing Cai)\n- ccf8582158cc67d73bb8124ba1ea233dc7c0f73b Fix NPE in geometry_invalid_reason (James Gill)\n- b89db16de87fed22b4413196c0ab78c16a1652fe Add session property to ignore table bucketing (Vic Zhang)\n- a6b43aa097a0600489f800ea52d61db5374e366f Include table name and parquet file name in error message on schema mismatch (Vivek Bharathan)\n- f5060f3d203b0b5018eb9ec06a56a060ff2c4bb5 Fix session properties for determinism analysis main queries (Leiqing Cai)\n- 93f1fb10cd16e6c4f0d6fd00b52630dae5ccdb37 Add session property for targetResultSize (Tim Meehan)\n- b476b693a72d9349fe7dee3e3f29e968bce3bf13 Replace FileSystemContext in presto-raptor by HdfsContext (fvcortes)\n- 686152df953301020542d62f48731d57dd1130dd DDL support for DWRF encryption (Mayank Garg)\n- 3e1227c7519e9c0d9a0dc82524d2adc901244dd7 Base classes for passing encryption information to `HiveSplit` (Mayank Garg)\n- e7edcaa2f6d284f4bd4358cc27109b1d56d3b68d Pass requested columns to HiveTableLayoutHandle (Mayank Garg)\n- ada52f6a4355eb379bd0d16bca8e603de0caa024 Add Presto JDBC URL flag to add session property overrides (Tim Meehan)\n- 5be42efa5697c27ce63d359f56486e4ff385dfb9 Refactor map parsing in Presto JDBC connection URL (Tim Meehan)\n- 00c3dfcf8bc40cbd02ccd88f36ec8f08fd978d8b Fix header handling in Presto Proxy (Tim Meehan)\n- f0ebe6dd7542dfa6e144774c2db66baa04b50825 Treat SQL function parameter case-insensitive (Rongrong Zhong)\n- f209297a6cb02a0a02415c48db15724547e3e0de Add support for creating external function (Rongrong Zhong)\n- cb69e899efbabf35a09003852194800309f53dad Add session property to set bucket function type (Vic Zhang)\n- 3a83e7273714ef7b3edb7526ce92a04a0d7711aa Make Proxy timeout configurable (Tim Meehan)\n- 9e62ba7beea530b8780db558bbd8565cc718bce2 Upgrade ZSTD version (Rohit Jain)\n- 837184878c79c3d7c5e71cd3d1549377c0f9b8b2 Refactor presto-common predicate tests (lucasdamo)\n- 74e0bf4f402dbce31d6f00af1011ae814f710439 Adding time type support for Pinot predicate pushdown (Xiang Fu)\n- 5526987a37f10c94aa10a2651f27da722941672c Add an optimizer rule to remove redundant assignments in ProjectNode (Rongrong Zhong)\n- bc5da24d0221d2311d13335d6c86dcc312ad03cd Close PrestoSparkService on the Driver (Andrii Rosa)\n- c8c88a5957f1dab8b6c32967dd70a41900f03fbe Join optimisation: filtering null rows explicitly (Saumitra Shahapure)\n- 28fc102fd2475f4efb16abe78c3c0c5c70010090 Add additional connectors for presto-spark-package (Ravion)\n- 1b0752f38ec7d04d4e14c32d19c7a99b26f4c675 Use alluxio version 2.2.2 (Rohit Jain)\n- d783c836436cfe463b1e300e4fedb10d027abb43 Fix integer overflow error in Bing Tiles (James Gill)\n- 53252da438e5cd2a8ac501b6b15f6f766f6e77c1 Move pure BingTile functions to BingTileUtils (James A. Gill)\n- b6f7986d69120bc851934979488d751d85805b5a Parquet: Handle nested column schema changes when subfield pruning is enabled (Venki Korukanti)\n- 093ac4aa9e3e63eea25e24d5da5008c5f5dfacfb Ignore HiveBucketFilter for buckted temporary table (Vic Zhang)\n- 9d097a57910eac079666dc88534f2d707603ccd6 Refactor getPartitionMetadata method (Vic Zhang)\n- db1466879d750b3c52a5395567cd636faddf52a4 Refactor HiveBucketProperty constructor method (Vic Zhang)\n- 98c8c0327d8783a7d8f08d6c3d6b643adcbcfce2 Support all column types for hive temporary bucket table (Vic Zhang)\n- b7ceddfb52ec3d381afd41f18cf19b2d604a6c5e Avoid wrapping iterators (Andrii Rosa)\n- 298de509d09d09bd1ac57ba08643045caeb2a68f Adding a config to retry on pinot exceptions (Xiang Fu)\n- 942cd399b9adaa12b427bd743c1ac1fe154f1cc5 Reduce redundant calls to setNull in DoubleSumAggregation (Sreeni Viswanadha)\n- 9d6b3edf8669233b9aa3a054736f58b0260cde6a Fix for tasks not showing in Query details (Cem Cayiroglu)\n- 9c5aee1bff00bc50e4e7c7ab527aeba12ae8f78a Make TaskStatus Leaner (Ajay George)\n- 239519575308058cf97d46e9d5aecf0cf924c73d Use UncheckedStackArrayAllocator instead of SimpleArrayAllocator (Ying Su)\n- 72bb533e14c69063357240f48b8a5ed1b668502e Revert \"Add max buffer count config property for optimized repartitioning\" (Ying Su)\n- 7b87673681ee9374cf6f340beabb4a9b0d7d94dd Introduce UncheckedStackArrayAllocator (Ying Su)\n- 76750f6141fa1464e80a07ab7c9032fcd75be0c3 Refactor TestSimpleArrayAllocator (Ying Su)\n- a5dccb36139f0206e3108033b7ea77d346f43496 Pre-calculate totalSliceLength before ensureCapacity (Ying Su)\n- 5d20801c400de01f5614db638114bd9ed197f583 Avoid unnecessary page to row conversions (Andrii Rosa)\n- 3607e0850158c6b400167cebef3bd8dc768879fb Optimize PrestoSparkOutputOperator (Andrii Rosa)\n- f5db3bdd038bbab6377881633df02ecfb5381d73 Optimize PrestoSparkRemoteSourceOperator (Andrii Rosa)\n- 81261a56210cff6ebf9f0215fe8d17e00e2db8f1 Add custom shuffle serialization for Presto on Spark (Andrii Rosa)\n- 2081d20d17b25bc110fb1747f32e3ce158d2c268 Rename PrestoSparkRow to PrestoSparkMutableRow (Andrii Rosa)\n- 314769eec29af002201ab4a6a29935cfec8eb895 Implement MutablePartitionId (Andrii Rosa)\n- 5e45c8a0758ebea01ccf43afeeb9cb514485766b Fix flaky TestFileSingleStreamSpiller (Andrii Rosa)\n- a1d5f8a82f96e5e53e50f8ece51cadc54186a180 Print estimated buffer max capacities in BlockEncodingBuffers.toString() (Ying Su)\n- ad8989be817494db115d36a4fc19420cf01e97c8 Add toString() to AbstractBlockEncodingBuffer (Ying Su)\n- e10751b99e7d87e0a96d9cba3305b100f1875a9c Use ToStringHelper in BlockEncodingBuffers.toString() (Ying Su)\n- 4bd9d192342f2c136357b6ddab8129a898c1135f Add peakTaskTotalMemoryBytes to Verifier output (Leiqing Cai)\n- 5ee73fdd7cb1ec9b78cdc96073f76b73c80ed0d4 Upgrade drift to 1.25 (Mayank Garg)\n- ae77e90a24683f5e2ad3c0b697f208c8c073f630 Add unit test for Alluxio data validation (zhiyua-git)\n- 8fb6f1b0cbeb6deb6ad49f1aab2247c5c4ad6bef Add data validation to Alluxio data caching (Zhiyuan Hu)\n- c6f1cbc8fa318d773b8899f97155a9380d6bea84 Canonicalize body of LambdaDefinitionExpression (Rongrong Zhong)\n- 4b0f7c192280a6ffb046766c2fb263ba1f0a4614 Enable collocated join for Presto on Spark (Andrii Rosa)\n- cb2da8321eadd3ded00bed3ed8667f4497cdd2fa Add support for bucketed tables in Presto on Spark (Andrii Rosa)\n- 0b753220953fe3d5d7b8b81dc10a6169f950b6cf Refactor PrestoSparkRddFactory (Andrii Rosa)\n- 21f0b0475b637cdb9ecc464398eb26906402a2c6 Add ConnectorNodePartitioningProvider#getBucketCount (Andrii Rosa)\n- cbe1cb1a440d3b0e1136e5668a1a9dc1303558ef Fix SingleMapBlock.seekKeyExact bug (Ying Su)\n- 35cd0888fb729ecb1a9760c61afb6062d56a4728 Change BlockBuilder's expectedEntries to be the number of values (Ying Su)\n- 79b4a1872c63ae13dcb7a232210ba7dd085f17f9 Enforce memory limits on broadcasted tables for lookup join (Peizhen Guo)\n- 21ca972b98e146eaa2b9dd732e3c66256b2e0fdc Fix session properties for query execution (Leiqing Cai)", "NaN"], ["14771", "Allow configure SQL invoked function implementation type", "Rongrong Zhong", "rongrong", "07/06/20, 11:12:15 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14773", "Minor geospatial refactor", "James Gill", "jagill", "07/02/20, 07:04:38 PM", "Three small code cleanups that are useful in their own right, but also preparation for a later PR.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14775", "Fix thread snapshot UI due to variable initialization", "Partha Kanuparthy", "kvbp2kfb", "07/07/20, 12:11:35 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* The worker page's thread snapshot UI does not work (no stack trace displayed on click) when there is active query load (tested under Chrome). This patch fixes an uninitialized variable in client JS that was causing this UI behavior.\r\n```", "NaN"], ["14783", "Fix #14693: support TRUNCATE function for DOUBLE and REAL", "countryman4687", "fgwang7w", "10/13/20, 07:00:52 PM", "\r\n== RELEASE NOTES ==\r\n```\r\nGeneral Changes\r\n```\r\nThe solution is to fix issue #14693 by adding an enhancement to TRUNCATE function for allowing DOUBLE and REAL types.\r\nFollowing examples show how user may use this feature:<br>\r\n\r\n`truncate(DOUBLE '1234.56', 1)` -> The outcome of the evaluation is `1234.5`<br>\r\n\r\n`truncate(REAL '-12.333', -1)` -> The outcome of the evaluation is `-10.0`<br>\r\n\r\n<br><br>\r\nNOTE: Neither does this patch change any behaviors of current mathemtical functionalities, nor is the existing workload being impacted. In addition, there is zero impact to data/workload migration.\r", "NaN"], ["14784", "Fix query completion events for non-dispatched queries", "Timothy Meehan", "tdcmeehan", "07/06/20, 06:07:42 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix missing query completion events for queries which fail prior to dispatching\r\n```\r\n\r", "NaN"], ["14786", "Implement additional serialization methods for Spark shuffle", "Andrii Rosa", "arhimondr", "07/09/20, 05:06:21 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14788", "Fixes for completion events post-dispatching", "Timothy Meehan", "tdcmeehan", "07/13/20, 11:07:43 PM", "Improvement on #14784 \r\n\r\n* Removes a potential duplicate completion event\r\n* Additional fix for certain DDL and pre-execution SQL execution problems where completion event may not be logged", "NaN"], ["14789", "Allow DictionaryBlock for MinMaxBy aggregation", "Vic Zhang", "viczhang861", "07/10/20, 03:05:33 PM", "Fixes https://github.com/prestodb/presto/issues/14787\r\n\r\nSince the issue is data dependent, verified using raw table.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14791", "Add a test for Presto-on-Spark", "Wenlei Xie", "wenleix", "07/07/20, 04:26:39 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14793", "Fix bytecode generation for SQL functions", null, "prithvip", "07/12/20, 10:44:48 PM", "A lambda expression inside the body of a SQL function might have\r\nthe same argument name, as one of the arguments of the function\r\nitself. Bytecode generation fails in this case.\r\n    \r\nExample:\r\nCREATE FUNCTION testing.test.array_sum(x array<int>) RETURNS int\r\nRETURN reduce(x, 0, (s, x) -> s + x, s -> s)\r\n    \r\nHere \"x\" is an argument for both the function and the lambda\r\nexpression in the function body. This change fixes the bug by\r\nproperly scoping variables in the lambda expression body, so that\r\nthere is no conflict with variables of the same name defined\r\noutside the lambda expression's scope.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14795", "Add support for Hudi MOR queries", null, "bschell", "08/06/20, 01:47:13 AM", "Allows presto-hive to support the use of custom input formats with custom\r\nfile splits and record readers. Tested using Hudi merge-on-read table\r\ninput format.\r\n\r\nThis is a rebase of the hudi presto realtime query patch on presto mainline. Tests are passing but need to double check for any possible new issues for Hudi given the rebase. Just wanted to get this opened for feedback ASAP.\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Allows presto-hive to use custom parquet input formats \r\n* Add support for Hudi realtime input format for hudi realtime queries\r\n\r", "NaN"], ["14799", "Add getSupportedColumnStatisticsForTemporaryTable function", "Peizhen Guo", "pguofb", "07/07/20, 11:09:24 PM", "Hive does not support all presto types, but temporary table could use unsupported presto types via native format. Hence, directly calling `metastore.getSupportedColumnStatistics()` to determine supported column statistics for each column will cause problems for temporary tables.\r\n\r\nThis PR adds a `getSupportedColumnStatisticsForTemporaryTable` function to regulate how supported column statistics types are determined for temporary table, which is called by two paths: 1. When PlanFragmenter creates temporary table; 2. When PlanFragmenter configures statisticsAggregations in temporary table writer/finish nodes.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14800", "Partition filtering warning", null, "aweisberg", "07/15/20, 05:31:39 PM", "Add support for warning on queries that scan a partitioned table without filtering on at least one of a configured set of partition columns.\r\n\r\nThis is to warn on queries that may scan a very large table when they should have done at least some filtering on the partition columns.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for warning on unfiltered partition keys using `warn-on-no-table-layout-filter` system property. This property specifies a ',' separated list of column names that are assumed to be good for filtering on partitioned tables. If one or more of these columns are present in the partition key when reading from a partitioned table and none are filtered then a warning is emitted. The Hive connector is currently the only connector that supports emitting this warning.\r\n```", "NaN"], ["14802", "Make TestElasticsearchIntegrationSmokeTest single threaded", "Zhenxiao Luo", "zhenxiao", "07/07/20, 11:29:10 PM", "Fix https://github.com/prestodb/presto/issues/14801\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14803", "Run PruneRedundantProjectionAssignments after RowExpression change", "Rongrong Zhong", "rongrong", "07/08/20, 08:23:24 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14805", "Add Connector Session info to SQL String", "Naveen821", "Naveen007", "07/08/20, 11:55:42 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1065\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14808", "Various DWRF encryption/decryption fixes", "Rebecca Schlussel", "rschlussel", "07/09/20, 12:12:27 AM", " * Empty files don't have encryptiongroups, so ignore any keys \r\n* Adding encrypted streams was getting short circuited if unencrypted streams had a row group dictionary\r\n* FileStatistics for encryption groups wasn't implemented correctly (format requires a list of encrypted file statistics objects corresponding to then nodes of the encryption group, not a single FileStatistics object for all the nodes)\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14811", "Elasticsearch connector improvements", "Zhenxiao Luo", "zhenxiao", "07/10/20, 03:20:12 AM", "\r\n```\r\n== RELEASE NOTES ==\r\n\r\nElasticsearch Changes\r\n* Support Elasticsearch numeric keyword\r\n* Support composite publish_address in Elasticsearch\r\n* Add configurations to improve concurrency in Elasticsearch\r\n```", "NaN"], ["14812", "Use the same RowExpressionCompiler in CursorProcessorCompiler", "Rongrong Zhong", "rongrong", "07/09/20, 12:11:07 AM", "In CursorProcessorCompiler, we created new RowExpressionCompiler when compiling\r\nevery expression. Now that RowExpressionCompiler is no longer stateless\r\n(compiledLambdaMap needs to be updated), this becomes problematic. Conceptually,\r\nRowExpressionCompiler is adding methods to a generated class, and all codegen\r\nrelated to the same generated class should use the same RowExpressionCompiler.\r\nModify the code accordingly.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14814", "Use the same RowExpressionCompiler in CursorProcessorCompiler", "Rongrong Zhong", "rongrong", "07/09/20, 06:02:34 PM", "NaN", "NaN"], ["14815", "Skip isSplittable reflection for RCFileInputFormat ", "Rohit Jain", "jainxrohit", "07/13/20, 07:55:06 PM", "Currently, presto relies on the method reflection to find out if the input format is splittable for all input formats except OrcInputFormat. Using the method reflection is a costly operation, hence avoiding the method reflection for RCFileInputFormat as well.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14816", "Include scheduling stats of bucketed table to NodeSelectionStats", "Ke", "kewang1024", "07/13/20, 06:21:26 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14817", "fix inconsistent session value at server and client side", null, "weidongduan37", "07/14/20, 02:32:12 PM", "Fixes #14807", "NaN"], ["14818", "fix druid incorrect real data value for Druid connector", null, "weidongduan37", "07/10/20, 05:36:33 PM", "fix druid incorrect real data value for DruidBrokerPageSource\r", "NaN"], ["14819", "Avoid fetching encryption information when no columns requested", null, "mayankgarg1990", "07/09/20, 09:40:42 PM", "This can happen if the user is only asking for partition columns or something like count(*). As of now,\r\nif the table has table level encryption key, we will try getting the key metadata for that key where\r\nas it is not needed. This change will allow users who don't have access to the table data to still be able\r\nto get the partition columns or count(*) and bring the implementation at par with column level encryption\r\nkey specification\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14820", "Add support of caching for glue metastore", null, "adlymousa", "07/23/20, 04:46:46 PM", "Hive `GlueMatastoreModule` doesn't support caching, which caused performance issues in planning time, read throughput, and parallelism when filtering on a partitioning column.\r\n\r\nThis PR adds the support of caching to the `GlueMatastoreModule`. Adding this support won't enable caching unless `hive.metastore-cache-ttl` property is not 0.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Add support for caching the Glue metastore.\r\n```\r", "NaN"], ["14821", "Fix reading encrypted streams at offset 0", "Rebecca Schlussel", "rschlussel", "07/10/20, 02:31:25 AM", "if a stripe encryption group has streams at offset 0, then the\r\noffset doesn't need to be specified.  As such, the reader should\r\nstreams from each group separately, and if no offset is specified, begin\r\nat offset 0..\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14822", "Support return type coercion in CREATE FUNCTION", "Teja Sasank", "jetsasank", "07/15/20, 06:28:54 PM", "Currently, within CREATE FUNCTION syntax, we currently restrict that the return type declared by the RETURNS clause must exactly match the expression type in the body. The change here will be relaxing that restrictions. \r\n\r\n```\r\n=== RELEASE NOTES ===\r\n\r\nGeneral Change\r\n* Add support to create functions whose function body type is coercible to the declared return type.\r\n```\r", "NaN"], ["14823", "Support customizable ways of launching Presto queries in Verifier", "Leiqing Cai", "caithagoras", "07/30/20, 08:57:53 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1081\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Fix an issue where Verifier fails to start when failure resolver is disabled.\r\n* Add support to implement customized way of launching Presto queries.\r\n* Add support to run helper queries on a separate cluster other than the control cluster.\r\n```\r", "NaN"], ["14825", "Presto spark query info", "Andrii Rosa", "arhimondr", "07/14/20, 04:15:36 PM", "Depended by facebookexternal/presto-facebook#1067\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14827", "Fix document in Presto-on-Spark", "Wenlei Xie", "wenleix", "07/10/20, 10:14:51 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\npng files are removed in https://github.com/prestodb/presto/pull/14824, but the reference is not. This cause Travis failure. \r\n\r\ncc @Ravion ", "NaN"], ["14829", "Pushdown dereference", "Zhenxiao Luo", "zhenxiao", "08/01/20, 08:09:00 AM", "\r\n== RELEASE NOTES ==\r\n```\r\nGeneral Changes\r\n* push down dereference expression\r\n```\r", "NaN"], ["14831", "Move DistinctLimitNode to spi", "Xiang Fu", "xiangfu0", "07/12/20, 07:36:17 AM", "Fixes #14809\r\n```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n* Move `DistinctLimitNode` to `presto-spi` module for connectors to push down.\r", "NaN"], ["14833", "Fetch results eagerly from coordinator", "Timothy Meehan", "tdcmeehan", "07/27/20, 07:19:04 PM", "Before, the query results would only begin to be fetched once the client was\r\nredirected to the executing query endpoint.  This introduces latency and may\r\nnot be acceptable for very low latency use cases.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14834", "Partial Aggregation Pushdown for ORC/Parquet", "Vivek", "ClarenceThreepwood", "09/08/20, 10:11:37 PM", "Please make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nDepended by facebookexternal/presto-facebook#1153", "NaN"], ["14836", "Implement ORDER BY spilling", "Saksham", "sachdevs", "07/24/20, 08:04:35 AM", "PrestoSQL PR https://github.com/prestosql/presto/pull/228\r\n\r\nTODO\r\n* Add docs for ORDER BY spilling\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add local disk spilling support for `ORDER BY` syntax.\r\n```\r", "NaN"], ["14838", "Fix bad verify check in DwrfEncryptionInfo", "Rebecca Schlussel", "rschlussel", "07/14/20, 06:12:05 PM", "The check was left over from when the encryptors were a list, and was\r\nincorrectly causing failures.\r\n\r", "NaN"], ["14840", "Do not prune duplicate ConstantExpression", "Rongrong Zhong", "rongrong", "07/15/20, 01:11:58 AM", "We added PruneRedundantProjectionAssignments to eliminate duplicate projections\r\nso PageFunctionCompiler would not need to handle compiling duplicate\r\nprojections. This should only apply to expressions that are not\r\nInputReferenceExpression (which maps to VariableReferenceExpression at planning)\r\nor ConstantExpression, because those are compiled differently. So it's safe to\r\navoid prune ConstantExpression here.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14841", "Allow configurable parser option in Presto-on-Spark", "Wenlei Xie", "wenleix", "07/15/20, 02:20:26 PM", "```\r\n== RELEASE NOTES ==\r\nPresto-on-Spark Changes:\r\n* Allow configerable parser option in Presto-on-Spark\r\n```\r\n\r", "NaN"], ["14842", "Add aggregation function SET_UNION", null, "prithvip", "07/24/20, 06:04:22 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add aggregation function SET_UNION \r\n```\r", "NaN"], ["14845", "Support evaluating min/max only metadata query", "Shixuan Fan", "shixuan-fan", "07/27/20, 10:32:55 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support to optimize min/max only metadata query. This is controlled by existing config ``optimizer.optimize-metadata-queries`` and session property ``optimize_metadata_queries``. Note that enabling this config/session property might change query result if there are metadata that refers to empty data, e.g. empty hive partition.\r\n```\r\n\r\nNote that enabling existing config `optimizer.optimize-metadata-queries` and session property `optimize_metadata_queries` might change query result if there are metadata that refers to empty data, e.g. empty hive partition. For example, if we have two Hive ds partitions, one is `2020-07-01` and the other is `2020-08-01`. Let's assume `2020-08-01` is an empty partition. Then when computing without metadata optimizer, the `ds` rows come from data, and since `2020-08-01` does not have any data, it won't be appearing in the result (e.g. `DISTINCT ds` would only return `2020-07-01`). However, if metadata optimizer is enabled, then `ds` rows come from metastore, and `DISTINCT ds` would return both rows.", "NaN"], ["14846", "Add runtimeOptimizedStages in QueryCompletedEvent", "Peizhen Guo", "pguofb", "07/23/20, 01:52:39 PM", "This PR adds runtimeOptimizedStages field in QueryMetadata of the QueryCompletedEvent, so that the runtime optimization information can be processed and logged later acordingly.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14847", "Fix configuration for Alluxio metastore module", "David Zhu", "yuzhu", "07/17/20, 02:38:50 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14849", "Honor ignore_stats_calculator_failures in getTableStatistics", "Rebecca Schlussel", "rschlussel", "07/17/20, 03:01:49 PM", "ignore_stats_calculator_failures was getting enforced in\r\ncachingStatsProvider and cachingCostProvider.  However, InputExtractor\r\nwas calling getTableStatistics directly, so queries could still fail due\r\nto errors computing stats.  This was particularly a problem when filters\r\nwere pushed down to the table scan, because we compute filter stats as\r\npart of getTableStatistics().\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix an issue where the property ``ignore_stats_calculator_failures`` would not be honored for certain queries that had filters pushed down to the table scan. \r\n```\r", "NaN"], ["14850", "Distribute splits to presto on spark tasks based on size", "Vic Zhang", "viczhang861", "07/23/20, 04:09:38 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14851", "Move authentication support from Presto to Airlift", "Zac", "zacw7", "07/22/20, 10:18:24 PM", "Depends on [facebookexternal/presto-facebook#1079](https://github.com/facebookexternal/presto-facebook/pull/1079)", "NaN"], ["14853", "Fix optimizing like expression during stats calculation", "Rebecca Schlussel", "rschlussel", "07/20/20, 02:35:49 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14854", "Remove DistributionStat for PeakRunningTasks", "Vic Zhang", "viczhang861", "07/20/20, 11:45:42 PM", "DistributionStat is relatively more expensive, use QueryManager:RunningTaskCount instead.\r\n\r\nThis stat is not very useful in practice as one minute resolution may average out, please use eventlistener/QueryStatistics to identify query with highest peak running task count.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14855", "Avoid creating tiny splits at the end of block boundaries", "James Petty", "pettyjamesm", "07/20/20, 06:25:54 PM", "Previously, generating splits would carve out either the entire max split size or the remaining bytes in a block, whichever was smaller. This could create \"tiny\" last splits for unfortunate sized blocks. Eg: a max split size of 64MB would turn a 65MB block into one 64MB split and one 1MB split.\r\n\r\nNow, the last two splits in a block are evenly apportioned. In the example above, the same block will now generate generate two 32.5MB splits.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14856", "Clean up KdbTree node intersection logic", "James Gill", "jagill", "07/27/20, 02:36:49 PM", "KdbTree nodes don't contain their right or upper boundaries.  This\r\ncommit moves that logic internal to the KdbTree, so that callers\r\ndon't have to know about this and have logic handling cases where\r\na point might intersect more than one node.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14857", "Upgrade JTS to 1.17.0", "James Gill", "jagill", "07/27/20, 02:36:18 PM", "This includes two bugfixes that block a serialization improvement.\r\n1.17.0 also includes an improved Geometry.buffer() implementation,\r\nand the current implementation has been causing memory issues for\r\nlarge polygons.\r\n\r\nThe bugfixes include one for an\r\n[NPE in UnaryUnionOp](https://github.com/locationtech/jts/issues/490)\r\nand one that used 3-dimensions, not 2, when deserializing WKTs into\r\na PackedCoordinateSequence.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Update JTS to 1.17.0. This changes the implementation of ST_Buffer: the output might change by a small (1e-10) amount.\r\n```\r", "NaN"], ["14858", "Pinot Connector bug fixing: escape pinot query string before set it into json request", "Xiang Fu", "xiangfu0", "07/20/20, 10:37:54 PM", "Pinot broker request is in JSON format, and the query is set as the field `pql`/`sql` in it.\r\nSince query is not escaped for the special characters when setting to the request.\r\nIt would cause Pinot side throw json parsing exceptions and Presto side failed with `PINOT_HTTP_ERROR`.\r\n\r\nFailed query from presto-cli:\r\n```\r\npresto:default> select count(*) from airlinestats where origin = '{\"random\": \"random\"}' limit 1;\r\n```\r\n\r\nException got:\r\n```\r\nPrestoExternalError(type=EXTERNAL, name=PINOT_HTTP_ERROR, message=\"Unexpected response status: 500 for request {\"pql\" : \"SELECT count(*) FROM airlinestats WHERE (origin = '{\"random\": \"random\"}')\" } to url http://localhost:8099/query, with headers {Accept=[application/json], Content-Type=[application/json]}, full response \", query_id=20200715_203703_00913_7d5x3)\r\n```\r\n\r\nAfter this fix,\r\nSample Pinot JSON Requests generated:\r\n```\r\n{\r\n\t\"sql\": \"SELECT * FROM myTable\"\r\n}\r\n```\r\n```\r\n{\r\n\t\"sql\": \"SELECT count(*) FROM airlinestats WHERE origin = '\\\"{\\\"random\\\" : \\\"random\\\"}\\\"'\"\r\n}\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14859", "Remove unused PushdownFilterResult", "James Sun", "highker", "07/20/20, 04:09:43 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14860", "Fix subfield pruning to be enabled by featureConfig", null, "mayankgarg1990", "07/20/20, 10:32:56 PM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14862", "Pass user info into PrestoSparkRunner", "Wenlei Xie", "wenleix", "07/21/20, 05:50:37 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14863", "Pushdown DistinctLimitNode in Pinot Connector", "Xiang Fu", "xiangfu0", "07/24/20, 05:37:30 AM", "Push down DistinctLimitNode to Pinot Query.\r\n\r\nFor Presto query: `SELECT DISTINCT flightnum FROM airlinestats LIMIT 10`,\r\nWe will pushdown below query to Pinot:\r\n-  SQL format: `SELECT FlightNum FROM airlineStats GROUP BY FlightNum LIMIT 10`.\r\n-  PQL format: `SELECT count(*) FROM airlineStats GROUP BY FlightNum TOP 10`.\r\n\r\nBelow is the generated query plan for SQL mode.\r\n```\r\npresto:default> explain select distinct flightnum from airlinestats limit 10;\r\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n - Output[flightnum] => [flightnum:integer]\r\n         Estimates: {rows: ? (?), cpu: ?, memory: 0.00, network: ?}\r\n     - RemoteStreamingExchange[GATHER] => [flightnum:integer]\r\n             Estimates: {rows: ? (?), cpu: ?, memory: 0.00, network: ?}\r\n         - TableScan[TableHandle {connectorId='pinot', connectorHandle='PinotTableHandle{connectorId=pinot, schemaName=default, tableName=airlineStats, isQueryShort=Optional[true], expectedColumnHandles=Optional[[PinotColumnHandle{columnName=FlightNum, dataType=integer, type=REGULAR}]], pinotQuery=Optional[GeneratedPinotQuery{query=SELECT FlightNum FROM airlineStats GROUP BY FlightNum LIMIT 10, format=SQL, table=airlineStats, expectedColumnIndices=[], groupByClauses=1, haveFilter=false, isQueryShort=true}]}', layout='Optional[PinotTableHandle{connectorId=pinot, schemaName=default, tableName=airlineStats, isQueryShort=Optional[true], expectedColumnHandles=Optional[[PinotColumnHandle{columnName=FlightNum, dataType=integer, type=REGULAR}]], pinotQuery=Optional[GeneratedPinotQuery{query=SELECT FlightNum FROM airlineStats GROUP BY FlightNum LIMIT 10, format=SQL, table=airlineStats, expectedColumnIndices=[], groupByClauses=1, haveFilter=false, isQueryShort=true}]}]'}] => [flightnum:integer]\r\n                 Estimates: {rows: ? (?), cpu: ?, memory: 0.00, network: 0.00}\r\n                 flightnum := PinotColumnHandle{columnName=FlightNum, dataType=integer, type=REGULAR}\r\n```\r\n\r\nBelow is the generated query plan for PQL mode.\r\n```\r\npresto:default> explain select distinct flightnum from airlinestats limit 10;\r\n\r\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n - Output[flightnum] => [flightnum:integer]\r\n         Estimates: {rows: ? (?), cpu: ?, memory: 0.00, network: ?}\r\n     - RemoteStreamingExchange[GATHER] => [flightnum:integer]\r\n             Estimates: {rows: ? (?), cpu: ?, memory: 0.00, network: ?}\r\n         - TableScan[TableHandle {connectorId='pinot', connectorHandle='PinotTableHandle{connectorId=pinot, schemaName=default, tableName=airlineStats, isQueryShort=Optional[true], expectedColumnHandles=Optional[[PinotColumnHandle{columnName=FlightNum, dataType=integer, type=REGULAR}]], pinotQuery=Optional[GeneratedPinotQuery{query=SELECT count(*) FROM airlineStats GROUP BY FlightNum TOP 10, format=PQL, table=airlineStats, expectedColumnIndices=[0, -1], groupByClauses=1, haveFilter=false, isQueryShort=true}]}', layout='Optional[PinotTableHandle{connectorId=pinot, schemaName=default, tableName=airlineStats, isQueryShort=Optional[true], expectedColumnHandles=Optional[[PinotColumnHandle{columnName=FlightNum, dataType=integer, type=REGULAR}]], pinotQuery=Optional[GeneratedPinotQuery{query=SELECT count(*) FROM airlineStats GROUP BY FlightNum TOP 10, format=PQL, table=airlineStats, expectedColumnIndices=[0, -1], groupByClauses=1, haveFilter=false, isQueryShort=true}]}]'}] => [flightnum:integer]\r\n                 Estimates: {rows: ? (?), cpu: ?, memory: 0.00, network: 0.00}\r\n                 flightnum := PinotColumnHandle{columnName=FlightNum, dataType=integer, type=REGULAR}\r\n\r\n(1 row)\r\n```\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n* Pushdown DistinctLimitNode to Pinot Query in SQL mode.\r\n```\r", "NaN"], ["14866", "Make skip.header.line.count=1 files splittable", "James Petty", "pettyjamesm", "07/30/20, 09:05:51 PM", "In general, files with arbitrarily many header lines are not currently considered splittable because the hive record reading logic does not work when rows might span over multiple splits. However, the relatively common case of having a single header row to skip is actually safe and can be considered splittable.\r\nObserve:\r\n- when skip.header.line.count = 1, the hive split boundary handling works even if the header line extends into the subsequent split because the reader of the next split will only process rows that start within the boundary of the split, not rows that extend into the split from the previous one. No additional logic required.\r\n- when skip.header.line.count > 1 and the header rows extend past the first split end boundary, it becomes impossible for a reader to know which rows should be skipped without reading from the beginning. This is still considered unsplittable.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Adds support for splitting hive files when skip.header.line.count=1\r\n```\r", "NaN"], ["14867", "Change TableStatistics constructor to private", "Peizhen Guo", "pguofb", "07/21/20, 05:12:50 PM", "It should be avoided to explicitly call the constructor of\r\nTableStatistics, but instead should use TableStatistics.builder method.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14869", "Upgrade airlift to 0.193 and discovery-server to 1.32", "Zac", "zacw7", "07/22/20, 07:47:24 PM", "NaN", "NaN"], ["14872", "Allow export clientInfo, userAgent and spark queue in Presto-on-Spark", "Wenlei Xie", "wenleix", "07/22/20, 09:48:25 PM", "userAgent can be used to store Spark application id.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14876", "Improve Alluxio cache documentation", "Bin Fan", "apc999", "07/23/20, 07:25:09 PM", "Documentation improvement on Alluxio caching\r\n\r\nCo-authored-by: Haoyuan Li <haoyuan@alluxio.com>\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14877", "Avoid checking isSplittable for files smaller than the split max size", "James Petty", "pettyjamesm", "07/30/20, 03:26:54 PM", "For some input formats, the isSplittable check is non-trivial and can add a significant amount of time to split generation. This change allows files smaller than the max split size to avoid that check and simply call them unsplittable since they're within the split target range already.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Improves split generation by avoiding an unncessary splittable check when files are smaller than the initial split max size, regardless of their input format.\r\n```\r", "NaN"], ["14879", "Fix infinite loop in non-legacy SqlQueryScheduler", "Rebecca Schlussel", "rschlussel", "07/27/20, 01:59:47 PM", "Queries that don't have table scan inputs could enter an infinite loop inthe scheduler if they hit the queryStateMachine.isDone() check and had any sections ready for execution. If the query is done (e.g. canceled or abandoned at this point), we abort the section.  Since \"aborted\" is a failed state, the section would continue to be considered \"ready for execution\"  in our next time through, and the loop would continue indefinitely. We need to explicitly exit the scheduling loop if the query is finished.\r\n\r\nQueries with inputs didn't have this issue because the split scheduler would be closed when the query finished, so the scheduler would hit an error when it tried to create the scheduler for the scan stages.\r\n\r\nI don't have a test because I was only able to reproduce the issue by explicitly calling queryStateMachine.transitionToFailed() here so we would enter the if statement.  Recommendations about how to test this are appreciated.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Fix potential infinite loop when the setting ``use_legacy_scheduler`` is set to false.\r\n```\r\n\r", "NaN"], ["14880", "Enable async page transport by default", "Vic Zhang", "viczhang861", "07/28/20, 01:50:46 AM", "\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Enable async page transport with non-blocking IO by default. This can be disabled by setting ``exchange.async-page-transport-enabled`` configuration property to false.\r\n```\r", "NaN"], ["14881", "Simplify integration with Alluxio local cache", "Bin Fan", "apc999", "07/31/20, 06:45:26 PM", "Bumping Alluxio dependency to 2.2.2-2 to improve integration between Presto and Alluxio\r\n- remove code redundancy on the presto-side and reuse code in Alluxio\r\n- simplify initialization of a new Alluxio File System instance\r\n\r\nCo-authored-by: Haoyuan Li <haoyuan@alluxio.com>\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14884", "Fix flaky testRuntimeOptimizedStagesCorrectness", "Peizhen Guo", "pguofb", "07/27/20, 03:37:33 PM", "Fixes #14882\r\n\r\nEssentially, this PR let the `setUp()` function wait for up to 3min so that all warmup queries (tpch copying) will finish populating their `QueryCompletedEvent`. This will avoid warm-up query completed events get accidentally captured as that of our testing queries. Further, we add additional checks on the queryId of the testing query and the query id of the captured query completed event, and skip the test when in extreme cases (e.g. warmup query completed event is delayed over 3min) where the two ids do not match.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14886", "Unify grouped execution properties", "Rebecca Schlussel", "rschlussel", "07/27/20, 05:51:57 PM", "Turn grouped execution on by default and introduce a new property grouped_execution to eventually replace all other grouped execution properties\r\n\r\n#14885 \r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Enable ``dynamic-schedule-for-grouped-execution`` by default.  In future releases, we will remove this property, and grouped execution will always use dynamic scheduling.\r\n* Enable ``grouped-execution-for-aggregation`` and ``experimental.grouped-execution-for-eligible-table-scans`` by default\r\n* Introduce new configuration property ``grouped-execution-enabled`` and session property ``grouped_execution`` to turn grouped execution on or off.  This property is true by default.  If set to false, it is equivalent to setting all of ``grouped-execution-for-aggregation``, ``grouped-execution-for-join``, and ``experimental.grouped-execution-for-eligible-table-scans`` to false.  In future releases we will remove these other properties and only have a single switch for enabling and disabling grouped execution.\r\n```\r", "NaN"], ["14888", "Minor refactor to RemoteSourceFactory", "Wenlei Xie", "wenleix", "07/27/20, 05:33:25 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14889", "Handle NaN in Parquet statistics", "Zhenxiao Luo", "zhenxiao", "07/25/20, 06:12:16 AM", "Cherry-pick of https://github.com/prestosql/presto/commit/c457710fab971c95cc6b869a269269f2be1c7ef8\r\n\r\nCo-authored-by: Alex Albu <alex.albu@starburstdata.com>", "NaN"], ["14890", "Upgrade presto-hadoop-apache2 for GCS token refreshing", "Beinan", "beinan", "07/29/20, 06:20:57 AM", "Fix #14832 \r\nUpgrade presto-hadoop-apache2 to pick up the commit prestodb/presto-hadoop-apache2#43 , which will fix the GCS token not refreshing issue.", "NaN"], ["14895", "Add support for limit pushdown through union", "countryman4687", "fgwang7w", "08/05/20, 07:24:55 PM", "```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Add support for limit pushdown through union\r\n```\r\n\r\nresolve #14894 ", "NaN"], ["14896", "Add bing_tile_children and bing_tile_parent functions", "James Gill", "jagill", "07/30/20, 02:25:59 AM", "Small helper functions to find the children and parent of a BingTile.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeospatial Changes\r\n* Introduce ``bing_tile_children`` and ``bing_tile_parent`` functions to get parents and children of a Bing tile.\r\n```\r", "NaN"], ["14899", "Add geometry_to_dissolved_bing_tiles and improve geometry_to_bing_tiles", "James Gill", "jagill", "07/30/20, 06:06:08 PM", "NB: The first commit of this PR is in #14896; that should be merged first.\r\n\r\nA useful feature (that is requested by users) is to have a tile covering\r\nof a geometry that recursively dissolves complete sets of children into their\r\nparents.  This results in a smaller set of tiles with non-uniform zoom\r\nlevel.  This can be done efficiently, and actually can form the basis\r\nof the current (uniform zoom level) minimal tile covering function.\r\n\r\nThis PR adds geometry_to_dissolved_bing_tiles, and refactors\r\ngeometry_to_bing_tiles to use an intermediate result in that algorithm.\r\nThis increases the performance by 50x on large/complex polygons,\r\nremoves the complexity restriction on the polygons, and fixes a\r\ncorrectness bug (see below).\r\n\r\nThe benchmarks before and after:\r\n\r\n    Baseline\r\n    Benchmark                                         Mode  Cnt    Score    Error  Units\r\n    BenchmarkGeometryToBingTiles.envelopeToBingTiles  avgt   10    0.013 \u00b1  0.001  ms/op\r\n    BenchmarkGeometryToBingTiles.geometryToBingTiles  avgt   10  330.286 \u00b1 18.704  ms/op\r\n\r\n    New Algorithm\r\n    Benchmark                                         Mode  Cnt  Score   Error  Units\r\n    BenchmarkGeometryToBingTiles.envelopeToBingTiles  avgt   10  0.044 \u00b1 0.002  ms/op\r\n    BenchmarkGeometryToBingTiles.geometryToBingTiles  avgt   10  7.697 \u00b1 0.635  ms/op\r\n\r\nThe correctness bug is as follows: We consider Bing tiles to not include their southern and\r\neastern border.  This prevents duplicating points that are on the border of one or more tiles.\r\nHowever, this restriction was not in the previous geometry_to_bing_tiles algorithm, so the\r\nzoom=1 covering of `POLYGON ((0 0, 1 0, 1 1, 0 1, 0 0))` was the tile `\"1\"`, but the covering\r\nfor `POINT (0 0)` (clearly intersecting the above polygon) was the tile `\"3\"`.  This would result\r\nin some intersections being missed in a tile-based join.  The new algorithm expands them\r\ncorrectly, which leads to the correct but non-intuitive result of the covering of the polygon of a\r\nBing tile to not only be that tile, but its east, south, and southeast neighbors.  The intuitive\r\nconflict can be resolved by noting that while the Bing tile does not include its eastern or southern\r\nborder, the polygon derived from it does, so to cover that, we must include those adjacent tiles.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeospatial Changes\r\n* Add geometry_to_dissolved_bing_tiles function, which dissolves complete sets of child tiles to their parent.\r\n* Improve geometry_to_bing_tiles.  It is 50x faster on complex polygons, the limit on polygon complexity is removed, and some correctness bugs have been fixed.\r\n```\r", "NaN"], ["14900", "Revert \"Support grouped execution for eligible table scans\"", "Rebecca Schlussel", "rschlussel", "07/27/20, 09:57:34 PM", "Remove experimental feature grouped_execution_for_eligibile_table_scans, since this feature could cause query failures where queries exceed the split buffering limit and has not been sufficiently useful in practice. We may choose to add this feature back later in a more robust way. \r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Remove experimental feature to perform grouped execution for eligible table scans and its associated configuration property ``experimental.grouped-execution-for-elligible-table-scans`` and session property ``grouped_execution_for_eligible_table_scans``\r\n```\r", "NaN"], ["14901", "Kafka insert", "Zhenxiao Luo", "zhenxiao", "07/29/20, 03:25:12 AM", "\r\n```\r\n== RELEASE NOTES ==\r\n\r\nKafka Changes\r\n* Support insert in Kafka connector\r\n```", "NaN"], ["14903", "Fix Parquet long statistics handling when min/max not set", "Zhenxiao Luo", "zhenxiao", "07/28/20, 04:55:28 PM", "Cherry-pick of https://github.com/prestosql/presto/commit/3bc0e128855599115565a77485469096c25d5fd1\r\n\r\nCo-authored-by: talbm <tal@varada.io>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Fix parquet statistics when min/max is not set\r\n```\r", "NaN"], ["14904", "Move TranslateExpressions above all PickTableLayouts", "James Sun", "highker", "07/29/20, 02:45:15 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14906", "Add a fast path using totalSize to plan join swapping", "Peizhen Guo", "pguofb", "08/05/20, 03:15:26 PM", "For simple plans (join on two tables directly without intermediate operations), directly leverage the `totalSize` statistics to plan the join swapping, which avoids either the per-column statistics collection cost or the inaccurate size estimations from CBO statsCalculator (especially for nested data structures e.g., map<varchar, array<bigint>>).\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14907", "Fix flaky testTransactionMetadataCleanup", "Vic Zhang", "viczhang861", "07/31/20, 02:25:16 PM", "Attempt to fix https://github.com/prestodb/presto/issues/14835\r\n\r\nTest is set to run 1000 times,  no failure observed in logging https://api.travis-ci.org/v3/job/712638157/log.txt  (terminated due to exceeding maximal log size)\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14908", "Add release notes for 0.239", "Leiqing Cai", "caithagoras", "08/07/20, 05:44:16 PM", "# Missing Release Notes\n## Leiqing Cai\n- [x] https://github.com/prestodb/presto/pull/14670 BenchmarkRunner fixes and make it extensible (Merged by: Leiqing Cai)\n\n## Nikhil Collooru\n- [x] https://github.com/prestodb/presto/pull/14618 Collect file stats (Merged by: James Sun)\n\n## Ravion\n- [ ] 5d162735a0e6677753d64c2cd358fc62d2328d4e Added documentation for Presto on Spark\n\n## Rebecca Schlussel\n- [x] https://github.com/prestodb/presto/pull/14838 Fix bad verify check in DwrfEncryptionInfo (Merged by: Rebecca Schlussel)\n- [x] https://github.com/prestodb/presto/pull/14068 Bump zookeeper from 3.4.13 to 3.4.14 (Merged by: Rebecca Schlussel)\n\n## Tim Meehan\n- [ ] https://github.com/prestodb/presto/pull/14788 Fixes for completion events post-dispatching (Merged by: Timothy Meehan)\n\n## Weidong Duan\n- [ ] https://github.com/prestodb/presto/pull/14817 fix inconsistent session value at server and client side (Merged by: Maria Basmanova)\n- [ ] https://github.com/prestodb/presto/pull/14818 fix druid incorrect real data value for Druid connector (Merged by: Zhenxiao Luo)\n\n## Wenlei Xie\n- [ ] https://github.com/prestodb/presto/pull/14841 Allow configurable parser option in Presto-on-Spark (Merged by: Andrii Rosa)\n\n## Zhenxiao Luo\n- [ ] https://github.com/prestodb/presto/pull/14889 Handle NaN in Parquet statistics (Merged by: Zhenxiao Luo)\n\n## Zhi Wen\n- [ ] https://github.com/prestodb/presto/pull/14851 Move authentication support from Presto to Airlift (Merged by: Andrii Rosa)\n- [ ] https://github.com/prestodb/presto/pull/14869 Upgrade airlift to 0.193 and discovery-server to 1.32 (Merged by: Andrii Rosa)\n\n## tgorthi\n- [ ] https://github.com/prestodb/presto/pull/14822 Support return type coercion in CREATE FUNCTION (Merged by: Leiqing Cai)\n\n# Extracted Release Notes\n- #14527 (Author: Saksham Sachdev): Aggregation ORDER BY & DISTINCT spilling\n  - Add local disk spilling support for aggregation functions with `ORDER BY` or `DISTINCT` syntax.\n- #14632 (Author: Zhi Wen): Add allowed roles for HTTP endpoints\n  - Specify allowed roles for HTTP endpoints.\n- #14696 (Author: frank.hu): Support common sub-expression optimization in CursorProcessorCompiler\n  - Add optimization for cursor projection & filter by extract and compute common subexpressions among all projections & filter first. This optimization can be turned off by session property ``optimize_common_sub_expressions``.\n- #14704 (Author: Xiang Fu): Adding Pinot SQL endpoint support\n  - Add Pinot SQL endpoint support.\n- #14740 (Author: fornaix): Fix confusion matrix computation for classification function\n  - Fix  :func:`classification_miss_rate` and :func:`classification_fall_out` functions (:pr:`14740`).\n- #14743 (Author: Sanket Dige): Add invoker security mode for views\n  - Add support for 2 security modes for views. The default `DEFINER` security mode is the same as the previous behavior. Tables referenced in the view are accessed using the permissions of the view owner (the **creator** or.\n  - Definer** of the view) rather than the user executing the query. In the `INVOKER` security mode, tables referenced in the view are accessed using the permissions of the query user (the **invoker** of the view).\n- #14752 (Author: Swapnil Tailor): Revert \"Reliable Resource Groups with versioning\"\n  - Reverting Reliable Resource Group Versioning.\n- #14753 (Author: Adam J. Shook): Implement PrestoDatabaseMetaData getClientInfoProperties\n  - Implemented DatabaseMetaData.getClientInfoProperties API.\n- #14754 (Author: Partha Kanuparthy): Fix thread listing under heavy workloads (thread churn)\n  - Fix NPE in `/v1/thread` end point.\n- #14775 (Author: Partha Kanuparthy): Fix thread snapshot UI due to variable initialization\n  - The worker page's thread snapshot UI does not work (no stack trace displayed on click) when there is active query load (tested under Chrome). This patch fixes an uninitialized variable in client JS that was causing this UI behavior.\n- #14784 (Author: Tim Meehan): Fix query completion events for non-dispatched queries\n  - Fix missing query completion events for queries which fail prior to dispatching.\n- #14800 (Author: Ariel Weisberg): Partition filtering warning\n  - Add support for warning on unfiltered partition keys using `partition-keys-to-warn-on-no-filtering` system property.\n- #14811 (Author: Zhenxiao Luo): Elasticsearch connector improvements\n  - Support Elasticsearch numeric keyword.\n  - Support composite publish_address in Elasticsearch.\n  - Add configurations to improve concurrency in Elasticsearch.\n- #14820 (Author: Adli Mousa): Add support of caching for glue metastore\n  - Add support for caching the Glue metastore.\n- #14831 (Author: Xiang Fu): Move DistinctLimitNode to spi\n  - Move `DistinctLimitNode` to `presto-spi` module for connectors to push down.\n- #14836 (Author: Saksham Sachdev): Implement ORDER BY spilling\n  - Add local disk spilling support for `ORDER BY` syntax.\n- #14842 (Author: prithvip): Add aggregation function SET_UNION\n  - Add aggregation function SET_UNION.\n- #14845 (Author: Shixuan Fan): Support evaluating min/max only metadata query\n  - Add support to optimize min/max only metadata query. This is controlled by existing config ``optimizer.optimize-metadata-queries`` and session property ``optimize_metadata_queries``. Note that enabling this config/session property might change query result if there are metadata that refers to empty data, e.g. empty hive partition.\n- #14849 (Author: Rebecca Schlussel): Honor ignore_stats_calculator_failures in getTableStatistics\n  - Fix an issue where the property ``ignore_stats_calculator_failures`` would not be honored for certain queries that had filters pushed down to the table scan.\n- #14857 (Author: James Gill): Upgrade JTS to 1.17.0\n  - Update JTS to 1.17.0. This changes the implementation of ST_Buffer: the output might change by a small (1e-10) amount.\n- #14863 (Author: Xiang Fu): Pushdown DistinctLimitNode in Pinot Connector\n  - Pushdown DistinctLimitNode to Pinot Query in SQL mode.\n- #14879 (Author: Rebecca Schlussel): Fix infinite loop in non-legacy SqlQueryScheduler\n  - Fix potential infinite loop when the setting ``use_legacy_scheduler`` is set to false.\n- #14880 (Author: Vic Zhang): Enable async page transport by default\n  - Enable async page transport with non-blocking IO by default. This can be disabled by setting ``exchange.async-page-transport-enabled`` configuration property to false.\n- #14886 (Author: Rebecca Schlussel): Unify grouped execution properties\n  - Enable ``dynamic-schedule-for-grouped-execution`` by default.  In future releases, we will remove this property, and grouped execution will always use dynamic scheduling.\n  - Enable ``grouped-execution-for-aggregation`` and ``experimental.grouped-execution-for-eligible-table-scans`` by default.\n  - Introduce new configuration property ``grouped-execution-enabled`` and session property ``grouped_execution`` to turn grouped execution on or off.  This property is true by default.  If set to false, it is equivalent to setting all of ``grouped-execution-for-aggregation``, ``grouped-execution-for-join``, and ``experimental.grouped-execution-for-eligible-table-scans`` to false.  In future releases we will remove these other properties and only have a single switch for enabling and disabling grouped execution.\n- #14900 (Author: Rebecca Schlussel): Revert \"Support grouped execution for eligible table scans\"\n  - Remove experimental feature to perform grouped execution for eligible table scans and its associated configuration property ``experimental.grouped-execution-for-elligible-table-scans`` and session property ``grouped_execution_for_eligible_table_scans``.\n\n# All Commits\n- 6d5cdaac0a679ce830eeac193b4bb092ef978c25 Increase default size for http timeout executors (Vic Zhang)\n- c963c79cb9449a7e9cbe54bf30b0e5555db7f3c8 Enable async page transport by default (Vic Zhang)\n- ef4b5378011084e2eb20b7307aa866431091e582 Support evaluating min/max only metadata query (Shixuan Fan)\n- b1fc2020f98fa6f1a4bb59419cfa488480539d65 Remove unused field in MetadataQueryOptimizer (Shixuan Fan)\n- 259f461209ce2a7e14006574664aba89f013121e Allow EnforceSingleRowNode for predicate extractor (Shixuan Fan)\n- 5944e152d1a67a5bdea0c2714f1512dd61867e51 Push expression translation above MetadataQueryOptimizer (Shixuan Fan)\n- 03e80fcb3c62dc4f5e0d5e21d60d08dd27e88c23 Revert \"Support grouped execution for eligible table scans\" (Rebecca Schlussel)\n- ee0a3af0c62a1d84784f986488994a18f8587ef5 Fetch results eagerly from coordinator (Tim Meehan)\n- 7cb1d7a1500a0d552f735ffec74642ba426c29aa Unify grouped execution properties (Rebecca Schlussel)\n- c8152d85ec6f0480190f4b06a291ad076ba1fd3c Enable grouped execution by default (Rebecca Schlussel)\n- f38828ffaa0063f825f954088bb7edc8d2be98aa Don't try grouped execution for forceSingleNode (Rebecca Schlussel)\n- 82cd9bc2543d2c925d0399899996a0b9c0c047d9 Minor refactor to RemoteSourceFactory (Wenlei Xie)\n- 57c24f15a0e5bf54bc3a2805defc92f49f463d60 Fix flaky testRuntimeOptimizedStagesCorrectness (Peizhen Guo)\n- 9ae3a2185d2371cb79a8d0b294881d1f99064b1c Clean up KdbTree node intersection logic (James Gill)\n- 7e3d217da686361273507eb153e0bf1336a74d58 Upgrade JTS to 1.17.0 (James Gill)\n- 23082e899e62049d09e51a5e80b67cba221444f4 Fix infinite loop in scheduler for finished query (Rebecca Schlussel)\n- 6f01b1e3eae309427cf5761a5174cb5afb51bd4b Remove extra new line (Rebecca Schlussel)\n- 81f6349a2542a2d65eaf161e83fb617657f9cc3a Handle NaN in Parquet statistics (Zhenxiao Luo)\n- 4c2b8c224f7633c2672b02ec145849524c06e373 Add aggregation function SET_UNION (prithvip)\n- 392ec928df28d9a9122db4b2597234b1f1c051dc Close spiller on Operator#close for ORDER BY spilling (Saksham Sachdev)\n- e418fe73def934b18c8b5dbb95ddb6df60478e7e Extract order by queries tests to separate class (Saksham Sachdev)\n- 4722c757f3d674f5bcf74190bb38d346737ca110 Convert revocable memory to user memory on OrderBy finish (Saksham Sachdev)\n- 1a56a5e2ec462dce9463f764c3ba491754cda4f9 Use OrderingCompiler in OrderBy spilling (Saksham Sachdev)\n- 9d11bdb592d2949d9c4e83316866be14aaa6a25e Add Spill To Disk for ORDER BY (Saksham Sachdev)\n- 37f6c26195f6dbacf3cf91b4983e10dc43db100f Use WorkProcessor in OrderByOperator (Saksham Sachdev)\n- 3b7f8461d2123cd08420405d93efa81665816e94 Extract DummySpillerFactory from TestHashAggregationOperator (Saksham Sachdev)\n- 4d8ead5b460f6ef903ff0e98c51408b2a07967b1 Produce more than single page in testHashAggregation (Saksham Sachdev)\n- 93255a21294f05011ee473783caca056eb254c43 Allow memory revoke only during operator finish phase (Saksham Sachdev)\n- 9b5009442d2c7abac2b9717884b2c9f6d6be1a37 Revoke memory after initial output page has been produced in tests (Saksham Sachdev)\n- 6a55eb422b9043f49db02b5245bfd59c091b8776 Adding DistinctLimit pushdown support in Pinot Connector (Xiang Fu)\n- 8045dbe36d7d2fb48ce83303a00c4de7ee2f8478 Collect statistics of files committed by OrcFileWriter (Nikhil Collooru)\n- 89632d3a92c14df9be9c07dfe6832c0dc8cd9cb5 Improve Alluxio cache documentation (Bin Fan)\n- 9831c835957e8ca939a5c9e4a4ea6b0c5a96a0bc support caching for glue metastore (Adli Mousa)\n- 63547278dd82157491c88d689ce0f9247cae10b7 Add getSplitSizeInBytes in SPI ConnectorSplit (Vic Zhang)\n- d35acde119d770c551e1ddf4e511de88bdacd211 Add properties for Presto-on-Spark split distribution (Vic Zhang)\n- 58585b29c14bc1de960605b1746de7c35b86db02 Add test checking QueryCompletedEvent (Peizhen Guo)\n- ae5770a5b9eaeeed7d44a6e2c74a4f61952b8016 Add runtimeOptimizedStages in QueryCompletedEvent (Peizhen Guo)\n- 2cb629e8658bc0781f310ae4b08d8da4b9972d73 Make maxConcurrency optional in ConcurrentExecutionPhase (Leiqing Cai)\n- 4514b992dfd2603e602707d66a6f61bfef455802 Refactor PhaseExecutor (Leiqing Cai)\n- a4e7a7858d4e15487d9c2dfa577d266f03d20f4c Allow session properties to be specified with queries (Leiqing Cai)\n- f8cfad6f5b88a051f8b1982b4c1a33376ee1eb1c Refactor BenchmarkSuite (Leiqing Cai)\n- 515ccf97a2d9dcc61a2ad502e7b53e459232065b Fix PhaseSpecification (Leiqing Cai)\n- c61baf447443a0ae8bd6fa4dbf5f661d564fae77 Make benchmark runner extensible (Leiqing Cai)\n- af8058a55dd4d934ae22ef59eac80330d65a9746 Make BenchmarkSuiteSupplier extensible (Leiqing Cai)\n- b7435884f747e9e5031bd028aa035ed51e335687 Specify allowed roles for HTTP endpoints (Zhi Wen)\n- f28613ed547f91121f5807f01c326256568505b1 Move authentication support from Presto to Airlift (Zhi Wen)\n- 3469944fd4c7f5444239f9b83175b09c61c7bcd5 Refactor userAgent handle in Presto-on-Spark (Wenlei Xie)\n- 9d2efe68bc023b34db4d44ab7fd0a4bebfc85696 Allow export Spark queue in Presto-on-Spark query event (Wenlei Xie)\n- 38ee80c1ba957a7b6f1dbd08e56fce3b5c6f1923 Allow customizing clientInfo and userAgent in Presto-on-Spark (Wenlei Xie)\n- 4f1b626ea1d4bc23a3ab90577c018769131f2bea Upgrade airlift to 0.193 and discovery-server to 1.32 (Zhi Wen)\n- bad48ac5c9b030f8d63c2b0c51957cf460dfda24 Pass user info into PrestoSparkRunner (Wenlei Xie)\n- 7d9d8dcbe9a00b94cb93b96ea72dc110e2e094fb Change TableStatistics constructor to private (Peizhen Guo)\n- 1ef8df5f6c7487195d87a98889c1b015dd3e64f9 Remove DistributionStat for PeakRunningTasks (Vic Zhang)\n- 6e98e41748c25281dbc638dc81f02bbd92ad5596 Bugfixing: escape pinot query string before set it into json request (Xiang Fu)\n- 94999230b1b1f78834c5bb6a852362ce4ac4a634 Fix subfield pruning to be enabled by featureConfig (Mayank Garg)\n- bfc5ea8e3a40f18173805ad441f3d0f071ad3c25 Avoid creating tiny splits at the end of block boundaries (James Petty)\n- 91b058e83f22776ca701c501269c015611248831 Add invoker security mode for views (Sanket Dige)\n- d7949f77315565e6dacc63529b71de138e3c8501 Remove unused PushdownFilterResult (James Sun)\n- 2d2693f0531377ce8cf04519cd670c58d948825e Fix failure during optimization of LIKE expression (Rebecca Schlussel)\n- 9e012a2e9e6058c016fcdc73c170ebb3c8a0b141 Don't ignore stats failures in tests (Rebecca Schlussel)\n- aa049ccae726b8f9a382117f495b2453b2ac784a Honor ignore_stats_calculator_failures in getTableStatistics (Rebecca Schlussel)\n- 589325b3b27c117c71ecc399254567ea0a49f5e0 Support Pinot new SQL endpoint for broker queries. (Xiang Fu)\n- 573b8d158ec089c30164f8343d31a12801fa0d2c Fix configuration for Alluxio metastore module (David Zhu)\n- f5868d440a8db1c46220cfe068bc73aeac5b7c2e Explicitly not supporting external functions in lambda and join filter (Rongrong Zhong)\n- b8f4c99e749036dad3569da0830297d7380fb489 Add rule to rewrite filter with remote function to project (Rongrong Zhong)\n- 4aa2dedfec0bbd4f71fc614d93aa67514337218e Plan projection with remote functions (Rongrong Zhong)\n- 2608642304e1147d717665024e84084f2be83dba Add Locality to ProjectNode (Rongrong Zhong)\n- 0890b3dc4bb18e1e6567fe8e2ca82a81dcd3a9c0 Invoke CBO at SqlQueryScheduler for Join Swapping (Peizhen Guo)\n- d86aeee0d405f319125ac5dad02bd6859e5b1bdf Add a session property for runtime optimizer (Peizhen Guo)\n- 3cf806989cab06f269cfcb4a2a7ac7335252cd3f Support return type coercion in CREATE FUNCTION (tgorthi)\n- 19228af885c56372cf5f1de6e0b1dbac0136b185 Warn on configured unfiltered partitions (Ariel Weisberg)\n- 816e9771c7d4d043a7906fc1f96ecf49ab6f6165 Support for checking partition key filter coverage (Ariel Weisberg)\n- 7cc8bd339dde19e9c06e0581bc01592940fb3dba Rename PlanSanityChecker to PlanChecker (Ariel Weisberg)\n- 4f2f854fcb819efb841f1741d13508eeef73473a Introduce CacheQuotaRequirementProvider (Ke Wang)\n- a63cb8ad1e66cbc888b04b2e3a2c05dc1c8043e5 Allow configurable parser option in Presto-on-Spark (Wenlei Xie)\n- b14f68a18ac742d31d9fc4b9b53c593bda94d5f0 Do not prune duplicate ConstantExpression (Rongrong Zhong)\n- 28694009ec07ac5a8e875e7d5c01d1c47ac6cd71 Fix bad verify check in DwrfEncryptionInfo (Rebecca Schlussel)\n- 670ecb5dd26da8ecbe0233c6c60502e5c93cf2d5 Allow query info to be stored in a file upon query finish (Andrii Rosa)\n- 6e98dd127a2dec675a628fb69685b01d0df61396 Add ability to register event listener in Presto on Spark (Andrii Rosa)\n- 3f69d1f0d0bd57e8908492424c233d46b3e214d6 Implement QueryMonitor callbacks for Presto on Spark (Andrii Rosa)\n- 2f0575cd521742ac1685a01ff188196c0464c3a9 Collect TaskInfo for Presto on Spark tasks (Andrii Rosa)\n- 323592f182bd205edd2e7b7ca2b086b88e48603c Prepare to collect TaskInfo from Presto on Spark tasks (Andrii Rosa)\n- 41483c3fa9c6b8b064c66298dc1f88a904f1eb16 Apply compression to Presto on Spark splits (Andrii Rosa)\n- 829b667c6b3275ba7bc82e5af39f6b944757f822 Fix inconsistent session value from both server and client side (Weidong Duan)\n- 20a172696aac308738b6bae08318a917be752910 Fix query completion events prior to execution (Tim Meehan)\n- 3177a906b3adde0a6a5fa43a8f57c722ffccf1bb Skip isSplittable reflection for RCFileInputFormat (Rohit Jain)\n- 058bda43f81e2819f7a2cb8deccc3f014e06815e Include scheduling stats of bucketed table to NodeSelectionStats (Ke Wang)\n- dece3cd3e27d3cc02e3cb0f80e4e6faf1f331519 Fix bytecode generation for SQL functions (prithvip)\n- bd300f74bae348514dfa9f13b8c5a3f0dbbb2007 Move DistinctLimitNode to spi (Xiang Fu)\n- 728b8b0db3e03922d3f8afc0a0ad4188dcdd9753 Fix document in Presto-on-Spark (Wenlei Xie)\n- 3943983ef11f58dbd843910e4971f10a33b34192 fix druid incorrect real data value for DruidBrokerPageSource (Weidong Duan)\n- 5d162735a0e6677753d64c2cd358fc62d2328d4e Added documentation for Presto on Spark (Ravion)\n- 84b7b0ac0ac07b0c42a7f60169fc2b04a35d3360 Allow DictionaryBlock for MinMaxBy aggregation (Vic Zhang)\n- 13ea2ea7164d2d631b31b5a1eedb0dc4b9de44ed Add documentation for Elasticsearch connector concurrency configs (Zhenxiao Luo)\n- 0e4e6460ff9ac81bdb74bd663ecaf29185b1e44b Improve concurrency for Elasticsearch calls (Zhenxiao Luo)\n- 93f78434dcb26e1c897e933252b727117c414696 Enable testShowCreateTable for Elasticsearch connector (Zhenxiao Luo)\n- 277da41576d2cc5a10b12a6f05499a536ef8a279 Remove fake properties from Elastic Search column metadata (Zhenxiao Luo)\n- 15ff958b5ea83fbe49df4701ade39be52175d086 Optimize Elasticsearch queries that fetch no columns (Zhenxiao Luo)\n- d58624da7ce04b45694476c5ac3fcffe317d25d7 Fix ElasticsearchClient package name (Zhenxiao Luo)\n- 2cafb6923e4af1019fe19b7e9f9c4d4a535c757b Support composite publish_address in Elasticsearch (Zhenxiao Luo)\n- 25b636f4d089c7d95dff5c8cd0e477fc13b85f98 Support elasticsearch numeric keyword (Zhenxiao Luo)\n- e182413e722e49d80d898695b5b06b6412dde513 Fix reading encrypted streams at offset 0 (Rebecca Schlussel)\n- 614304448532309ff542e524bcc355281cd4803d Avoid fetching encryption information when no columns requested (Mayank Garg)\n- 86a3cb0a1cfd593347c8ca38782bfd2b4815d736 Implement additional serialization methods for Spark shuffle (Andrii Rosa)\n- 56f902210fb8baeea808fa936cd84a8dbec05be8 Fix file stats for encryption groups (Rebecca Schlussel)\n- 443d105dade21f961a1bfbbcc73265ff017a9cfb Fix encryption with dictionary encodings (Rebecca Schlussel)\n- 863a5341542ff75b6ca7ff7b0046d65bf54f04ec Fix dwrf decryption for empty files (Rebecca Schlussel)\n- a54efd95c9ac0b87ad24d6d191c467bfddc690a9 Minor cleanup in TestDecryption (Rebecca Schlussel)\n- 4a3d46c2298f69bf83e5c18584c677e4bf757efe Use the same RowExpressionCompiler in CursorProcessorCompiler (Rongrong Zhong)\n- b8b30599be6ea33932623124a2e3c60da3f3afa0 Attach user and query ID to JDBC SQL strings (Naveen Cherukuri)\n- e22ebb0afc6fb229135a506d17a784c4ca6d6e6b Add documentation for driver.max-page-partitioning-buffer-count (Ying Su)\n- bbda1570796d08da32ad97fba09be8665f2aeef1 Run PruneRedundantProjectionAssignments after RowExpression change (Rongrong Zhong)\n- f2b3910f5019adb89dd1b49c85c28d751cd04764 Make TestElasticsearchIntegrationSmokeTest single threaded (Zhenxiao Luo)\n- 92c3d3cf1bbe0b8d8375bd8c9902108f2c04ce1c Change getSupportedColumnStatistics for temp table (Peizhen Guo)\n- 7fdb787ac4f4b526346468f1057e8175ef89f5e1 Fix flaky testAndInFilter test (Rebecca Schlussel)\n- b9cde3a1da402d3b273066b38bbc385aad2e5e51 Add support for column names with spaces to SubfieldTokenizer (Masha Basmanova)\n- 57428cc9d064db809a7f3e98edffeadefcba5358 Add a test for Presto-on-Spark (Wenlei Xie)\n- 2f5d368fdd345f84d269a2a3c30138abac7e4a9b adding dist code (Partha Kanuparthy)\n- 22b9d9c31f87c1dc8f6976d8426c74999bee525b Fix thread snapshot UI due to variable initialization (Partha Kanuparthy)\n- 7b71284fccf6bca607596132449fb893315eec04 Revert \"Fix thread snapshot UI due to variable initialization\" (Partha Kanuparthy)\n- 97969996b55142971edaee9d78e12580a45fa6bb Fix thread snapshot UI due to variable initialization (Partha Kanuparthy)\n- 709d2297b0718ed91c50af577cbf69fcd605ac6c fix for thread listing under heavy workloads (Partha Kanuparthy)\n- ee71bf56891477f1cceb95695de89588ec60ed59 Allow configure SQL invoked function implementation type (Rongrong Zhong)\n- 54f7e416ac9f92b5e2e9dfdc6fa4de1176946faa Add support for DWRF encryption without compression (Rebecca Schlussel)\n- a5434172d75ca4bc8f0cba930abb2a07e6ba4965 Pass DwrfWriterEncryption into OrcWriter (Rebecca Schlussel)\n- c561c50582ddbe35cf2d2885e9f929c9a453a9fa Write support for DWRF encryption at rest (Mayank Garg)\n- 9f89256256c4bb577b8a9e78c24047c49da93275 Add TestingHiveEventListener (James Sun)\n- 64e1f905bb8463887af0f1d3ff1e465a8f6d79cf Fix query completion events for non-dispatched queries (Tim Meehan)\n- a101186ca2f489f39ff0ca52527796ed0f50f897 Bump zookeeper from 3.4.13 to 3.4.14 (Rebecca Schlussel)\n- dc2a10a0c54381f866fd3e640eb2119824da4151 Set sequenceId for dwrf columnEncodings (Rebecca Schlussel)\n- 1f5a29cc2dd4f78bfe94b7d3093b92f341f4234d Fix stats for dwrf encryption (Rebecca Schlussel)\n- 6b51cbf52a138150962a4c1145b4517bc6da89c8 Refactor buffer max capacity calculation (Ying Su)\n- 91cb2ec400ca034295bb690682c756e5eb660c25 Allow additional error margin for estimatedMaxCapacity (Ying Su)\n- cfcf39256cf045d459113a802199d9177855663c Always make space for nullsBuffer and hashTablesBuffer (Ying Su)\n- 724e1f91e56bbfbeef962d8d9a5d92be552c737b Add tests for max buffer capacity estimation (Ying Su)\n- c5b89795521472323d392daa860240d64d3876f0 Fix estimatedSerializedSizeInBytes for RLE and Dictionary Blocks (Ying Su)\n- 184c000004ab8e92e6e06bdca7bf8f79d37e7432 Fix getLogicalSizeInBytes() for Blocks (Ying Su)\n- 33bca28b0101dbd28f555816e9e7aa43cc486313 Remove childrenEstimatedSerializedSizeInBytes from DecodedBlockNode (Ying Su)\n- ea35ddf06829a73ed3f22bc743a9dbaeb04acba2 Fix serialized size estimation in BlockEncodingBuffers (Ying Su)\n- 3f7dca0ae7ed2535257fc66aedbba0e378a25449 Implement ORDER BY and DISTINCT spilling for aggregation (Saksham Sachdev)\n- 53e23a124a5c74224e52dc9c428cb4b8a799def2 Clean up create geometry calls (James A. Gill)\n- c899dc4a05be2b6ce6991ce4d4e157914d6a0096 Remove unneeded Esri NaN writing (James A. Gill)\n- 4803c01b28d5c4f53427c8ca53c0519d6197f2c7 Refactor case statement to use enums (James Gill)\n- 4cad3f8cb039c7fea47c144f4f3be3e25aa773d9 Parquet: Handle missing struct columns as part of the nested column pruning (Venki Korukanti)\n- 5aef7a4aeae1f11a48b91886d38090920cd09d78 Implement PrestoDatabaseMetaData getClientInfoProperties (Adam J. Shook)\n- 83d37f704a5ccf2d659b9c6239b92d778c373f0a Remove workaround for ConcurrentMap.compute (Rebecca Schlussel)\n- 1bc3803755541582ddf1d290d428b9e960d0602b Remove unused methods (Rebecca Schlussel)\n- e72fe866205fc83623fdaed3771936be74b73b7b Use byte[] instead of slice for EncryptionLibrary (Rebecca Schlussel)\n- da1ccfa6fd85cccc06257afab2781bfee47279ac Fix reading and writing of column encodings for DWRF (Rebecca Schlussel)\n- 168a0e9e346b98933f0ef5d848ffacee0645f159 Fix encryption/decryption of encrypted metadata (Rebecca Schlussel)\n- 583ee11b4e7caa319f0f539447fa18442237bfe5 Decrypt separately from compression (Rebecca Schlussel)\n- 21f4cd2e035f6578abca683d8b410d7e16a4534a Generate DataEncryptionKeys in OrcWriter (Rebecca Schlussel)\n- d72661fb703880cfcd76b713768622f8db37145e Support common sub-expression optimization in CursorProcessorCompiler (frank.hu)\n- d60030f584ddf546da500b16a487c41b9ba6e65c Revert \"Reliable Resource Groups with versioning\" (Swapnil Tailor)\n- f88def2ac7ad5f6c7cdc262d01752775bbb9b5f1 Fix artificially high queued query metrics (Jeremy DeGroot)\n- 9e77bf21699a54c03c640cb712d9d508895561dd Turn off failure detector by default in tests (Rebecca Schlussel)\n- d80fc51bcadcfb4a8003eb12da98d12d0dc8afbc Fix confusion matrix computation for classification function (fornaix)\n- e34525404c016e0cdd88bf2682d4efddf393d7c8 Make TestRaptorIntegrationSmokeTestMySql single threaded (James Sun)\n- 02e0460f261c329614a56df5086762b4046eef44 Add documentation for a deployment example (Adam J. Shook)\n- aabe21429d2a3a12ea4eb33eabafd2f2f361e967 Report field name when decoding Elasticsearch value fails (Zhenxiao Luo)\n- 31e6bc0510c3264dd0be090827e807ed438fa8fb Add support for nested types in Elasticsearch (Zhenxiao Luo)\n- d136b07a34afd1aa3fc56adbc696fcd1d53e65f3 Fix error message typo in Elasticsearch decoders (Zhenxiao Luo)\n- 997ed9f047f687e3313b8ee1d606ce25fcbf727a Handle empty object field in Elasticsearch (Zhenxiao Luo)\n- b7f6c007e4c9f63ec701af76c700d9154fe2e073 Add Elasticsearch array support using definitions in the _meta field (Zhenxiao Luo)\n- d3890515bb39702393341bbe8740727dfc19b854 Add option to ignore Elasticsearch publish_address (Zhenxiao Luo)\n- acd5590df269c1b88b28205b7e70a65e8e0f992d Iterate over Elasticsearch documents in index order (Zhenxiao Luo)\n- 05f5f46b346848bf31740eef0bbcd09d22c29a71 Refresh Elasticsearch index before test queries (Zhenxiao Luo)\n- e2772b05bc17568293195d55761a73de8fc33258 Add support for querying Elasticsearch aliases (Zhenxiao Luo)\n- 1785a9494f3ffb04ea77896f6b538279ea751363 Use filter clause for Elasticsearch queries (Zhenxiao Luo)\n- 38d8f40a6ea2f4845208c6ffc5ea7f3ce86c6607 Fix predicate pushdown for Elasticsearch (Zhenxiao Luo)\n- b825bdd6ed77ca0f1672199af0a7be9ee8e452c8 Handle mixed-case columns in Elasticsearch (Zhenxiao Luo)\n- d5ab3597ca79d41d15caa189a52f07d3ebb50bec Add support for datetimes exposed as numbers in Elasticsearch (Zhenxiao Luo)\n- 54879c59a37fdfbc5d4f8b32e66c31334dc69814 Add support for AWS IAM authorization to Elasticsearch connector (Zhenxiao Luo)\n- 4e42579c0ff5d1ad0dbf6562912e95714a57cda7 Move ElasticsearchClient to client package (Zhenxiao Luo)\n- 2824ab2bcaf8ef883c08208bc14e4663083ee3db Support nodes with no public http address (Zhenxiao Luo)\n- 03c26f42edac35b8e98b5636cbb35399eae3d1bd Use absolute paths for Elasticsearch requests (Zhenxiao Luo)\n- 83aea2f80f5009d21ae8922cfac0811854b0b980 Add system.nodes table to Elasticsearch (Zhenxiao Luo)\n- 5da00caf945da71134cd9d712ff27a6ce553506a Move IndexMetadata, NodesResponse, SearchShardsResponse, Shard, ElasticsearchNode into Elasticsearch client directory (Zhenxiao Luo)\n- dd0e2e6b39af2b7992c3c2d8375b57d0205bf9d5 Refresh Elasticsearch nodes periodically (Zhenxiao Luo)\n- ba21188c3843b48f53d2700f636e573c690c55f1 Make Elasticsearch retry timeout configurable (Zhenxiao Luo)\n- 639897b168fcd26c536b5fe08cbc6e2311ad15db Add support for Elasticsearch query string syntax (Zhenxiao Luo)\n- 7f56aabf4084901b76807d82b8f298601e4fdb04 Adjust Elasticsearch timeout defaults (Zhenxiao Luo)\n- d914af49470462ef0d955454a7d9c37f37f66558 Load tables dynamically in Elasticsearch (Zhenxiao Luo)\n- ca40434225fa8375d84d4f3c48d3e9f778061a18 bulk load elasticsearch test data (Zhenxiao Luo)\n- 0cbc76fc12a1b0bf9ca27a90012475807ba72aa7 Update Elasticsearch connector to use Elasticsearch Http client (Zhenxiao Luo)\n- 24d807aabe95fc0bf6f76152aa0d4c3a503df9fb Move metadata specific methods to ElasticsearchMetadata (Zhenxiao Luo)\n- 017661c9f36ed7cc335e2107537098edd8d54644 Embed index and type in Elasticsearch connector handle (Zhenxiao Luo)\n- 03c8f38047a30e21d4ac165fc16bb9e4c7d2124f Use shard primary host in Elasticsearch connector to save extra hop in Elasticsearch (Zhenxiao Luo)\n- f8bded155de9279979c8803707f83b0f20385612 Encapsulate logic for fetching shards (Zhenxiao Luo)\n- 00d45bd67160e337bc0f982186c306d9ae51096d inline method for Elasticsearch getSearchShards() (Zhenxiao Luo)\n- cc6be46697a69a585bb7f474dfdc86065ee9d591 Remove indexExactMatch from Elasticsearch connector (Zhenxiao Luo)\n- 42fd6814e4904e4d9843021f71b492bc178ed454 Simplify logic for Elasticsearch connector getSearchShards (Zhenxiao Luo)\n- bd4a17f873d333f16b2ac782cd50b905dec9775d Remove unused parameter in Elasticsearch getSearchShards() (Zhenxiao Luo)\n- 3f7665a3f5c05f2856e9c361d6f9cdebf22da826 clear Elasticsearch scroll when connector query close (Zhenxiao Luo)\n- dd5bc6cd37aa32908f9eb8a53aeedb8fa92d2791 Make ElasticsearchQueryBuilder stateless (Zhenxiao Luo)\n- 25b179234ff99d26094259c312d62d9ba591c8e6 Cache Elasticsearch clients (Zhenxiao Luo)\n- c5c5ce5c1a2e9b5ca65d954a93813a3c6128045d Make Elasticsearch config catalog wide (Zhenxiao Luo)\n- f92194acec9b38f447b9d1d26e91788bfcb6d110 Rename ElasticSearchConnectorConfig to ElasticSearchConfig (Zhenxiao Luo)\n- 8e34f48b32c50b52ebee041b5badd32cee8b03f0 Fix ObjectMapper typo in ElasticsearchClient (Zhenxiao Luo)\n- 83931c67f29152ccc53ffc19b4f1943407d3c4b9 Revert \"Fix ObjectMapper typo in ElasticsearchClient\" (Zhenxiao Luo)\n- fa5f78b319ec08550d7def93db56feec4f83d4e3 Revert \"Rename ElasticSearchConnectorConfig to ElasticSearchConfig\" (Zhenxiao Luo)\n- 8367c655150d29376b743bb715708a284fdf2855 Revert \"Make Elasticsearch config catalog wide\" (Zhenxiao Luo)\n- ca6a8bf61c640c792884f21100d1c6fe959b4d23 Revert \"Cache Elasticsearch clients\" (Zhenxiao Luo)\n- 911db66a6568af766d2508051a443269af3bda23 Revert \"Make ElasticsearchQueryBuilder stateless\" (Zhenxiao Luo)\n- 60e7b591cd584dc379e85cb33f0f86062960bb77 Revert \"clear Elasticsearch scroll when connector query close\" (Zhenxiao Luo)\n- c8bb7623092c47c6b431103fcfc22d94e9989605 Revert \"Remove unused parameter in Elasticsearch getSearchShards()\" (Zhenxiao Luo)\n- 091933fc3dd336ad8b656e879cf006922f5d2fbd Revert \"Simplify logic for Elasticsearch connector getSearchShards\" (Zhenxiao Luo)\n- 890782e249b0bbe1e8cb4a40e02ea91fc6652989 Revert \"Remove indexExactMatch from Elasticsearch connector\" (Zhenxiao Luo)\n- 31e4ab9bc1db7c60024f75bbfd3dd2241d1536b3 Revert \"inline method for Elasticsearch getSearchShards()\" (Zhenxiao Luo)\n- 738be0bac220e1b52697dede42fe5967b13d954f Revert \"Encapsulate logic for fetching shards\" (Zhenxiao Luo)\n- 326dc71baef2f7f2df64a79b3bfedef8cc33b222 Revert \"Use shard primary host in Elasticsearch connector to save extra hop in Elasticsearch\" (Zhenxiao Luo)\n- 6eba4212e8c3f977c402f20d008f481360577a47 Revert \"Embed index and type in Elasticsearch connector handle\" (Zhenxiao Luo)\n- 54f1cbb7caaadf2d4322ab835566f73cd4432c03 Revert \"Move metadata specific methods to ElasticsearchMetadata\" (Zhenxiao Luo)\n- 59154cf5dc0a5905ac285c633cd1e1e50607127e Revert \"Update Elasticsearch connector to use Elasticsearch Http client\" (Zhenxiao Luo)\n- 8186abe245d3067cb2776a2cb67941b24573ffb9 Revert \"bulk load elasticsearch test data\" (Zhenxiao Luo)\n- 27771369b3cac035338365b41903571a68c54f76 Revert \"Load tables dynamically in Elasticsearch\" (Zhenxiao Luo)\n- 9ed2baf513972a9bc20f679db5374c25e68f6df5 Revert \"Adjust Elasticsearch timeout defaults\" (Zhenxiao Luo)\n- d51a968beb7c2abd3aadb21741087e1a5ec64346 Revert \"Add support for Elasticsearch query string syntax\" (Zhenxiao Luo)\n- 561a4b4bac0a050f5b81e9e73ae830e07ceda001 Revert \"Make Elasticsearch retry timeout configurable\" (Zhenxiao Luo)\n- 5510d4d85be0a0f75c1407fac6e40f82f42cf5a0 Revert \"Refresh Elasticsearch nodes periodically\" (Zhenxiao Luo)\n- 6ac8dbe253ecb03da2277a46e990a6df7e4ca12a Revert \"Move IndexMetadata, NodesResponse, SearchShardsResponse, Shard, ElasticsearchNode into Elasticsearch client directory\" (Zhenxiao Luo)\n- 7df099e0f6a32c940a42006aae4289817dffc918 Revert \"Add system.nodes table to Elasticsearch\" (Zhenxiao Luo)\n- f7521f272eee37393d16e2f9f329167b8c7a3b25 Revert \"Use absolute paths for Elasticsearch requests\" (Zhenxiao Luo)\n- 323b60aaef55293e7c9c000000564d0a5ea316b2 Revert \"Support nodes with no public http address\" (Zhenxiao Luo)\n- 2a9081fbbe0ad000cc0abcd1cb0d84f56147618f Revert \"Move ElasticsearchClient to client package\" (Zhenxiao Luo)\n- 7e1e8ce29d9d8228d8794a25244a3b4236ce38c0 Revert \"Add support for AWS IAM authorization to Elasticsearch connector\" (Zhenxiao Luo)\n- 5b8c54c75c827e6733c90a41abfebcf61a13ff9f Revert \"Add support for datetimes exposed as numbers in Elasticsearch\" (Zhenxiao Luo)\n- 76d8dc6f3d9f9fe9800d9bef4a1e0dff87fbbbd8 Revert \"Handle mixed-case columns in Elasticsearch\" (Zhenxiao Luo)\n- c32856db8079f3bad25ccc96d162aad18a54013c Revert \"Fix predicate pushdown for Elasticsearch\" (Zhenxiao Luo)\n- b3705cc7811a0ea806a3cf5a06717fc93125ddce Revert \"Use filter clause for Elasticsearch queries\" (Zhenxiao Luo)\n- 6be17652b40541af668b5d0d701a30f8a6f0471d Revert \"Add support for querying Elasticsearch aliases\" (Zhenxiao Luo)\n- 5e9e0974663915c4cb997416894ab684c59b1fde Revert \"Refresh Elasticsearch index before test queries\" (Zhenxiao Luo)\n- df2e257a18606ed765330722a35b77857d4aaf53 Revert \"Iterate over Elasticsearch documents in index order\" (Zhenxiao Luo)\n- 8138809ebeb4efdfb71f51db095445a891fc06be Revert \"Add option to ignore Elasticsearch publish_address\" (Zhenxiao Luo)\n- ac103a5b4c27201f59cfce345bf3941751e6db68 Revert \"Add Elasticsearch array support using definitions in the _meta field\" (Zhenxiao Luo)\n- 4e726f1097213c94bb92d338fa2ddc057ed2c662 Revert \"Handle empty object field in Elasticsearch\" (Zhenxiao Luo)\n- 51fdf3a43a9954b50061fb3739cb64e7437a6e46 Revert \"Fix error message typo in Elasticsearch decoders\" (Zhenxiao Luo)\n- 5c2233c72c31286b1432191d990a82b332837d46 Revert \"Add support for nested types in Elasticsearch\" (Zhenxiao Luo)\n- 6b887546612808441a46c13b11966ca38327eb40 Revert \"Report field name when decoding Elasticsearch value fails\" (Zhenxiao Luo)\n- ca600190df37eb5daaf2f767b948b08c6942f57b Pass encryption info from hive to ORC reader/writer (Rebecca Schlussel)\n- 38358b8ffcccb22a8624eea787aadbe59d4ee2fa Add more testing for encryption/decryption (Rebecca Schlussel)\n- 00d0011dec60b009cfdbbdd7ff95b4dbbca3e999 Add DWRF encryption support to ORC writer (Rebecca Schlussel)\n- d4a18225ba57d5f6eee54175d297ce0905e306ce Add encryption support to ORC ColumnWriters (Rebecca Schlussel)\n- 9ca3b95481c63f8d0c94286aefaab406b343e3ea Add DWRF encryption support to ORC reader (Rebecca Schlussel)\n- 81a6b0da7531598be04d8a42b88d6da5e48bd43a Move creation of includedOrcColumns to RecordReader (Rebecca Schlussel)\n- f51c5c5e73fe45e194a010f42d7df8da249c1cb7 Make columnEncodings into a map (Rebecca Schlussel)\n- 37b5701850d046f5a5c998221593313b8461a67d Add metadata support for encrypted DWRF files (Rebecca Schlussel)\n- c6415c4c828d277bf36070059201b6cd0ecc28c3 Remove unused field (Rebecca Schlussel)\n- f1bfd8c1cef4234579ee45469fa51c0b4dd08960 Fix copywrite for TestByteArrayUtils (Rebecca Schlussel)\n- 7213ae3867dab716ada62e4d5c509878cb0354ac Enable statistics aggregation for temporary table. (Peizhen Guo)\n- 1fbfb409ebd6a050ef9b7e67bd906fd905a66b84 Disable encryption tests temporarily (Mayank Garg)", "NaN"], ["14909", "Implement Window Function Spilling", "Saksham", "sachdevs", "08/04/20, 01:08:19 AM", "Original PR: https://github.com/prestosql/presto/pull/228/\r\nBroken up into ORDER BY spilling (merged in https://github.com/prestodb/presto/pull/14836) and Window function spilling (this one)\r\n\r\nPart of https://github.com/prestodb/presto/issues/14935\r\n\r\n\r\nTODO\r\n- Release notes\r\n- Window function docs\r", "NaN"], ["14910", "Minor refactor to LocalQueryRunner", "Wenlei Xie", "wenleix", "07/30/20, 03:09:58 PM", "Make UnsupportedRemoteSourceFactory as a explicit inner static class\r\n(instead of an anonymous class) to make the LocalExecutionPlanner#plan\r\ncall more concise in LocalQueryRunner.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14911", "Add logging on Presto-on-Spark worker", "Wenlei Xie", "wenleix", "08/05/20, 06:11:19 PM", "* Total split size. This can help debug skewed workers.\r\n* The plan fragment. The RDD stage id is not matched with Presto plan\r\n  fragment id. Thus today we will need to look at the worker log, get\r\n  the Presto stage id, and look at the plan. This eases debugging by\r\n  eliminating one step. In the future, we should investigate better\r\n  integration with Spark UI.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14912", "Handle CAST in canonicalized LambdaDefinitionExpression", "Rongrong Zhong", "rongrong", "07/29/20, 01:28:15 AM", "Include CallExpression return type in canonicalized LambdaDefinitionExpression\r\nso CAST with same input type would not be mistakenly canonicalized.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix LambdaDefinitionExpression canonicalization did not handle CAST\r\n```\r", "NaN"], ["14913", "Put time limit on `TestDistributedSpillQueries#testJoinPredicatePushdown`", null, "mayankgarg1990", "07/29/20, 07:03:44 PM", "We are seeing this test get stuck multiple times and putting this limit\r\nwill help it fail fast and help us investigate. The normal success runtimes\r\nfor this test are less than 5 seconds - so this is way more buffer than\r\nneeded\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14914", "Move TranslateExpressions above all PredicatePushDowns", "James Sun", "highker", "07/29/20, 10:51:50 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14915", "Avoid planning unnecessary LIMIT/TopN/Sort/DistinctLimit", "countryman4687", "fgwang7w", "12/28/20, 10:03:47 PM", "Cherry-pick of https://github.com/prestosql/presto/pull/441 and https://github.com/prestosql/presto/pull/818\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Avoid planning unnecessary LIMIT/TopN/Sort/DistinctLimit when relation is know to single row or less rows than requested\r\n* The analyzer will emit a warning if a redundant ORDER BY is present\r\n```\r\nresolves: #14897 \r", "NaN"], ["14919", "Add \"Test plan\" field to PR template", null, "mayankgarg1990", "07/29/20, 08:03:08 PM", "Based on the discussion in the TSC meeting of presto on 7/7/2020, adding\r\na new field called as Test plan to motivate contributors to think better\r\nabout the testing method for their PRs and giving committers an\r\nopportunity to assess the same and recommend changes/suggestions to the\r\nsame.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14922", "Handle create page source for Druid segment on s3", null, "jinyangli34", "08/01/20, 05:34:10 AM", "Please make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nDruid Changes\r\n* Fix errors when reading Druid segment file on S3\r\n```\r\n\r\n\r\nTo address: https://github.com/prestodb/presto/issues/14921\r", "NaN"], ["14923", "Add geometry_nearest_points function", "James Gill", "jagill", "08/21/20, 02:04:06 PM", "This function is similar to PostGIS ST_ClosestPoints.  Given two\r\ngeometries, find a pair of points on them that have the minimum\r\ndistance.  If either geometry is empty, return null.\r\n\r\nFixes #14887\r\n\r\nTest plan - Unit tests in TestGeoFunctions\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeospatial Changes\r\n* Add :func:`geometry_nearest_points` to find nearest points of a pair of geometries.\r\n```\r", "NaN"], ["14924", "Remove allFragments from PlanPrinter#formatFragment", "Wenlei Xie", "wenleix", "07/30/20, 03:36:25 PM", "When stats and cost is displayed in https://github.com/prestodb/presto/pull/11268, `allFragments` is introduced to `PlanPrinter#formatFragment` since `FragmentedPlanStatsCalculator` and `FragmentedPlanCostCalculator` needs all fragments. It also changes the `TypeProvider` to use all variables in all fragments, looks like this change is by accident, see discussions in https://github.com/prestodb/presto/pull/11268/files#r462651639\r\n\r\nFragment stats and cost are now precomputed and stored in `PlanFragment` through https://github.com/prestodb/presto/pull/11511 . As a result, `allFragments` is not required any more by `PlanPrinter#formatFragment`.\r\n\r\ncc @rschlussel \r\n\r\nTest plan - Travis passed. Also  spot check a plan for EXPLAIN (TYPE DISTRIBUTED) with multiple fragments to see that the stats look correct.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14925", "Dynamic filtering planner", "Ke", "kewang1024", "08/04/20, 06:37:57 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\nTodos:\r\n\r\n- [x] Add more tests", "NaN"], ["14927", "Add release notes for 0.238.2", "Leiqing Cai", "caithagoras", "07/30/20, 09:35:30 AM", "NaN", "NaN"], ["14928", "Fix nondeterminism in TestQueryResource", "Timothy Meehan", "tdcmeehan", "08/03/20, 06:23:00 PM", "Test plan - Local run\r\n\r\nThis test used to rely on the query status reliably reporting the status as queued for the first two requests (even if it's not queued within a resource group manager).  With #14833, this behavior is not guaranteed anymore, and it's possible that within the first two GETs we make sufficient progress to complete the query.  This commit makes the queueing behavior in the test more reliable by making a query which is actually queued in a `ResourceGroupManager`.\r\n\r\nFixes #14940\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14931", "Inline SQL functions at plan time", null, "prithvip", "08/13/20, 12:41:27 AM", "```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Add support for inlining SQL functions at query planning time. This feature is enabled by default, and can be disabled with the ``inline_sql_functions`` session property.\r\n```", "NaN"], ["14936", "Disable TestDistributedSpilledQueries#testJoinPredicatePushdown", "Saksham", "sachdevs", "07/31/20, 03:18:18 PM", "Test plan - (N/A)\r\n\r\nThis test was creating timeouts and blocking release. Disabling since join spilling is going to be reworked anyways.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14937", "Fix typo in \"longitudeToTileY\" function name", "James Gill", "jagill", "07/31/20, 02:04:38 PM", "It should be `latitudeToTileY`.  Also, clean up the documentation.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14938", "Collect shuffle statistics in Presto on Spark", "Andrii Rosa", "arhimondr", "08/05/20, 08:18:30 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14939", "Add Session property to ignore non-readable hive partitions and pass the warning to the user", "Naveen821", "Naveen007", "09/02/20, 04:38:42 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1124\r\n\r\nAdd Session property to ignore non-readable hive partitions and pass that as a warning back to the user\r\n\r\nHive Config\r\nhive.ignore-unreadable-partition\r\n\r\nSession property\r\nignore_unreadable_partition\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14941", "Populate ClientInfo in Verifier JDBC queries", "Leiqing Cai", "caithagoras", "08/03/20, 06:47:02 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to populate client info for the queries issued by Verifier.\r\n* Add configuration property ``test_name``, to be passed in to the client info blob.\r\n```\r", "NaN"], ["14942", "Flatten TaskStats and PipelineStats", "Ajay George", "ajaygeorge", "08/05/20, 07:51:00 PM", "Flatten TaskStats and PipelineStats\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["14943", "Add max Spark input partition count for auto tune", "Wenlei Xie", "wenleix", "08/24/20, 11:30:44 PM", "When scanning over very large tables (hundreds of TBs or even PBs),\r\nusing the default `spark.max-splits-data-size-per-partition` might\r\nresult in too many Spark executors (e.g. >50K) and cause jobs to be\r\nunstable.\r\n\r\nThe long term solution is to set\r\n`spark.max-splits-data-size-per-partition` to be reasonably large, and\r\nleverage historic based optimizer to figure out the optimal split size\r\nper partition. In a short term, having a max Spark input partition count\r\nmakes it makes it easier for job auto migration.\r\n\r\nTest plan - Tested with a large-scale Spark job.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14944", "Shade classes in presto-jdbc uber jar", null, "mayankgarg1990", "08/03/20, 03:57:20 PM", "https://github.com/prestodb/presto/pull/14585 introduced new dependencies that entered\r\npresto-jdbc uber jar. presto-jdbc shades out all dependencies other than the core jdbc\r\nclasses itself and this PR does the same.\r\n\r\nTest plan - I will let all unittests succeed before landing this PR\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14945", "Update airbase to 99", null, "mayankgarg1990", "08/03/20, 07:58:07 PM", "Depends on https://github.com/prestodb/airbase/pull/9\r\n\r\nTest plan - The attached unittests should be enough for this\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14950", "Connect to Cassandra cluster using SSL", "SandishKumarHN", "SandishKumarHN", "08/06/20, 10:35:36 PM", "Connect to Cassandra cluster with tls security\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nCassandra Changes\r\n* Add TLS security support.\r\n```\r", "NaN"], ["14952", "Timestamp literal pushdown for Druid connector", "Beinan", "beinan", "08/07/20, 11:05:35 PM", "Push down the timestamp to druid\r\n`select * from druid.wikipedia where __time > timestamp '2016-06-26 18:00:00.000'`\r\nor the timestamp with timezone:\r\n`select * from druid.wikipedia where __time > timestamp '2016-06-26 19:00:00.000 UTC';`\r\nor cast:\r\n`select * from druid.wikipedia where __time > CAST('2016-06-26 18:00:00.000' as TIMESTAMP);`\r\n\r\nIn this PR, I also added the druid connector to presto server's config (for devel only) and improved the error message handling.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14953", "Add spilled bytes to QueryStatistics", "Saksham", "sachdevs", "08/04/20, 08:59:23 PM", "This PR enables tracking bytes spilled in QueryStatistics.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14955", "Push dereferences into table scan for parquet tables", "Venki Korukanti", "vkorukanti", "08/15/20, 02:01:05 AM", "Remaining patches to resolve  #14517. It works on top of #14829 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* This change adds planner side support for pushing dereferences into Parquet table scan. Pushing deferences into table scan enables efficient scans as only the required nested column is read when required independent of the other projected nested columns in the same base column. Currently this functionality is behind a configuration variable `hive.enable-parquet-dereference-pushdown`\r\n```\r", "NaN"], ["14956", "Allow extra stats to be emitted even when query stats is missing", "Leiqing Cai", "caithagoras", "08/05/20, 06:49:24 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1109\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14958", "Support skipping control in Verifier", "Leiqing Cai", "caithagoras", "08/05/20, 09:03:41 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to skip running control queries and comparing results. This can be enabled by configuration property ``skip-control``.\r\n```\r", "NaN"], ["14959", "Use the same RowExpressionCompiler when compiling expressions", "Rongrong Zhong", "rongrong", "08/05/20, 07:26:12 PM", "Earlier commit changed CursorProcessorCompiler to use the same\r\nRowExpressionCompiler to compile all expressions. We should just\r\nuse the same RowExpressionCompiler for all expressions within the\r\nsame class.\r\n\r\nTest plan - See added test\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix compiler error in certain situations where sql functions with same lambda are used multiple times\r\n```\r", "NaN"], ["14961", "Dynamic filtering implementation", "Ke", "kewang1024", "08/05/20, 08:42:25 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r", "NaN"], ["14963", "Dynamic filtering planner fix", "Ke", "kewang1024", "08/17/20, 04:18:14 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14967", "Fix druid connector bugs related to pushed down queries", null, "weidongduan37", "08/07/20, 11:04:45 PM", "NaN", "NaN"], ["14968", "Added conversion functions between Geometry and GeoJSON format. ", "Rong Rong", "walterddr", "08/10/20, 11:25:23 PM", "Test plan - Unit tests added in `TestGeoFunctions` for forward and backward conversion between JSON and Geometry format.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Added :func:`geometry_from_geojson` and :func:`geometry_as_geojson` to convert geometries from and to GeoJSON format.\r\n```\r\n\r", "NaN"], ["14969", "Dynamic filtering connector implementation", "Ke", "kewang1024", "08/07/20, 08:59:51 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r", "NaN"], ["14970", "Resubmit verification if test query fails with HIVE_PARTITION_OFFLINE", "Leiqing Cai", "caithagoras", "08/06/20, 07:41:30 PM", "If control query runs but test query fails with HIVE_PARTITION_OFFLINE,\r\nwe should resubmit the query for verification. Most likely, the control\r\nquery will then fail and the verification will be skipped.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to resubmit verification if test query fails with ``HIVE_PARTITION_OFFLINE``.\r\n```", "NaN"], ["14971", "Convert revocable memory to user memory on hash aggregation finish", "Saksham", "sachdevs", "08/06/20, 06:32:12 AM", "Spillable hash aggregation keeps allocated memory in revocable memory pool instead of converting it during finish. This is a bug. I have also moved some of our tracking logic  for when operator is finishing to SpillableHashAggregationBuilder.\r\n\r\nAdapted from second commit in https://github.com/prestosql/presto/pull/164.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14972", "Allow Raptor to run custom check before table creation", "Shixuan Fan", "shixuan-fan", "08/06/20, 04:37:37 PM", "Test plan - This is a no-op for default set up in open source repo. For Facebook internal extension, added unit tests and will do deployment test\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14974", "Rename files written by PageSink", "Nikhil Collooru", "NikhilCollooru", "10/01/20, 05:40:59 PM", "This PR enables the workers to send / receive metadata requests /responses from coordinator. \r\nIf this feature is enabled for Hive, then all the files written by the HivePageSink for a given partition will have increasing whole numbers as file names. (For ex: Lets say this we enabled this feature and ds=2020-09-30 is written by HivePageSink. Then we can expect the file names of that partition to be 0,1,2,3..N etc ).\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1125\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Add support for file renaming for Hive connector. This can be enabled with ``hive.file_renaming_enabled`` configuration property\r\n```\r", "NaN"], ["14975", "Fix memorySize calculation for quantiles", null, "bhhari", "08/10/20, 02:39:20 PM", "Use long type to account for memory as long to int underflow causes query failures.\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1113\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14976", "Fix generation of dynamic filter", "Ke", "kewang1024", "08/07/20, 06:45:12 PM", "1. Only create DynamicFilters when turned on to avoid unnecessary compute\r\n2. Add dynamicFilterEnabled check\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14980", "Support more than 1 druid source", null, "weidongduan37", "08/13/20, 05:05:29 PM", "Allow union and union all operation", "NaN"], ["14981", "Minor page optimizations", "James Petty", "pettyjamesm", "08/10/20, 03:55:08 PM", "Adds two new utility methods to `Page`:\r\n- `copyPositions(int[] retainedPositions, int offset, int length)` which mirrors `getRegion`\r\n- `extractChannels(int[] channels)` which creates a new page with arbitrary channel selection / reordering\r\n\r\nThe follow up commit then updates various operators and classes that explicitly created and manipulated Block arrays to use equivalent Page methods that avoid extra defensive copies.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14983", "Add warning message for UNION queries without ALL/DISTINCT keyword", null, "prithvip", "08/14/20, 06:02:03 PM", "Co-authored-by: Sundeep Katepalli\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Added new warning message for UNION queries without ALL/DISTINCT keyword.\r\n```\r", "NaN"], ["14985", "Thrift udf api", "Rongrong Zhong", "rongrong", "08/06/20, 10:33:53 PM", "Test plan - travis\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nThrift Connector Changes\r\n* Rename ``presto-thrift-connector-api`` to ``presto-thrift-api`` and have separate packages for datatypes, valuesets and connector.\r\n```\r", "NaN"], ["14986", "Export slim query info to file upon PoS query completion", "Wenlei Xie", "wenleix", "08/07/20, 11:05:21 PM", "Presto-on-Spark supports exporting JSON serialized QueryInfo since\r\n670ecb5dd26da8ecbe0233c6c60502e5c93cf2d5. In practice, we found full\r\nQueryInfo can frequently be too verbose (e.g. >16MB). Thus we export a\r\nslim version instead.\r\n\r\nTest plan - test with a query, and see the following \r\n```\r\n{\"queryId\":\"20200807_031720_00000_g5vxr\",\"state\":\"FINISHED\",\"nodes\":1,\"totalSplits\":16441,\"cpuTimeMillis\":7451871,\"wallTimeMillis\":39090156,\"elapsedTimeMillis\":120011,\"processedRows\":39079360449,\"processedBytes\":6094416586,\"peakMemoryBytes\":0,\"peakTotalMemoryBytes\":0,\"peakTaskTotalMemoryBytes\":0}\r\n```\r\n\r\nRelated PR is https://github.com/prestodb/presto/pull/14825\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14987", "Fix error message in SessionPropertyDefaults", "Leiqing Cai", "caithagoras", "08/07/20, 05:37:02 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14988", "Allow procedures to have optional arguments with default values", "Vivek", "ClarenceThreepwood", "08/11/20, 04:50:37 PM", "Cherry pick https://github.com/prestosql/presto/commit/7a7404d78c8477a509378fbaddca6e1f1499689d\r\n\r\nThis change allows procedures to be defined with optional arguments or default values for arguments\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Allow procedures to have optional arguments with default values\r\n```\r\n\r\n\r", "NaN"], ["14989", "Batch presto spark rows", "Andrii Rosa", "arhimondr", "08/14/20, 08:17:25 PM", "Producing tiny rows to the shuffle has significant performance implications, as the shuffle algorithm has non zero cost of appending a row. By grouping multiple rows for a single partition together into a single row batch increases shuffle efficiency and allows to achieve higher shuffle throughput.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14990", "Support legacy date/timestamp to varchar coercion", "Shixuan Fan", "shixuan-fan", "08/11/20, 04:49:15 AM", "For certain connector migration, we are facing incompatible semantic\r\nwhere column type changed. This is to provide a legacy mode so\r\nthat we could disaggregate connector migration from actual query\r\nmigration.\r\n\r\nWarnings will be issued for queries relying on this legacy behavior.\r\n\r\nTest plan - This functionality has been tested by shadowing prod traffic for several months before June. Will do another shadow test on the latest master.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14991", "Remove unnecessary annotation in ThriftUdfService", "Rongrong Zhong", "rongrong", "08/10/20, 06:17:04 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14995", "Add support for druid data ingestion", "Beinan", "beinan", "08/14/20, 06:19:11 PM", "* Add support for 'insert into select'\r\n* Add support for CTAS\r\n* Add support for native batch (parallel) ingestion\r\n   * Ingest data from a local holder\r\n\r\nIngest by CTAS\r\n`create table new_dataset7 as select localtimestamp as __time, 'beinan' as name, bigint '18' as age, double '188.88' as height;`\r\n\r\n```\r\npresto:druid> desc new_dataset7;\r\n Column |   Type    | Extra | Comment\r\n--------+-----------+-------+---------\r\n __time | timestamp |       |\r\n age    | bigint    |       |\r\n height | double    |       |\r\n name   | varchar   |       |\r\n```\r\n\r\nIngest by Insert:\r\n` insert into new_dataset7 select localtimestamp as __time, bigint '18' as age, double '288.88' as height, \"Mike\" as name;`\r\nor even more\r\n`insert into new_dataset7 SELECT cast(date_column as timestamp),bigint '18' as age, double '188.88' as height, 'aaa' as name FROM (VALUES (SEQUENCE(FROM_ISO8601_DATE('2010-01-01'), FROM_ISO8601_DATE('2020-12-31'), INTERVAL '1' DAY) ) ) AS t1(date_array) CROSS JOIN UNNEST(date_array) AS t2(date_column);`\r\n\r\n\r\nLimitation:\r\nCurrently only supports limited data types such as timestamp, varchar, bigint, double and float.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nDruid Changes\r\n* Add support for data ingestion\r\n```\r\n\r\n\r", "NaN"], ["14996", "Add support for ip data type in ES Connector", "Reetika", "agrawalreetika", "08/11/20, 06:08:39 AM", "Add support for IP data type in ES Connector. #14806 \r", "NaN"], ["14997", "Dynamic filtering for semi join", "Ke", "kewang1024", "08/12/20, 06:19:37 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["14998", "#13359 : Upgrade Kudu to latest release 1.12.0", "SandishKumarHN", "SandishKumarHN", "08/10/20, 07:45:41 PM", "```\r\n== NO RELEASE NOTE ==\r\n\r\nUpgrade Kudu to latest release 1.12.0\r\n```\r", "NaN"], ["15000", "Add plan check for runtime optimization", "Peizhen Guo", "pguofb", "08/20/20, 03:48:52 PM", "This PR implements the following:\r\n1) Refactor the add/remove local exchange logic in RuntimeReorderJoinSides rule to use the same checking logic as the planning time optimization rule AddLocalExchange. Namely, the StreamPropertyDerivations class.\r\n2) Extend the plan checker to be also invoked during runtime for each rewritten fragment.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15001", "Fix boilerplate codes for CacheKey and JoinFilterCacheKey", "Shixuan Fan", "shixuan-fan", "08/11/20, 10:40:07 PM", "Test plan - (Please fill in how you tested your changes)\r\n\r\nTravic CI\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15003", "Add IF EXISTS and IF NOT EXISTS checks to ALTER TABLE", "Vivek", "ClarenceThreepwood", "08/12/20, 05:37:58 PM", "\r\nChery pick of https://github.com/prestosql/presto/commit/cbc1c79af21587d3fe09e4b97ed83c1504319f47\r\n\r\nCo-Authored-By:  y1275963 <1275963@gmail.com>\r\n\r\nFixes #15002 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add `IF EXISTS `and `IF NOT EXISTS` syntax to `ALTER TABLE`\r\n\r\n```\r", "NaN"], ["15004", "Add error code for Presto-on-Spark", "Wenlei Xie", "wenleix", "08/19/20, 07:37:50 PM", "Test plan - \r\n![testinprod](https://user-images.githubusercontent.com/799346/89843770-8552dc00-db2e-11ea-897a-e03139a2216a.jpeg)\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15005", "Minor changes on Sql Functions", "Rongrong Zhong", "rongrong", "08/11/20, 10:53:31 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1121\r\n\r\nTest plan - Travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15010", "Move Presto Spark execution exceptions to presto-spark-classloader-interface", "Andrii Rosa", "arhimondr", "08/12/20, 01:07:33 AM", "As they have to be serializable\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15012", "Deprecating NullableValue class", null, "ssaumitra", "08/12/20, 07:44:22 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15013", "Add Hive procedure to sync table partitions", "Vivek", "ClarenceThreepwood", "08/31/20, 12:41:10 AM", "Cherry pick of\r\n  https://github.com/prestosql/presto/commit/78cde41c3c0d05e0314920dca824a029fd392da1\r\n  https://github.com/prestosql/presto/commit/bf7944cbc180182bee4f7c91f1b8c7479e80a85a\r\n  https://github.com/prestosql/presto/commit/86886ef0cf5783dedb723bbf316a094f9e66d628\r\n  https://github.com/prestosql/presto/commit/fa66094120d584ff60cd3303204d6f8cb3bc9ade\r\n\r\nCo-authored by: \r\n           Hao Luo <hluo@twitter.com>\r\n           Raunaq Morarka <raunaqmorarka@gmail.com>\r\n           Alex Albu <alex.albu@starburstdata.com>\r\n\r\nFixes #14946 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add procedure system.sync_partition_metadata() to synchronize the partitions in the metastore with the partitions that are physically on the file system.\r\n```\r\n\r", "NaN"], ["15014", "Add Oracle connector support", "countryman4687", "fgwang7w", "08/19/20, 05:03:54 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nJDBC Changes\r\n* Add Oracle JDBC Plugin Support for Presto that allows users to use Oracle Jdbc Connection\r\n```\r\nTest plan - \r\nBaseOracleIntegrationSmokeTest: test Describe, Show Create table\r\nTestOracleConfig: test default config and explicit property setting\r\nTestOracleDistributedQueries: test various senarios of DDL and querying workload\r\nTestOracleIntegrationSmokeTest: test Describe table function\r\nTestOraclePlugin: test oracle plugin\r\nTestOracleTypes: data type checks for Oracle connection\r\n\r\nresolve: #13959 ", "NaN"], ["15017", "Dynamic filter bucket pruning", "Ke", "kewang1024", "08/14/20, 05:49:32 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15018", "Update java version parser for presto running on java 10, 11 or later", "Beinan", "beinan", "08/13/20, 06:29:43 PM", "Update java version parser to fix the java version check failure on Java 11.\r\n\r\nJava version specs for your quick reference:\r\nhttps://openjdk.java.net/jeps/322\r\nhttps://openjdk.java.net/jeps/223\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15020", "Fix OrcSelectivePageSource incorrect memory reporting", null, "sujay-jain", "08/12/20, 07:48:11 PM", "We are noticing that ```OrcSelectivePageSource``` is reporting 0 memory usage. ```All OrcSelectivePageSource``` objects we saw in some heapdumps were reporting 0 usedBytes. This change is to fix that.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15022", "Move QueryNotAdequatelyPushedDownErrorCode to PinotErrorCode", "Xiang Fu", "xiangfu0", "08/24/20, 05:31:13 AM", "Per slack discussion, Pinot connector accidentally took the error code space `0x0625`. \r\n\r\nThis PR moves the corresponding error to PinotErrorCode space `0x0505`.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15024", "Implement PrestoS3FileSystem#listFiles for direct recursive listings", "James Petty", "pettyjamesm", "08/21/20, 11:49:18 PM", "Implements `FileSystem#listFiles(Path, boolean recursive)` for `PrestoS3FileSystem` which in theory adds support for directly listing S3 files underneath a prefix without recursive calls through each \"directory\". This direct traversal requires much fewer requests when dealing with nested directories but may violate some `PathFilter` implementation's expectation of being called at each directory level, and may perform worse when a large number objects are contained within hidden paths (since filtering would be performed after the fact). I'm open to suggestions about how to balance this trade-off and integrate this with the DirectoryLister.\r\n\r\nIncidentally, a straightforward improvement to getFileStatus fell out of the implementation allowing the isDir check for a path prefix (with no object present) to be done by limiting the listing result size to 1 instead of listing the full default 1000.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Add support for direct recursive file listings in PrestoS3FileSystem\r\n```\r", "NaN"], ["15027", "Implement JDBC ResultSet.getStatement", "Adam J. Shook", "adamjshook", "08/17/20, 06:16:53 PM", "Fixed #14804\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nJDBC Changes\r\n* Implemented ResultSet getStatement\r\n```", "NaN"], ["15028", "Revert \"Upgrade ZSTD version\"", "Rohit Jain", "jainxrohit", "08/14/20, 10:15:13 PM", "The zstd version, 1.4.5-4 is casuing full GCs in the release 238.\r\nHence reverting it to the previous stable version.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Downgrade ZSTD JNI compressor version to resolve the frequent excessive GC events introduced in version 0.238.\r\n```", "NaN"], ["15030", "Improve shuffle statistics collection", "Andrii Rosa", "arhimondr", "08/14/20, 10:00:08 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15031", "Fix TABLESAMPLE SYSTEM for Presto on Spark", "Andrii Rosa", "arhimondr", "08/19/20, 01:31:28 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15032", "Add release notes for 0.240", "Leiqing Cai", "caithagoras", "08/25/20, 09:35:45 PM", "# Missing Release Notes\n## Beinan Wang\n- [x] https://github.com/prestodb/presto/pull/14890 Upgrade presto-hadoop-apache2 for GCS token refreshing (Merged by: Zhenxiao Luo)\n\n## George Wang\n- [x] https://github.com/prestodb/presto/pull/14895 Add support for limit pushdown through union (Merged by: Rongrong Zhong)\n\n## Jinyang Li\n- [x] https://github.com/prestodb/presto/pull/14922 Handle create page source for Druid segment on s3 (Merged by: Zhenxiao Luo)\n\n## Saksham Sachdev\n- [x] https://github.com/prestodb/presto/pull/14909 Implement Window Function Spilling (Merged by: James Sun)\n\n## Weidong Duan\n- [x] https://github.com/prestodb/presto/pull/14980 Support more than 1 druid source (Merged by: Zhenxiao Luo)\n- [x] https://github.com/prestodb/presto/pull/14967 Fix druid connector bugs related to pushed down queries (Merged by: Zhenxiao Luo)\n\n## agrawalreetika\n- [x] https://github.com/prestodb/presto/pull/14996 Add support for ip data type in ES Connector (Merged by: Zhenxiao Luo)\n\n# Extracted Release Notes\n- #14585 (Author: Beinan Wang): Use local private credentials (json key file) to refresh GCS access token\n  - Use local private credentials (json key file) to refresh GCS access token presto-cli --extra-credential hive.gcs.credentials.path=\"${PRIVATE_KEY_JSON_PATH}\".\n- #14739 (Author: George Wang): Add a limit on total number of bytes read from storage in table scan\n  - Add `query.max-scan-physical-bytes` configuration and `query_max_scan_physical_bytes` session properties to limit total number of bytes read from storage during table scan. The default limit is 1PB.\n- #14795 (Author: Brandon Scheller): Add support for Hudi MOR queries\n  - Allows presto-hive to use custom parquet input formats.\n  - Add support for Hudi realtime input format for hudi realtime queries.\n- #14823 (Author: Leiqing Cai): Support customizable ways of launching Presto queries in Verifier\n  - Fix an issue where Verifier fails to start when failure resolver is disabled.\n  - Add support to implement customized way of launching Presto queries.\n  - Add support to run helper queries on a separate cluster other than the control cluster.\n- #14829 (Author: Zhenxiao Luo): Pushdown dereference\n  - Push down dereference expression.\n- #14866 (Author: James Petty): Make skip.header.line.count=1 files splittable\n  - Adds support for splitting hive files when skip.header.line.count=1.\n- #14877 (Author: James Petty): Avoid checking isSplittable for files smaller than the split max size\n  - Improves split generation by avoiding an unncessary splittable check when files are smaller than the initial split max size, regardless of their input format.\n- #14896 (Author: James Gill): Add bing_tile_children and bing_tile_parent functions\n  - Introduce ``bing_tile_children`` and ``bing_tile_parent`` functions to get parents and children of a Bing tile.\n- #14899 (Author: James Gill): Add geometry_to_dissolved_bing_tiles and improve geometry_to_bing_tiles\n  - Add geometry_to_dissolved_bing_tiles function, which dissolves complete sets of child tiles to their parent.\n  - Improve geometry_to_bing_tiles.  It is 50x faster on complex polygons, the limit on polygon complexity is removed, and some correctness bugs have been fixed.\n- #14901 (Author: Zhenxiao Luo): Kafka insert\n  - Support insert in Kafka connector.\n- #14903 (Author: Zhenxiao Luo): Fix Parquet long statistics handling when min/max not set\n  - Fix parquet statistics when min/max is not set.\n- #14912 (Author: Rongrong Zhong): Handle CAST in canonicalized LambdaDefinitionExpression\n  - Fix LambdaDefinitionExpression canonicalization did not handle CAST.\n- #14931 (Author: prithvip): Inline SQL functions at plan time\n  - Add support for inlining SQL functions at query planning time. This feature is enabled by default, and can be disabled with the ``inline_sql_functions`` session property.\n- #14941 (Author: Leiqing Cai): Populate ClientInfo in Verifier JDBC queries\n  - Add support to populate client info for the queries issued by Verifier.\n  - Add configuration property ``test_name``, to be passed in to the client info blob.\n- #14950 (Author: SandishKumarHN): Connect to Cassandra cluster using SSL\n  - Add TLS security support.\n- #14958 (Author: Leiqing Cai): Support skipping control in Verifier\n  - Add support to skip running control queries and comparing results. This can be enabled by configuration property ``skip-control``.\n- #14959 (Author: Rongrong Zhong): Use the same RowExpressionCompiler when compiling expressions\n  - Fix compiler error in certain situations where sql functions with same lambda are used multiple times.\n- #14968 (Author: Rong Rong): Added conversion functions between Geometry and GeoJSON format. \n  - Added :func:`geometry_from_geojson` and :func:`geometry_as_geojson` to convert geometries from and to GeoJSON format.\n- #14970 (Author: Leiqing Cai): Resubmit verification if test query fails with HIVE_PARTITION_OFFLINE\n  - Add support to resubmit verification if test query fails with ``HIVE_PARTITION_OFFLINE``.\n- #14985 (Author: Rongrong Zhong): Thrift udf api\n  - Rename ``presto-thrift-connector-api`` to ``presto-thrift-api`` and have separate packages for datatypes, valuesets and connector.\n- #14988 (Author: Vivek): Allow procedures to have optional arguments with default values\n  - Allow procedures to have optional arguments with default values.\n- #15003 (Author: Vivek): Add IF EXISTS and IF NOT EXISTS checks to ALTER TABLE\n  - Add `IF EXISTS `and `IF NOT EXISTS` syntax to `ALTER TABLE`.\n\n# All Commits\n- 95e5ee485fc6099f5699c1641f09a9e5f501b8f2 Update java version parser for java 10, 11 or later (Beinan Wang)\n- 7a0c1405f75c232c9521b82490a7dcb9898586d8 Support more than 1 druid source, allow union and union all operation (Weidong Duan)\n- a9d5c4c405a1bd2affd2b4f9718090959fa8869c Inline SQL functions at plan time (prithvip)\n- dfee819bb5b1a9debffa47a9cb9f6b1fcc98546d Add casts to expressions in SQL functions if coercible (prithvip)\n- 6f1db903bd4fc2df2bb4fd817c8dc8852a0ebdec Fix improperly scoped argument binding in lambda expressions (prithvip)\n- 4c869d6bb80b7c86beadf69eb9891504bfa4bfeb Fix OrcSelectivePageSource incorrect memory reporting (Sujay Jain)\n- 06b6f96d2c6dc94ea78fd3f41d5ecef287054443 Deprecating NullableValue class (Saumitra Shahapure)\n- 32dfebde7018103a5ae44d2eb5a2f2bc618ede69 Add IF EXISTS and IF NOT EXISTS checks to ALTER TABLE (Vivek)\n- ec5b590d15c244fdf565805dc51c1ebf7e61935e Implement dynamic filtering collection for semi join (Ke Wang)\n- 849b6a42f1eaa40295f4588ccd3e91bb52765dc2 Add semi join dynamic filter placeholder (Ke Wang)\n- 603b77ce7c88bcdaf6a5a652daa5dccb6f9f0220 Move Presto Spark execution exceptions to presto-spark-classloader-interface (Andrii Rosa)\n- c838111a7a8430ba5de8b4ed97055784e61f38a0 Change SQL function version to String (Rongrong Zhong)\n- 569d71e950d89c0f8840d723727bcc807a95e4e5 Add language to FunctionMetadata (Rongrong Zhong)\n- 22c79d824ec0748ffeb19c00fb68e5f98352d04a Fix equals, hashCode and toString for JoinFilterCacheKey (Shixuan Fan)\n- a70b4156d4df51f32b349d2c220602168bedbbfd Fix hashCode for CacheKey in ExpressionCompiler (Shixuan Fan)\n- 1b66b42ef8edba89c7a0ae8407b42ff58dd6e417 Allow procedures to have optional arguments with default values (Vivek)\n- eed2581ebc6ba32f85f716b2e96ab562cba9f7b2 Add support for ip data type in ES Connector (agrawalreetika)\n- 53467ec1162faf148059b945860c809a8730316c Support legacy date/timestamp to varchar coercion (Shixuan Fan)\n- d9c1e8489e5cc890560f0a5fd57c94684116945f Add functions geometry_from_geojson and geometry_as_geojson (Rong Rong)\n- 4c143d9ddd2aa9104098e3b97ca22e0058b7dca8 Upgrade Kudu to latest release 1.12.0 (SandishKumarHN)\n- 922afacdd7cd2a9ef7f34e21ae948e15e08c773b Remove unnecessary annotation in ThriftUdfService (Rongrong Zhong)\n- 4479e85c7b22e6cfb109c7459e94f65cdf2dedb7 Ensure exception thrown by scheduled tasks are logged (Andrii Rosa)\n- 269c3acc1427830fb64b53ffa495fbe4e40ec0c8 Prefer builtin Page methods that avoid Block[] copies (James Petty)\n- 91eb4b8e12b0c06a366366f9899e2db432bc14f8 Add copyPositions and extractChannels utility methods to Page (James Petty)\n- 0f8a20e9925f9422abb58fea22140aa7988c939f Fix memorySize calculation for quantiles (Bhavani Hari)\n- b99054213edc4009304be4a2fd40cbc96d04433e Add test for timestamp literal pushdown (beinan)\n- be949c0035a7b7d0ab6ee2835adca6d24c5f2b52 Remove the unnecessary logic on time literal pushdown (beinan)\n- 67c9fad3742e0a3ddfa66c5c0bf43eb231795f59 Timestamp literal push down for Druid connector (beinan)\n- 7074294ac086f201006d9df56a827ba2fdc968fd Handle the error message from Druid broker (beinan)\n- 42a472999fde5ac8e0dafd1f86421f81ae7ae318 Add druid connector to devel configs (beinan)\n- b15ee864b8255a1ede251f399496d0c38e59589d Export slim query info to file upon PoS query completion (Wenlei Xie)\n- 521df87997f05fa4393826e858135f041dd27c52 Support FilterNode on top of AggregationNode (Weidong Duan)\n- d560b6e644e49cab94fa9076ec62406cd62afa68 Fix unhandled HTTP response error for druid client (Weidong Duan)\n- 53c89ab0b52bed2d2bb3eae61a4ec1eada204285 Run distributed Hive join tests with and without dynamic filtering (Ke Wang)\n- 8792a928c0c2ba36c792efebba98ae1a29f26412 Push dynamic filter to HivePageSourceProvider (Ke Wang)\n- 740e18c29f53a587c8398317e567cb84f2bf3f42 Fix generation of dynamic filter (Ke Wang)\n- eb4e582942793631a8de8cf6e6e1c06ce3053d75 Fix error message in SessionPropertyDefaults (Leiqing Cai)\n- 1d52c576eca07e9cb77fea89a5e8330409033f82 prestodb connect to cassandra cluster with tls security (SandishKumarHN)\n- dd95430e90d91ffe588e6999a9b23f51719c1081 Introduce Thrift UDF service APIs (Rongrong Zhong)\n- b8a4ab3fe6fb3d4ae390e66b8b54c1ab87e02422 Refactor presto-thrift-connector-api (Rongrong Zhong)\n- e1b40669f099ea7bdf9e01ce4a0e4b275f92db9e Resubmit verification if test query fails with HIVE_PARTITION_OFFLINE (Leiqing Cai)\n- e12f5f4b264ea29c0638a2db80c6e8b1f263b44b Allow Raptor to run custom check before table creation (Shixuan Fan)\n- a2183fa4541352f35f749de50ce1a22f4dada98a Convert memory to user pool on HashAggregation spill finish (Saksham Sachdev)\n- cfb2e7aa077954a02c048e81c97a47994d329852 Add hive custom split support for Hudi (Brandon Scheller)\n- c406344b34646fceda8f0ab537b1fea8ce6ed24b Support skipping control in Verifier (Leiqing Cai)\n- 35ce159dd2df82a037d2840ddf324b4858341187 Collect shuffle statistics in Presto on Spark (Andrii Rosa)\n- ea81b9f02eb88e2c2903a7eb4bbee3d4476c4901 Propagate fragment id to PrestoSparkShufflePageInput (Andrii Rosa)\n- 1a909d55ea83c9066189163b75bd5f1b0f56e3d9 Refactor PrestoSparkMutableRowPageInput (Andrii Rosa)\n- 46499f5c1f761620d9b7b2b9fd7759dabc35bc82 Update QueryDetails UI to use new TaskStats response structure (Ajay George)\n- 88e0bd0fef55ac30b2f512fe6b1056aee142ce62 Flatten TaskStats and PipelineStats (Ajay George)\n- c960ab417a0e47a2b338d1639ad56e24eea99407 Use the same RowExpressionCompiler when compiling expressions (Rongrong Zhong)\n- e9f49df3d33baffa5236ce3874d9fe4dd1ebfaa1 Add support for limit pushdown through union (George Wang)\n- e0718bbc4f42231688ede393f2b8ff2df3cdb3b0 Add a constructor to PrestoQueryException (Leiqing Cai)\n- d45ad4709a21d9600bd5ae87c57c0739ddc43a5e Make PrestoQueryException.queryActionStats non-optional (Leiqing Cai)\n- 4959b793a4966da780784f0dc8add5b071ac58d8 Allow extra stats to be emitted even when query stats are missing (Leiqing Cai)\n- 1c406b0c6ce2365f6b315c036f5b3e2e863bdc2c Allow TableScanNode#getCurrentConstraint returns null on worker (Wenlei Xie)\n- 2c247c51967e4c44011c940bcb76b590680b6cd6 Add logging on Presto-on-Spark worker (Wenlei Xie)\n- fecb4a9c690ef29952b2f25d63e83cc2dc68acb0 Use totalSize statistics for simple join plan (Peizhen Guo)\n- fb604581abbf29a8253e80b71fe523a8de021d05 Add unit tests for LocalDynamicFilter and LocalDynamicFilterCollector (Ke Wang)\n- c9db6e2d6d6feb54ecd3ef934030443d995e7bea Short circuit page source when dynamic filter is none and Clean up LocalDynamicFiltersCollector (Ke Wang)\n- 43324080a1717d0f430b7d47381afe19a41182da Implement local dynamic filtering for broadcast inner-joins (Ke Wang)\n- 882fb2389d1e155462684a6a4190176ebbf8a2bb Allow creating unlimited TypedSet (Ke Wang)\n- 56503cfc8808b3c416006bd358e11c56ae01360b Add spilled bytes to QueryStatistics (Saksham Sachdev)\n- 58237417f836633d0785c43c5cad9d462f7500d5 Extract dynamic filters in Hive ORC reader (Ke Wang)\n- 66c22abe81919da6e6bcc14dca4b3baf93acc021 Extract dynamic filters in LocalExecutionPlanner (Ke Wang)\n- dab5a607c93df0288a2d3d7d8b395a1ffbe2bc70 Remove unsupported dynamic filters (Ke Wang)\n- 05553f80e3e2289d84183b71d9882096ed879529 Add query plan test for DynamicFilter (Ke Wang)\n- 4d4f29f540585bb3e918aabfb09ae24c80168766 Generate DynamicFilter in optimization and cost calculation process (Ke Wang)\n- c2ad7846291e99c248f94c149680d5ef7138a036 Introduce dynamicFilters into JoinNode and PlanPrinter (Ke Wang)\n- 1f5da2e17dc99e490576f80ad62559aa11861be4 Introduce DynamicFilter placeholder and its builtin function (Ke Wang)\n- af8d095b277dd7b7784fa212bc51abb90a26b5d5 Add session properties for dynamic filtering (Ke Wang)\n- 28b60627a52edc25aee976c4d0da225fd32b2942 Close spiller in Operator#close() for window spilling (Saksham Sachdev)\n- 9925cd319772d4e8f3aecce803a0b735c0f41979 Extract window queries tests to separate class (Saksham Sachdev)\n- 0eeaf84ad44a19f5c73813d32eeceb06c958cbfe Convert revocable memory to user memory on fully buffered group (Saksham Sachdev)\n- 19097b000b595e1eb9e29328dc449f94d211999b Use OrderingCompiler in WindowOperator spilling (Saksham Sachdev)\n- 085e1e6281c8306e1525a4f825f5bfa4d8b1388c Implement spill to disk For WindowOperator (Saksham Sachdev)\n- d8b4fa095ca0785e19eb023730da6cc737190d9e Introduce revocable aggregated memory context in OperatorContext (Saksham Sachdev)\n- c4fc2d9183513b1436afbdbf06375ce8b50b885f Separate input PageSource work processor (Saksham Sachdev)\n- 5351fa4ac7e575ef506c0eecbf53064e8a19afd3 Port window operator to work processors (Saksham Sachdev)\n- 74cc2dd88b88bfad5c1e3608641d8c70f7b12aa0 Update airbase to 99 (Mayank Garg)\n- 8e272aab8a8b3bde26305282789a200b3f632953 Add a user specified test name that is passed into client info (Leiqing Cai)\n- 0cade1d9fc77ebd63d2f9cb7d2abd532c662eb90 Populate ClientInfo in Verifier JDBC queries (Leiqing Cai)\n- 9a3b041b7d1fa99001c46366267647257869ea8f Fix nondeterminism in TestQueryResource (Tim Meehan)\n- 5ebb3e2a51c4f1f230fae8d201005aff4d867685 Shade classes in presto-jdbc uber jar (Mayank Garg)\n- 24ac335a4da1c2de3f02d9d2dd61fc949bce45ba Add blank line for format (Beinan Wang)\n- e50f5dbcbc61edd9c297887ef77d1665c0c267fc Add Google OAuth scopes to extra credentials (Beinan Wang)\n- 756f9f0281e1b7baf9368fc9e988994a2763aa41 Fix transitive dependencies conflict of google oauth libraries (Beinan Wang)\n- d49e7686ad43d069d91c71d740295f8c27e1c53c Make presto jdbc driver support Google OAuth private key handling (Beinan Wang)\n- d0dfd13752c2b3f2d0393bce2752dc6eecaab0be Make QueryRunner support GCS private key handling (Beinan Wang)\n- 29c5f3ee61c5a4111e5975f3fcd92fee5b23f2dd Implement GCS OAuth token handler (Beinan Wang)\n- 48be2a96e05b33584d06bb7c21a8756b7ba1fd5c Complete removal of joda-to-java-time-bridge. (Beinan Wang)\n- fb43861d7d2354704031c8703f1bd3dcfbb730f3 Add the timezone Asia/Qostanay into zone-index (Beinan Wang)\n- 0e3cd141eea2da944deb1e407b2dba21cb4db767 Fix incompatible types error on jdk 9 and 11 (Beinan Wang)\n- fc699525ef6f5cef9a8e3c0f62845056be7c0616 Add dependency of javax.annotation into presto-catch's pom to fix build failure on java 11 (Beinan Wang)\n- bd0c3e217f464985cb36641ffae769a5565e532b Move dereference pushdown below TranslateExpressions (Zhenxiao Luo)\n- 6a2e94e2023fefdae97858c65fbef1ae6aa14359 Add session property to enable dereference pushdown (Zhenxiao Luo)\n- d45962c68b0e7f788b544fc3fefc4ca4d135f36f Push down dereference expression (Zhenxiao Luo)\n- a4f14e433587ad325a4f2ec18f1ef300cab36809 Handle create page source for Druid segment on s3 (Jinyang Li)\n- 4a37013ba4c1095c23e50e76291b80f89e50bdb1 Simplify integration with Alluxio local cache (Bin Fan)\n- fb8bb9fd0b94519cacb8e7fbb6fb7858d704baa4 Add a limit on total number of bytes read from storage in table scan (George Wang)\n- 404581f4d323eca820f5a997f77d22e36df58b03 Disable TestDistributedSpilledQueries#testJoinPredicatePushdown (Saksham Sachdev)\n- 186ea6dfac549ec70090297b3eb34804a2f7e7c5 Fix flaky testTransactionMetadataCleanup (Vic Zhang)\n- a3feff4b1c1dc0f8191fef2bf14425e9e17a9163 Fix typo in \"longitudeToTileY\" function name (James Gill)\n- d43913fd0778cb4de5966654afb2de9051735440 Make skip.header.line.count=1 files splittable (James Petty)\n- 52a7529cbd372cb8c654163d48f3e7f9e79e91f3 Introduce additional methods in SqlExceptionClassifier interface (Leiqing Cai)\n- 1704705f5fa7d4853d65e89a683bf8acdc417df4 Move Query#toStatementStats to a utility class (Leiqing Cai)\n- 9f97c9c61b1a856bfbd71e0376cfa976b7542b2b Fix binding failure when failure resolver is disabled (Leiqing Cai)\n- b027e1eb8f72820082984911af0cff93bdd74a39 Move the logic of mangling session properties into a utility method (Leiqing Cai)\n- 6a2ed8b3e2403ec33f5096a9c88b70632437af5e Use a single metadata-timeout and checksum-timeout (Leiqing Cai)\n- 6be7b056f27470884b2649df9f30fe81c05706f1 Allow additional data to be emitted in QueryStatsEvent (Leiqing Cai)\n- 8bd1a527ca0ddf59b4848179d70ebd7c32f6fa4e Support customizable ways of launching Presto queries in Verifier (Leiqing Cai)\n- 7a4dc5a4ef983c45c6392c3150ebf53987af0891 Make the entire JDBC QueryStats available in verification results (Leiqing Cai)\n- e85e4e14bcfb074088a7004be384d8ccf2105abe Make control.http-port optional (Leiqing Cai)\n- 426b48b43214aac074e396371abe4afb6488e25c Style improvments in DeterminismAnalyzer (Leiqing Cai)\n- 8473d79a2b4088696601000f44cb99cfe66b222c Refactor geometry_to_bing_tiles to use new algorithm (James Gill)\n- 5e5bb96a5c049d1e43a2f0e73d85c6efa1e73e0d Reformatting some geometry_to_bing_tiles tests (James Gill)\n- 22c1167da87b9051b5d5e0fb77fe513424a90f91 Add test cases for geometry_to_bing_tiles (James Gill)\n- b96c2a27e63f3e1c6704d534cfee03b09230dac6 Add geometry_to_dissolved_bing_tiles (James Gill)\n- 52e6eb9667e9d82eb4f258c551a911ef97b71b55 Extract accelerateGeometry into GeometryUtils (James A. Gill)\n- c53732e58dc722512ef8333a566681e88fd0d2fd [Test Only] Remove allFragments from PlanPrinter#formatFragment (Wenlei Xie)\n- 4110ec688b528abd8298989b1245a5368a6b8816 Avoid checking isSplittable for files smaller than initial split size (James Petty)\n- e4aa4e36fe618d87c7b32672064d8946f8f5f928 Minor refactor to LocalQueryRunner (Wenlei Xie)\n- 53a26bb340d016789b0597dc247b46953a125c9a Add BingTile.findChildren() and findParent() (James Gill)\n- 040a48ace574b3a5730df5be815732ecf46286c7 Rename RowExpressionPredicatePushDown to PredicatePushDown (James Sun)\n- 0cfbeff302c0b3cb321b3644eba1b1fb5d57c828 Remove unused functions in ExpressionUtils (James Sun)\n- 990a119bdd007a226087bc27be45371f13fbfc11 Remove SubExpressionExtractor (James Sun)\n- 04beda34e1931abd180482de7a2ad3f7bf30dafe Rename RowExpressionEqualityInference to EqualityInference (James Sun)\n- ae6d0ff88916a171f433d7897a455a2393934b78 Remove EqualityInference (James Sun)\n- 502bc0601813a679faf400e23092aa86f49a3b7c Rename RowExpressionPredicateExtractor to EffectivePredicateExtractor (James Sun)\n- 40abdcef862986258e2eb4c00a361519101e7a89 Remove EffectivePredicateExtractor (James Sun)\n- 9ae92b5f248fee15ea1b90a796a0a0959095697b Move TranslateExpressions above all PredicatePushDowns (James Sun)\n- 6528824c2360774a5040206c9ff0ad7d222924d6 Add \"Test plan\" field to PR template (Mayank Garg)\n- d34897d37a57694c9ee2533c96d18f4c8889d359 Put time limit on `TestDistributedSpillQueries#testJoinPredicatePushdown` (Mayank Garg)\n- 0438c0e259159428f140499af495bae5c8126435 Upgrade hadoop-apache for refreshing GCS OAuth token automatically (Beinan Wang)\n- 55a685d4b31a67ca415ba01cb5fadcc2aa94129e Fix Kafka product tests avro schema (Zhenxiao Luo)\n- b52bab6802e2bb7e1f938ada1f146efd43ab8386 Add Kafka raw encoder (Zhenxiao Luo)\n- ad1bca38d06d15cbcd3d9f8aadc75d1184c30150 Improve Kafka round trip test (Zhenxiao Luo)\n- 27865e4543b343ed4930e865606f362c9cfa7c16 Fix resource warning in KafkaPageSink#appendPage (Zhenxiao Luo)\n- 3c4d47bd6b34ae65aefb16ac8b28bc41171a6200 Add row to Kafka round trip test (Zhenxiao Luo)\n- 45637696b23284408b1d31e9731d499e362e7d1e Add Kafka Avro encoder (Zhenxiao Luo)\n- 6ff59216b3bbcc7a7001d2fadc3350154fb3ecdd Make Kafka RowEncoder Closeable (Zhenxiao Luo)\n- 5a00a63d7c79efcf0d87f6f9e491842f6aa99486 Move column handle validation for Kafka encoders into constructor (Zhenxiao Luo)\n- 9c74b623c307a20f4891f0ddad2a8a3a09459d45 Remove Kafka connector suppress warnings labels (Zhenxiao Luo)\n- a1635df838f981c6595e31d9b5f6808d91562b49 Add Kafka CSV encoder (Zhenxiao Luo)\n- 8eee85915f9a65746ad9497515c3f589b7a24539 Implements Inserts for Kafka connector (Zhenxiao Luo)\n- 611d210fa73479a784c9dcba2505399cf69a6baa Remove unused functions in ExpressionUtils (James Sun)\n- 56bfdebed18e5f86bd8f6826d0f10aec2482d55a Move TranslateExpressions above all PickTableLayouts (James Sun)\n- ddd2766d69418a70532e0b5b3a8b112f0fd6d36c Move TranslateExpressions above PushAggregationThroughOuterJoin (James Sun)\n- 5f30def53593ec4d7621b1a15c490710d0da7765 Move TranslateExpressions below PushAggregationThroughOuterJoin (James Sun)\n- fe32cbd0f31647bcdc04ce2a2880974618420fc4 Move TranslateExpressions above IndexJoinOptimizer (James Sun)\n- 3f1023d4273ae163c06dc9d7f8bb5c6b7add42d9 Move TranslateExpressions above SimplifyCountOverConstant (James Sun)\n- a07948a2353db039d87d323bb14e5e5a07d122e0 Move TranslateExpressions above LimitPushDown (James Sun)\n- d46226cd4dfe723ec72651fcfb0fca5bfd97e8e0 Move TranslateExpressions above WindowFilterPushDown (Yi He)\n- 19634dcde35ed768b7c54134242ccfa6b7ba2dfb Move TranslateExpressions above GatherAndMergeWindows (James Sun)\n- b39ecadfabe11f56a4de7dcc65304d06c93211e7 Move TranslateExpressions below GatherAndMergeWindows (James Sun)\n- fa288b807416141287681d220c3e968aead544fc Handle CAST in canonicalized LambdaDefinitionExpression (Rongrong Zhong)\n- 3eb8442ad46eb47761faf6a4086591a34c80f604 Fix Parquet long statistics handling when min/max not set (Zhenxiao Luo)", "NaN"], ["15033", "Add support for temporary (session-scoped) functions", null, "prithvip", "01/28/21, 12:18:37 AM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1179\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for temporary (session-scoped) functions\r\n```", "NaN"], ["15034", "Make request tracker infrastructure generic", "Timothy Meehan", "tdcmeehan", "09/02/20, 07:12:34 PM", "Test plan - Local runs and unit tests\r\n\r\nThere's not much that's specific to tasks in this code, and it could be useful to reuse this code for other sorts of request tracking in general.\r\n\r\nRelated to #15071 \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15036", "Catch and throw storage connection error properly", "Nikhil Collooru", "NikhilCollooru", "08/15/20, 06:21:01 AM", "Currently, storage connection error is being returned as HIVE_BAD_DATA error. So fixed it to return it as HIVE_FILESYSTEM_ERROR instead.\r\n\r\nThis PR will fix https://github.com/prestodb/presto/issues/15021\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15037", "Fix null analysis for cast and try_cast", "James Sun", "highker", "08/15/20, 08:15:29 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15039", "In PruneUnreferencedOutputs pass the parent context to child when WindowNode is skipped", "Venki Korukanti", "vkorukanti", "08/18/20, 06:44:11 AM", "\r\nPart of the one user query got into the following plan:\r\n```\r\n  Project(user_uuid_506)\r\n    Filter (rank_554 = 1)\r\n      Window(rank_554=rank(), orderingScheme=expr_538,random_553)\r\n        Project(user_uuid_506, expr_538, random_553=random())\r\n          Values(user_uuid_506, expr_538(cast(name as date)))\r\n```\r\nAs the `Values()` operator doesn't have any data (original table got pruned), the `Filter` is removed as part of the `PredicatePushdown` rule\r\n```\r\n  Project(user_uuid_506)\r\n    Window(rank_554=rank(), orderingScheme=expr_538,random_553)\r\n      Project(user_uuid_506, expr_538, random_553=random())\r\n        Values(user_uuid_506, expr_538(cast(name as date)))\r\n```\r\n\r\nIn the remaining plan when trying to prune the unreferenced outputs (`PruneUnreferencedOutputs` rule), we remove the `WindowNode` as no one is looking at the `rank_554`, but the unreferenced variables `expr_538` and `random_553` are not removed as we pass the context that has variables from `Project(user_uuid_506)` and `Window(rank_554=rank(), orderingScheme=expr_538,random_553)`. This causes the issues later on in `PushdownSubfields` rule which expects the plan to have unreferenced output variables pruned.\r\n\r\nOne another way to resolve this is add `PruneUnreferencedVariables` rule just before the `PushdownSubfields` rule\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15040", "Separate operator and stage statistics from query statistics", null, "mayankgarg1990", "08/20/20, 04:16:05 PM", "Add new classes `StageStatistics` and `OperatorStatistics` which helps add\r\nstage and operator level stats when the query completes. In addition to this,\r\nremoved all operator and stage related stats from QueryCompletedEvent\r\n\r\nThis change is necessary because of 2 reasons:\r\n\r\n1. By moving from JSON to structured objects, it helps us to better understand\r\n   what is being logged and the users of this data, can log this structured data\r\n   more efficiently\r\n2. It should make adding new stats a lot easier since we have the 3 layers of\r\n   abstraction clearly there now - query, stage, operator\r\n\r\nThis PR does duplicate a lot of fields in `StageExecutionStats` as `StageStatistics` and `OperatorStats` in `OperatorStatistics`, but I feel that the separate classes are a better way forward since all the fields are not really suitable for end of query stats (like `runningDrivers`). In addition to that, while adding a new entry to the `*Statistics` class will make people ensure that this is indeed information that the developer wants to export in stats.\r\n\r\nTest plan - Unittests in Travis and ran local test runs where I ensured that the appropriate fields are being logged correctly.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n* Add ``StageStatistics`` and ``OperatorStatistics`` to ``QueryCompletedEvent`` and remove stage and operator statistics from ``QueryStatistics``.\r\n```", "NaN"], ["15042", "Add documentation for Presto authorization", null, "mayankgarg1990", "08/19/20, 08:31:16 PM", "Co-Authored-By: Zhi Wen <zacw@fb.com>\r\n\r\nTest plan - Locally executed `sphinx-build -b html -n -d target/doctrees  -j auto src/main/sphinx target/html` and ensured that the output is as expected\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nSecurity Changes\r\n* Implement REST endpoint authorization in Presto. See :doc:`/security/authorization`\r\n```", "NaN"], ["15044", "Add support for versioning metastore API", "Nikhil Collooru", "NikhilCollooru", "09/18/20, 05:24:50 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for partition versioning. This can be enabled with ``hive.partition-versioning-enabled`` configuration property\r\n* Add support for caching HiveMetastore calls at a more granular level . Supported levels:  'PARTITION' and 'ALL'(default). This can be set with ``hive.metastore-cache-scope`` configuration property.\r\n```\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1127", "NaN"], ["15045", "Enable dereference pushdown in more testcases", "Zhenxiao Luo", "zhenxiao", "08/18/20, 12:46:40 AM", "Test plan - \r\n```\r\nTestHivePushdownDistributedQueries\r\nTestHivePushdownFilterQueries\r\nTestHivePushdownIntegrationSmokeTest\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15048", "Add release notes for 0.238.3 and 0.239.1", "Leiqing Cai", "caithagoras", "08/18/20, 09:02:49 PM", "NaN", "NaN"], ["15049", "Adds option to read map entries with null keys from orc file", "Dmitry Borovsky", "borovsky-d", "08/26/20, 08:18:57 PM", "Add option to presto orc reader to allow map entries with null key (OrcReaderOptions#mapNullKeysEnabled)\r\n\r\nThis change doesn't aim to change presto-sql (original behavior is maintained) but makes presto-orc more attractive as general purpose orc file reader.\r\n\r\nTest plan - I added unit test TestOrcMapNullKey.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15050", "Remove OriginalExpression from multiple classes", "James Sun", "highker", "08/19/20, 06:54:35 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15053", "Fix for Pinot queries where order by column is pruned in projection", "Dharak Kharod", "dharakk", "08/20/20, 05:42:57 PM", "Queries where order by columns are not part of the selection were failing with `NullPointerException` because column names for order by clause were being taken from final selections. \r\n\r\nTo fix this now we are storing the selections for topN at the time we encounter topN node, which guarantees that the order by columns will be in the contemporary selections.\r\n\r\nTest plan - Tested with failing query shapes, example as below\r\n```\r\nselect city\r\nfrom cityFares\r\nwhere secondsSinceEpoch > 1597047119\r\norder by secondsSinceEpoch desc, 1 desc\r\nlimit 1\r\n```\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15056", "Improve error handling during determinism analysis", "Leiqing Cai", "caithagoras", "08/25/20, 05:48:06 PM", "We rerun control queries first and then the limit query analysis,\r\n\r\nIn one case, we see a determinism analysis failed because the control\r\nrerun failed due to a transient generic internal error. The query\r\ncontains a LIMIT clause, but limit query analysis was never run as\r\nthe exception is caught and the analysis is marked as\r\nANALYSIS_FAILED_QUERY_FAILURE.\r\n\r\nTo mitigate this issue:\r\n- Run limit query analysis before control reruns, since it is a\r\n  light-weight analysis.\r\n- If analysis query is failing during limit query analysis, do\r\n  not return and proceed to the control reruns.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Fix an issue during determinism analysis where queries with LIMIT clause are not identified\r\n  as non-deterministic when a rerun of the control query fails.\r\n```", "NaN"], ["15057", "Fix multi-join dynamic filtering", "Ke", "kewang1024", "08/21/20, 05:11:22 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15058", "Improve PrestoSparkRunner capabilities", "Andrii Rosa", "arhimondr", "08/25/20, 08:09:39 PM", "- Support writing query results into a file\r\n- Improve PrestoSparkQueryInfo, make it more consistent with QueryResults from conventional Presto\r\n- Expose more session parameters in the interface\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15059", "Remove QueryInfo from DispatchManager", "Timothy Meehan", "tdcmeehan", "09/02/20, 05:36:32 AM", "QueryInfo is a large class which is expensive to serialize,\r\nand is not necessary prior to query execution.\r\n\r\nTest plan - Locally run TpchQueryRunner and inject a condition to gurantee queueing; observing correct behavior in UI and endpoint.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15063", "Refactor WarningCollector to spi module ", "Naveen821", "Naveen007", "08/22/20, 02:23:23 AM", "Refactor WarningCollector to spi module enabling connectors to pass warnings back to the engine\r\n\r\nDepended by facebookexternal/presto-facebook#1132\r\n\r\n== NO RELEASE NOTE ==", "NaN"], ["15065", "Support non-hive types in hive views", "Rebecca Schlussel", "rschlussel", "09/01/20, 09:38:47 PM", "Test plan - unit test\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\n\r\nHive Changes\r\n* Add support for non-hive types to hive views.  This support had been removed in 0.233.  If a view uses an unsupported type for any columns ,we will store only a single dummy column for that view in the metastore.\r\n```\r", "NaN"], ["15066", "Add dynamic filter canonicalization in UnaliasSymbolReferences", "Ke", "kewang1024", "08/23/20, 04:48:09 AM", "== RELEASE NOTES ==\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15067", "Recognize Presto-on-Spark error code in verifier", "Wenlei Xie", "wenleix", "08/24/20, 06:20:47 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15070", "Clean up TrackedQuery", "Timothy Meehan", "tdcmeehan", "08/24/20, 11:38:01 PM", "getRunningTaskCount is only applicable if the query is a QueryExecution\r\n\r\nTest plan - unit tests and local query submissions to TpchQueryRunner\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15074", "Add support in parquet reader for reading TIMESTAMP_MICROS type.", null, "s-sanjay", "09/18/20, 02:58:24 AM", "Summary:\r\nRight now presto always assumes TIMESTAMP_MILLIS as the OriginalType when int64 is used to\r\nrepresent timestamp type in the schema. This causes an issue.\r\n\r\nWhen we use createParquetPageSource we use our own type convertor and this does not have a check\r\nfor TIMESTAMP_MICROS and this can fail the query\r\n\r\nFix: In this change I am adding three ValueDecoders one for plain and one for RLE compression format and\r\none for the non batch reader. All of these are in 3 new classes so that during creation we\r\ninstantiate the class based on the OriginalType. One Alternate approach is to do this check inside\r\nLongValueDecoders but that would make this check inside the most critical path and can affect query\r\nperformance. The fix simply divides the micros seconds by 1000 to get the milliseconds and this should\r\nbe ok because in presto we anyway operate at the millisecond granularity. Note that the non batch reader\r\nalso checks for whether timezone is present in the data while the batch readers don't because that\r\ninformation is not available.\r\n\r\nFor testing, the ValueDecoders have their own unit test. Apart from that have also added a new test in\r\nParquetTester to check if timestamp stored as int64 with OriginalType TIMESTAMP_MICROS works.\r\n\r\nNote: We use hive parquet writer and schema definition as defined in the presto-hive package\r\nwhich is a shaded jar with a old version of hive. This packs an old version of parquet-mr that does not\r\nhave the TIMESTAMP_MICROS type. However, in the read path, we use the independent parquet dependency\r\nwhich is more recent and so we use the version where the new enum is available. If we upgrade all the tests\r\nto use the new version of parquet, we would not be testing parquet version written\r\nby the packaged hive writer. We would need a version of the test that would work with the new definitions.\r\nFor now I have done this only for the timestamp type to test TIMESTAMP_MICROS. As part of seperate change\r\nwe can make sure all the test are tested with latest parquet writer. This change uses ExampleParquetWriter\r\nwhich is supposed to be used only for demo or test purposes. We cannot use presto's parquet writer as well because\r\nthe one used in test always stored timestamp as millis and does not store any annotated type and using\r\nthat we will not be able to test our code.\r\n\r\nTest plan - tested in local docker container, also added unit test to test this out.\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* parquet files written by parquet-avro library that uses TIMESTAMP_MICROS as the OriginalType to represent timestamp can now be queried by presto\r\n```\r", "NaN"], ["15075", "Fix precision definition in classification_precision function", "Vic Zhang", "viczhang861", "08/25/20, 12:37:53 PM", "See explnation in https://github.com/prestodb/presto/pull/14740\r\n\r\nDefinition of precision = TP / (TP + FP)\r\n\r\n```\r\n== RELEASE NOTE ==\r\n```\r\n\r\nGeneral Changes\r\n* Fix incorrect results from function classification_precision() introduced in release 0.239", "NaN"], ["15076", "Track cache objects sizes in CachingOrcDataSource", "Ying", "yingsu00", "09/01/20, 09:28:27 PM", "The cache object in CachingOrcDataSource was not tracked before. This PR updates the PageSource's system memory usage whenever a new cache\r\nobject is allocated.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\n\r\nHive Changes\r\n* Fixed a memory tracking issue in OrcPageSource\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15077", "Dynamic filtering integration with Aria", "Ke", "kewang1024", "09/04/20, 05:40:23 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add dynamic filtering and bucket pruning support for inner join and semi join. The feature avoids full table scan on probe side for broadcast join or colocated join when the build side is small. Set config `experimental.enable-dynamic-filtering` to `True` to enable the feature. Configs `experimental.dynamic-filtering-max-per-driver-row-count` and `experimental.dynamic-filtering-max-per-driver-size` are available to tune the size on the build side join key space. Currently, only Hive connector can benefit from the feature.\r\n```\r", "NaN"], ["15078", "Add memory tracking for OrcRecordReader", "Ying", "yingsu00", "09/02/20, 03:56:38 PM", "\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Added memory tracking for OrcRecordReader\r\n```\r", "NaN"], ["15083", "Add release notes for 0.239.2", "Vic Zhang", "viczhang861", "08/26/20, 08:04:01 PM", "NaN", "NaN"], ["15084", "Adds support for microsecond timestamp precision", "Dmitry Borovsky", "borovsky-d", "08/28/20, 03:32:32 AM", "Adds support for microseconds precision for presto-orc reader. There are no plans to enable it for presto sql, but other projects may employ the setting for higher precision.\r\n\r\nTest plan - added unit test TestApacheHiveTimestampDecoder\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15085", "Improve logging of queued queries", "Timothy Meehan", "tdcmeehan", "08/27/20, 12:40:56 AM", "Test plan - Local reproduction of race condition, unit tests\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15086", "Fix page splitter creating large dictionary page list", "James Sun", "highker", "08/26/20, 07:37:45 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15088", "Fix PrestoSparkQueryStatusInfo deserialization", "Andrii Rosa", "arhimondr", "08/26/20, 09:25:11 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15090", "Fixup rebase conflicts on 0.240", "Timothy Meehan", "tdcmeehan", "08/27/20, 02:15:25 PM", "Test plan - local tests", "NaN"], ["15091", "Categorize the AccessControlException as user error", "Venki Korukanti", "vkorukanti", "08/27/20, 11:11:23 PM", "Currently it shows up as external\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15092", "Fix TestOrcMapNullKey", "Maria Basmanova", "mbasmanova", "08/27/20, 07:27:13 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15093", "Adding multivalued column support in Pinot connector", "Dharak Kharod", "dharakk", "09/12/20, 01:17:56 AM", "Test plan - Tested by selecting a multivalued columns\r\n\r\nMultivalued columns in pinot are best described a set of same primitive type elements. Picking Array here as the closest presto type. With this diff only selecting the column is possible and not UDFs such as length, which will be added in subsequent PRs\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15094", "Add config property for min spark partition count", "Vic Zhang", "viczhang861", "08/31/20, 10:52:04 PM", "Implementation choice:\r\n     When number of splits are less than min_partition_count, every partition is required to have at least one split. Therefore,  actual partition count <= number of splits\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15095", "Implement pass through mode UNION ALL on Presto-on-Spark", "Vic Zhang", "viczhang861", "09/23/20, 08:33:01 PM", "Extract pass through mode commit from https://github.com/prestodb/presto/pull/14999\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15096", "Add more error message for Presto-on-Spark", "Wenlei Xie", "wenleix", "08/31/20, 08:17:33 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15099", "Fix memory counting for SliceDictionaryBatchStreamReader", "Ying", "yingsu00", "09/01/20, 01:52:15 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Fixed a memory counting bug in SliceDictionaryBatchStreamReader \r\n```\r", "NaN"], ["15100", "Do not write buffered data when task is aborted", "Vic Zhang", "viczhang861", "09/02/20, 01:54:51 PM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15101", "Support explain verification", "Leiqing Cai", "caithagoras", "09/10/20, 09:17:19 PM", "Add a new functionality to the verifier to allow explain verification.\r\n\r\nFor each query pairs to be verified, explain both control and test, and\r\ncompare the query plans. Verification is marked as succeeded as long as\r\nthe test query can be explained.\r\n\r\nIntroduced a new field matchType in the output event, which can be used\r\nto indicate whether there is plan difference.\r\n\r\nFor non-DML queries, we don't care much about plan difference and hence\r\nthe control query and the plan comparison are skipped. \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to run explain verification. (:pr:`15101`). This can be enabled by configuration property ``explain``.\r\n```\r", "NaN"], ["15104", "Fix incorrect intersection between two envelopes", "James Gill", "jagill", "09/08/20, 08:37:14 PM", "When two envelopes intersected at a single point, the intersection would\r\nbe incorrectly calculated as `POINT(x, x)` instead of `POINT(x, y)`.\r\nThis also simplifies the logic when two envelopes intersect in a line.\r\n\r\nTest plan - Added unit test for failing case.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeospatial Changes\r\n* Fix bug when two envelopes intersect at a point for :func:`ST_Intersection`.\r\n```\r", "NaN"], ["15108", "Add property to limit output size of a query", null, "mayankgarg1990", "09/11/20, 07:54:53 PM", "Test plan -\r\n\r\n1) Added unit tests\r\n2) Manually tested by using the TpchQueryRunner:\r\n\r\n```\r\n$ cat /tmp/query.sql \r\nSET SESSION query_max_output_bytes='1000B';\r\nSELECT * FROM orders;\r\n\r\n$ ./presto-cli/target/presto-cli-0.241-SNAPSHOT-executable.jar --catalog tpch --schema sf30000 -f /tmp/query.sql > /tmp/out\r\nSET SESSION\r\nQuery 20200902_000014_00001_9ki9x failed: Query has exceeded output size Limit of 1000B\r\n```\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add session property ``query_max_output_bytes`` and configuration property ``query.max-output-bytes`` to control how much data a query can output.\r\n```", "NaN"], ["15111", "Add zstd compression for PAGEFILE", "Vic Zhang", "viczhang861", "09/03/20, 06:55:10 PM", "1. Test covered by `TestHiveIntegrationSmokeTest::testPageFileCompression`\r\n2. Make default compression algorithm for temporary file as zstd, all tests pass. \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15112", "Add debug logging when running Presto queries in Verifier", "Leiqing Cai", "caithagoras", "09/03/20, 01:09:24 PM", "This allows us to debug long running queries on Verifier.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15113", "Support multiple control and test clusters in Verifier", "Leiqing Cai", "caithagoras", "09/04/20, 12:23:42 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to allow multiple control and test clusters.\r\n```\r", "NaN"], ["15114", "Add waitForMinimumCoordinators to ClusterSizeMonitor", "Timothy Meehan", "tdcmeehan", "09/16/20, 04:16:54 AM", "Test plan - Unit test coverage included in this PR\r\n\r\nExtracted from #15071\r\n\r\nRelated to #10174\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15116", "Make equals() and hashCode() consistent in TableHandle", "Shixuan Fan", "shixuan-fan", "09/03/20, 08:28:39 PM", "Test plan - Travis CI\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15117", "Check grouped execution for dynamic filtering", "Ke", "kewang1024", "09/03/20, 07:15:03 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15122", "Add HiveMetadataUpdater to handle metadata updates", "Nikhil Collooru", "NikhilCollooru", "09/05/20, 02:47:30 AM", "```\r\n== NO RELEASE NOTE ==\r\n\r\n```\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1156\r", "NaN"], ["15125", "Remove getFileStatus on openFile with Alluxio Cache", "Bin Fan", "apc999", "09/05/20, 02:45:30 AM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1158\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15128", "Fixing bug: IS DISTINCT FROM NULL should work for NULL column values", null, "ssaumitra", "09/17/20, 06:16:30 PM", "Fixes #15107.\r\n\r\nTest plan - Tested using the new test in `AbstractTestQueries`. Also ran the tests from `TestExpressionInterpreter.testIsDistinctFrom`\r\n```\r\n== RELEASE NOTES ==\r\n* Fix incorrect behavior of `expression IS DISTINCT FROM NULL` when expression is not constant.\r\n```\r", "NaN"], ["15129", "ParquetWriters cleanup", "Zhenxiao Luo", "zhenxiao", "09/08/20, 12:37:30 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15130", "GC issue fixes in SliceDictionarySelectiveReader", "Ying", "yingsu00", "09/15/20, 01:39:18 PM", "Previously we always allocate a dictionary for every rowgroup when opening a new rowgroup during read. When these dictionaries are humongous, the allocations could cause reliability and performance issues. This PR does the following:\r\n\r\n-  Defer the allocation of the dictionary to getBlock() so that lazyBlocks don't have to allocate memory if they don't need to be loaded.\r\n-  Do not create Slice objects when evaluating the filters.\r\n- Materializes the dictionaries if they are too large. Instead of outputting \r\n    a DictionaryBlock, it will output a plain VariableWidthBlock if the dictionaries \r\n    size is above certain threshold. \r\n\r\nThe experiment on user reported query shows over 10x reduction in allocations and over 2x CPU reduction in scan. The query that was reported problematic was\r\n\r\n```\r\nSELECT COUNT(*) FROM t WHERE c IS NOT NULL;\r\n```\r\n\r\nInput data was 58.4M rows, 4.29GB. \r\n\r\n\r\n|                            |   Total CPU  |   Wall Time | TableScan CPU |\r\n|----------------|-------------|------------|-----------------|\r\n|Aria off                |            429s  |            64s  |          7.14m       |\r\n|Aria on No fix      |            407s  |            68s  |          6.77m |\r\n|Aria on With Fix  |        155s  |            21s   |          2.57m |\r\n\r\n\r\n\r\nWithout fix, byte[] (out) was 291,270,027,152 bytes, with fix 13,340,988,744 bytes.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15131", "Restore commented out tests", "Ying", "yingsu00", "09/09/20, 11:22:00 AM", "Restore the tests commented out in PR https://github.com/prestodb/presto/pull/14808, commit `443d105 Fix encryption with dictionary encodings`\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15132", "Support fetching queries to be verified using a Presto query", "Leiqing Cai", "caithagoras", "09/10/20, 06:16:18 PM", "Currently, we support fetch source queries (pairs for queries to be\r\nverified) from an MySQL table by a given suite. This is insufficient\r\nin some cases, as it requires MySQL and is also bound by the limitation\r\nof MySQL.\r\n\r\nAdd support to fetch source queries by running a Presto query. The\r\nfetching query will be run on the helper cluster.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to fetch the list of queries to be verified by running a Presto query.\r\n```", "NaN"], ["15135", "Pinot Connector: Adding limit to push down order by for broker query", "Xiang Fu", "xiangfu0", "09/26/20, 09:33:07 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n* Adding config: `pinot.pushdown-topn-broker-queries` to support pushing down TOPN queries.\r\n\r\nPresto doesn't retain the order of query response from Pinot for large number of records returned. So only enable this when doing small group by and order by results\r\n```\r", "NaN"], ["15136", "Warn on logical operators AND and OR expressions without explicit parenthesization ", "Ankit Kothari", "ankit0811", "09/16/20, 08:11:17 PM", "Test plan - Tested locally\r\nAdded test cases to check is any warnings were thrown in `TestWarnings.java`\r\nFixes:\r\nIssue: https://github.com/prestodb/presto/issues/15106\r\nIssue: https://github.com/prestodb/presto/issues/15105\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15141", "Add test infrastructure to run queries using native workers", "Maria Basmanova", "mbasmanova", "09/10/20, 06:18:58 PM", "Extend HiveQueryRunner and DistributedQueryRunner to allow for setting up a Java-based coordinator and a set of native workers. Native workers are created by launching separate processes using native binary. The path to the binary is specified using PRESTO_SERVER environment variable. The new test that uses this infrastructure, TestHiveNativeWorkersQueries.java, is disabled by default. In the future, we'll figure out how to get native worker binary on a CI machine to enable this test.\r\n\r\nThe new test passed locally.\r\n\r\n<img width=\"553\" alt=\"Screen Shot 2020-09-08 at 7 57 32 PM\" src=\"https://user-images.githubusercontent.com/27965151/92538813-aa11a080-f20d-11ea-8edd-20916217fced.png\">\r\n\r\nCC: @andrewmc-facebook\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15143", "Add release notes for 0.241", "Leiqing Cai", "caithagoras", "09/24/20, 09:24:03 PM", "# Missing Release Notes\n## Daniel Ohayon\n- [x] https://github.com/prestodb/presto/pull/14728 Enums support #1: base types and operators (Merged by: Rongrong Zhong)\n\n## George Wang\n- [x] https://github.com/prestodb/presto/pull/15014 Add Oracle connector support (Merged by: James Sun)\n\n## Vic Zhang\n- [x] https://github.com/prestodb/presto/pull/15075 Fix precision definition in classification_precision function (Merged by: Maria Basmanova)\n\n# Extracted Release Notes\n- #14923 (Author: James Gill): Add geometry_nearest_points function\n  - Add :func:`geometry_nearest_points` to find nearest points of a pair of geometries.\n- #14955 (Author: Venki Korukanti): Push dereferences into table scan for parquet tables\n  - This change adds planner side support for pushing dereferences into Parquet table scan. Pushing deferences into table scan enables efficient scans as only the required nested column is read when required independent of the other projected nested columns in the same base column. Currently this functionality is behind a configuration variable `hive.enable-parquet-dereference-pushdown`.\n- #14983 (Author: prithvip): Add warning message for UNION queries without ALL/DISTINCT keyword\n  - Added new warning message for UNION queries without ALL/DISTINCT keyword.\n- #14995 (Author: Beinan Wang): Add support for druid data ingestion\n  - Add support for data ingestion.\n- #15013 (Author: Vivek Bharathan): Add Hive procedure to sync table partitions\n  - Add procedure system.sync_partition_metadata() to synchronize the partitions in the metastore with the partitions that are physically on the file system.\n- #15024 (Author: James Petty): Implement PrestoS3FileSystem#listFiles for direct recursive listings\n  - Add support for direct recursive file listings in PrestoS3FileSystem.\n- #15027 (Author: Adam J. Shook): Implement JDBC ResultSet.getStatement\n  - Implemented ResultSet getStatement.\n- #15028 (Author: Rohit Jain): Revert \"Upgrade ZSTD version\"\n  - Downgrade ZSTD JNI compressor version to resolve the frequent excessive GC events introduced in version 0.238.\n- #15040 (Author: Mayank Garg): Separate operator and stage statistics from query statistics\n  - Add ``StageStatistics`` and ``OperatorStatistics`` to ``QueryCompletedEvent`` and remove stage and operator statistics from ``QueryStatistics``.\n- #15042 (Author: Mayank Garg): Add documentation for Presto authorization\n  - Implement REST endpoint authorization in Presto. See :doc:`/security/authorization`.\n- #15056 (Author: Leiqing Cai): Improve error handling during determinism analysis\n  - Fix an issue during determinism analysis where queries with LIMIT clause are not identified as non-deterministic when a rerun of the control query fails.\n- #15065 (Author: Rebecca Schlussel): Support non-hive types in hive views\n  - Add support for non-hive types to hive views.  This support had been removed in 0.233.  If a view uses an unsupported type for any columns ,we will store only a single dummy column for that view in the metastore.\n- #15077 (Author: Ke Wang): Dynamic filtering integration with Aria\n  - Add dynamic filtering and bucket pruning support for inner join and semi join. The feature avoids full table scan on probe side for broadcast join or colocated join when the build side is small. Set config `experimental.enable-dynamic-filtering` to `True` to enable the feature. Configs `experimental.dynamic-filtering-max-per-driver-row-count` and `experimental.dynamic-filtering-max-per-driver-size` are available to tune the size on the build side join key space. Currently, only Hive connector can benefit from the feature.\n- #15078 (Author: Ying Su): Add memory tracking for OrcRecordReader\n  - Added memory tracking for OrcRecordReader.\n- #15099 (Author: Ying Su): Fix memory counting for SliceDictionaryBatchStreamReader\n  - Fixed a memory counting bug in SliceDictionaryBatchStreamReader.\n- #15104 (Author: James Gill): Fix incorrect intersection between two envelopes\n  - Fix bug when two envelopes intersect at a point for :func:`ST_Intersection`.\n- #15113 (Author: Leiqing Cai): Support multiple control and test clusters in Verifier\n  - Add support to allow multiple control and test clusters.\n\n# All Commits\n- 61d5b873a8702c34f28a88e8fc79ec7fd48db410 Partial Aggregation Pushdown for ORC/Parquet (Vivek Bharathan)\n- 731ea6b0fa8ff351c9b0fa4519b62429905c36e3 Fix incorrect intersection between two envelopes (James Gill)\n- 7b07513ff9ae5c5c8990ed5821b40c37338ccc8e Add support to bounded varchar to BenchmarkSelectiveStreamReaders (Ying Su)\n- a8dce28097f712f727e629d5ecd0bfa5e2462d1e Support multiple columns in BenchmarkSelectiveStreamReaders (Ying Su)\n- 921393d63182df1f0bf86da0043ba567edfa4207 Allow custom filter rate for BenchmarkSelectiveStreamReaders (Ying Su)\n- bc44dabd5617482bd255fedccbf93d4ff8eb0eac Add verification method to BenchmarkSelectiveStreamReaders (Ying Su)\n- cf491372b7276d074b31e8ceb6d55132eecf4f7c Refactor BenchmarkSelectiveStreamReaders (Ying Su)\n- 6c8ad659aeac266e61821925d5654e17358aa410 ParquetWriters cleanup (Zhenxiao Luo)\n- eadfd1bbb5a948f45a10861e7455ac914311730b Add ConnectorMetadataUpdateHandle resolver (Nikhil Collooru)\n- f02c93ddf08223574eb1b2d7eda1b467d869226c Add ConnectorMetadataUpdater to handle worker's metadata updates (Nikhil Collooru)\n- e6995f4b62589a7abd809ccde12223b6d64921e8 Rename PageSinkProperties to PageSinkContext (Nikhil Collooru)\n- ae0bdf8126b277ebadfa02b0aca65c7ad10e775e Remove getFileStatus on openFile with Alluxio Cache (Bin Fan)\n- f74a84e8b610dcddff5d29d76b345fe7ac2de5a4 Dynamic filtering integration with hive filter pushdown (Ke Wang)\n- 76442b33ae4a6a45f649805d516ca405dc29ed06 Support multiple control and test clusters in Verifier (Leiqing Cai)\n- 2663a6679bd1ff8063e62e94f1818fd2620d178a Make equals() and hashCode() consistent in TableHandle (Shixuan Fan)\n- 344a3f8bc9d8daa568d7fd9c3d52d1b464f4de0e Add check that dynamic filtering is not enabled with grouped execution (Ke Wang)\n- 6b6b143f9cedc19098460725fb73a12bf7417c71 Rename dynamicFilterSupplier in ScanFilterAndProjectOperator (Ke Wang)\n- e548af5502685683251be3c5e5d38fe87e8ac666 Wrap OrcZstdDecompressor with airlift Decompressor (Vic Zhang)\n- b2139549f976acb307bc79ca3a3ae3d94223c7b1 Add zstd compression for PAGEFILE (Vic Zhang)\n- 287c4711ccd8e8a2e407ba67d274a8bb26acf9cd Add debug logging when running Presto queries in Verifier (Leiqing Cai)\n- eed4ae1a6ff52257ac7adf8f25ceadb10913d64a Make SimpleHttpResponseHandler generic (Tim Meehan)\n- 5805f747b64f1109b764b55f2fef8cf364e4fe79 Make RequestErrorTracker generic (Tim Meehan)\n- f2c70979d6fee1d2b95c8be1fe48debb074c265f Add tests for session propery ignore_unreadable_partition (Naveen007)\n- 8ff1be57f652b07295ede68d54202405f564cf59 Add Session property to ignore non-readable hive partitions (Naveen007)\n- 3d4931b1d753235aa9e6f8e8ad95bafca5a53247 Add testMemoryTracking in TestSelectiveOrcReader (Ying Su)\n- 696ffca57d1b195a958854acb41ea290a68b19c5 Fix memory tracking for some SelectiveStreamReader's (Ying Su)\n- d3756467f3b708db0fc3cd5612615269fd22f09c Add memory tracking for OrcSelectiveRecordReader (Ying Su)\n- 9aaa21880110ad15de70840a78790be0b5c17298 Do not write buffered data when task is aborted (Vic Zhang)\n- 1ac2744a1131dc3eda3d834ed03bef7c478ec12d Add ExecutionFailureInfo to BasicQueryInfo (Tim Meehan)\n- 5d6ec74cadca9dc3f07869f7e93cd941719ab054 Remove QueryInfo from DispatchManager (Tim Meehan)\n- 4375cd29031c826c14a33a2ab7f3bf09ed4b9475 Fix formatting in TestHiveIntegrationSmokeTest (Rebecca Schlussel)\n- 256fb82858b7fa0ca36bf3a26c3ee73bdcc20b52 Suport non-hive types for Hive views (Rebecca Schlussel)\n- a458ecbe56105226c88b0f89bcf13dc09b3fc103 Track cache objects sizes in CachingOrcDataSource (Ying Su)\n- 91a1c79e431463eceb507a153f34ca6150df51c4 Create OrcPageSource using the OrcDataSource from OrcReader (Ying Su)\n- b552b63595a5bb0eaa8ecb77453342b63a5be686 Fix memory counting for SliceDictionaryBatchStreamReader (Ying Su)\n- 21c9c62b11afec44bd9d4385f67c73c5d0e37535 Update Presto on Spark splits assignment test (Vic Zhang)\n- fe13c969b6acaa0fce8676dd9fc73e33a4c41266 Add config property for min spark partition count (Vic Zhang)\n- dcf54c535b1e57ac14238e8a34805a3b6703a255 Add more error message for Presto-on-Spark (Wenlei Xie)\n- 99e67bc9d490d51920473999cb43905037bb7ee5 Add enum operators (Daniel Ohayon)\n- 3649366842b1267813e43f1fa3ea36be2015d0f3 Support enum literals in queries (Daniel Ohayon)\n- f99842b17afdbb6b3bdca063049503ec86cf7a87 Support type bound in TypeVariableConstraint (Daniel Ohayon)\n- 7ced455b02032a99b7ce8991412897baba60d14d Add long and varchar enum types (Daniel Ohayon)\n- 9bde82176cc95b9fc3709793901eba6cf28ef4c9 Add Hive procedure to sync table partitions (Vivek)\n- a703fc48010461d1100aa5268c4a50970fdc36aa Adds support for microsecond timestamp precision (Dmitry Borovsky)\n- da8303be9054634e2b5c35b89906668ba22d1733 Categorize the AccessControlException as user error (Venki Korukanti)\n- 9b61fd745160e547d4235ac05061b3f5e4303be8 Fix TestOrcMapNullKey (Masha Basmanova)\n- 738bb9cca9804a94649bfb856f1733769a6d4628 Improve logging of queued queries (Tim Meehan)\n- fbe8766f983ed862848f91bd805aecabad1b8085 Fix PrestoSparkQueryStatusInfo deserialization (Andrii Rosa)\n- 1a8987f745365ec08467429cfc6c39fcf80addf9 Adds option to read null map keys from orc file (Dmitry Borovsky)\n- 9d92f0ef5f2064346b8cbd32a9e5ff919973a890 Fix page splitter creating large dictionary page list (James Sun)\n- d8a12ee72c91a82a09c9f86e43f8697070a90a33 Add test cases for making temp dir under both exist and non-exist folder (Beinan Wang)\n- 3f34a616bb9a79b4736e60e7f44a1d1a1f76a0e0 Create temporary root folder when it does not exist and add unit test (Beinan Wang)\n- c0ae3044443c163d227553d63ddeb4512fbd2c6e fix CTAS failures when using viewfs (Beinan Wang)\n- 6923691a0e8f6e83340180aacbea5d1ad9852519 Allow to store query results into a file in Presto on Spark (Andrii Rosa)\n- 6fa3632e2b5dababdf847ff1b4bd3b6605cb6710 Restructure PrestoSparkQueryInfo to resemble QueryResults (Andrii Rosa)\n- 2222b94325783b3c664e184fa65e73bd9c1d704b Expose more session parameters in PrestoSparkRunner (Andrii Rosa)\n- f8793a7477209fa91b800c997d34be46348c5795 Improve error handling during determinism analysis (Leiqing Cai)\n- b18ccbe27262affe2943a0e86f846a196347da8f Fix precision definition in classification_precision function (Vic Zhang)\n- b7d55392b38e16a17b35c2c08751347ff2f283f6 Clean up TrackedQuery (Tim Meehan)\n- 86914fbd787a254c9691759cf274d133b2e7a332 Add max Spark input partition count for auto tune (Wenlei Xie)\n- f16f757d171af7ce93dbe27f3b0bdef875b64ceb Recognize Presto-on-Spark error code in verifier (Wenlei Xie)\n- 39872c40ec70acbb5579720d30dd75763e6305ef Move QueryNotAdequatelyPushedDownErrorCode to PinotErrorCode (Xiang Fu)\n- ad31ac01727b5c500ed075a2fb3f3ca40a4af980 Add dynamic filter canonicalization in UnaliasSymbolReferences (Ke Wang)\n- 0cfa23fd729a4ae6ab55c4c55ff8c1ccfd6b8be0 Refactor WarningCollector to spi module enabling connectors to pass warnings back to the engine (Naveen007)\n- 95725f66bceceadd311eaaff9f7e5706b63a407a Implement PrestoS3FileSystem#listFiles for direct recursive listings (James Petty)\n- e7af71b06a68b7f48cf432c4e4bd8a996b61a573 Add geometry_nearest_points function (James Gill)\n- f25aa0cde71510e852803dcdf1fff8cfea10997f Fix multi-join dynamic filtering (Ke Wang)\n- f4fd78618cb2dc1cb8f98b6bfe44ef02bd1e7358 Fix for Pinot queries where order by column is pruned in projection (Dharak Kharod)\n- 4778753b805674babb8a4c036af9a7ec7dc3ec1f Separate operator and stage statistics from query statistics (Mayank Garg)\n- 4066bd3a1130db56e2b81b99bfe3de4e813a9235 Invoke runtime plan checker in SqlQueryScheduler (Peizhen Guo)\n- 9b29a7485953309f4ca0d17ef7def889d89d25d8 Refactor RuntimeReorderJoin use PropertyDerivation (Peizhen Guo)\n- 7dd650c93f0d9f8048d2a7d9947483dc9e302abd Add documentation for Presto authorization (Mayank Garg)\n- c02e90ef01a8f46e2a33879d5ff6dd888c3f9c56 Add error code for Presto-on-Spark (Wenlei Xie)\n- ad1ab8b8e12e1f12506ef2f72c9ef42dfdfbf438 Remove OriginalExpression from PropertyDerivations (James Sun)\n- d60c948fbbae5fd41d81f327c0dc20e97cf4d154 Remove OriginalExpression from StreamPropertyDerivations (James Sun)\n- 5de7bca66493bfb2e502f3e0f332719b03535b9e Remove OriginalExpression from PushProjectionThroughUnion (James Sun)\n- 0bd085450071e7db1ab19427fbf5a91632ea72bc Remove OriginalExpression from PushProjectionThroughExchange (James Sun)\n- 49a2f198a7ac95ff38b4cb1eaee23aa7c7e9487d Add Oracle connector (George Wang)\n- c79b71428ed777cfe8154e257aa39f1a4a2693f3 Fix TABLESAMPLE SYSTEM for Presto on Spark (Andrii Rosa)\n- 36b78d2770132aa7866aff0084ed60649c2b1012 Fix pruning unreferenced variables when WindowNode is skipped in PruneUnreferencedOutputs (Venki Korukanti)\n- 7e7be195b08ec491c54e64d2b3d3b6963dc8d732 Enable dereference pushdown in more testcases (Zhenxiao Luo)\n- d07fbc573552fa380cb9d6f4806da419155494fc Implement JDBC ResultSet.getStatement (Adam J. Shook)\n- 8338c0bc22f41c305e9802a47111413608f990a2 Improve DynamicFiltersChecker to catch unsupported dynamic filters (Ke Wang)\n- 08e260c5fcedbb962002ac2763a05b43a2bdd4ed Cleanup nested dynamic filters in RemoveUnsupportedDynamicFilters (Ke Wang)\n- 19cfbbc3e766b18be799106ec452975a52f55e62 Handle null analysis for try_cast (James Sun)\n- 5dc949e2d7f6d3d19e34ae55124c205a59479e1f Fix NullabilityAnalyzer for RowExpression (James Sun)\n- a1b15d6347532834f24a50fd3667f9f8143c8da8 Remove unused NullabilityAnalyzer for Expression (James Sun)\n- f6fa7c821c59b723e287156c1cf6ff5371c1566f Catch and throw storage connection error properly (Nikhil Collooru)\n- 99c0d2a123f31586afd7f349633e98dcc123b2b4 Use SYNTHESIZED type to represent pushed down Subfield in HiveColumnHandle (Venki Korukanti)\n- 7d4a89246fce33b759cf5c3d88fadc796a8d49ca RowGroup pruning using the filter on pushed down subfield (Venki Korukanti)\n- 04ed142e96e23a087dd541713ab40150ed278694 Fix the dereference validity check in PushdownDeferences rule (Venki Korukanti)\n- a444c048cbc66df50f06986206651d0316fdf354 Update Parquet reader to read pushed down dereference columns (Venki Korukanti)\n- 009eb3f0ab34dfb5ee853da2acf702c0a84f9741 Pushdown dereferences into table scan for parquet tables (Venki Korukanti)\n- 871dbf3f7cb917ace5a4b5aed6499ab368ada1f2 Add an option to control Parquet dereferance pushdown (Venki Korukanti)\n- 9f867d4ceba0a76d0a38ca2b32249253f0d107b1 Revert \"Upgrade ZSTD version\" (Rohit Jain)\n- 172a552a266dac2a342e00397a3017cc0f3c2448 Improve shuffle statistics collection (Andrii Rosa)\n- c8d9198860b46f6da72be2605ac2ac642c422184 Implement batching of tiny rows for Presto on Spark (Andrii Rosa)\n- d2964bab2de8747645a110683e9893146d4a2a84 Use offset instead of size in PrestoSparkRowBatch (Andrii Rosa)\n- de096455a8990263b4bd89e78be6d4d3aea524c4 Implement hdfs input source for druid ingestion (beinan)\n- 8fb458d9b2d26e4a45d0b01a5fee9ade0125faf3 Implement druid ingestion by CTAS (beinan)\n- d29764e698d169506744ba9fb773d50892ed753e Implement sending ingestion task to druid (beinan)\n- 572a4cb8cf5a8807bb48e0cf30330382d8182f82 Write druid page data to gzip files (beinan)\n- 84408b8d16283130017fb5c67e8423bc4bed129c Add ingestion storage path to DruidConfig (beinan)\n- d5928f12cf505984d1d9ef2d3dc0b1925815648e Add druid table insert/ingestion skeleton code (beinan)\n- 27e922f63cf90273b7746667c390bf3246117c4e Add warning message for UNION queries without ALL/DISTINCT keyword (prithvip)\n- c6c34d6bc25486415497a4195eff32e4ca4d9764 Modify regex for compatibility with both mysql, mariadb (Amit Sadaphule)\n- 0cd9fb183a7d1ce6f5b5865fc35c166920345ef6 Dynamic bucket pruning on workers (Ke Wang)", "NaN"], ["15144", "Add support for CREATE TABLE in Cassandra", "SandishKumarHN", "SandishKumarHN", "09/24/20, 12:18:10 AM", "Add support for CREATE TABLE in Cassandra\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15145", "Skip unsupported types when pushing down in Cassandra", "SandishKumarHN", "SandishKumarHN", "09/18/20, 07:18:03 AM", "Skip unsupported types when pushing down in Cassandra \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15147", "Support smallint, tinyint, date type of Cassandra", "SandishKumarHN", "SandishKumarHN", "09/16/20, 10:50:31 PM", "Support smallint, tinyint, date type of Cassandra\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nCassandra Change\r\n* Add `SMALLINT`, `TINYINT`, and `DATE` type support to Cassandra connector.\r\n```", "NaN"], ["15148", "Bump Apache Druid to 0.19.0", "Benedict Jin", "asdf2014", "09/13/20, 07:55:55 AM", "== RELEASE NOTES ==\r\n\r\nhttps://github.com/apache/druid/releases/tag/druid-0.19.0", "NaN"], ["15151", "Dynamic partition pruning on workers", "Ke", "kewang1024", "09/11/20, 06:04:06 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15152", "Remove dependency on javafx", "Vivek", "ClarenceThreepwood", "09/10/20, 02:17:23 AM", "Follow up to #14543\r\n\r\nFixes #14543\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15153", "Update drift library to 1.28", "Vic Zhang", "viczhang861", "09/20/20, 03:38:01 PM", "Test plan - Travis\r\n\r\npresto-facebook is using drift 1.28\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15155", "Support fragment result caching", "Shixuan Fan", "shixuan-fan", "10/01/20, 04:41:05 PM", "Test plan - Multiple unit test for `CanonicalPlanGenerator`, `FileFragmentResultCacheManager` and `Driver`. Ran shadow test for internal queries. Ran verifier test for internal queries.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for fragment result caching. When enabled, if the same plan fragment and same connector split hit the same worker, engine would directly fetch result from cache and skip computation. Currently only partial aggregation is supported. Cache could be enabled by setting ``fragment-result-cache.enabled`` to ``true`` and tuned by other configs started with ``fragment-result-cache``. \r\n Query could use fragment result cache by setting config ``experimental.fragment-result-caching-enabled`` or session property ``fragment_result_caching_enabled`` to ``true``.\r\n\r\nSPI changes\r\n* Add ``getSplitIdentifier`` to ``ConnectorSplit``. Split identifier is used in fragment result caching to identify if splits are identical.\r\n* Add ``getIdentifier`` to ``ConnectorTableLayoutHandle``. Layout identifier is used in fragment result caching to construct canonical plan. \r\n```\r", "NaN"], ["15156", "partial aggregation pushdown for ORC/Parquet cleanup", "Zhenxiao Luo", "zhenxiao", "09/10/20, 04:53:43 PM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15161", "TupleDomain cleanup", "Zhenxiao Luo", "zhenxiao", "09/11/20, 08:36:38 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15162", "Add tests for sum, min, max and avg queries running on native workers", "Maria Basmanova", "mbasmanova", "09/12/20, 12:48:45 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15163", "Fix performance regression when hive SerDe doesn't prefer Writables", "James Petty", "pettyjamesm", "09/17/20, 01:10:19 AM", "Introduced in https://github.com/prestodb/presto/pull/8206 `GenericHiveRecordCursor` was modified to avoid extra overhead when the SerDe provided a more efficient String handling implementation with Writables. However, when the SerDe does not provide such an implementation and instead already returned String instances directly, this change introduced an extra conversion from bytes to String just to be converted back to bytes.\r\n\r\nThis change alters the behavior of `GenericHiveRecordCursor#parseString` to respect the PrimitiveObjectInspector's preference for using writables.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Fix a performance regression for String field handling in GenericHiveRecordCursor when the SerDe does not provide an efficient Writable implementation\r\n```\r", "NaN"], ["15165", "Disallow partial aggregation pushdown in more cases", "Vivek", "ClarenceThreepwood", "09/21/20, 06:41:28 AM", "Disallow partial aggregation pushdown when\r\n1. Filters are pushed into tableScan\r\n2. Storage format metadata is inconsistent with the file format\r\n\r\nAlso enable partial aggregation flags in more tests\r\n\r\nFixes #15157 \r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15167", "Fix wrong group by column definition in druid connector", null, "weidongduan37", "09/16/20, 06:40:49 PM", "Previous code may generate wrong group by name that end with suffix _number, and the dql could not be executed at druid side\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15168", "Automatically switch to PAGEFILE format for hive unsupported type", "Vic Zhang", "viczhang861", "09/18/20, 06:27:21 PM", "When ORC is the default format for temporary table and there is unsupported column type, add an option to automatically use PAGEFILE format.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15171", "Fix IllegalArgumentException in RelationPlanner", "Rongrong Zhong", "rongrong", "09/15/20, 09:15:58 PM", "in RelationPlanner, when we process values node, we would add coercions for\r\ntype only coercion expression. This could change the expression, which does\r\nnot exist in the analysis. For Enum literals, we want to rewrite the\r\ndereference to enum literal. Previously this is done after the coercion rewrite.\r\nThus if the original expression has type only coercions, we could get\r\nIllegalArgumentException when trying to get the type of the node. Moving the\r\ndereference to enum rewrite before the coercion should solve this problem.\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15173", "Implement Spilling Strategies", "Saksham", "sachdevs", "09/29/20, 11:35:36 PM", "This PR adds multiple different spilling strategies that can be swapped between using the config `experimental.spiller.task-spilling-strategy`.\r\n\r\nORDER_BY_CREATE_TIME - current default strategy. Watch memory pools for revocable memory exceeding threshold, sort tasks by create time, revoke individual operators until we reach the lower threshold.\r\n\r\nORDER_BY_REVOCABLE_BYTES - NEW. Watch memory pools for revocable memory exceeding threshold, sort tasks by most allocated revocable bytes, revoke individual operators until we reach the lower threshold.\r\n\r\nPER_TASK_MEMORY_THRESHOLD - NEW. Watch revocable memory pool for memory exceeding per task memory threshold defined by `experimental.spiller.max-revocable-task-memory`. Spill operators in task until it lowers to this threshold.\r\n\r\nTODO\r\n* ~finish integration tests~\r\n* ~better commit message~\r\n* ~Release note~\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add config `experimental.spiller.task-spilling-strategy` for choosing different spilling strategy to use.\r\n```\r", "NaN"], ["15174", "[Verifier] Fix broken link in presto-verifier README", "Palash Goel", "palashgoel7", "09/16/20, 06:33:04 PM", "Fix broken link in presto-verifier README\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\nChecked that the link works in my fork\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15179", "Add more functions to TestHiveExternalWorkersQueries", "Maria Basmanova", "mbasmanova", "09/16/20, 02:51:07 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15180", "Disable Elasticsearch connector flaky test", "Zhenxiao Luo", "zhenxiao", "09/25/20, 02:23:30 AM", "https://github.com/prestodb/presto/issues/15177\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15181", "Fix LimitQueryDeterminismAnalyzer", "Leiqing Cai", "caithagoras", "09/17/20, 05:10:07 PM", "Fix LimitQueryDeterminismAnalyzer to return `DETERMINISTIC` when the control query row count is less then the value of the limit clause. Before the fix, LimitQueryDeterminismAnalyzer concluded `ANALYSIS_FAILED_DATA_CHANGED` instead where there the data is not actually changed.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Fix an issue in determinism analysis would indicate failing due to data being changed while the data is not changed.\r\n```", "NaN"], ["15182", "Update default Presto Pinot Connector Configs", "Xiang Fu", "xiangfu0", "09/21/20, 06:29:01 PM", "- Update default Presto Pinot Connector Configs\r\n- Add sample pinot connector configs as example in `presto-main`.\r\n\r\nTest plan:\r\nStarted Presto server and tested queries with default configs:\r\n```\r\nconnector.name=pinot\r\n\r\n# Pinot controller endpoint\r\npinot.controller-urls=localhost:9000\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r", "NaN"], ["15183", "Fix compiler error in LambdaBytecodeGenerator", "Rongrong Zhong", "rongrong", "09/17/20, 09:27:54 PM", "When there are lambda expressions from different SQL functions, and they are\r\ncompiled into the same class due to CSE, we need to make sure the generated\r\nfunction names are always unique.\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix compiler error in LambdaBytecodeGenerator when CSE is enabled and multiple SQL functions contain lambda expressions.\r\n```\r", "NaN"], ["15184", "Revert enum", "Ying", "yingsu00", "09/17/20, 08:03:30 PM", "During the releasing process of v0.241, we found this PR caused `TOO_MANY_REQUESTS_FAILED` on some queries and the result was deterministic. To remove release blocker we will revert this PR in v0.241.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15185", "Remove unused configuration property from properties.rst", "Ying", "yingsu00", "09/17/20, 08:04:10 PM", "driver.max-page-partitioning-buffer-count was removed in 0.238 but\r\nthe documentation for it was not removed. This commit removes it\r\nfrom the properties.rst documentation.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15187", "Consider predicate columns for encryption", null, "mayankgarg1990", "09/21/20, 04:33:51 PM", "In encryption we pass the list of columns that are being read to help with\r\ngetting the credentials only for those columns and allow for more granular\r\naccess control.\r\n\r\nUptil now, only the columns being requested in the select statement were\r\nthe only ones being considered. This commit adds support for columns in\r\nthe predicate as well. We now consider the predicate columns as well.\r\n\r\nA special case to callout is `ROW`. Presto currently supports encryption\r\ngranularity of a subfield, however, it is non-trivial to get the exact\r\nsubfields being accessed for a `ROW`. This PR puts the whole `ROW` column\r\nif any subfield is being used in the predicate. This is a limitation for\r\nnow.\r\n\r\nTest plan - It is very difficult to test this change in a unit test in its current format. A\r\nbigger refactor is needed to be able to create tables with dummy encryption providers.\r\n\r\nFor this specific PR, I performed extensive tests against Facebook infrastructure and\r\nensured that all specific situations are succeeding.\r", "NaN"], ["15188", "Remove unused Parquet classes", "Vivek", "ClarenceThreepwood", "09/18/20, 02:57:27 AM", "Clean up dead code\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15189", "Parquet code cleanup", "Zhenxiao Luo", "zhenxiao", "09/18/20, 06:23:13 PM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15190", "Refactor more classes to use Page helper methods", "James Petty", "pettyjamesm", "09/21/20, 05:32:06 PM", "Comparable to changes in https://github.com/prestosql/presto/pull/5218\r\n\r\nTwo commits:\r\n- Adds `Page#extractChannel(int)` helper method to allow efficient single channel Page extraction\r\n- Refactors more classes and operators to use application `Page` class helper methods which avoid extra allocations and copies of Block arrays\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15191", "[Verifier] Add application-name config", "Palash Goel", "palashgoel7", "09/28/20, 08:09:51 PM", "Test plan - (Please fill in how you tested your changes)\r\nVerified Jar is created and works with config in Docs. Also, verified generated config in logs.\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add application-name config to override source passed in ClientInfo\r\n\r\n```\r\n\r", "NaN"], ["15193", "Add docs for exchange materialization", "Wenlei Xie", "wenleix", "09/25/20, 09:15:00 PM", "Test plan - build the doc locally\r\n\r\n![Screen Shot 2020-09-19 at 3 54 07 PM](https://user-images.githubusercontent.com/799346/93690714-68e08280-fa90-11ea-87aa-01a4edb5ef6b.png)\r\n\r\n-----------------------\r\n\r\n![Screen Shot 2020-09-19 at 3 54 15 PM](https://user-images.githubusercontent.com/799346/93690715-6a11af80-fa90-11ea-8626-682d288ed710.png)\r\n\r\n-----------------------\r\n\r\n\r\n![Screen Shot 2020-09-19 at 3 54 22 PM](https://user-images.githubusercontent.com/799346/93690717-6c740980-fa90-11ea-9f57-b9eb075c8049.png)\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15195", "Support CREATE VIEW and CREATE TABLE verification", "Leiqing Cai", "caithagoras", "10/06/20, 01:42:15 AM", "## Verify CREATE VIEW\r\nIf the specified view already exists, create a temporary view to match\r\nthe existing view. Otherwise, do nothing for setup queries. Rewrite the\r\ntarget view name, and run both control and test queries. Run a SHOW\r\nCREATE VIEW query and the returned CREATE VIEW statement needs to match.\r\n\r\n## Verify CREATE TABLE\r\nRewrite the target table of the CREATE TABLE statement, run both\r\ncontrol and test queries, run SHOW CREATE TABLE query as the check.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add support to verify ``CREATE VIEW`` and ``CREATE TABLE`` queries.\r\n```\r", "NaN"], ["15196", "Do not support dynamic filtering with grouped-execution", "Ke", "kewang1024", "09/21/20, 05:52:10 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15198", "Improve plan comparison for explain verification", "Leiqing Cai", "caithagoras", "09/23/20, 07:57:43 PM", "Plain text comparison for JSON plan is inaccurate. There are too many\r\nfactors in two different deployments to cause query plan to not match\r\nexactly.\r\n\r\nReplace MatchType PLAN_CHANGED with STRUCTURE_MISMATCH and\r\nDETAILS_MISMATCH.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Improve JSON plan comparison for explain verification. (:pr:`15198`)\r\n```\r", "NaN"], ["15199", "Fix encrypt large value", "Rebecca Schlussel", "rschlussel", "09/23/20, 02:23:38 PM", "Test plan - new unit test in AbstractTestOrcReader\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Fix a bug where DWRF encryption would fail for large uncompressed column values\r\n```\r", "NaN"], ["15200", "Upgrade Esri to 2.2.4", "James Gill", "jagill", "09/28/20, 09:51:09 PM", "This includes several fixes, two that affect our users that we reported:\r\nhttps://github.com/Esri/geometry-api-java/issues/247\r\nhttps://github.com/Esri/geometry-api-java/issues/266\r\n\r\nThe second, in particular, caused workers to hang when they tried to\r\nperform a union of two geometries.\r\n\r\nRelease notes:\r\nhttps://github.com/Esri/geometry-api-java/releases/tag/v2.2.4\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\nAdded test case that reproduces the hang.  It does not hang anymore!\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1182\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeospatial Changes\r\n* Upgrade Esri to [2.2.4](https://github.com/Esri/geometry-api-java/releases/tag/v2.2.4).  This includes two fixes for bug (https://github.com/Esri/geometry-api-java/issues/266 and https://github.com/Esri/geometry-api-java/issues/247) that were seen in production.\r\n```\r", "NaN"], ["15204", "Improve Elasticsearch connector documentation", null, "fornaix", "09/22/20, 06:41:17 AM", "Fixes https://github.com/prestodb/presto/issues/15120\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15205", "Improve BenchmarkPageProcessor", "Ying", "yingsu00", "10/02/20, 10:05:35 AM", "Previously BenchmarkPageProcessor filters out 100% rows, and this makes\r\nproject was not tested at all. This PR adds a new option for the\r\nfilter to pass all rows so that projection can be tested. It also moves the creation of data and processors into a new inner class BenchmarkData, which allows for easier future parameter based expansions.\r\n \r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15206", "Reserve memory before projecting rows", "Ying", "yingsu00", "09/30/20, 11:57:24 PM", "Previously the BlockBuilders always grow their internal arrays from 1\r\nelement, and call growCapacity() when adding new elements. growCapacity()\r\nis every expensive in both CPU and memory allocations. Sometimes we\r\nknow the number of elements that are going to be added in a batch, and\r\nwe can pre-reserved the memory to avoid repeating memory growth and\r\ncopying.\r\n\r\nBenchmarkPageProcessor shows 33% improvement in projection's throughput\r\n(higher is better) and 4x reduction in gc.alloc.rate.norm\r\n(lower is better) in GC benchmarks.\r\n\r\nBefore\r\n```\r\nBenchmark                                                         (projectionDataType)   Mode  Cnt       Score        Error   Units\r\nBenchmarkPageProcessor.compiled                                                 BIGINT  thrpt   10    8374.449 \u00b1   1023.859   ops/s\r\nBenchmarkPageProcessor.compiled:\u00b7gc.alloc.rate                                  BIGINT  thrpt   10    1451.783 \u00b1    179.079  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.alloc.rate.norm                             BIGINT  thrpt   10  363640.395 \u00b1      1.385    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Eden_Space                         BIGINT  thrpt   10    1448.711 \u00b1   1904.852  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Eden_Space.norm                    BIGINT  thrpt   10  368448.852 \u00b1 487748.838    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Survivor_Space                     BIGINT  thrpt   10       0.019 \u00b1      0.054  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Survivor_Space.norm                BIGINT  thrpt   10       4.904 \u00b1     13.838    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.count                                       BIGINT  thrpt   10       6.000               counts\r\nBenchmarkPageProcessor.compiled:\u00b7gc.time                                        BIGINT  thrpt   10      27.000                   ms\r\nBenchmarkPageProcessor.compiled                                                 DOUBLE  thrpt   10    8805.494 \u00b1    916.612   ops/s\r\nBenchmarkPageProcessor.compiled:\u00b7gc.alloc.rate                                  DOUBLE  thrpt   10    1527.888 \u00b1    159.617  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.alloc.rate.norm                             DOUBLE  thrpt   10  363640.100 \u00b1      0.011    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Eden_Space                         DOUBLE  thrpt   10    1716.294 \u00b1   1812.190  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Eden_Space.norm                    DOUBLE  thrpt   10  414368.217 \u00b1 436717.530    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Survivor_Space                     DOUBLE  thrpt   10       0.008 \u00b1      0.023  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Survivor_Space.norm                DOUBLE  thrpt   10       1.942 \u00b1      5.771    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.count                                       DOUBLE  thrpt   10       7.000               counts\r\nBenchmarkPageProcessor.compiled:\u00b7gc.time                                        DOUBLE  thrpt   10      30.000                   ms\r\n```\r\nAfter\r\n```\r\nBenchmark                                                         (projectionDataType)   Mode  Cnt       Score        Error   Units\r\nBenchmarkPageProcessor.compiled                                                 BIGINT  thrpt   10   11140.511 \u00b1   1245.100   ops/s\r\nBenchmarkPageProcessor.compiled:\u00b7gc.alloc.rate                                  BIGINT  thrpt   10     494.381 \u00b1     55.174  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.alloc.rate.norm                             BIGINT  thrpt   10   93152.321 \u00b1      1.149    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Eden_Space                         BIGINT  thrpt   10     534.785 \u00b1    705.834  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Eden_Space.norm                    BIGINT  thrpt   10  105433.423 \u00b1 139043.448    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Survivor_Space                     BIGINT  thrpt   10       0.012 \u00b1      0.045  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Survivor_Space.norm                BIGINT  thrpt   10       2.275 \u00b1      8.238    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.count                                       BIGINT  thrpt   10       6.000               counts\r\nBenchmarkPageProcessor.compiled:\u00b7gc.time                                        BIGINT  thrpt   10      27.000                   ms\r\nBenchmarkPageProcessor.compiled                                                 DOUBLE  thrpt   10   11050.149 \u00b1   1224.166   ops/s\r\nBenchmarkPageProcessor.compiled:\u00b7gc.alloc.rate                                  DOUBLE  thrpt   10     491.177 \u00b1     54.807  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.alloc.rate.norm                             DOUBLE  thrpt   10   93152.079 \u00b1      0.009    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Eden_Space                         DOUBLE  thrpt   10     365.061 \u00b1    889.401  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Eden_Space.norm                    DOUBLE  thrpt   10   68525.405 \u00b1 167416.354    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Survivor_Space                     DOUBLE  thrpt   10       0.006 \u00b1      0.020  MB/sec\r\nBenchmarkPageProcessor.compiled:\u00b7gc.churn.PS_Survivor_Space.norm                DOUBLE  thrpt   10       1.133 \u00b1      3.612    B/op\r\nBenchmarkPageProcessor.compiled:\u00b7gc.count                                       DOUBLE  thrpt   10       3.000               counts\r\nBenchmarkPageProcessor.compiled:\u00b7gc.time                                        DOUBLE  thrpt   10      13.000                   ms\r\n```\r\n\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15208", "Clamp BackgroundHiveSplitLoader concurrency to usable parallelism", "James Petty", "pettyjamesm", "09/23/20, 03:58:33 AM", "Comparable to changes contributed in https://github.com/prestosql/presto/pull/5260\r\n\r\nAvoids allocation a greater level of concurrency to split loading than can actually be used based on the number of partititions being loaded. The effective usable concurrency in split loading can be significantly lower than the configured split loader concurrency value when the number of partitions scanned is small since each partition will be loaded by at most 1 thread at a time.\r\n\r\nThis change makes it feasible to configure a much higher concurrency value than before without overcommitting threads to loading tasks that provide no additional actual parallelism.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Improve split loading efficiency by only using as many threads as are required\r\n```\r", "NaN"], ["15209", "Collapse nested DEREF expressions into a single one", "Sreeni Viswanadha", "kaikalur", "09/30/20, 05:26:24 AM", "Currently, for expressions like a.b.c.d - we get multiple nested derefs (one for each '.'). And that results in excessive code generation especially because of the null checks. So the idea is t merge all these intermediated DEREFs into the same code block as the ultimate result of any of these being null is to just return null. This reduced code by 22% in some of the most expensive queries that make heavy use of ROW types.\r\n\r\nTest plan - \r\n\r\nAbstractTestQueries has several test cases for this now.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15210", "Report peak task memory in Presto-on-Spark", "Wenlei Xie", "wenleix", "09/25/20, 09:11:55 PM", "    The peak memory stats in TaskContext only get updated when\r\n    TaskContext::getTaskStats(). Presto-on-Spark doesn't pull task stats\r\n    actively so it needs to be updated explicitly.\r\n\r\n---------------------\r\n\r\nTest plan - Unit test. Also do production query test.\r\n\r\n![Screen Shot 2020-09-24 at 10 12 31 PM](https://user-images.githubusercontent.com/799346/94228972-0eda2580-feb3-11ea-98f3-9e1afe63c3b8.png)\r\n\r\nNote in the Presto-on-Spark context, \"peak total memory\" really mean \"sum of peak task memory\" \r\n\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15211", "Handle storage connection exceptions correctly", "Nikhil Collooru", "NikhilCollooru", "09/23/20, 05:32:39 PM", "Storage connection timeout errors are being thrown as GENERIC_INTERNAL_ERROR. Fix it and throw them as EXTERNAL_ERROR.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15212", "Enable nullifying iterator for broadcast join", "Andrii Rosa", "arhimondr", "09/24/20, 04:24:45 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15213", "Improve nested loop operator performance", "James Petty", "pettyjamesm", "11/05/20, 05:04:01 PM", "Adaptation of comparable changes in https://github.com/prestosql/presto/pull/5276\r\n\r\nImprovements:\r\n- Modifies `NestedLoopJoinPagesBuilder` to combine empty pages (aka: positionCount only pages) when the build side of the nested loop join is empty\r\n- Handles the case where either probe or build side outputs are empty and position counts are fewer by emitting the same page repeatedly, avoiding unnecessary per-iteration allocations\r\n- Reduces the amount of block array copies made in the standard case by reusing a block buffer to build each page and letting the Page constructor clone it (ie: 1/2 as many allocations)\r\n- Nullifies the output iterator as well as the probe page when finished iterating through it to make the referenced pages elligible for GC. Also nullifies more fields when the operator is closed for the same reason.\r\n- Adds a constructor `Page(int positionCount)` that avoids per-invocation empty varargs array creation and copies.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve performance of NestedLoopJoinOperator\r\n```\r\n\r", "NaN"], ["15214", "Fix order of dwrf encryption group stats", "Rebecca Schlussel", "rschlussel", "09/23/20, 09:34:55 PM", "Test plan - Added unit test for this case and added stats verification for all decryption tests\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Fix a bug where non-Presto readers could not read encrypted DWRF files written by Presto if the encryption group listed columns out of order.\r\n```\r", "NaN"], ["15216", "Adding approxMostFrequent aggregate from prestoSQL", "Ankit Kothari", "ankit0811", "10/13/20, 06:40:37 PM", "Test plan - Added tests in `AbstractTestQueries`\r\n\r\nBack-porting `approx_most_frequent` function from prestosql\r\nhttps://github.com/prestosql/presto/pull/3425\r\nhttps://github.com/prestosql/presto/pull/4499\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\nBack-porting `approx_most_frequent` function from prestosql\r\n\r\n```", "NaN"], ["15217", "Make TestQueryManager#testFailQueryPrerun more reliable", "Timothy Meehan", "tdcmeehan", "09/28/20, 11:12:26 PM", "Fixes #15215\r\n\r\nTest plan - unit test runs\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15219", "Core support for enums, take #2", "Daniel Ohayon", "daniel-ohayon", "09/29/20, 05:55:23 PM", "Second attempt to merge #14728\r\n\r\nThis was previously reverted because of a performance regression detected in `TypeSignature.parseTypeSignature`\r\n\r\nThis regression is addressed in a newly added commit, which includes a benchmark.\r\n\r\nPerformance stats on BenchmarkTypeSignatureParsing are as follows on my machine:\r\n\r\n\r\n```\r\nparseRowTypeSignature\r\n==================\r\n* with enum parsing, unoptimized parsing: 600ms/op\r\n* with enum parsing, optimized parsing: 2ms/op\r\n* without enum parsing: 2ms/op\r\n\r\nparseRowTypeSignatureWithEnums\r\n==========================\r\n* with enum parsing, optimized parsing: 2ms/op\r\n```\r", "NaN"], ["15220", "Fix potential deadlock when PageBufferClient exceeds memory limit", "James Petty", "pettyjamesm", "09/25/20, 06:29:50 AM", "Comparable changes to https://github.com/prestosql/presto/pull/5289\r\n\r\nPrior to this change, if the `PageBufferClient` caused a task to exceed its memory limit inside of `addPages`, the exception would bubble to the root of the exchange client callback executor without ever marking the task as failed. If no other operator also saw the memory limit exceeded condition, then the task would become stuck in a deadlocked state.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix possible permanent stuck queries when memory limit exceeded during shuffle.\r\n```\r", "NaN"], ["15223", "Merge FunctionManager and TypeRegistry into FunctionAndTypeManager", "Rongrong Zhong", "rongrong", "10/13/20, 05:51:39 AM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1201\r\n\r\nTest plan - Travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15226", "Remove BlockEncodingManager's dependency on TypeManager", "Rongrong Zhong", "rongrong", "09/29/20, 05:53:49 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1194\r\nTest plan - Travis, presto verifier 0.242-20200928.195351-231\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15227", "Optimize MetadataQueryOptimizer with filter pushdown enabled", "Shixuan Fan", "shixuan-fan", "10/09/20, 12:51:25 AM", "Test plan - Unit test + verifier run on internal queries\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Optimize metadata query optimizer so that it does not fetch all partitions from metastore when hive filter pushdown is enabled.\r\n```\r", "NaN"], ["15230", "Simplify PinotQueryGenerator pushdown logic", "Zhenxiao Luo", "zhenxiao", "09/27/20, 06:44:14 AM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15231", "Support pushdown approx_distinct(x, e) into pinot", "Xiang Fu", "xiangfu0", "10/04/20, 10:48:25 PM", "Current Pinot aggregation pushdown only supports `approx_distinct(x)` but not `approx_distinct(x, e)`.\r\n\r\nSince pinot supports configurable approximate distinct with query function syntax `distinctCountHll(x, log2m)` (https://github.com/apache/incubator-pinot/pull/5564), we can push down `approx_distinct(x, e)` by converting `e` to `log2m` then call pinot function: `distinctCountHll`\r\n\r\nSome ref to log2m:\r\nhttps://github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/stream/cardinality/HyperLogLog.java#L98\r\n\r\n\r\nTest plan\r\n\r\n- Adding unit tests to check the correct expression to pushdown.\r\n- Tested locally with a presto server and pinot up and run the query with real data set.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n* Support pushing down aggregation function `approx_distinct(x, e)` to Pinot connector.\r\n```\r", "NaN"], ["15236", "added scale_tdigest function", null, "leonpanokarren", "10/14/20, 09:16:18 PM", "### Summary\r\n* This change adds the `scale_tdigest` function in support of prestodb/presto/issues/15234\r\n* It adds - \r\n  1. a `scale` function to `TDigest`\r\n  2. a `scale_tdigest` function to the library of `TDigestFunctions` scalar operators\r\n  3. a `getFrequencies` method and two `tests` to `TestDigestFunctions` to keep up tests\r\n\r\n### Test plan\r\n```\r\nmvn -Dtest=TestTDigestFunctions#testScaleNegative+testScale test &> ~/presto_tests.log\r\n# Result: https://gist.github.com/leonpanokarren/80c22e2ef7cda43c5008a2281585ce73#file-scale_tdigest_results-test-log-L226-L233\r\n```\r\n\r\nAlso per my discussion with @tdcmeehan , I ran the [following distribution](https://gist.github.com/leonpanokarren/1e6512747cd7c191d497cf8588b96a55) tests\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r", "NaN"], ["15238", "Support for CSV format in hive", "Ashish", "ashishtadose", "10/05/20, 04:17:21 PM", "PrestoSQL PR - https://github.com/prestosql/presto/pull/920\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add support for CSV format in Hive connector\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\nresolve #10905", "NaN"], ["15240", "Fix incompatible reflection for modifying static final fields since JDK 12", null, "fornaix", "12/08/20, 09:06:54 PM", "Fixes: https://github.com/prestodb/presto/issues/15239\r\nReferences: \r\n* https://github.com/powermock/powermock/issues/939\r\n* https://bugs.openjdk.java.net/browse/JDK-8217225\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15243", "Link to attribution guidelines in PR template", null, "aweisberg", "09/29/20, 07:24:52 PM", "Make clear when co-author tags should be added.", "NaN"], ["15244", "Use newer Alluxio client", "Bin Fan", "apc999", "09/30/20, 04:31:50 AM", "Test plan - tested offline with @kewang1024 \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15246", "Remove TypeManager.resolveOperator", "Rongrong Zhong", "rongrong", "09/30/20, 07:04:33 PM", "This API is a hack. TypeManager.resolveOperator internally will call\r\nFunctionManager.resolveOperator. However, FunctionManager is not initialized\r\nin TypeRegistry constructor, but set in FunctionManager's constructo, due to\r\ncyclic dependency. So to be able to call this API, we see following hacky code\r\nin test everywhere:\r\n```\r\n    TypeManager typeManager = new TypeRegistr();\r\n    // associate typeManager with a function manager\r\n    new FunctionManager(typeManager, ....);\r\n```\r\nThe only reason that this API exists is because MapParametricType.createType\r\nneeds to use it. So we did\r\n1) adding TypeManager to ParamatricType.createType\r\n2) adding TypeManager.resolveOperator\r\n\r\njust so MapParametricType could provide MethodHandle that would be used to\r\nbuild hashtable in Map block, while no other API needs\r\nTypeManager.resolveOperator, and no other ParametricType needs access to\r\nTypeManager. Another problem is that the API directly returns a MethodHandle,\r\nwhich means this API will not work once we support user defined types with\r\noperator implementation that's not a Java MethodHandle.\r\n\r\nSince the solution will not work for future generalized caase, removing it for\r\nnow, and directly check for MapParametricType in TypeRegistry to special handle\r\nmap types.\r\n\r\nTest plan - travis\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15247", "Add config and session property to toggle join spilling", "Saksham", "sachdevs", "09/30/20, 04:33:09 AM", "Adding extra property to guard join spilling. This is so rest of spilling can verified without it.\r\nTest plan - Going to deploy to test cluster.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15250", "Do not ignore table bucketing when query uses bucket column", "Vic Zhang", "viczhang861", "09/30/20, 10:05:46 PM", "Test plan - \r\nIntegration test added. Table bucketing for table that uses $bucket column is not ignored.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15255", "Fix double counting of charPositionInLine", "Ajay George", "ajaygeorge", "10/28/20, 05:43:12 PM", "Fix double counting of charPositionInLine\r\n\r\nParsingException when constructed through the NodeLocation\r\nconstructor ends up double counting the column number. This\r\ncommit tries to fix that by decrementing the column number\r\nwhile constructing the object.\r\n\r\nTest plan - unit testing and travis tests.\r\n\r\nfixes https://github.com/prestodb/presto/issues/15254\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["15256", "Add thrift serde support for TaskStatus", "Ajay George", "ajaygeorge", "10/14/20, 03:06:16 AM", "depended by facebookexternal/presto-facebook#1231\r\n\r\nAdd thrift serde support for TaskStatus.\r\n\r\nAdded Drift annotations to TaskStatus and related classes to support thrift serde.\r\n\r\nTest plan - Unit tests\r\nVerifier tests : https://www.internalfb.com/intern/presto/verifier/results/?test_id=46590\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["15257", "Add unit tests on async cache restore", "Bin Fan", "apc999", "10/02/20, 10:47:15 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15258", "Add a new deployment example of running Presto on a laptop", "Bin Fan", "apc999", "10/08/20, 07:23:30 AM", "Add documentation for a new deployment example in addition to docker, \r\nallowing users to deploy Presto to connect Hive Metastore and query S3.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15260", "Adding array functions pushdown to pinot", "Xiang Fu", "xiangfu0", "10/04/20, 10:46:56 PM", "Support pushing down Array functions to Pinot.\r\n- Array functions(`array_sum`, `array_min`, `array_max`, `array_average`) inside aggregation functions or group by clause  to Pinot.\r\n- Boolean functions in Predicate: `contains`\r\n\r\nE.g. For Presto query: `SELECT sum(array_max(col)) FROM myTable WHERE contains(col, 1)` , the pushdown pinot query is: `SELECT sum(arrayMax(col)) FROM myTable WHERE col = 1`\r\n\r\n\r\nTest plan:\r\nUnit tests and tested with local setup for array functions(`array_sum`, `array_min`, `array_max`, `array_average`, `contains`) pushdown to Pinot:\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n* Support pushing down array functions `array_sum`, `array_min`, `array_max`, `array_average`, and `contains` to Pinot connector.\r\n```\r\n\r", "NaN"], ["15262", "Disable broadcast join optimization for Presto on Spark", "Andrii Rosa", "arhimondr", "10/05/20, 05:13:33 PM", "Nullifying iterator does not work if the executor containers are reused, as the broadcast variables persistent between tasks. Nullifying a broadcast variable will make it invalid for the next task run on the same executor.\r\n\r\nAs a workaround for increased broadcast memory on executors compression has been added and additional check for broadcast size is added.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15266", "Add identity projection with partial filter failure rate to BenchmarkPageProcessor", "Ying", "yingsu00", "10/06/20, 06:33:24 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15269", "Add option to do more precise ACL checks", null, "prithvip", "10/17/20, 12:47:11 AM", "Before this change, Presto would check for column access permission on\r\nall columns referenced in any part of the query.\r\nThis behavior can sometimes be undesirable, for example, in this query:\r\n\r\n``SELECT name FROM (SELECT * FROM nation)``\r\n\r\nDuring execution of this query, access checks would be performed on all\r\ncolumns in the table nation, even though only the column ``name`` would\r\nactually be read during the execution of the query, and the other\r\ncolumns in the table have no impact on the query results.\r\n\r\nThis change introduces a new sesion property,\r\n``check_access_control_on_utilized_columns_only``, which, when\r\nenabled, will only perform access control checks on columns that would\r\nactually be required to produce the query output, ignoring columns that\r\nare referenced in the query, but are not required to compute the query\r\nresults.\r\n\r\n\r\nTest plan - \r\n1. Unit tests to cover a good range of query shapes. \r\n2. Test on production workload queries \r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add session property ``check_access_control_on_utilized_columns_only``, which, when enabled, only performs access control checks on columns that would actually be required to produce the query output, ignoring columns that are referenced in the query, but are not required to compute the query results.\r\n```\r", "NaN"], ["15270", "Add release notes for 0.242", "Leiqing Cai", "caithagoras", "10/10/20, 01:23:20 AM", "# Missing Release Notes\n## Ariel Weisberg\n- [x] https://github.com/prestodb/presto/pull/15243 Link to attribution guidelines in PR template (Merged by: Andrii Rosa)\n\n## Daniel Ohayon\n- [x] https://github.com/prestodb/presto/pull/15219 Core support for enums, take #2 (Merged by: Rongrong Zhong)\n\n## Mayank Garg\n- [x] https://github.com/prestodb/presto/pull/15187 Consider predicate columns for encryption (Merged by: Rebecca Schlussel)\n\n## Nikhil Collooru\n- [x] https://github.com/prestodb/presto/pull/15044 Add support for versioning metastore API (Merged by: Shixuan Fan)\n\n## Rongrong Zhong\n- [x] https://github.com/prestodb/presto/pull/14012 Remote plan execution (Merged by: Rongrong Zhong)\n\n## Saumitra Shahapure\n- [x] https://github.com/prestodb/presto/pull/15128 Fixing bug: IS DISTINCT FROM NULL should work for NULL column values (Merged by: Rongrong Zhong)\n\n## Xiang Fu\n- [x] https://github.com/prestodb/presto/pull/15135 Pinot Connector: Adding limit to push down order by for broker query (Merged by: Zhenxiao Luo)\n\n## Yuya Ebihara\n- [x] https://github.com/prestodb/presto/pull/15147 Support smallint, tinyint, date type of Cassandra (Merged by: James Sun)\n\n## asdf2014\n- [x] https://github.com/prestodb/presto/pull/15148 Bump Apache Druid to 0.19.0 (Merged by: Zhenxiao Luo)\n\n# Extracted Release Notes\n- #14974 (Author: Nikhil Collooru): Rename files written by PageSink\n  - Add support for file renaming for Hive connector. This can be enabled with ``hive.file_renaming_enabled`` configuration property.\n- #15074 (Author: Sanjay Sundaresan): Add support in parquet reader for reading TIMESTAMP_MICROS type.\n  - Parquet files written by parquet-avro library that uses TIMESTAMP_MICROS as the OriginalType to represent timestamp can now be queried by presto.\n- #15101 (Author: Leiqing Cai): Support explain verification\n  - Add support to run explain verification. (:pr:`15101`). This can be enabled by configuration property ``explain``.\n- #15108 (Author: Mayank Garg): Add property to limit output size of a query\n  - Add session property ``query_max_output_bytes`` and configuration property ``query.max-output-bytes`` to control how much data a query can output.\n- #15132 (Author: Leiqing Cai): Support fetching queries to be verified using a Presto query\n  - Add support to fetch the list of queries to be verified by running a Presto query.\n- #15155 (Author: Shixuan Fan): Support fragment result caching\n  - Add support for fragment result caching. When enabled, if the same plan fragment and same connector split hit the same worker, engine would directly fetch result from cache and skip computation. Currently only partial aggregation is supported. Cache could be enabled by setting ``fragment-result-cache.enabled`` to ``true`` and tuned by other configs started with ``fragment-result-cache``. Query could use fragment result cache by setting config ``experimental.fragment-result-caching-enabled`` or session property ``fragment_result_caching_enabled`` to ``true``.\n  - Add ``getSplitIdentifier`` to ``ConnectorSplit``. Split identifier is used in fragment result caching to identify if splits are identical.\n  - Add ``getIdentifier`` to ``ConnectorTableLayoutHandle``. Layout identifier is used in fragment result caching to construct canonical plan.\n- #15163 (Author: James Petty): Fix performance regression when hive SerDe doesn't prefer Writables\n  - Fix a performance regression for String field handling in GenericHiveRecordCursor when the SerDe does not provide an efficient Writable implementation.\n- #15173 (Author: Saksham Sachdev): Implement Spilling Strategies\n  - Add config `experimental.spiller.task-spilling-strategy` for choosing different spilling strategy to use.\n- #15181 (Author: Leiqing Cai): Fix LimitQueryDeterminismAnalyzer\n  - Fix an issue in determinism analysis would indicate failing due to data being changed while the data is not changed.\n- #15183 (Author: Rongrong Zhong): Fix compiler error in LambdaBytecodeGenerator\n  - Fix compiler error in LambdaBytecodeGenerator when CSE is enabled and multiple SQL functions contain lambda expressions.\n- #15191 (Author: Palash Goel): [Verifier] Add application-name config\n  - Add application-name config to override source passed in ClientInfo.\n- #15198 (Author: Leiqing Cai): Improve plan comparison for explain verification\n  - Improve JSON plan comparison for explain verification. (:pr:`15198`).\n- #15199 (Author: Rebecca Schlussel): Fix encrypt large value\n  - Fix a bug where DWRF encryption would fail for large uncompressed column values.\n- #15200 (Author: James Gill): Upgrade Esri to 2.2.4\n  - Upgrade Esri to [2.2.4](https://github.com/Esri/geometry-api-java/releases/tag/v2.2.4).  This includes two fixes for bug (https://github.com/Esri/geometry-api-java/issues/266 and https://github.com/Esri/geometry-api-java/issues/247) that were seen in production.\n- #15208 (Author: James Petty): Clamp BackgroundHiveSplitLoader concurrency to usable parallelism\n  - Improve split loading thread utilization by reducing the number of tasks submitted for small scans.\n- #15214 (Author: Rebecca Schlussel): Fix order of dwrf encryption group stats\n  - Fix a bug where non-Presto readers could not read encrypted DWRF files written by Presto if the encryption group listed columns out of order.\n- #15220 (Author: James Petty): Fix potential deadlock when PageBufferClient exceeds memory limit\n  - Fix possible permanent stuck queries when memory limit exceeded during shuffle.\n\n# All Commits\n- 4cd2141ab8b3f7d596568b9acebe247941528789 Add integration tests for file renaming (Nikhil Collooru)\n- d3ef69a8796f25f8773b95fef8addd5a55646df3 Add config parameters to enable hive file renaming (Nikhil Collooru)\n- a78669969ae212476c6f8301d4fb14f88e8528f5 POST the metadata results to worker (Nikhil Collooru)\n- c2b2b0d9cc80f08edb931a849edfd2ebe55d5352 Rename files written by HivePageSink (Nikhil Collooru)\n- a74b034525bfe1541a510eaa408a97cacb1a77b0 Add new POST {taskId}/metadataresults endpoint on worker (Nikhil Collooru)\n- f15fe1f678751dff123f2920c2996d6d90e498f6 Piggyback MetadataUpdateRequests to coordinator as part of TaskInfo (Nikhil Collooru)\n- 9eaa655ac8f7719898abd5fea6923dc3d408f0d6 Support fragment result caching (Shixuan Fan)\n- 99445d9d1f56b0701cc1b5e18cdbd09aa6bcf089 Use static import and lambda in TestDriver (Shixuan Fan)\n- 1489cac84ee7332493113bd5f849e74cf637a54e Introduce fragment result cache manager (Shixuan Fan)\n- 069629bae15bca84a4c9128c8eb46ba595dd0380 Introduce split identifier (Shixuan Fan)\n- a1b91767c69a48d2e57d0d287b64bdef172d4b9e Introduce CanonicalPlanGenerator (Shixuan Fan)\n- 53aa7f67c8156a904c9114e4faae2586c630968c Reserve memory before projecting rows (Ying Su)\n- 00e61e4ff38bb9ec0782f57db11f81156a689f7b Do not ignore table bucketing when query uses bucket column (Vic Zhang)\n- d57ce0f0ed34647883daeb821dfd92196893072c Remove TypeManager.resolveOperator (Rongrong Zhong)\n- e6ad497de2d3fba67be46a018878e7648154cd1a Collapse nested DEREF expressions into a single one. (Sreeni Viswanadha)\n- d8be70d5fc70731e292d1a101d9213cb77bae802 Add extra config and session property to toggle join spilling (Saksham Sachdev)\n- 9a1190801eb856db518d2e673a1481d9f65b7453 Use newer Alluxio client (Bin Fan)\n- 544b5a4313fac313ddf6534e7740c2ddfd0c0bd5 Introduce TaskSpillingStrategy and multiple spilling strategies (Saksham Sachdev)\n- 30e46063c0b3872a7d1422a1a83855ecbfe98911 Link to attribution guidelines in PR template (Ariel Weisberg)\n- cef58ff2037ed4445d7871c8dae1b75a5d14b4ef Add enum operators (Daniel Ohayon)\n- 9fe32b502815daa67d52ccd084e662aa9f0782d4 Support enum literals in queries (Daniel Ohayon)\n- 7e880e6de3fc41b5585da0ae6c502bc75a508f77 Support type bound in TypeVariableConstraint (Daniel Ohayon)\n- 5e209739f57bcb0924a5b4725a1af20e0960f981 Add long and varchar enum types (Daniel Ohayon)\n- 2dfb7cd69c3b2a321e169f4f248446d62c33e1b8 Remove BlockEncodingManager's dependency on TypeManager (Rongrong Zhong)\n- ef5843ad6b1c44743909ba0f81f0c9ad510cb9f0 Make TestQueryManager#testFailQueryPrerun more reliable (Tim Meehan)\n- b57f7ff9d1c6de6f00e3f9f43b8e4a3f04f3f467 Upgrade Esri to 2.2.4 (James Gill)\n- 139f15aad6b3dd0d77e32a959f6ac28a3aecd13b [Verifier] Add application-name config (Palash Goel)\n- e2ff32eb8e15a7efe8f34d9ad3015dd004eda923 Simplify PinotQueryGenerator pushdown logic (Zhenxiao Luo)\n- 2d1416eb77e5dc06f0398b054216b54bd278f2cd Pinot Connector: Adding config to push down order by for broker query (Xiang Fu)\n- bb0d04ada5c72a24d99494630f54a11203340ab2 Add docs for exchange materialization (Wenlei Xie)\n- 6b97e58656d7c102dce0649eb41128bad7634562 Update peak memory in TaskContext for Presto-on-Spark task (Wenlei Xie)\n- c9e550bf49d35bf426e43feb131b8d5921259c96 Minor tweak in Presto-on-Spark peak task user memory report (Wenlei Xie)\n- ebf428a76a4a862e681546b16ff4805c2fba7d4c Add peak task user memory to TaskStats (Wenlei Xie)\n- 46bcc6314a41f42aa756fbdaa05b3dcd003857d9 Add comment to SerializedTaskInfo (Wenlei Xie)\n- 746824c1c0331f1449b5b0b97f21bf9a7f55fd5d Fix potential deadlock when PageBufferClient exceeds memory limit (James Petty)\n- 35f267df72b8140fcc9610a92dcc787daa22753b Disable Elasticsearch connector flaky test (Zhenxiao Luo)\n- 8ae554dc9c1434c3e22b1c6c94fc45ed478012fa Enable nullifying iterator for broadcast join (Andrii Rosa)\n- 77bfa04063ef5b7d636b3b3034c9292f4ce730ca Cassandra connector ccreate table (Yuya Ebihara)\n- c857741afb79f656e0ec21d5b1cb1d7366fc439e Fix order of dwrf encryption group stats (Rebecca Schlussel)\n- d57bbfda5fcf839246e169a134c44947a768dbda Implement pass through mode UNION ALL on Presto-on-Spark (Vic Zhang)\n- 4d91a6016c8bfeb77a80c59fd7aa1cac91df2a4b Improve plan matching for explain verification (Leiqing Cai)\n- fdfd9bec9140be74ed2d4b92907fcc4b98609a51 Remove special rule to skip control queries for explain verification (Leiqing Cai)\n- 1fc69dcce88e596fd9232c5696ead3bd1df78025 Handle storage connection exceptions correctly (Nikhil Collooru)\n- 6bb12b633eec281d0619b84d2851219511cbc018 Fix error message formatting (Rebecca Schlussel)\n- da0d5c2945610e3aa8a88aa3108784bc72eebcea Fix dwrf encryption for large column values (Rebecca Schlussel)\n- 913be2838e939cdba851a978dbfc46b018bcea94 Clamp BackgroundHiveSplitLoader concurrency to usable parallelism (James Petty)\n- 79c3a3f26e4e771eed3f568d0ead36f41131d72c Improve Elasticsearch connector documentation (fornaix)\n- 00fbb9b371b6676129403ff48cafb2f0b89def43 Update default Presto Pinot Connector Configs (Xiang Fu)\n- 14ba79ce5c811636e2281b4fc1e7e519e01b16c8 Refactor more operators to use applicate Page method helpers (James Petty)\n- 20a6c406baf2a4cb5facc2b13925dc42d66cea09 Add Page#extractChannel(int) to enable efficient single channel extraction (James Petty)\n- fa513e2fe824845676c2299f12754853a83547e4 Consider predicate columns for encryption (Mayank Garg)\n- c0b38fe9c9e9c1b5b194c359f3f63eeb13c671b8 Disallow partial aggregation pushdown when (Vivek)\n- c065b1ddd8b618b6471250197f00a2e2a2fe7ff3 Do not support dynamic filtering with grouped-execution (Ke Wang)\n- 244d8804f26f7c43ffebdf118656e33b56e5db14 Update drift library to 1.28 (Vic Zhang)\n- 95bcfcda9b6d42de7233f2f9a779813f463b4202 Include storage format name to temporary table name (Vic Zhang)\n- 28bd920dd26c1de4336bb7986a5cf8dc6434af36 Use PAGEFILE format for hive unsupported type (Vic Zhang)\n- 2d228dba8ca5203e33818a12ca8d7c806c9c45d1 Add session property to automatically use PAGEFILE (Vic Zhang)\n- 20b72bdc27d410397f0ebe0db4acf6765286a80a Parquet code cleanup (Zhenxiao Luo)\n- 4b8021ff468b089eb4abef521c2bd1628a18317c Add unit tests for partition versioning enabled Metastore (Nikhil Collooru)\n- 584fd8cb9b02479f8042be4e016aebb5aa3b9f87 Add new getPartitionNamesWithVersionByFilter metastore api (Nikhil Collooru)\n- cdbd17701c5575c357ac84fe2cad9b20ee86da49 Add config to enable Partition versioning (Nikhil Collooru)\n- 9dcc558428e282cd34378e66787da034a096420e Add optional version parameter to metastore Partition (Nikhil Collooru)\n- 9ae10f0fbc5e2daa2108a75a4f0dd8b06edbed35 Skip unsupported types when pushing down in Cassandra (Yuya Ebihara)\n- ad2a1c7b913a9d3eb3f137eca07953f97779025a Add support in parquet reader for reading TIMESTAMP_MICROS type. (Sanjay Sundaresan)\n- d9c8807bb75c4abe8c95e50866e824b78482d5f2 Remove unused Parquet classes (Vivek)\n- b88a9a91b24500d3c46df8cd613783effdc9d70c Fix compiler error in LambdaBytecodeGenerator (Rongrong Zhong)\n- 30d258803edab0d22d8725344d120515611cab52 Remove unused configuration property from properties.rst (Ying Su)\n- 4ea4139e5fd9893e350a2af710c5ccadbbc2fa73 Revert \"Add long and varchar enum types\" (Ying Su)\n- 748e69eee4358eb9ba10d0685a4c942b20ce45ec Revert \"Support type bound in TypeVariableConstraint\" (Ying Su)\n- 93bf9be6b0e511578ab801a31e64d41a8c3021b7 Revert \"Support enum literals in queries\" (Ying Su)\n- 923d3ed904882deb2b15de3cc8ad6e37ae93e326 Revert \"Add enum operators\" (Ying Su)\n- 77c442cf6b64c3d57d10da186310ef735baa3222 Revert \"Fix IllegalArgumentException in RelationPlanner\" (Ying Su)\n- d79804dc483ade179834fef2c5469632a41e0416 Fixing bug: IS DISTINCT FROM NULL should work for NULL column values (Saumitra Shahapure)\n- ee1627e0fd74ab65446213d5fcac6ed1ab101fd7 Bug fix in LimitQueryDeterminismAnalyzer (Leiqing Cai)\n- 600c15755840cd3ff4857ded1c06934b2eb7ff98 Avoid DateTimeZone.getDefault() in GenericHiveRecordCursor hot path (James Petty)\n- e7e8ecc97f720cdc94386a5e892e9dea65370b5a Fix performance regression when Hive SerDe doesn't prefer Writables (James Petty)\n- 06d5f49a22845d3f923dd1259ca8fcd1a9869ba1 Support smallint, tinyint, date type of Cassandra (Yuya Ebihara)\n- 5bdb41d3c8d003290affac6f179b030b9de41a20 Parser warnings when the query contains mixed AND/OR operator without proper parenthesis. (ankit0811)\n- df8228fe7265ced46c1dc2ed92371d37998212e6 Fix wrong group by column definition in druid connector (Weidong Duan)\n- af273e1a86b5f161d2f8402bd3ae052af2eb9a65 Fix broken link in presto-verifier README (Palash Goel)\n- faebf2a918196895a6929069346c9cc249329cda Add more functions to TestHiveExternalWorkersQueries (Masha Basmanova)\n- 2108955ddce372cfe76275370e04988f8c08dab3 Add waitForMinimumCoordinators to ClusterSizeMonitor (Tim Meehan)\n- dd7dad97aaf9e4b03d8ac87bac71feb9ae7a96e4 Fix IllegalArgumentException in RelationPlanner (Rongrong Zhong)\n- aaeff3f8360a09c1f64c8feb05354e1ca527d37d Support Remote function execution (Rongrong Zhong)\n- 3825bdb0078665a12e2671468589f59e977fbdb8 Remote function execution with thrift executor (Rongrong Zhong)\n- b1b5e8f276485d8faf73f3fe3abf5812a9e09b34 Fix ProjectNode locality in PlanFragmenter (Rongrong Zhong)\n- 219c7d76eb42df406213e3ff4f9e8428028fc54e Introduce large dictionary mode in SliceDictionarySelectiveReader (Ying Su)\n- 0006741cc9a2ed8474a15c8e7556bd20f9b8c0f7 Remove stripeDictionaryData buffer in SliceDictionarySelectiveReader (Ying Su)\n- 79947540306b3ccc774102c6ca069d9147ea0352 Defer the creation of dictionary in SliceDictionarySelectiveReader (Ying Su)\n- 93d1b046a53d72b101bd2bba5cb21e78a5737849 Bump Apache Druid to 0.19.0 (asdf2014)\n- 405ebe6f8c4af2d0ee9aa4717c0da2d57bc2e42c Adding multivalued column support in Pinot connector (Dharak Kharod)\n- ec9b5cf8177ff3bfb63a5a4c9a52c77d9986fa33 Add tests for sum, min, max and avg queries running on native workers (Masha Basmanova)\n- 926df095ab79752545d61eba0513362415736ca3 TupleDomain cleanup (Zhenxiao Luo)\n- a7bf06f81b37364fecaf242fdc2cc2f3342ab7fc Add property to limit output size of a query (Mayank Garg)\n- 1bc26f0c8223d5fe52957b7b3778b626fb0fddd6 Dynamic partition pruning on workers (Ke Wang)\n- 710f04e002587a7617db107b76b60bd3ebb67c41 Support explain verification (Leiqing Cai)\n- b388a1bca8e98e403dcbe332d43510ba3c2dfe9d Extract class QueryBundle (Leiqing Cai)\n- d3855832408038a07a6cb39597646d574ee78143 Extract interface MatchResult (Leiqing Cai)\n- f2d19cff05a3687fe8c80f3a4a4d80c4eb682283 Extract determinism analysis from AbstractVerification (Leiqing Cai)\n- b050fc1ba814f7093066766b670578199c8caab7 Extract query rewrite from AbstractVerification (Leiqing Cai)\n- aae01573a3e9d2e40061d84fead33ed665b62a47 Minor fixes in Verifier (Leiqing Cai)\n- f3a43a8cc0a3e3cdb65deb48a24b949f5b999600 Add test infrastructure to run queries using native workers (Masha Basmanova)\n- 2e179d3ff936ab790baf23beaa6908385cbc3d8e Support fetching queries to be verified using a Presto query (Leiqing Cai)\n- 6a763b274c161a783c1fca1eedea277f8c990346 Cleanup of partial aggregation pushdown for ORC/Parquet (Zhenxiao Luo)\n- b66f1028ecaac6f0475ea04dd761ffff3b375be1 Remove dependency on javafx (Vivek)\n- baece53fc9cb467052dab7538f8703102665d819 Restore commented out tests (Ying Su)", "NaN"], ["15271", "Remove unnecessary config and session property", "Rongrong Zhong", "rongrong", "10/07/20, 07:22:48 PM", "optimizer.optimize-full-outer-join-with-coalesce and the corresponding session\r\npropoerty optimize_full_outer_join_with_coalesce are no longer needed as the\r\nfeature is already fully rolled out and enable it is always more beneficial when\r\nit could be applied.\r\n\r\nTest plan - Travis. I `zbgs`ed in Facebook codebase and didn't find any reference of this session config.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Remove config property `optimizer.optimize-full-outer-join-with-coalesce` and the corresponding session property `optimize_full_outer_join_with_coalesce`. The feature will always be enabled.\r\n\r\n```\r", "NaN"], ["15272", "Improve identity projection when the selectedPositions is a list", "Ying", "yingsu00", "10/14/20, 02:14:46 PM", "This PR optimizes identity projection with two ways:\r\n\r\n1. Create DictionaryBlock when the selectedPositions is a list;\r\n2. Use a faster and more memory efficient block size calculation.\r\n\r\nBenchmarkPageProcessor shows 21x improvement in throughput, 46x reduction in gc.alloc.rate.norm, and 9x reduction in\r\nGC count and time:\r\n\r\nBefore\r\n\r\n```\r\n    Benchmark                                                                    Mode  Cnt       Score      Error   Units\r\n    BenchmarkPageProcessor.identityProjection                                   thrpt   50  125261.816 \u00b1 5342.200   ops/s\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.alloc.rate                    thrpt   50    2439.004 \u00b1  103.990  MB/sec\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.alloc.rate.norm               thrpt   50   40832.029 \u00b1    0.033    B/op\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.churn.PS_Eden_Space           thrpt   50    2461.428 \u00b1  364.315  MB/sec\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.churn.PS_Eden_Space.norm      thrpt   50   41439.660 \u00b1 6521.978    B/op\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.churn.PS_Survivor_Space       thrpt   50       0.016 \u00b1    0.016  MB/sec\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.churn.PS_Survivor_Space.norm  thrpt   50       0.257 \u00b1    0.267    B/op\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.count                         thrpt   50      46.000             counts\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.time                          thrpt   50     181.000                 ms\r\n```\r\n\r\nAfter\r\n\r\n```\r\n    Benchmark                                                                    Mode  Cnt        Score        Error   Units\r\n    BenchmarkPageProcessor.identityProjection                                   thrpt   10  2687127.250 \u00b1 337956.918   ops/s\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.alloc.rate                    thrpt   10     1118.285 \u00b1    141.900  MB/sec\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.alloc.rate.norm               thrpt   10      872.001 \u00b1      0.005    B/op\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.churn.PS_Eden_Space           thrpt   10     1050.013 \u00b1   1683.604  MB/sec\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.churn.PS_Eden_Space.norm      thrpt   10      827.947 \u00b1   1329.288    B/op\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.churn.PS_Survivor_Space       thrpt   10        0.012 \u00b1      0.040  MB/sec\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.churn.PS_Survivor_Space.norm  thrpt   10        0.011 \u00b1      0.034    B/op\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.count                         thrpt   10        5.000               counts\r\n    BenchmarkPageProcessor.identityProjection:\u00b7gc.time                          thrpt   10       21.000                   ms\r\n```\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15274", "Add eligibleToIgnore flag to Hive partitions", "James Sun", "highker", "10/07/20, 01:48:53 AM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1217\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15276", "Add local cache for Parquet Metadata", "Vivek", "ClarenceThreepwood", "10/14/20, 12:55:23 AM", "Test plan - TravisCI and unit tests in AbstractTestParquetReader\r\n\r\nFixes #13921\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add caching module for Parquet metadata\r\n```\r", "NaN"], ["15278", "Rename PartitionVersionFetcher to PartitionMutator", "James Sun", "highker", "10/07/20, 07:43:24 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1220\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15279", "Add a test for adding a new column", null, "mayankgarg1990", "10/15/20, 03:42:31 PM", "Test plan - This PR adds a test and will ensure that it succeeds\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15281", "Ignore broadcast memory limit in verifier", null, "aweisberg", "10/07/20, 10:58:15 PM", "Verifier unit tests\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier\r\n* Improve verifier handling of broadcast memory limit by removing it if it is set via session property to avoid CBO broadcast induced failures on smaller verifier clusters.\r\n```\r\n\r", "NaN"], ["15286", "Enable reading of FileStatistics in presto-orc", "Sergii Druzkin", "sdruzkin", "10/20/20, 09:46:02 PM", "Move intermediate encryption keys from the OrcRecordReader to the OrcReader's constructor.\r\nEnable reading of FileStatistics for both encrypted and unencrypted ORC files.\r\n\r\n== Test plan ==\r\n```\r\nmvn clean install -DskipTests\r\nmvn clean install -pl presto-orc -pl presto-hive -pl presto-raptor\r\n```\r\n\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Move IEK from OrcRecordReader to OrcReader's constructor\r\n* Enable reading of FileStatistics for ORC files\r", "NaN"], ["15287", "Allow nullable cache base directory", "Bin Fan", "apc999", "10/10/20, 12:19:06 AM", "Test plan - Tested locally\r\n\r\nAvoid throwing NPE when user didn't set `cache.base-directory`. In this case, cache base directory will  be the default value set by `alluxio.user.client.cache.dir` property in Alluxio \r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15288", "S3 assume iam role", "Ashish", "ashishtadose", "10/12/20, 08:16:36 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* AWS SDK upgrade to leverage EKS grant feature\r\n* S3 configuration support to assume IAM role\r\n* Disabled default S3 instance credential behavior to provide configurability to choose from instance creds or IAM role\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nresolves #15158 ", "NaN"], ["15289", "Revert \"GC issue fixes in SliceDictionarySelectiveReader\"", "Ke", "kewang1024", "10/08/20, 06:53:16 PM", "Reverts prestodb/presto#15130", "NaN"], ["15292", "Add syntax support for CREATE MATERIALIZED VIEW", "Ge Gao", "gggrace14", "11/23/20, 07:54:19 AM", "Make Presto parser support CREATE MATERIALIZED VIEW syntax, ex.\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS mv\r\nCOMMENT 'A simple materialized view'\r\nWITH (\r\n\tpartitioned_by = ARRAY['ds'],\r\n\tretention_days = 90\r\n)\r\nAS SELECT * FROM t\r\n\r\nWe do not support REPLACE MATERIALIZED VIEW at the moment,\r\nsince that is possible to trigger the entire view refresh.\r\nUsers could use DROP and CREATE a new materialzied view instead.\r\n\r\nThis commit rewrites a subset of\r\nprestosql/presto@88116a4\r\ndue to above reason.\r\n\r\nTest plan -\r\n1. Add unit tests to TestSqlParser\r\n2. mvn test\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15293", "Support formatting Use statement", "Leiqing Cai", "caithagoras", "10/12/20, 07:27:54 PM", "Before the fix, `explain use catalog.schema` would fail with generic internal error.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix query failure when explaining ``Use`` statement.\r\n```\r", "NaN"], ["15295", "Restore #15130 with null handling fix", "Ying", "yingsu00", "10/30/20, 08:42:36 AM", "#15130 was reverted due to a bug handling nulls. It introduced a nullsCount variable that intended to early return a RunLengthEncodedBlock when all output positions are null even when allNulls is false. This was theoretically possible: suppose outputPositions = [1, 3], outputPositionCount = 2, but both position 1 and 3 are null,\r\nthen there is no need to create the normal block. However there are some cases that the outputPositionCount was dropped after applying filter functions. In such cases, if the dropped outputPositionCount happens to be equal to nullsCount, a RunLengthEncodedBlock of all nulls would be incorrectly created. \r\n\r\nThis PR restores #15130, and fixed the above mentioned bug. It just removes this optimization and only return the RunLengthEncodedBlock of nulls when allNulls is true.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15296", "Minor improvements for partitioned output buffers and operator", "James Petty", "pettyjamesm", "10/20/20, 01:27:26 PM", "Comparable changes to https://github.com/prestosql/presto/pull/5499\r\n\r\n- Adds fast path for partitioning without constant arguments\r\n- Simplify iterating through `PartitionedOutputBuffer` pages to one pass\r\n- Use known-size builders where possible\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15302", "Allow qualified names in types", "Daniel Ohayon", "daniel-ohayon", "10/16/20, 06:58:29 PM", "Currently, type names used in CREATE TABLE or CAST statements are expected to be `identifiers`.\r\nInstead, we want them to be `qualifiedNames` so that we can use user-defined types with a namespace, like `testing.test.mytype`.\r", "NaN"], ["15303", "Prefer verifier cluster memory limits", null, "aweisberg", "10/20/20, 06:50:35 PM", "Verifier unit tests\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier\r\n* Improve verifier handling of memory limits by using the limits from the verifier cluster which are frequently better matched to the queries picked to be run in the verifier suite\r\n```", "NaN"], ["15304", "Handle cases when flat map doesn't have dummy encoding for sequence 0", null, "zzhao0", "10/14/20, 05:53:38 PM", "Current reader assumes that the value node of a flat map always have dummy\r\nencoding metadata for sequence 0. However, that may not be true especially\r\ngiven that such metadata is not needed in the actual read process. Handle case\r\nwhen sequence 0 is missing.\r\n\r\nTest plan\r\n\r\n- Added unit tests that read against a file with missing sequence 0.\r\n\r\n== NO RELEASE NOTE ==\r", "NaN"], ["15305", "Add session property remote_functions_enabled", "Rongrong Zhong", "rongrong", "10/13/20, 06:40:55 PM", "Test plan - travis\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add session property ``remote_functions_enabled`` to configure whether remote functions are allowed.\r\n```\r", "NaN"], ["15309", "Bug fixes for file_renaming feature", "Nikhil Collooru", "NikhilCollooru", "10/14/20, 08:15:08 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15311", "Refactor ParquetPageSource", "Zhenxiao Luo", "zhenxiao", "10/15/20, 12:48:26 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15313", "Add type to FunctionNamespaceManager", "Rongrong Zhong", "rongrong", "11/10/20, 08:10:38 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1264\r\n\r\nTest plan - Travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15316", "Fix caching and case-sensitivity issues with enum type signatures", "Daniel Ohayon", "daniel-ohayon", "10/23/20, 05:09:30 PM", "* This adds the hashCode and equals method to enum types, which fixes a caching bug for enum UDF lookup \u2013 without this, `CAST(string as SomeEnum)` always resolved to the same cast function implementation of the first enum encountered\r\n* By fixing this bug, I uncovered another bug: in a number of places, we assume that TypeSignatures are case-insensitive and we lowercase them \u2013 however the way we represent varchar enum type signatures is not case-insensitive because we store the map's values. Here I encode them in Base 32, which is a [case-insensitive encoding](https://en.wikipedia.org/wiki/Base32#Advantages). I figured this would be more robust than trying to patch all the places where we make the assumption, and break it only in specific cases.\r\n\r\n\r\nIf instead of using Base32, we go for requiring case-sensitive signatures, we'd need at least to change logic in `sql/tree/Cast.java`, `sql/tree/QualifiedName.java` and `common/function/QualifiedFunctionName.java` \u2013 one complication there is that we'd also make function signatures case-sensitive, especially `$literal` function names \u2013 this seemed like a problematic thing to introduce just for this one type, so I went for the Base32 \"trick\" instead.\r\n\r\n\r\n## Benchmarking\r\n\r\nI added benchmarks for `parseTypeSignature` on varchar and long enum signatures with 35,000 keys. Results on my Mac are as follows (added the row type signature times for comparison). Takes around 150ms per call for the varchar enum signature, and 60ms per call for the long enum signature:\r\n```\r\nBenchmark                                                       Mode  Cnt    Score    Error  Units\r\nBenchmarkTypeSignatureParsing.parseBigLongEnumTypeSignature     avgt   10   60.033 \u00b1 10.942  ms/op\r\nBenchmarkTypeSignatureParsing.parseBigVarcharEnumTypeSignature  avgt   10  155.382 \u00b1  6.885  ms/op\r\n\r\nBenchmarkTypeSignatureParsing.parseRowTypeSignature             avgt   10    2.269 \u00b1  0.297  ms/op\r\nBenchmarkTypeSignatureParsing.parseRowTypeSignatureWithEnums    avgt   10    2.596 \u00b1  0.204  ms/op\r\n```\r\n\r\nThese numbers are a bit concerning  given that last time we saw timeouts in communication between worker-coordinator when parsing the type signatures took more than 500ms, but I don't know that we can do much better if we keep the full enum definition within the signature.\r\n\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Fix caching and case-sensitivity bugs with type signatures for enum types.", "NaN"], ["15317", "Remove deprecated grouped execution properties", "Rebecca Schlussel", "rschlussel", "10/16/20, 08:39:27 PM", "grouped_execution_for_aggregation and grouped_execution_for_join have\r\nbeen replaced by a single property grouped_execution\r\n\r\ndynamic_schedule_for_grouped_execution is enabled by default and doesn't\r\nneed a replacement.\r\n\r\nTest plan - unit tests\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Remove deprecated configuration properties ``grouped-execution-for-aggregation-enabled`` and ``grouped-execution-for-join-enabled`` and their corresponding session properties ``grouped_execution_for_aggregation`` and ``grouped_execution_for_join``.  These have been replaced by a single configuration property ``grouped-execution-enabled`` and session property ``grouped-execution``\r\n* Remove deprecated configuration property ``dynamic-schedule-for-grouped-execution`` and session property ``dynamic_schedule_for_grouped_execution``.  No replacement is needed.\r\n```\r", "NaN"], ["15319", "Pinot streaming connector", "Xiang Fu", "xiangfu0", "11/03/20, 07:29:10 PM", "Pinot recently added support of gRpc based streaming query endpoint at Pinot server level, which allows user to pull data in a streaming fashion. (https://github.com/apache/incubator-pinot/pull/5599, https://github.com/apache/incubator-pinot/pull/6027 )\r\n\r\nThis is extremely good for the use cases which requires large data scan and try to retrieve a lot of data from Pinot.\r\n\r\nComparing to the non-streaming way, Presto segment level query requires pinot server to process the query on the entire data segment and all the results have to be in memory and serialized then send back to client. In the streaming fashion, client side can specify a block size and only call next to fetch the given size of results to process then call next to fetch the next block. Pinot server side only advance and process the data  processing when client calls next. In this way, Pinot server side cpu and memory pressure is much less.\r\n\r\n\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\nTested locally with Pinot 0.6.0 with Streaming server supported.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPinot Changes\r\n* Support query Pinot server with streaming API (Requires Pinot Version >= 0.6.0.)\r\n* Enabled it by setting `pinot.use-streaming-for-segment-queries=true`\r\n```\r", "NaN"], ["15321", "Add sealedPartition flag to HivePartition", "Nikhil Collooru", "NikhilCollooru", "10/16/20, 06:50:54 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1243", "NaN"], ["15322", "Only allow constants for prepared statement parameters", "Rebecca Schlussel", "rschlussel", "12/08/20, 02:26:38 PM", "We were allowing anything without an explicit column reference, which\r\nincluded SELECT * from table and SELECT 1 from table.  Now only\r\nexpressions that reference neither tables nor columns (nor *) will be\r\nallowed.\r\n\r\nTest plan -  unit tests\r\n\r\nFixes #15297 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix an issue where prepared statements would allow some non-constant parameters.\r\n```\r", "NaN"], ["15323", "Add peak node total memory to statistics", null, "aweisberg", "10/28/20, 07:14:53 PM", "Unit tested, also deployed and checked to see that the statistics appears in downstream consumers.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add `peak_node_total_memory` to statistics to allow monitoring the peak sum of memory used across tasks at any node by a query\r\n\r\n```", "NaN"], ["15325", "Disable listFiles call to underlying storage", "Nikhil Collooru", "NikhilCollooru", "11/20/20, 11:36:00 PM", "Get the filenames and filesizes from Metastore partition object and use them to create splits. \r\nThis way we can avoid list directory call to underlying storage.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15327", "Add BigQuery Connector Support", "countryman4687", "fgwang7w", "03/24/21, 12:46:49 AM", "Test plan :\r\n- mock presto server to run UT\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* add BigQuery Connector Plugin\r\n```\r\n\r", "NaN"], ["15329", "Refactor DataSink/DataOutput to presto-common/spi", "Wenlei Xie", "wenleix", "10/19/20, 06:44:25 PM", "Test plan- Travis\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15330", "Reserve memory for nested BlockBuilders in projection", "Ying", "yingsu00", "10/23/20, 11:47:32 PM", "In bf4bf6a610 \"Reserve memory before projecting rows\" we created the\r\n    BlockBuilders new every time. However, nested BlockBuilders for complex\r\n    types would lose the status from previously created BlockBuilders such\r\n    that memory growth increased. For example, for an ARRAY(BIGINT)\r\n    BlockBuilder, the nested LongArrayBlockBuilder would just have the same\r\n    expectedEntries as the ArrayBlockBuilder although the entries in the\r\n    LongArrayBlockBuilder could be a lot more.\r\n\r\nThis commit uses the newly introduced newBlockBuilderLike() method that\r\n    estimates the expectedEntries for nested BlockBuilders from previously\r\n    created BlockBuilders. The following simple ArrayTransform query shows\r\n    the allocated long[] reduced from 118GB to 93GB on TPCH 100GB.\r\n\r\n```\r\n    select transform(l_array, x-> x+1) from lineitem_map_array;\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15332", "Fix druid reading issue", null, "adlymousa", "10/26/20, 04:42:04 PM", "This PR solves the problem of reading from druid caused by using the ARRAY_LINES result format. When applying any unsupported transformation(expression) to one of the selected columns and filtering on other columns, this will cause the dql to be build and sent to druid with all the columns in select and filter statements, leading to a larger number of columns returned to be processed, hence, parsing the wrong column if we are depending on the column indices (since columnHandles size differ from the returned data array size).\r\n\r\nTest plan:\r\nApplying the following query on wikipedia test dataset provided by druid, before and after the change:\r\n```\r\nselect if(cityname = '', 'hi', cityname), count\r\nfrom druid.wikipedia\r\nwhere __time > timestamp '2016-06-27 13:00:00';\r\n```\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nDruid Changes\r\n* Fix druid reading issue\r\n```\r", "NaN"], ["15333", "Fix access control checks for USING clause", null, "prithvip", "10/21/20, 06:36:17 PM", "This change fixes a bug where columns referenced in the USING clause of\r\na join were not checked for column access permissions. Since an\r\nidentifier in a USING clause refers to multiple columns, the\r\n``columnReferences`` field in Analysis is changed to a Multimap.\r\n\r\nTest plan - \r\nAdded unit tests \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15334", "Add release notes for 0.243", "Leiqing Cai", "caithagoras", "11/04/20, 05:43:48 PM", "# Missing Release Notes\r\n## Daniel Ohayon\r\n- [x] https://github.com/prestodb/presto/pull/15302 Allow qualified names in types (Merged by: Rongrong Zhong)\r\n\r\n## George Wang\r\n- [x] https://github.com/prestodb/presto/pull/14783 Fix #14693: support TRUNCATE function for DOUBLE and REAL (Merged by: Rongrong Zhong)\r\n\r\n## Ke\r\n- [x] https://github.com/prestodb/presto/pull/15289 Revert \"GC issue fixes in SliceDictionarySelectiveReader\" (Merged by: Leiqing Cai)\r\n\r\n## ankit0811\r\n- [x] https://github.com/prestodb/presto/pull/15216 Adding approxMostFrequent aggregate from prestoSQL (Merged by: Rongrong Zhong)\r\n\r\n# Extracted Release Notes\r\n- #14485 (Author: James A. Gill): Fix partitioned spatial joins with small spatial index\r\n  - Allow geometries outside of the spatial partitioning to match in a partitioned spatial join.\r\n- #15195 (Author: Leiqing Cai): Support CREATE VIEW and CREATE TABLE verification\r\n  - Add support to verify ``CREATE VIEW`` and ``CREATE TABLE`` queries.\r\n- #15227 (Author: Shixuan Fan): Optimize MetadataQueryOptimizer with filter pushdown enabled\r\n  - Optimize metadata query optimizer so that it does not fetch all partitions from metastore when hive filter pushdown is enabled.\r\n- #15231 (Author: Xiang Fu): Support pushdown approx_distinct(x, e) into pinot\r\n  - Support pushing down aggregation function `approx_distinct(x, e)` to Pinot connector.\r\n- #15260 (Author: Xiang Fu): Adding array functions pushdown to pinot\r\n  - Support pushing down array functions `array_sum`, `array_min`, `array_max`, `array_average`, and `contains` to Pinot connector.\r\n- #15269 (Author: prithvip): Add option to do more precise ACL checks\r\n  - Add session property ``check_access_control_on_utilized_columns_only``, which, when enabled, only performs access control checks on columns that would actually be required to produce the query output, ignoring columns that are referenced in the query, but are not required to compute the query results.\r\n- #15271 (Author: Rongrong Zhong): Remove unnecessary config and session property\r\n  - Remove config property `optimizer.optimize-full-outer-join-with-coalesce` and the corresponding session property `optimize_full_outer_join_with_coalesce`. The feature will always be enabled.\r\n- #15276 (Author: Vivek): Add local cache for Parquet Metadata\r\n  - Add caching module for Parquet metadata.\r\n- #15281 (Author: Ariel Weisberg): Ignore broadcast memory limit in verifier\r\n  - Improve verifier handling of broadcast memory limit by removing it if it is set via session property to avoid CBO broadcast induced failures on smaller verifier clusters.\r\n- #15293 (Author: Leiqing Cai): Support formatting Use statement\r\n  - Fix query failure when explaining ``Use`` statement.\r\n- #15305 (Author: Rongrong Zhong): Add session property remote_functions_enabled\r\n  - Add session property ``remote_functions_enabled`` to configure whether remote functions are allowed.\r\n- #15317 (Author: Rebecca Schlussel): Remove deprecated grouped execution properties\r\n  - Remove deprecated configuration properties ``grouped-execution-for-aggregation-enabled`` and ``grouped-execution-for-join-enabled`` and their corresponding session properties ``grouped_execution_for_aggregation`` and ``grouped_execution_for_join``.  These have been replaced by a single configuration property ``grouped-execution-enabled`` and session property ``grouped-execution``.\r\n  - Remove deprecated configuration property ``dynamic-schedule-for-grouped-execution`` and session property ``dynamic_schedule_for_grouped_execution``.  No replacement is needed.\r\n\r\n# All Commits\r\n- d5773063bb5fda35bcd974ef1ac73fff6fbca9a8 Move PageDataOutput to presto-spi (Wenlei Xie)\r\n- d879d7f5e3eb9621ac8ca133ed9132a2de4a4cfd Move DataSink/DataOutput to presto-common (Wenlei Xie)\r\n- bef17e674ead757b28a1042a5d7b909d2274548d Add option to do more precise ACL checks (prithvip)\r\n- 4479fb8749ea913a12361de5d012785659a27ce5 Remove deprecated grouped execution properties (Rebecca Schlussel)\r\n- 061f432fd3f4f1bc2f5b79687909d98f569a5151 Allow qualified names in types parsing (Daniel Ohayon)\r\n- 4fdfb89574b0e9d82b71e28cda5887de83c30028 Add sealedPartition flag to HivePartition (Nikhil Collooru)\r\n- 029ae8d9962fffcbc793e46c10ed6481d27a19b0 Add a test for adding a new column (Mayank Garg)\r\n- 84ade36f51c10c449f62aa840426fa798d4c078d Refactor ParquetPageSource (Zhenxiao Luo)\r\n- 2e0c44f9a2438909dcf8d1e1e845d83902880408 Add scale_tdigest function (leonpanokarren)\r\n- d3bd4ec0840c349ec91f938244e468824f595ab6 Bug fixes for file_renaming feature (Nikhil Collooru)\r\n- 7a122bb50b63426c64551a02a3ce62e082e4f023 Handle cases when flat map doesn't have dummy encoding for sequence 0 (Zhenyuan Zhao)\r\n- ee74e4bdcdb13ef0414506dccee058d4f1e2bbc9 Use getApproximateLogicalSizeInBytes() in PageProcessor (Ying Su)\r\n- 7bffeb531fdab7a1982130f4bc74ea61c2bae4c1 Add getApproximateRegionLogicalSizeInBytes() to Block (Ying Su)\r\n- 465644ca69ea57f76fc5b8f5b7e312c6800c0716 Create DictionaryBlock for identity projections (Ying Su)\r\n- 57a1b204f542344f1607e478744100fa78dd1138 Update drift-api module's scope to provided (Ajay George)\r\n- 118b48a2c95eef21e0051a2a28443b19453119cf Add thrift serde support for TaskStatus (Ajay George)\r\n- 213660bceff1f5f1b16c89a71406b7c9d6538d5b Add local cache for Parquet Metadata (Vivek)\r\n- f2965f681745b4054d2c62134f828cb9ffaec609 Add support for TRUNCATE function for DOUBLE and REAL (George Wang)\r\n- 4631b6d104b810b5dbe7400bb0a0bd4c6cfd4667 Add session property remote_functions_enabled (Rongrong Zhong)\r\n- 321c8b633d98220753564bce48cf5c5eed222633 Adding approxMostFrequent aggregate from prestoSQL (ankit0811)\r\n- f1fb2e045232df0ac9f4f89ce1e699855bc9e4d1 Remove setFunctionAndTypeManager from TypeRegistry (Rongrong Zhong)\r\n- ac735dbd270d3ef435102f2061818b92dcd9090e Use FunctionAndTypeManager instead of TypeRegistry (Rongrong Zhong)\r\n- 7fffbd80b8ca42bb0e6c8381d1edfd066825f9da Remove TypeManager from BuiltInFunctionNamespaceManager (Rongrong Zhong)\r\n- ca07a42e351af0863b8b29d7b397d86baf2b9328 Remove TypeManager from APIs when FunctionAndTypeManager is available (Rongrong Zhong)\r\n- 63af9357ae7e1aaf951d7da01833a2427ab262a7 Change FunctionManager to FunctionAndTypeManager (Rongrong Zhong)\r\n- 4720be3b94b1c8d97d83d98895934c5705cb09a3 Allow PrestoS3FileSystem and GlueHiveMetastore to assume an aws role (Ashish Tadose)\r\n- 8cbf330cda791a86dfe3c219b5a89b976b152b34 Allow PrestoS3FileSystem and GlueHiveMetastore to assume an aws role (Ashish Tadose)\r\n- 3ec373e05c77079dc000865bf82dc5720418779f bump aws-sdk to 1.11.697 (Ashish Tadose)\r\n- ac60c7560103ce0a08acf3bdbfbe77c2bbf7e391 Support formatting Use statement (Leiqing Cai)\r\n- ce96d881e74b3bb98c142e0c1c255d83769a3979 Fix a potential NPE by allowing nulllable cache base directory (Bin Fan)\r\n- 4de1f4d1a6c34aafa7d6fe0b6197d0e7c3e9364a Optimize MetadataQueryOptimizer with filter pushdown enabled (Shixuan Fan)\r\n- 05f387555515cf7fedfa2f9b723f40dea4095706 Fix ValuesMatcher (Shixuan Fan)\r\n- 8ab725f37277ae91e7218d156ba8158778e86704 Revert \"Defer the creation of dictionary in SliceDictionarySelectiveReader\" (Ke)\r\n- 460a4d064bc9ac547e938a98267f651b7e0dd638 Revert \"Remove stripeDictionaryData buffer in SliceDictionarySelectiveReader\" (Ke)\r\n- a8825abcfd380f3a948e3a43c1544aa169ea1e8a Revert \"Introduce large dictionary mode in SliceDictionarySelectiveReader\" (Ke)\r\n- 83e4afc56e97764a276dc69e8789d53731425676 Add a new deployment example (Bin Fan)\r\n- 03fb2f6a992d244ed867b27086caf8ea64a025f0 Ignore broadcast memory limit in verifier (Ariel Weisberg)\r\n- 9870fb8319b38b609239015f123b20eaff209a0c Rename PartitionVersionFetcher to PartitionMutator (James Sun)\r\n- 55508dfadb0cb4bd7b5d83563befdf4c9926976b Remove unnecessary config and session property (Rongrong Zhong)\r\n- 63565a9381adf51422d083a64e2602bed11f733f Improve spatial join tests (James A. Gill)\r\n- 4cd357c4b97e9eda45929d035387a37f1ac3edad Assign partition index to geometries outside of KdbTree (James A. Gill)\r\n- 89c399680986651d38b4d325d561131abe7f8e1f Add eligibleToIgnore flag to Hive partitions (James Sun)\r\n- ca06ed5756b56c7c43cdf898e6944c6877618e61 Allow partial filter failures in BenchmarkPageProcessor (Ying Su)\r\n- c0455e93f2a8e79fe0ff2398942e4d37f85cba69 Add identity projection to BenchmarkPageProcessor (Ying Su)\r\n- 883e88322b229b01a790c12a37fd2ee2ab51b26e Support CREATE TABLE verification (Leiqing Cai)\r\n- a50fb09da6a576d8c2e17d9401d8c9b625a2d966 Support CREATE VIEW verification (Leiqing Cai)\r\n- 2f6965dbfa4c67772339a56becc606f42667408e Fix injection error when failure resolvers are disabled (Leiqing Cai)\r\n- facde22ff8c2b6958ecdf7e983c973fc2dec2f11 Rename DataQueryBundle to QueryObjectBundle (Leiqing Cai)\r\n- a81016cb84fde647f91ce6ecf88a20c6a05050b3 Remove unsupported query types (Leiqing Cai)\r\n- 79a8c8f5d30878e60e1baca06d5e1f4e7c457ec5 Enforce broadcast memory limits in Presto on Spark (Andrii Rosa)\r\n- 13a8f649f87c4fc26cda51ba356b6cdcc55c72d4 Compress broadcast pages in Presto on Spark to save memory (Andrii Rosa)\r\n- d87cee599ed7cc3682bc9be26ad7eac1f697d961 Make sure PagesSerde is not used in a non thread safe manner (Andrii Rosa)\r\n- 322321d2cb7e07c93908c0f854d3e33d8a06ed7e Revert \"Enable nullifying iterator for broadcast join\" (Andrii Rosa)\r\n- 299f751df9e8858b7621a599c1748d89c137acab Support for CSV format in hive (Ashish Tadose)\r\n- 9f4203ed016f0e1df551aeb5fd8b880023bcd492 Support pushdown approx_distinct(x, e) into pinot (Xiang Fu)\r\n- 5c29aa2d06d2c9e2634b56611355226a7bc5b881 Adding array functions pushdown to pinot (Xiang Fu)\r\n- a93082917f2a62a8fd525b733ba9a849ef649180 Add unit tests on async cache restore (Bin Fan)\r\n- 6ff3052e9df1a04613157746e7e779fd54c0c2a1 Add BIGINT benchmark for projection (Ying Su)\r\n- 3a44824409b8f66fe5b8081573e0401b4cd14c90 Add GCProfilter to BenchmarkPageProcessor (Ying Su)\r\n- 186ee27aa9e7667c140925351e18f950410f9610 Add verify methods for BenchmarkPageProcessor (Ying Su)\r\n- 632c403a9e33816533739b884b4f045109bd4b78 Allow 100% filter pass rate in BenchmarkPageProcessor (Ying Su)\r\n- 1d3ef17c190ff928a3beb7cbb1e89cbe7c866cb0 Move data preparation to BenchmarkData for BenchmarkPageProcessor (Ying Su)", "NaN"], ["15335", "Introduce temporary store based spilling", "Wenlei Xie", "wenleix", "10/26/20, 09:53:09 PM", "Test plan - Travis \r\n\r\nThe first commit is based on https://github.com/prestodb/presto/pull/15312 and https://github.com/arhimondr/presto/commit/d8262c81368f6f0854a43ae114fe4304fddce97b\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15336", "Variable name fix for taskSpillingStrategy", "Wenlei Xie", "wenleix", "10/20/20, 05:18:41 PM", "Test plan - Travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15338", "Change warning to ignore unreadable partitions to be a digest", "Mohamed Shafee", "Moe82", "10/28/20, 04:03:11 AM", "This PR is for issue [#15285](https://github.com/prestodb/presto/issues/15285)\r\n\r\nHiveSplitManager creates a PrestoWarning for each unreadable partition within a partitionBatch:\r\n\r\n```\r\nTable <table name> partition <partition name> is not readable: <reason>\r\nTable <table name> partition <partition name> is not readable: <reason>\r\nTable <table name> partition <partition name> is not readable: <reason>\r\n```\r\nThis is excessive and can result in memory/readability issues for a large number of unreadable partitions. With the changes in this PR,\r\n1 PrestoWarning is added to the warningCollector for each partitionBatch. The warning message for the PrestoWarning follows this general format:\r\n\r\n```\r\nTable <table name> has <number of unreadable partitions> out of <total partitions> partitions unreadable: \r\n<reason_a>: <partition name> <partition name>  <\"...\" if more than 2 unreadable partitions have this error>\r\n<reason_b>: <partition name> <partition name>  <\"...\" if more than 2 unreadable partitions have this error>\r\n<\"...\" if more than 2 types of errors>\r\n```\r\nA partition can be unreadable for more than one reason (**in theory**). If there is only 1 reason, a maximum of 2 partitions will be displayed, and if there are 2 reasons, a maximum of 4 partitions will be displayed. If there are more than 2 partitions that are unreadable for the same reason, `<partition name> <partition name> ...`\r\n\r\n**In practice**, the PrestoWarning message will likely follow:\r\n```\r\nTable <table name> has <number of unreadable partitions> out of <total partitions> partitions unreadable: \r\n<reason>: <partition name> <partition name> ...\r\n```\r\n(I think..)\r\n\r\nOther changes: Strings partName and partitionNotReadable changed to partitionName and reason to be more descriptive. MetastoreUtil.makePartName() should also be modified. Tests added in TestHiveClientFileMetastore to validate the new format of the PrestoWarning message.\r", "NaN"], ["15339", "Catch all possible failures for Presto-on-Spark", "Vic Zhang", "viczhang861", "10/23/20, 11:38:48 PM", "The goal is to let all failed queries have query completion events logged.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15340", "Fix bug where MemoryRevokingSchedulers stopped being installed", "Saksham", "sachdevs", "10/20/20, 05:39:17 PM", "Test plan - None, fixed bug that prevented spilling schedulers from being bound.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15341", "Emit session properties in VerifierQueryEvent", "Leiqing Cai", "caithagoras", "10/22/20, 06:31:46 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add control and test session properties to Verifier output.\r\n```\r", "NaN"], ["15342", "[Verifier] Fix JDBC URL selection", "Leiqing Cai", "caithagoras", "10/22/20, 06:31:29 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Fix an issue where queries are always directed to the first cluster when multiple control or test clusters are specified.\r\n```", "NaN"], ["15344", "Bump Alluxio client version to 2.3.0-2", "Bin Fan", "apc999", "10/31/20, 12:24:27 AM", "After bumping Alluxio client version to 2.3.0-2, we aim to \r\n\r\n- Enable calling openFile with hiveFileContext in local cache\r\n- Enable timeout for cache operations to fall back normally in case of hitting OS or hardware issues\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15345", "Catch up TestHiveExternalWorkersQueries with presto_cpp", "Maria Basmanova", "mbasmanova", "10/23/20, 12:47:21 AM", "Native worker now reads properties from config.properties\r\nand node.properties. Updated the test to generate these.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15346", "Refactor function resolution logic into FunctionResolver", "Rongrong Zhong", "rongrong", "10/23/20, 04:59:34 AM", "Test plan - Travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15347", "Fix hamming_distance to handle code point zero", "Shixuan Fan", "shixuan-fan", "10/24/20, 12:33:48 AM", "Currently when code point zero is encountered in hamming_distance,\r\nwe treat it as 0-length, which would fail the position check for\r\nlegit comparison.\r\n\r\nTest plan - Added unit test\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix hamming_distance to handle code point zero correctly.\r\n```\r", "NaN"], ["15348", "Support access control for Presto-on-Spark", "Vic Zhang", "viczhang861", "10/26/20, 07:12:58 PM", "- The default value for config `allow-unsecured-access` is true. Need to configure Presto-on-Spark after this is merged.\r\n\r\n- `TestingAccessControlModule` is removed.\r\n\r\nDepended by https://github.com/facebookexternal/presto-facebook/pull/1257\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15349", "Changes to pass through sqlparser options", "Dong Shi", "BlueberryDS", "11/16/20, 03:41:55 PM", "Parsing for prepared statements has differences from the regular statement handler in that configured parsing options are not performed consistently. This change passes the SqlParsingOptions through to the prepared statement handling code to make that logic consistent. \r\n\r\nTest Plan - Unit tests\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Pass SqlParserOptions to prepared statement handler so that parsing logic is consistent with regular requests\r\n\r\n```\r", "NaN"], ["15350", "Add Thrift transport support for TaskStatus", "Ajay George", "ajaygeorge", "10/29/20, 07:06:02 PM", "depended by https://github.com/facebookexternal/presto-facebook/pull/1260\r\n\r\nTest plan - Verifier testing \r\nhttps://www.internalfb.com/intern/presto/verifier/results/?test_id=47005\r\nhttps://www.internalfb.com/intern/presto/verifier/results/?test_id=47196\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add thrift transport support for TaskStatus which can be enabled using config property ``experimental.internal-communication.thrift-transport-enabled``\r\n```\r\n\r", "NaN"], ["15351", "Disable file renaming for temp files", "Nikhil Collooru", "NikhilCollooru", "10/24/20, 09:17:34 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15352", "Update property names for presto version and worker ip", null, "bhhari", "10/27/20, 10:05:58 PM", "```\r\n== NO RELEASE NOTE ==\r\nUpdate the property names that are read for presto worker startup\r\n* presto.version\r\n* node.ip\r\n```\r", "NaN"], ["15353", "Documentation for Metastore configurations", "Reetika", "agrawalreetika", "03/08/21, 02:35:19 PM", "Documentation for Metastore configurations. \r\nFixes #15331 \r\n\r\n== NO RELEASE NOTE ==\r", "NaN"], ["15355", "Add SDK and API metrics to GlueHiveMetastore", "James Petty", "pettyjamesm", "11/02/20, 04:30:03 PM", "Add support for AWS SDK client request metrics and Glue API metrics to `GlueHiveMetastore`.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Add support for AWS SDK client request metrics and Glue API metrics to GlueHiveMetastore\r\n```\r", "NaN"], ["15356", "Fix memory accounting in SpillableFinalOnlyGroupedAccumulator", "Saksham", "sachdevs", "10/28/20, 07:18:59 AM", "This fixes a bug where these accumulators were underreporting\r\nmemory usage resulting in less spilling events.\r\n\r\nTest plan - Travis + deploy to flash cluster for verification\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15358", "Fix client-info test-name output", "Guy \u2600\ufe0f Moore", "g1y", "10/29/20, 09:33:54 PM", "The serialization of the Optional test-name field was outputting an\r\nobject with its only key value pair being \"Present\". We want the\r\nactual test-name string to be serialized to json instead. Airlift's\r\ntoJson seems to handle serialization a bit better than ObjectMapper's\r\n\r\nTest plan - Ran verifier with a test-name and found it correctly serializes without putting in \"Present\". \r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15360", "Fix integer overflow in newBlockBuilderLike()", "Ying", "yingsu00", "11/07/20, 01:31:56 AM", "When `newBlockBuilderLike()` is called on BlockBuilders for complex types,\r\n`toIntExact()` could throw \"Integer Overflow\" exception when the expected\r\nnew size exceeds integer boundary. Example stack trace is as follows:\r\n```\r\njava.lang.ArithmeticException: integer overflow\r\nat java.base/java.lang.Math.toIntExact(Math.java:1071)\r\nat com.facebook.presto.common.block.VariableWidthBlockBuilder.newBlockBuilderLike(VariableWidthBlockBuilder.java:393)\r\n```\r\nThis commit fixes the problem by introducing a new method\r\n`calculateBlockResetSize()` that calculates the new size and bound it\r\nby `MAX_ARRAY_SIZE`.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15362", "Optimize TypedSet and map_concat, array_union", "Ying", "yingsu00", "11/30/20, 08:43:54 PM", "This PR resolves the \"Avoid block building inside\" part in https://github.com/prestodb/presto/issues/15361. The optimizations included in this PR are\r\n- Avoid block building inside.\r\n\r\n  TypedSet has an internal BlockBuilder and appends each Block position being added to it. Block building using BlockBuilder is highly costly and inefficient operation. Here the BlockBuilder is needed to 1) resolve hash table probing collision 2) rehash. However, both are actually not a problem. In most use cases, whole blocks (instead of several positions of a block) are added to the TypedSet, and problem 1) can be resolved by keeping track of the blocks being added. 2) Rehashing is not needed since we can know the max number of entries before creating a TypedSet for most use cases. So we want to provide a method that adds a whole Block and just record the positions in the set using SelectedPositions objects. By providing this new interface, internal operations can be streamlined to more efficient loops. It also gives opportunity to get the result Block using more efficient APIs that allows encapsulated memory copying.\r\n\r\n  In the new OptimizedTypedSet class, we offer several operations: union, intersect and except. The operations can be called multiple times, but user does need to specify the max set size when creating the OptimizedTypedSet.\r\n\r\n- Avoid computing the hash positions multiple times\r\nThe previous TypedSet usage sometimes require calculating the hash position multiple times if there are multiple TypedSets. For example array_intersect and array_except. Such usages build one TypedSet R, and based on the probe result on R, insert new elements to another TypedSet B. It requires to calculate the hash position multiple times. This can be avoided if the new TypdedSet can encapsulate the operations inside. The hashPosition calculated in hashtable A can be reused by hashtable B if the size and hash functions are the same.\r\n\r\n- JMH benchmark shows up to 40% improvement for `array_union`:\r\n\r\n    Type       | Baseline  | Specialized Baseline | OptimizedTypedSet | Gain%\r\n    -----------|-----------|----------------------|-------------------|----------\r\n    BIGINT     |      5511 |                 3742 |            3320   |      40%\r\n    VARCHAR    |     20414 |                 N/A  |            14155  |      31%\r\n\r\n- JMH benchmark shows 82% improvement for non_empty case for `map_concat` when keyCount=100\r\n    and POSITIONS=1000:\r\n\r\n```\r\n    Baseline\r\n\r\n    Benchmark                     Mode  Cnt      Score      Error  Units\r\n    BenchmarkMapConcat.mapConcat  avgt   20  26710.925 \u00b1 2005.756  ns/op\r\n    Retained Size: 1,402,374 bytes\r\n\r\n    After\r\n    Benchmark                     Mode  Cnt      Score      Error  Units\r\n    BenchmarkMapConcat.mapConcat  avgt   20  14605.437 \u00b1 1209.786  ns/op\r\n    Retained Size: 1,373,273 bytes\r\n\r\n```\r\n When keyCount=1000 and POSITIONS=1000, **the baseline just OOMed. The optimized version succeeded.**\r\n\r\n- Array_intersect  showed up to 49% improvement in time and 72% savings in allocation rate. More detailed comparisons:\r\n![Screen Shot 2020-11-05 at 1 59 55 AM](https://user-images.githubusercontent.com/33299678/98228163-14447a00-1f0d-11eb-9389-20df2a1ae3c6.png)\r\n![Screen Shot 2020-11-05 at 2 00 02 AM](https://user-images.githubusercontent.com/33299678/98228174-173f6a80-1f0d-11eb-8ba0-a86cbdda2c6b.png)\r\n\r\n- array_except JMH benchmark shows 40% improvement:\r\n\r\n```\r\n    Before\r\n    Benchmark                               Mode  Cnt       Score        Error  Units\r\n    BenchmarkArrayIntersect.arrayIntersect  avgt   10  618074.452 \u00b1 119912.203  ns/op\r\n\r\n    After\r\n    Benchmark                               Mode  Cnt       Score       Error  Units\r\n    BenchmarkArrayIntersect.arrayIntersect  avgt   10  376854.064 \u00b1 21616.063  ns/op\r\n```\r\nProduction testing:\r\nTested using verifier on 1220 queries with map_concat, array_union, array_intersect, array_except. No failures found.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15366", "Fix query plan for dynamic filtering with filter pushed down", "Ke", "kewang1024", "10/28/20, 04:18:31 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15369", "Handle query level timeouts in Presto on Spark", "Andrii Rosa", "arhimondr", "10/30/20, 07:47:44 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15372", "Fix extracting logic in dynamic filtering when integrated with filter pushdown", "Ke", "kewang1024", "11/19/20, 04:37:57 PM", "Test plan - \r\n1. Integration Test\r\n2.Test in internal verifier runs\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15373", "Simplify HivePageSource bucket filtering", "James Petty", "pettyjamesm", "11/03/20, 01:58:59 PM", "Adds an early return path for when all rows or no rows are filtered as ineligible as part of bucket adaptation in `HivePageSource`\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15375", "Fix flaky test for TestTempStorageSingleStreamSpiller", "Wenlei Xie", "wenleix", "10/30/20, 06:19:23 PM", "Use different temporary directory every time.\r\n\r\nTest plan - Travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15376", "Add Top N queries to TestHiveExternalWorkersQueries", "Maria Basmanova", "mbasmanova", "10/30/20, 01:40:17 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15377", "Use more efficient getApproximateLogicalSizeInBytes in OrcWriter", "Ying", "yingsu00", "11/02/20, 10:17:12 PM", "OrcWriter.estimateAverageLogicalSizePerRow has the top memory allocation\r\n    in some KDS pipeline. Since the original code was to estimate the page's\r\n    logical size and do not require accurate size to be calculated, this\r\n    commit changes the call of the expensive getLogicalSizeInBytes method to\r\n    a much faster and memory friendly method getApproximateLogicalSizeInBytes\r\n    that estimate the approximate logical size of a page.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15378", "Update TestHiveExternalWorkersQueries to create hive.properties file", "Maria Basmanova", "mbasmanova", "10/30/20, 06:02:11 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15379", "Check requirements under try-catch", "Andrii Rosa", "arhimondr", "10/30/20, 06:06:26 PM", "To make sure that query events are generated\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15380", "Replace Joda-Time libraries with java.time", null, "stone-shi", "11/05/20, 09:27:15 PM", "Since Java 8 we have java.time packages which are equivalent to Joda and\r\nthe recommendation from the author of the Joda-Time is to migrate to\r\njava.time(JSR-310) library.\r\n\r\nTest plan - \r\nRun unit test using \r\nmvn  install -pl presto-kafka  -T1C\r\nAll unit test passed\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["15383", "Make encryption key explicit when reading DWRF stripes", "Rebecca Schlussel", "rschlussel", "11/04/20, 06:08:58 PM", "DWRF StripeInformation only contains keymetadata if the keymetadata for\r\nthis stripe would be different from the previous stripe.  However, when\r\nwe read stripes in the record reader, we exclude stripes that are not\r\npart of the split, so we may not have info about what the keys were in\r\nthe previous stripe.  Therefore, we explicitly set the implied key\r\ninformation.\r\n\r\nTest plan - Added unit test and verified with failing production query for table written by another system that now succeeds.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Fix a bug with reading encrypted DWRF tables where queries could fail with a NullPointerException\r\n```\r", "NaN"], ["15384", "Refactor TempStorage related interfaces ", "Wenlei Xie", "wenleix", "11/16/20, 09:33:26 PM", "Test plan - Travis\r\n\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15388", "Replace BenchmarkArrayIntersectFunctions with BenchmarkArraySetFunctions", "Ying", "yingsu00", "11/03/20, 10:54:08 PM", "This adds benchmarks for array_except and array_union as well.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15389", "Reuse immutable partition fields in Glue partition conversion", "James Petty", "pettyjamesm", "11/13/20, 12:29:18 AM", "Extracted comparable changes from https://github.com/prestosql/presto/pull/5794\r\n\r\nWhen loading a large number of partitions from Glue (and especially when storing the loaded values into a cache), partition\r\ninstances often share equivalent immutable field values (eg: their columns list) which can consume a large amount of coordinator heap space.\r\n\r\nThis change enables opportunistic reuse of some partition fields by memoizing values during transformation and opportunistically reusing equivalent shared instances for consecutive converted partitions.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15390", "Add session property configuration manager for Presto-on-Spark", "Vic Zhang", "viczhang861", "11/04/20, 08:00:26 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1294\r\n\r\nGoal\r\n- Support session property override\r\n- Presto on Spark doesn't have resource group, make regex matching rule for resource group optional\r\n\r\nTest\r\n- End to end test done. See details in https://github.com/facebookexternal/presto-facebook/pull/1294\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15393", "Make query result HTTP response compression configurable", "James Petty", "pettyjamesm", "11/30/20, 11:52:49 PM", "Before this change, query result JSON responses were generally compressed (assuming the response met the minimum size threshold and passed the user agent checks), so that behavior is still the default. However, disabling GZIP compression can significantly improve throughput of sending query results, especially over localhost links where the overhead of compressing the response and then uncompressing it again on the client side is never worth the bandwidth savings.\r\n\r\nClients are allowed to *opt-out* of compression, but not request compression from a server which has decided to disable compressed query result responses. Both sides ultimately negotiate the result based on their `Accept-Encoding` or `Content-Encoding` headers and the way that the gzip compression middleware interprets them.\r\n\r\nFor queries that are bound only by result processing throughput (eg: `SELECT * FROM <large table>`) execution time can reduced by 20-50% when submitted over a localhost connection with compression disabled.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for disabling query result compression via client and server-side configuration properties. Clients\r\ncan disable compressed responses using the ``--disable-compression`` flag or ``disableCompression`` driver property. Compression can be disabled server-wide by using the configuration property: ``query-results.compression-enabled=false``\r\n```\r", "NaN"], ["15397", "Add release notes for 0.242.1", "Rebecca Schlussel", "rschlussel", "11/04/20, 08:53:07 PM", "Test plan - build and checked generated notes.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15401", "Add Local Revocable Memory Limit", "Saksham", "sachdevs", "11/16/20, 02:57:12 AM", "This config allows spilling queries to fail after allocating a sufficiently large amount of revocable memory.\r\n\r\nTest plan - Deployed to test cluster with 5MB limit:\r\n\r\nExample stack trace\r\n```\r\ncom.facebook.presto.ExceededMemoryLimitException: Query exceeded per-node revocable memory limit of 5MB [Allocated: 4.86MB, Delta: 149.53kB]\r\n\tat com.facebook.presto.ExceededMemoryLimitException.exceededLocalRevocableMemoryLimit(ExceededMemoryLimitException.java:60)\r\n\tat com.facebook.presto.memory.QueryContext.enforceRevocableMemoryLimit(QueryContext.java:442)\r\n\tat com.facebook.presto.memory.QueryContext.updateRevocableMemory(QueryContext.java:175)\r\n\tat com.facebook.presto.memory.QueryContext$QueryMemoryReservationHandler.reserveMemory(QueryContext.java:390)\r\n\tat com.facebook.presto.memory.context.RootAggregatedMemoryContext.updateBytes(RootAggregatedMemoryContext.java:37)\r\n\tat com.facebook.presto.memory.context.ChildAggregatedMemoryContext.updateBytes(ChildAggregatedMemoryContext.java:38)\r\n\tat com.facebook.presto.memory.context.ChildAggregatedMemoryContext.updateBytes(ChildAggregatedMemoryContext.java:38)\r\n\tat com.facebook.presto.memory.context.ChildAggregatedMemoryContext.updateBytes(ChildAggregatedMemoryContext.java:38)\r\n\tat com.facebook.presto.memory.context.ChildAggregatedMemoryContext.updateBytes(ChildAggregatedMemoryContext.java:38)\r\n\tat com.facebook.presto.memory.context.SimpleLocalMemoryContext.setBytes(SimpleLocalMemoryContext.java:82)\r\n\tat com.facebook.presto.operator.OperatorContext$InternalLocalMemoryContext.setBytes(OperatorContext.java:618)\r\n\tat com.facebook.presto.operator.OperatorContext$InternalLocalMemoryContext.setBytes(OperatorContext.java:612)\r\n\tat com.facebook.presto.operator.WindowOperator$SpillablePagesToPagesIndexes.updateMemoryUsage(WindowOperator.java:749)\r\n\tat com.facebook.presto.operator.WindowOperator$SpillablePagesToPagesIndexes.process(WindowOperator.java:639)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$3.process(WorkProcessorUtils.java:262)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$ProcessWorkProcessor.process(WorkProcessorUtils.java:315)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$3.process(WorkProcessorUtils.java:249)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$ProcessWorkProcessor.process(WorkProcessorUtils.java:315)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$3.process(WorkProcessorUtils.java:249)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$ProcessWorkProcessor.process(WorkProcessorUtils.java:315)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$3.process(WorkProcessorUtils.java:249)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$ProcessWorkProcessor.process(WorkProcessorUtils.java:315)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$3.process(WorkProcessorUtils.java:249)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$ProcessWorkProcessor.process(WorkProcessorUtils.java:315)\r\n\tat com.facebook.presto.operator.WindowOperator.getOutput(WindowOperator.java:358)\r\n\tat com.facebook.presto.operator.Driver.processInternal(Driver.java:417)\r\n\tat com.facebook.presto.operator.Driver.lambda$processFor$8(Driver.java:300)\r\n\tat com.facebook.presto.operator.Driver.tryWithLock(Driver.java:721)\r\n\tat com.facebook.presto.operator.Driver.processFor(Driver.java:293)\r\n\tat com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1077)\r\n\tat com.facebook.presto.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:162)\r\n\tat com.facebook.presto.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:545)\r\n\tat com.facebook.presto.$gen.Presto_0_245_SNAPSHOT_bcd98e2__0_245_20201112_215425_46____20201113_002019_1.run(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15403", "Fix int overflow error when spilling large page", "Rebecca Schlussel", "rschlussel", "11/06/20, 04:38:15 PM", "Page serialization requires page size to fit in an integer, so larger\r\npages could hit an int overflow error.\r\n\r\nTest plan - Not sure how to write a test because if I create a page that big I run out of java heapspace.  I plan to verify with a production query that encountered this issue to check if it fixes it, but need to wait for other testing to finish on that cluster first.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix an issue where queries could encounter an int overflow error during spilling\r\n\r\n```\r", "NaN"], ["15404", "Add queries with cast, between and values to TestHiveExternalWorkersQueries", "Maria Basmanova", "mbasmanova", "11/06/20, 11:00:05 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15405", "Fix local exchanges when materializing remote exchanges", "Rebecca Schlussel", "rschlussel", "11/06/20, 09:27:14 PM", "Remote streaming exchanges enforce a \"fixed\" distribution, but table scans\r\nonly enforce a \"multiple\" distribution.  This meant we were missing local\r\nexchanges in some cases when we materialized exchanges but required a fixed\r\ndistribution.  In particular this fixes join spilling when using\r\nmaterialized exchanges, since in that case the probe side requires a\r\nfixed distribution.\r\n\r\nTest plan - Added test for spill + materialized exchanges.  Also increased general join spill test coverage.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for spilling joins when using materialized exchanges.\r\n```\r\n\r\n```\r", "NaN"], ["15406", "Remove spill-enabled requirement for enabling join spill", "Rebecca Schlussel", "rschlussel", "11/12/20, 05:38:08 PM", "The requirement that to enable join spill by session property, regular spill needs to be enabled by config property is a problem when spill is disabled by default.\r\n\r\nTest plan - manual verification\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15410", "Load configuration manager for Presto-on-Spark driver only", "Vic Zhang", "viczhang861", "11/09/20, 05:28:10 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15413", "Fix the check to correctly capture NULL positions when presentStream is null and filter allows NULLs", null, "bhhari", "11/11/20, 07:58:29 PM", "```\r\n== NO RELEASE NOTE ==\r\nWhen the presentStream is null, we do not initialize the nulls member variable. During readWithFilter we were checking the inNullVector instead of presentStream, which is causing an NPE. The isNullVector is reused across stripes and is not null. Updated the if condition to use presentStream\r\n```\r", "NaN"], ["15414", "Fix LambdaDefinitionExpression canonicalization", "Rongrong Zhong", "rongrong", "11/10/20, 12:10:23 AM", "Fixes #15424\r\n\r\nTest plan - added new test in AbstractTestQueries\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix LambdaDefinitionExpression canonicalization to correctly canonicalize Block constant (:issue:`15424`).\r\n```\r", "NaN"], ["15415", "Add release notes for 0.243.1", "Rebecca Schlussel", "rschlussel", "11/09/20, 10:29:23 PM", "Test plan - build, look at the generated docs\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15418", "Support function namespace manager for Presto-on-Spark", "Vic Zhang", "viczhang861", "11/10/20, 07:52:35 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1303\r\n\r\nSummary\r\n- Similar to catalog configurations,  for `functionNamespaceProperties (Map<String, Map<String, String>>)` , key is namespace/catalog, value is properties\r\n\r\nTest plan\r\n- End to end test succeeded, see query result in facebookexternal/presto-facebook#1303\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15421", "Reduce parquet metadata read request count", "James Petty", "pettyjamesm", "11/12/20, 08:02:24 PM", "Attempts to reduce the number of reads required for parquet footers by pre-reading up to 16KiB from the end of the file. If the metadata section ends up being < 16KiB long, then one additional round trip has been saved. When the metadata section is longer, the additional read occurs to load the remaining portion of the metadata section.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Improve parquet metadata reader by pre-loading 16KiB from the end of the file. When the metadata section is smaller than the prefetch size, only a single read is performed compared to the previous two.\r\n```\r", "NaN"], ["15425", "Add release note for 0.243.2", "Rongrong Zhong", "rongrong", "11/11/20, 09:59:25 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15428", "Use serialized plan hash for fragment result caching", "Shixuan Fan", "shixuan-fan", "11/18/20, 11:25:04 PM", "Test plan - Enhanced existing unit test. Verifier run for internal queries passed.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Reduce memory usage for fragment result caching.\r\n```\r", "NaN"], ["15431", "Support different concurrency", "Andrii Rosa", "arhimondr", "12/10/20, 09:42:01 PM", "If TableWriter input is fixed distributed (e.g.: aggregation or join) the actual aggregation or join concurrency will be set to the table writer concurrency\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15433", "Add release notes for 0.244", "Leiqing Cai", "caithagoras", "11/19/20, 07:38:07 PM", "# Missing Release Notes\r\n## Daniel Ohayon\r\n- [x] https://github.com/prestodb/presto/pull/15316 Fix caching and case-sensitivity issues with enum type signatures (Merged by: Rongrong Zhong)\r\n\r\n## Mohamed Shafee\r\n- [x] https://github.com/prestodb/presto/pull/15338 Change warning to ignore unreadable partitions to be a digest (Merged by: James Sun)\r\n\r\n## Rongrong Zhong\r\n- [x] af78e1b53f2a60fa7ff7ea85aad8e67e820fa5b6 Chage Block's toString to be unique with respect to hashCode\r\n- [x] f439df5d3f68b1bb8bc331a1322c0607927ee3ff Fix LambdaDefinitionExpression canonicalization\r\n\r\n## Sergii Druzkin\r\n- [x] https://github.com/prestodb/presto/pull/15286 Enable reading of FileStatistics in presto-orc (Merged by: Rebecca Schlussel)\r\n- [x] https://github.com/prestodb/presto/pull/15286 Enable reading of FileStatistics in presto-orc (Merged by: Rebecca Schlussel)\r\n\r\n# Extracted Release Notes\r\n- #15213 (Author: James Petty): Improve nested loop operator performance\r\n  - Improve performance of NestedLoopJoinOperator.\r\n- #15213 (Author: James Petty): Improve nested loop operator performance\r\n  - Improve performance of NestedLoopJoinOperator.\r\n- #15303 (Author: Ariel Weisberg): Prefer verifier cluster memory limits\r\n  - Improve verifier handling of memory limits by using the limits from the verifier cluster which are frequently better matched to the queries picked to be run in the verifier suite.\r\n- #15319 (Author: Xiang Fu): Pinot streaming connector\r\n  - Support query Pinot server with streaming API (Requires Pinot Version >= 0.6.0.).\r\n  - Enabled it by setting `pinot.use-streaming-for-segment-queries=true`.\r\n- #15323 (Author: Ariel Weisberg): Add peak node total memory to statistics\r\n  - Add `peak_node_total_memory` to statistics to allow monitoring the peak sum of memory used across tasks at any node by a query.\r\n- #15323 (Author: Ariel Weisberg): Add peak node total memory to statistics\r\n  - Add `peak_node_total_memory` to statistics to allow monitoring the peak sum of memory used across tasks at any node by a query.\r\n- #15332 (Author: Adli Mousa): Fix druid reading issue\r\n  - Fix druid reading issue.\r\n- #15332 (Author: Adli Mousa): Fix druid reading issue\r\n  - Fix druid reading issue.\r\n- #15332 (Author: Adli Mousa): Fix druid reading issue\r\n  - Fix druid reading issue.\r\n- #15341 (Author: Leiqing Cai): Emit session properties in VerifierQueryEvent\r\n  - Add control and test session properties to Verifier output.\r\n- #15342 (Author: Leiqing Cai): [Verifier] Fix JDBC URL selection\r\n  - Fix an issue where queries are always directed to the first cluster when multiple control or test clusters are specified.\r\n- #15347 (Author: Shixuan Fan): Fix hamming_distance to handle code point zero\r\n  - Fix hamming_distance to handle code point zero correctly.\r\n- #15350 (Author: Ajay George): Add Thrift transport support for TaskStatus\r\n  - Add thrift transport support for TaskStatus which can be enabled using config property ``experimental.internal-communication.thrift-transport-enabled``.\r\n- #15355 (Author: James Petty): Add SDK and API metrics to GlueHiveMetastore\r\n  - Add support for AWS SDK client request metrics and Glue API metrics to GlueHiveMetastore.\r\n- #15383 (Author: Rebecca Schlussel): Make encryption key explicit when reading DWRF stripes\r\n  - Fix a bug with reading encrypted DWRF tables where queries could fail with a NullPointerException.\r\n- #15383 (Author: Rebecca Schlussel): Make encryption key explicit when reading DWRF stripes\r\n  - Fix a bug with reading encrypted DWRF tables where queries could fail with a NullPointerException.\r\n- #15403 (Author: Rebecca Schlussel): Fix int overflow error when spilling large page\r\n  - Fix an issue where queries could encounter an int overflow error during spilling.\r\n- #15405 (Author: Rebecca Schlussel): Fix local exchanges when materializing remote exchanges\r\n  - Add support for spilling joins when using materialized exchanges.\r\n\r\n# All Commits\r\n- af78e1b53f2a60fa7ff7ea85aad8e67e820fa5b6 Chage Block's toString to be unique with respect to hashCode (Rongrong Zhong)\r\n- f439df5d3f68b1bb8bc331a1322c0607927ee3ff Fix LambdaDefinitionExpression canonicalization (Rongrong Zhong)\r\n- 8e5b6902533149588d69dea19ec5581fd9fd925e Remove calculateBlockResetBytes in BlockUtil (Ying Su)\r\n- 190bcfdfdcbe6c9c453cfdae076854cba8c3a325 Fix integer overflow in newBlockBuilderLike() (Ying Su)\r\n- c2dd8681f274924d1d37dc2c9ecca6c930e7944f Fix local exchanges when materializing remote exchanges (Rebecca Schlussel)\r\n- f61df1b6b5e12f8c722f64b7dd93e868f58be5fe Fix int overflow error when spilling large page (Rebecca Schlussel)\r\n- c7c416339741df5ae0a3f3750b25b7ab0642d6c2 Add queries with cast, between and values to TestHiveExternalWorkersQueries (Masha Basmanova)\r\n- 4fee5e2eb232cad807790658090b8d0bf8d2af4d Replace Joda-Time libraries with java.time (Stone Shi)\r\n- f117ceb9d6a3e8c257968310cc4b3d877206ac96 Improve NestedLoopJoinOperator performance (James Petty)\r\n- 3b7b2d9241f0bd2ee2d0b43bb6071d75d5165d9a Add Page constructor for empty blocks case (James Petty)\r\n- 673cb90df3debd52aadfd2060254a14e09ce9e97 Add session property configuraiton manager for Presto-on-Spark (Vic Zhang)\r\n- 734b959f032ce13e3578313a0e4aed69b5f05119 Make ResourceGroupId optional in SessionConfigurationContext (Vic Zhang)\r\n- 030c95942c756b506011881b02d8ac2b5f544c56 Refactor SessionPropertyDefaults to load properties map (Vic Zhang)\r\n- c67b9f6916595c1e58fbaf1756451752e6f9bbd8 Make encryption key explicit when reading DWRF stripes (Rebecca Schlussel)\r\n- 8580bb52fdacbde3e043656baac37dd83099fb50 Cleanup unused variables in OrcReader (Rebecca Schlussel)\r\n- e01b612359b38e9020a3ea564689b5d0cdf958a5 Replace BenchmarkArrayIntersectFunctions with BenchmarkArraySetFunctions (Ying Su)\r\n- 780dc894b56da7db0da56aa20750499699a5eb26 Support Pinot Streaming connector for Segment query (Xiang Fu)\r\n- ae929b99a2c6081f8b43ed0c3233b314acee4ae5 Simplify HivePageSource bucket filtering (James Petty)\r\n- 0883047575832fd2060caeacedcdaca0bd7aa496 Fix MaterializedResult null page handling (James Petty)\r\n- 6d665b4a1dea06946ffa8a5dd3cc1e7b53f2ea08 Use more efficient getApproximateLogicalSizeInBytes in OrcWriter (Ying Su)\r\n- 11c80531cf0e179b43f9a5e6864f8ec0c1655af4 Add SDK and API metrics to GlueHiveMetastore (James Petty)\r\n- 36392a20e398ec64e930cfbd4cffcd4ec180a4d2 This patch bumps Alluxio dependency to 2.3.0-2 (Bin Fan)\r\n- aa55ea7d733a1a33068f7217afd31532a1643242 Handle query level timeouts in Presto on Spark (Andrii Rosa)\r\n- 193a4cdb762cce5b6d9c6628655429dda0461102 Fix flaky test for TestTempStorageSingleStreamSpiller (Wenlei Xie)\r\n- fff331fb491834877b3da5123996d5c275d3a688 Check requirements under try-catch (Andrii Rosa)\r\n- 746d7b5053956cb3e0f237ead76d93f325d692d4 Update TestHiveExternalWorkersQueries to create hive.properties file (Masha Basmanova)\r\n- a90d97a17e576ed5e0edf42a7ad09ffe740f4a88 Introduce large dictionary mode in SliceDictionarySelectiveReader (Ying Su)\r\n- 64835d1ff4c2d690f8121944e86906061d890400 Revert \"Revert \"Remove stripeDictionaryData buffer in SliceDictionarySelectiveReader\"\" (Ying Su)\r\n- a846da5db22735bcd609a87598f059fbc1200941 Revert \"Revert \"Defer the creation of dictionary in SliceDictionarySelectiveReader\"\" (Ying Su)\r\n- 8b62d43eaa4fff10cd9c91c79a13774360124169 Add Top N queries to TestHiveExternalWorkersQueries (Masha Basmanova)\r\n- 467277a808d23b2e5c86bfe147e7ae092ef38fb6 Fix client-info test-name output (Guy Moore)\r\n- fc947199a6493f88e54ffcabf174cc5d5a3f9cbb Add Thrift transport support for TaskStatus (Ajay George)\r\n- 94a3e0ac3544c8afb16751cb9d5e216cd2d69ce2 Add peak node total memory to statistics (Ariel Weisberg)\r\n- 36ea3cd1e633b69c3960be1c965b605f22472db4 Only fetch memory reservation when needed (Ariel Weisberg)\r\n- 151fa9707ffff50f66196b1059bde0877cbba990 Fix double counting of charPositionInLine (Ajay George)\r\n- f43600d3ceb2a75067f5b492d748ff8ca20efc83 Fix query plan for dynamic filtering with filter pushed down (Ke Wang)\r\n- be57a0e596c00d6fe9b57195fe11a3e4511f3281 Fix memory accounting in SpillableFinalOnlyGroupedAccumulator (Saksham Sachdev)\r\n- 355aec9cb932afc3196812cf81e1b0ab91c014b7 change warning to ignore unreadable partitions to be a digest (Mohamed Shafee)\r\n- a4dc3363845b48131073127371a0f52c6b500a73 Update property names for presto version and worker ip (Bhavani Hari)\r\n- 24d9a53d6958c8216aaf8c32a4bc03e1aabb6e4e Rename TemporaryStore to TempStorage (Wenlei Xie)\r\n- 1e11a1eaf31dcadb63a7407352ad744a59dd15e7 Make TemporaryStore pluggable (Wenlei Xie)\r\n- 4fa2574d2336be9ab26e7a46ec418aaa13f58f32 Add test for temporary store based spilling (Wenlei Xie)\r\n- 8504ea73b9e855663ff652ce499e7ba8cde7016f Introduce temporary store based spilling (Wenlei Xie)\r\n- e63a6bf7f58a0ea7dce09ed64cd94ba435fbc39a Support access control for Presto-on-Spark (Vic Zhang)\r\n- aeecfa9732a795990a5765a3b523a596514773d1 Refactor AccessControlManager to load properties map (Vic Zhang)\r\n- 8c620762c3e199743300d24c78b6c1cb886a4f8f inline columnHandlesHasErrorMessageField (Adli Mousa)\r\n- e2055f1a3e6e878daa921725e62a2594986c9f28 raise exception if returned from druid (Adli Mousa)\r\n- 6b87023f46fdb2b069cc06a202af6fa90c86056e Fix druid reading issue (Adli Mousa)\r\n- 5c4f8ecb63ae2fb1ae3e97ec240e7df27f79a20a Disable file renaming for temp files (Nikhil Collooru)\r\n- f9125243064a5e2f67cbdb80dfe672f6acd87918 Fix hamming_distance to handle code point zero (Shixuan Fan)\r\n- b3c49f13b05a226e901b7660c5132af93ba58b64 Reserve memory for nested BlockBuilders in projection (Ying Su)\r\n- 8ad50799bb66169eb941a1c0de84320decc737df Introduce newBlockBuilderLike() with expectedEntries (Ying Su)\r\n- ee5643611a64fbd04fe481ed1a392e59f57f60a7 Catch all possible failures for Presto-on-Spark (Vic Zhang)\r\n- fc757d05e44d1bc6c2902922a11a6bf1c70f7ad2 Add benchmarks for big enum signatures (Daniel Ohayon)\r\n- 6004a2d2355ab8692df11abf921a373398583b19 Use base32-encoding in enum type signatures (Daniel Ohayon)\r\n- b1b353bb31fbcfd89f507351f3d5cdc4c46fb615 Vendor in Base32 codec from apache.commons.codec (Daniel Ohayon)\r\n- 9c5f9f9273909675c38e7c9bafd912c324746f4e Handle enum literals in SqlToRowExpressionTranslator (Daniel Ohayon)\r\n- c32704a8d004dcbd74dfe2195aa9ddedf25f3c8f Add hashCode to VarcharEnumType and LongEnumType (Daniel Ohayon)\r\n- cf2213e9960f72f90ac87fd182d45bc30a2e9f74 Refactor function resolution logic into FunctionResolver (Rongrong Zhong)\r\n- 8236982282b395e959347ee88efc57f8415abff1 Catch up TestHiveExternalWorkersQueries with presto_cpp (Masha Basmanova)\r\n- 9625d29e19ab3e126d6c2b54148e7328d71760be Emit session properties in VerifierQueryEvent (Leiqing Cai)\r\n- fb936e0c06afdbf1750d0fe3b7521aab042aed5b Fix JDBC URL selection (Leiqing Cai)\r\n- 12088497e565e8727d71cecaa1abde5e720ce1fd Fix access control checks for USING clause (prithvip)\r\n- 3d4f3f3ff042c7c2b75c09a8d9de6aee53227267 Enable reading of FileStatistics in presto-orc (Sergii Druzkin)\r\n- a832992ecbdede195c8ca02f3bc998ce31533c5e Move encryption keys from OrcRecordReader to OrcReader (Sergii Druzkin)\r\n- be65862cbf1d7dd03686ac7ea730467286d9015c Prefer verifier cluster memory limits (Ariel Weisberg)\r\n- f8d936a8dda5716e2b4c1a522f5fd91e4235b958 Fix bug where MemoryRevokingSchedulers stopped being installed (Saksham Sachdev)\r\n- cc82b90eaeb99e70532ae6edb865dc53228fbd2c Variable name fix for taskSpillingStrategy (Wenlei Xie)\r\n- 0325c46479354b6557d7b60be054cedf34faa8f0 Minor improvements for partitioned output buffers and operator (James Petty)", "NaN"], ["15436", "Add warning for Presto on Spark", "Vic Zhang", "viczhang861", "11/13/20, 06:36:59 PM", "Test\r\n- Get expected warning in a real query. ` \"warnings\": [\r\n        {\r\n            \"warningCode\": {\r\n                \"code\": 3,\r\n                \"name\": \"PERFORMANCE_WARNING\"\r\n            },\r\n            \"message\": \"COUNT(DISTINCT xxx) can be a very expensive operation when the cardinality is high for xxx. In most scenarios, using approx_distinct instead would be enough\"\r\n        }\r\n    ]`\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15437", "Add cpu and memory stats to NodeTaskMap", "cem cayiroglu", "cemcayiroglu", "01/27/21, 06:39:33 PM", "NodeTaskMap holds the worker level aggregated split stats which is used by NodeSelectors. NodeSelector accesses to NodeTaskMap via NodeAssignmentStats. CPU and Memory stats are added to NodeTaskMap. CPU and Memory stats will used in planed new feature \"workload placement\".", "NaN"], ["15438", " Null unused array elements in SpillableFinalOnlyGroupedAccumulator", "Saksham", "sachdevs", "11/19/20, 02:08:07 AM", "This PR reduces memory usage in SpillableFinalOnlyGroupedAccumulator\r\nTest plan - deployed to test cluster\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15439", "Improve SliceDirectSelectiveStreamReader", "Ying", "yingsu00", "12/01/20, 11:02:08 AM", "This PR improves reading unbounded varchar  by 1) introducing the batch read \r\nmode in SliceDirectSelectiveStreamReader; and 2) reading the lengthVector with \r\nmore efficient `next()` API. \r\n\r\nBenchmarkSelectiveStreamReaders shows improvements for almost all input\r\nfilter rates for both `readNoFilter` and `readWithFilter` except only one\r\ncase, where filter rate = 0.1. The\r\nimprovement ratio is up to 2.36x for reading one column and 2.1x for reading two columns.\r\nTo make a overall good balance between performance and memory\r\nI set the batch mode threshold on filter rate to be in the range of [0, 0.5] for readNoFilter,\r\n[0, 0.05] and [0.15, 0.5] for readWithFilter.\r\n\r\nNo Nulls\r\n<img width=\"274\" alt=\"Screen Shot 2020-11-13 at 12 36 52 PM\" src=\"https://user-images.githubusercontent.com/33299678/99118793-01851180-25ad-11eb-8164-246cbe1e87a4.png\">\r\n\r\nWith Partial Nulls\r\n<img width=\"278\" alt=\"Screen Shot 2020-11-13 at 12 36 59 PM\" src=\"https://user-images.githubusercontent.com/33299678/99118798-04800200-25ad-11eb-8d6f-d5b098d8779f.png\">\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15443", "Update to airlift 0.198 and airbase 100", "Leiqing Cai", "caithagoras", "11/21/20, 11:10:19 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1315\r\n\r\nWaiting for airlift release for: https://github.com/prestodb/airlift/pull/34\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15444", "Add UDF to map enum value to key", "Daniel Ohayon", "daniel-ohayon", "11/20/20, 06:57:09 PM", "Adds a UDF to get the enum key for an enum as a string value.\r\nThis is useful in the case of integer enums in particular, to inspect data via clients that do not support enum pretty-printing, and would just show the enum value as a number.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Added a UDF to get the key corresponding to an enum value: `ENUM_KEY(EnumType) -> VARCHAR`\r\n```\r", "NaN"], ["15447", "Change task threshold revoking config type to DataSize", "Saksham", "sachdevs", "11/19/20, 02:08:15 AM", "Test plan - travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15448", "Fix not registering TempStorage in PluginManager", "Wenlei Xie", "wenleix", "11/18/20, 09:41:31 PM", "Test plan - Travis. Also test in production and see a query successfully spilled to disagg storage. \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15449", "Add config plugin.disabled-connectors to stop registration of disabled connectors", "Saksham", "sachdevs", "12/17/20, 07:31:00 PM", "plugin.disabled-connectors ensures disabled connectors are not registered.\r\n\r\nTest plan - Deploy to test cluster to ensure \"Skipping disabled connector\" log is printed.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15450", "Fix LongEnum deserialization in map keys", "Daniel Ohayon", "daniel-ohayon", "11/20/20, 06:56:53 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix enum deserialization bug in map keys\r\n```\r", "NaN"], ["15455", "Adding request throttling retry for verifier requests", "Swapnil", "swapsmagic", "01/15/21, 06:31:04 AM", "Test plan -\r\nVerified with unit tests\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Adding retry behavior for verifier requests in case of throttling error\r\n```", "NaN"], ["15457", "Allow serialized presto page in remote UDF thrift service", "Rongrong Zhong", "rongrong", "12/12/20, 12:57:28 AM", "Test plan - (Please fill in how you tested your changes)\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15459", "Add filter pushdown tests for native worker", "Maria Basmanova", "mbasmanova", "11/19/20, 02:23:25 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15460", "Store file names and sizes in partition parameters", "Nikhil Collooru", "NikhilCollooru", "11/19/20, 04:32:48 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15462", "Add release notes for 0.243.3", null, "prithvip", "11/20/20, 01:06:57 AM", "Test plan - Built the docs and made sure they looked ok\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15464", "Fix split cacheable setting for affinity scheduling", "Ke", "kewang1024", "11/20/20, 08:09:10 PM", "As of now, when we schedule a scan for bucketed table which then joins with a remote,\r\nwe don't set the cacheable to true even if we enable affinity scheduling, this PR introduce the support for it\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support in affinity scheduler to enable cache for bucketed table scan that has remote source\r\n```\r", "NaN"], ["15465", "Build presto-docs in Travis", "Leiqing Cai", "caithagoras", "11/20/20, 10:14:19 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15466", "Do not treat warning as errors when making presto-docs", "Leiqing Cai", "caithagoras", "11/20/20, 10:08:34 PM", "Tested with Sphinx v3.3.1\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15467", "Add verifier flag for removing memory related session property", "Wenlei Xie", "wenleix", "11/22/20, 08:07:19 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1330\r\n\r\nIn some cases we want to remove these session property when run verifier\r\ntest on smaller cluster. While in certain other cases we want to keep or\r\nadd them to test memory related improvements (while not changing the\r\ncluster config).\r\n\r\nTest plan - Travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15468", "Fix NPE when a FlatMap column is all empty", null, "bhhari", "12/01/20, 04:43:04 PM", "In few cases where the all the map values are empty the encoding is not present in the stripe footer causing NPE.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15469", "Fix base column name not present error when dereference pushdown enabled", "Beinan", "beinan", "11/28/20, 02:29:12 AM", "We're seeing the error below when the dereference push down is enabled\r\n\r\n`java.lang.IllegalArgumentException: nested column [SOME_COLUMN_455.SOME_SUBFIELD]'s base column SOME_COLUMN_455 is not present in table scan output\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitProject(HiveParquetDereferencePushDown.java:357)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitProject(HiveParquetDereferencePushDown.java:288)\r\n\tat com.facebook.presto.spi.plan.ProjectNode.accept(ProjectNode.java:92)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitPlan(HiveParquetDereferencePushDown.java:308)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitProject(HiveParquetDereferencePushDown.java:325)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitProject(HiveParquetDereferencePushDown.java:288)\r\n\tat com.facebook.presto.spi.plan.ProjectNode.accept(ProjectNode.java:92)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitPlan(HiveParquetDereferencePushDown.java:308)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitPlan(HiveParquetDereferencePushDown.java:288)\r\n\tat com.facebook.presto.spi.plan.PlanVisitor.visitFilter(PlanVisitor.java:35)\r\n\tat com.facebook.presto.spi.plan.FilterNode.accept(FilterNode.java:80)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitPlan(HiveParquetDereferencePushDown.java:308)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitProject(HiveParquetDereferencePushDown.java:325)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitProject(HiveParquetDereferencePushDown.java:288)\r\n\tat com.facebook.presto.spi.plan.ProjectNode.accept(ProjectNode.java:92)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitPlan(HiveParquetDereferencePushDown.java:308)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitProject(HiveParquetDereferencePushDown.java:325)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitProject(HiveParquetDereferencePushDown.java:288)\r\n\tat com.facebook.presto.spi.plan.ProjectNode.accept(ProjectNode.java:92)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitPlan(HiveParquetDereferencePushDown.java:308)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitProject(HiveParquetDereferencePushDown.java:325)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown$Visitor.visitProject(HiveParquetDereferencePushDown.java:288)\r\n\tat com.facebook.presto.spi.plan.ProjectNode.accept(ProjectNode.java:92)\r\n\tat com.facebook.presto.hive.rule.HiveParquetDereferencePushDown.optimize(HiveParquetDereferencePushDown.java:175)\r\n\tat com.facebook.presto.sql.planner.optimizations.ApplyConnectorOptimization.optimize(ApplyConnectorOptimization.java:134)\r\n\tat com.facebook.presto.sql.planner.LogicalPlanner.plan(LogicalPlanner.java:187)\r\n\tat com.facebook.presto.sql.planner.LogicalPlanner.plan(LogicalPlanner.java:176)\r\n\tat com.facebook.presto.execution.SqlQueryExecution.doAnalyzeQuery(SqlQueryExecution.java:392)\r\n\tat com.facebook.presto.execution.SqlQueryExecution.analyzeQuery(SqlQueryExecution.java:378)\r\n\tat com.facebook.presto.execution.SqlQueryExecution.start(SqlQueryExecution.java:328)\r\n\tat com.facebook.presto.$gen.Presto_0_241_66578ef____20201121_034334_1.run(Unknown Source)\r\n\tat com.facebook.presto.execution.SqlQueryManager.createQuery(SqlQueryManager.java:242)\r\n\tat com.facebook.presto.dispatcher.LocalDispatchQuery.lambda$startExecution$5(LocalDispatchQuery.java:114)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)`\r\n\r\n\r\nSo I think we should use SOME_COLUMN_SOME_NUMBER instead of SOME_COLUMN as the key of the map regularHiveColumnHandles\r\n\r\n\r\n@vkorukanti and @zhenxiao, could you guys help take a look? \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15470", "Ensure nullable partitioning column is handled properly", "countryman4687", "fgwang7w", "12/08/20, 03:04:58 AM", "```\r\n== RELEASE NOTE ==\r\n\r\nHive changes\r\n* Fix dynamic pruning failures for joining on null keys in hive partition\r\n```\r", "NaN"], ["15475", "Fixes for prefer_manifests_to_list_files property", "Nikhil Collooru", "NikhilCollooru", "11/24/20, 11:14:00 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15477", "Reduce output buffer lock contention", "James Petty", "pettyjamesm", "01/07/21, 08:46:19 PM", "Two commits attempting to reduce lock contention related to output buffers:\r\n- Refactors `LazyOutputBuffer` to attempt to read the delegate `OutputBuffer` reference before synchronizing\r\n- Refactors `SerializedPageReference` and usages to perform dereferencing in batches to avoid synchronizing on memory manager locks for each individual page.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15483", "Minor fix for memory property removal in Presto verifier", "Wenlei Xie", "wenleix", "12/07/20, 09:26:12 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15485", "Disallow ORDER BY literal in window functions", "Sreeni Viswanadha", "kaikalur", "12/30/20, 09:56:05 PM", "Disallow ORDER BY literal in Window functions as this is currently treated differently for int literals compared to ORDER BY in SELECT where it's treated it as an ordinal.\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\nTest included.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Disallow ORDER BY literals used with Window functions as it's not useful, expensive and most often used wrongly. \r\n```", "NaN"], ["15488", "Load function managers for Presto-on-Spark driver only", "Vic Zhang", "viczhang861", "12/04/20, 12:41:54 AM", "When inline sql function is enabled, no need to load\r\nfunction managers for executors.\r\n\r\nIssue\r\n- Loading function managers for PoS executor is not free,  reliability issue such as transportation error is observed when fetching client certificate\r\n\r\nTest plan \r\n- Build a package and query succeeds 20201130_212659_00000_ts7k9 when inline sql function is enabled.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15489", "Add release notes for 0.243.4", null, "bhhari", "12/01/20, 09:38:20 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15490", "Release 0.243", null, "bhhari", "12/01/20, 11:02:40 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15491", "Release 0.244", null, "bhhari", "12/02/20, 01:53:36 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15492", "Fix tests for prefer_manifests_to_list_files property", "Nikhil Collooru", "NikhilCollooru", "12/02/20, 08:52:34 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15494", "Update static docs site with new theme", null, "mzbeck", "01/14/21, 08:50:03 AM", "Test plan - Tested locally and with full build environment.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Change theme of the Presto docs site (https://prestodb.io/docs/), adding a table of contents, page navigation, Sphinx search, ability to copy code to clipboard, and a new color scheme.\r\n```\r", "NaN"], ["15495", "Add query schema to ConnectorSession", null, "mayankgarg1990", "12/03/20, 11:32:07 PM", "Having access to this information will allow us to make decisions based on the schema\r\nin the metadata layers\r\n\r\nTest plan - This is a simple change that flows the schema to the connector. Just ensured that\r\nthe current tests continue to succeed.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n* Add ``getSchema`` to ``ConnectorSession``\r\n```\r\n\r\nDepended by https://github.com/facebookexternal/presto-facebook/pull/1341\r", "NaN"], ["15498", "Handle nulls properly in flatmap reader", null, "zzhao0", "12/08/20, 07:59:54 PM", "In the case that a file has very sparse non-null flatmap, reader incorrectly calculates totalMapEntries if the entire batch only contains nulls. That causes exception when reading map values from the child readers.\r\n\r\nTest plan \r\n\r\nAdding a test file that contains all null flat maps except for the first row. This allows this particular edge case to be tested. Ie. the first batch returned contains 1024 rows, one of which is not null. The subsequent batches returned have only nulls.\r\n\r\n== NO RELEASE NOTE ==", "NaN"], ["15499", "Add release notes for 0.245", "Leiqing Cai", "caithagoras", "12/16/20, 01:33:17 AM", "# Missing Release Notes\n## Mayank Garg\n- [x] https://github.com/prestodb/presto/pull/15495 Add query schema to ConnectorSession (Merged by: Timothy Meehan)\n\n# Extracted Release Notes\n- #15349 (Author: Dong Shi): Changes to pass through sqlparser options\n  - Pass SqlParserOptions to prepared statement handler so that parsing logic is consistent with regular requests.\n- #15393 (Author: James Petty): Make query result HTTP response compression configurable\n  - Add support for disabling query result compression via client and server-side configuration properties.\n- #15393 (Author: James Petty): Make query result HTTP response compression configurable\n  - Add support for disabling query result compression via client and server-side configuration properties.\n- #15414 (Author: Rongrong Zhong): Fix LambdaDefinitionExpression canonicalization\n  - Fix LambdaDefinitionExpression canonicalization to correctly canonicalize Block constant (:issue:`15424`).\n- #15414 (Author: Rongrong Zhong): Fix LambdaDefinitionExpression canonicalization\n  - Fix LambdaDefinitionExpression canonicalization to correctly canonicalize Block constant (:issue:`15424`).\n- #15421 (Author: James Petty): Reduce parquet metadata read request count\n  - Improve parquet metadata reader by pre-loading 16KiB from the end of the file. When the metadata section is smaller than the prefetch size, only a single read is performed compared to the previous two.\n- #15428 (Author: Shixuan Fan): Use serialized plan hash for fragment result caching\n  - Reduce memory usage for fragment result caching.\n- #15444 (Author: Daniel Ohayon): Add UDF to map enum value to key\n  - Added a UDF to get the key corresponding to an enum value.\n- #15450 (Author: Daniel Ohayon): Fix LongEnum deserialization in map keys\n  - Fix enum deserialization bug in map keys.\n- #15464 (Author: Ke Wang): Fix split cacheable setting for affinity scheduling\n  - Add support in affinity scheduler to enable cache for bucketed table scan that has remote source.\n\n# All Commits\n- b2c7ab82fe10cf9103d7239897a448fada15f22a Load function managers for Presto-on-Spark driver only (Vic Zhang)\n- 8e1384e2fe241f6b28ae8b0ad4a78b3f217c4343 Add query schema to ConnectorSession (Mayank Garg)\n- 27ab25b538642fec4c4b96430013b877a9753d7f Fix tests for prefer_manifests_to_list_files property (Nikhil Collooru)\n- 996738a178d4b872ef26e83c39d20c1815637c84 Fix NPE when a FlatMap column is all empty (Bhavani Hari)\n- 388055f1c46d0481aa5f0d7d57c1968bde10cf6b Add more filter rates to BenchmarkSelectiveStreamReaders (Ying Su)\n- 297b089ea5fd786114d3944554d507470af25e76 Improve reading the lengthVector in SliceDirectSelectiveStreamReader (Ying Su)\n- 02e09811ff09d9f27ab5d99394e6b1cdc513a792 Improve reading unbounded varchar with batch read mode (Ying Su)\n- dc2d50bc51984e0e46577b3792eceabf33a679cb Make query result HTTP compression server configurable (James Petty)\n- d0c739356ee99e4da453477c14a935cfd76262eb Make query result HTTP compression client configurable (James Petty)\n- 1e00009c582c36ce585e85e9ea583450a59c02d5 Optimize array_intersect by using OptimizedTypedSet (Ying Su)\n- 87c776600afb054364e78ed155bc6076a16f14ae Optimize array_except by using OptimizedTypedSet (Ying Su)\n- aabf9d6314cd1dcf25284df3f634ee5b777d5961 Optimize array_union using OptimizedTypedSet (Ying Su)\n- 23e1ba71a6acf0e37b2147b18ec13bca7b6cbba2 Optimize map_concat using OptimizedTypedSet (Ying Su)\n- 30cacee442adf246b38b9fc986e2fea51aa454b0 Introducing OptimizedTypedSet (Ying Su)\n- ec7336d8a8302992cbce3dab1047f13f7ecc65ee Add self join and table scan assignment tests for dereference pushdown (Beinan Wang)\n- 130527265fd8aebee395ed2412f4058d0e269f51 Fix base column name not present error when dereference pushdown enabled (Beinan Wang)\n- 1a804ac9e5f34316af2b27793624990b5f6c5bf4 Fixes for prefer_manifests_to_list_files property (Nikhil Collooru)\n- cba87b4f111983b5483ab2ecf580573ac0afa595 Add syntax support for CREATE MATERIALIZED VIEW (Ge Gao)\n- fe1bc57c9442a368276be3a4536ae760104efaf0 Add verifier flag for removing memory related session property (Wenlei Xie)\n- ca64162a0c43482b75e8dd41313472ddba8f9d86 Update to airlift 0.198 and airbase 100 (Leiqing Cai)\n- 7b41e69c40eb9815e13dce6b41f9a2545c97c60d Add session property manifest_verification_enabled (Nikhil Collooru)\n- ce41e2f7af8815b96da95b5be389d5d57377e887 Add PartitionLoader to load partitions (Nikhil Collooru)\n- 04c9ef6307faf35d2fa04563f52002586010a63c Add new session property prefer_manifests_to_list_files (Nikhil Collooru)\n- 951ec9dfbbf66fb68c3df77d20274cf9f500d38c Build presto-docs in Travis (Leiqing Cai)\n- 1918b624f74aa069610a998a0187b2c79209ac41 Do not treat warning as errors when making presto-docs (Leiqing Cai)\n- 817d50ac5091cdb5d71e5e90a4c9e57875bb5805 Fix split cacheable setting for affinity scheduling (Ke Wang)\n- c0e24ec7b32d0361ef741d7430e255edfaf75ab9 Add UDF to map enum value to key (Daniel Ohayon)\n- 59e88dc9e1d441593868080897b44c2b93873db5 Fix LongEnum deserialization in map keys (Daniel Ohayon)\n- 9d5db3c1c4b809a8a0a503ef22039d334ffd3898 Fix extracting logic in dynamic filtering when integrated with filter pushdown (Ke Wang)\n- c96938bf960b62da8fb4700c752b61979753303c Store file names and sizes in partition parameters (Nikhil Collooru)\n- 36fdd4072cf09c4686762702b90f0d4de3fa1867 Add getFileSize() to retrieve the size of the file (Nikhil Collooru)\n- faee0a05677d0453832a7ad8cf35fd959c670d2a Add filter pushdown tests for native worker (Masha Basmanova)\n- 566b0d84827fc1c85af4c88b7a060970913a9897 Change task threshold revoking config type to DataSize (Saksham Sachdev)\n- 0bb6d8dac2fb4cf0c2bcfa3b5777940185e0f397 Null unused array elements in SpillableFinalOnlyGroupedAccumulator (Saksham Sachdev)\n- 2e67116d263b02b97ef507e8ca978af731747d06 Use serialized plan for fragment result caching (Shixuan Fan)\n- 15b0ea204136c1dc14ecffb109b81102bb385c69 Minor refactor to SingleStreamSpillerChoice (Wenlei Xie)\n- aef9bfc29149418b4014974c7db4bda5aabc5ee3 Fix not registering TempStorage in PluginManager (Wenlei Xie)\n- 6970b6da9f392a8f6eb41c974c7eeb15f69c62da Refactor TempStorage related interfaces (Wenlei Xie)\n- 548457ab4e63c9b045bfffa6f6d9a8fb8d6a7bb3 Pass in SqlParserOptions to HttpRequestSessionContext for Prepared Statements. (Dong Shi)\n- 2499b40de1599a99eea4f898c5e1cb24aefae9ea Add per-node revocable memory limit (Saksham Sachdev)\n- 32e32af04f82b2791a6c9e42ccd67f4794c3f7e5 Add warning for Presto on Spark (Vic Zhang)\n- 68b3c78fc6b189c56a5d96a2e932e204b9464605 Reuse immutable partition fields in Glue partition conversion (James Petty)\n- 1b842223640f2e6595e72e5f8b26d0f74662b8f8 Add pre-read hueristic to parquet MetadataReader (James Petty)\n- b23ce2e00046f108ebfd47497da7d6095f939f6e Remove spill-enabled requirement for enabling join spill (Rebecca Schlussel)\n- b2208ec92a3119f8365440cdcc8206781a215987 Fix the check to correctly capture NULL positions (Bhavani Hari)\n- 63d85a3c4998370738e0a859c1bda1061ee316cb Add ParametricType APIs to FunctionNamespaceManager (Rongrong Zhong)\n- a6d77a63e29adaaff1047c73f8448c66aebac479 Add QualifiedObjectName to TypeSignature (Rongrong Zhong)\n- b411d77178848abfa31d903d3d0c1a4fe1024390 Merge BuiltInTypeRegistry and BuiltinFunctionNamespaceManager (Rongrong Zhong)\n- 0ffe6dfca5a0220aeb54afe9d043ad6a19bb4574 Move QualifiedObjectName to presto-common (Rongrong Zhong)\n- bde30f6ba348ad20b729cdba4c0d54a574bb1cf2 Refactor type coercion logic to TypeCoercer (Rongrong Zhong)\n- d8da5bc2cbcf8fade280e57ffa4893abdffff240 Rename TypeRegistry to BuiltInTypeRegistry (Rongrong Zhong)\n- cd44d2409d5413385ade194eefd539ca017d639e Clean up TypeManager APIs (Rongrong Zhong)\n- ae66246ebbb9bcf4f1deabc399543d26beadebd3 Remove Metadata.getTypeManager (Rongrong Zhong)\n- e24dcc2b6411164812773667f2d385d5208a477e Support function namespace manager for Presto-on-Spark (Vic Zhang)\n- be1e61cdaea3f86e4433ba9728e1ac5d674c698e Refactor StaticFunctionNamespaceStore to load properties map (Vic Zhang)\n- 11535d41414a13709cf811e0aab3fbefa0bbaaf2 Chage Block's toString to be unique with respect to hashCode (Rongrong Zhong)\n- 5e1687b9f716c8fd32a3a7ef10840161e96e2b44 Fix LambdaDefinitionExpression canonicalization (Rongrong Zhong)\n- 455cd6c11cd08498caede087dd9f21f5b5a2eddc Load configuration manager for Presto-on-Spark driver (Vic Zhang)", "NaN"], ["15501", "Inject null provider when thrift execution is not needed", "Rongrong Zhong", "rongrong", "12/08/20, 11:30:22 PM", "Test plan - run Presto server\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix injection issue when using function namespace manager\r\n```\r", "NaN"], ["15502", "Delete external worker test", null, "bhhari", "01/16/21, 12:24:02 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15504", "Add file based password authenticator plugin", "Jalpreet Singh Nanda", "imjalpreet", "04/09/21, 06:29:31 PM", "Cherry-pick of https://github.com/prestosql/presto/pull/1912 (https://github.com/prestosql/presto/pull/1912)\r\n\r\nCo-authored-by: David Phillips <david@acz.org>\r\nCo-authored-by: Rupam Kundu <rkundu@qubole.com>\r\n\r\nRelated Issue: #15482\r\n\r\nThis PR also includes moving LDAP Authenticator to a separate package\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add file based password authenticator plugin", "NaN"], ["15505", "Add listener model for TaskThresholdMemoryRevokingScheduler", "Saksham", "sachdevs", "01/07/21, 09:07:03 AM", "Test plan - deployed to cluster, travis, wrote tests\r\n\r\nPreviously, TaskThresholdMemoryRevokingScheduler had only a polling based model for when a task allocates too much memory. This introduces a listener-based model instead which ensures that we do not have to wait for the polling thread to be scheduled.  Instead, we immediately schedule revoking.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add listener-based revocation model for spilling strategy PER_TASK_MEMORY_THRESHOLD\r\n```", "NaN"], ["15508", "Port reverse(varbinary) from Prestosql to Prestodb", null, "EmmyEmmy", "12/10/20, 12:36:35 AM", "Port commit from Prestosql https://github.com/prestosql/presto/commit/ac585502abd05b413644bb65a30a728b0213f19a to Prestodb", "NaN"], ["15509", "More lock contention reductions", "James Petty", "pettyjamesm", "01/12/21, 05:06:01 AM", "Continues lock contention reductions from https://github.com/prestodb/presto/pull/15477\r\n\r\nSeven commits:\r\n- Cache `isBlocked` future in various operators to avoid repeatedly contending for locks and potentially registering new callbacks once those operators are blocked\r\n- Add unsynchronized fast-paths in `ExchangeClient` to avoid unnecessary contention\r\n- Refactor `ExchangeClient#pollPage` to avoid redundant checks and synchronizations\r\n- Use `AtomicIntegerFieldUpdater` for local exchange `PageReference`\r\n- Reduce synchronization in `LocalExchangeMemoryManager` by making unblocking an edge-triggered event that only a single caller initiates.\r\n- Add unsynchronized common-case fast paths in `LocalExchangeSource`\r\n- Remove synchronized for final field read in `MemoryPool#getMaxBytes()`\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15511", "Deprecating Joda library in Presto Cassandra", null, "msolgi20", "12/09/20, 09:25:05 PM", "From https://www.joda.org/joda-time/\r\n\r\n> Note that from Java SE 8 onwards, users are asked to migrate to java.time (JSR-310) - a core part of the JDK which replaces this project.\r\n\r\nAccordingly, this PR removes dependency on Joda library in Presto Cassandra and uses java.time instead\r\n\r\nTest plan: \r\n`mvn test` passes all existing tests.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15515", "Fix broadcast memory bytes update", "Vic Zhang", "viczhang861", "12/14/20, 03:58:23 PM", "- It is not necessary to update broadcast memory bytes if the query has no broadcast join. This PR fixed it.\r\n- When a query exceeds max broadcast memory bytes, query will only be killed after verifying that the query has broadcast join,  therefore, no query is badly affected, this PR only fixed unnecessary update.\r\n\r\n- Max broadcast memory bytes for Presto on Spark is now correctly set.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15517", "Avoid costly resizes of the addresses dynamic array in PagesIndex", "Andrii Rosa", "arhimondr", "12/23/20, 12:21:55 AM", "On memory constraint environments resizing the `valueAddresses` can cause OOM. Also given the high expansion factor it may end up causing significant memory overhead.\r\n\r\nFor example if the current size of `valueAddresses` is ~2GB (260m elements) trying to expand it to 4GB can cause a premature OOM, as there might not be enough memory to accommodate 2 strongly reachable arrays (2GB + 4GB).\r\nAlso it is very likely to result in a significant memory waste if the expansion happens just to accommodate only a few overflowing elements. \r\n\r\nThis patch replaces a contiguous `LongArrayList` with the `AdaptiveLongBigArray`. `AdaptiveLongBigArray` allocates memory gradually. It is implemented by employing another level of indirection:\r\n\r\n```\r\n[1,2,3,4,5] -> [block1 -> [1,2,3], block2->[4,5]] \r\n```\r\n\r\nThis allows more gradual memory allocation, and eliminates the need of expensive array expansions. \r\n\r\nHowever everything comes at a price. Another indirection adds a cost of additional memory lookup. In theory that may negatively impact CPU cache locality and decrease performance.\r\n\r\nTo decrease the effects of the indirection the `AdaptiveLongBigArray` employs both techniques. It is expanding linear array (segment) up to 256MB, and only then adds another segment. With large segments the first table (array of long arrays) should be very small (~5 - 10 elements, 40 - 80 bytes)* and should easily fit into L1 CPU cache.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15520", "Replace Joda-Time libraries with java.time in Presto MongoDB", "Sorin Stoiana", "sstoiana", "12/16/20, 10:24:26 PM", "Replace Joda-Time libraries with java.time in Presto MongoDB\r\n    \r\n    Since Java 8 we have java.time packages which are equivalent to Joda and\r\n    the recommendation from the author of the Joda-Time is to migrate to\r\n    java.time(JSR-310) library.\r\n\r\nTest plan\r\n* I updated TestMongoIntegrationSmokeTest.java to include TIME\r\n* mvn test\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15521", "Fix accidental per-instance logger in SerializedPageWriteListener", "James Petty", "pettyjamesm", "12/15/20, 05:14:04 PM", "Identified as a contention bottleneck due to calling `java.util.logging.Logger.getLogger` under the hood on each request.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15522", "Initialize memory limits once", "James Petty", "pettyjamesm", "12/23/20, 08:05:38 PM", "Synchronizing on the `QueryContext` as part of every task update adds contention to the already heavily contended `QueryContext` instance lock (since each memory tracking update must synchronize as well). This change checks whether the memory limit initialization is necessary before synchronizing to avoid adding the extra contention after they\r\nhave been set at least once successfully.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15524", "Refactor AbstractTestQueryFramework to supply expectedQueryRunner", "john roll", "jbroll", "12/16/20, 07:46:19 PM", "Test plan - \r\n\r\nTravis\r\n\r\nRefactor AbstractTestQueryFramework to allow an expectedQueryRunnerSupplier to be provided in the \r\nconstructor instead of the hard coded H2QueryRunner.  H2 is still the default for the current constructor.  \r\nThis will allow presto_cpp to use AbstractTestQueryFramework and QueryAssertions to directly compare \r\nquery results from the Java and C++ workers.\r\n\r\nThis is used by presto_cpp here : https://github.com/facebookexternal/presto_cpp/pull/239\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15525", "Generate presto-internal.thrift file", "john roll", "jbroll", "01/25/21, 02:23:36 PM", "Test plan - \r\n\r\nrun \r\n```\r\nmvn process-classes -DskipTests -T1C\r\n```\r\n\r\ninspect presto-thrift-spec/target/presto-internal-0.247-SNAPSHOT.thrift\r\n\r\n```\r\nmvn clean install -DskipTests -T1C -pl '!presto-docs,!presto-server-rpm'\r\njroll@fb : ls -l ~/.m2/repository/com/facebook/presto/presto-thrift-spec/0.247-SNAPSHOT/presto-thrift-spec-0.247-SNAPSHOT.thrift\r\n-rw-r--r--  1 jroll  staff  1371 Jan 22 09:29 /Users/jroll/.m2/repository/com/facebook/presto/presto-thrift-spec/0.247-SNAPSHOT/presto-thrift-spec-0.247-SNAPSHOT.thrift\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add maven plugin for drift to generate the Thrift spec for a list of classes, current class list is **TaskStatus**\r\n\r\n\r", "NaN"], ["15526", "OrcWriter close errors are masked", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "01/12/21, 09:55:44 PM", "If there are errors in the ColumnWriter close, it is masked by reset's checkState\r\nhttps://github.com/prestodb/presto/blob/master/presto-orc/src/main/java/com/facebook/presto/orc/writer/SliceDictionaryColumnWriter.java#L559\r\n\r\nThis change Split's the code into flushColumnWriters and rest, so that flushColumnWriters can be moved outside of the\r\ntry block. Previously any failure in the flushColumnWriters part would have been masked  by the reset's IllegalStateException.\r\n\r\nIt is tedious to write test for this use case, unless the column writers are refactored into the ColumnWriterFactory and the factory is injected. So will rely on the existing tests. I spent close to an hour trying to find a suitable place to inject failure and found none.\r\n\r\nThis regression was introduced in https://github.com/prestodb/presto/pull/13508\r\n\r\nOther approach to this problem is make reset not throw, but I am not sure what the implications of such a change would be.\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines. Don't forget to follow our [attribution guidelines](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#attribution) for any code copied from other projects.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r", "NaN"], ["15527", "Make getExpectedOrdersTableDescription protected", "Rohit Jain", "jainxrohit", "12/17/20, 02:20:31 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1357\r", "NaN"], ["15529", "Split large pages in TempSingleStreamSpiller", "Rebecca Schlussel", "rschlussel", "12/17/20, 05:40:55 PM", "This prevents int overflow when serializing pages, since page\r\nserialization requires the page size in bytes to fit in an integer.\r\n\r\nTest plan - None\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix a possible int overflow error when spilling to temp storage\r\n```\r", "NaN"], ["15530", "Scan memory optimizations", "Andrii Rosa", "arhimondr", "12/22/20, 10:58:04 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15532", "Revert \"Fix temporary table deletion for exchange materialization\"", "Wenlei Xie", "wenleix", "12/16/20, 11:18:05 PM", "This reverts commit d2cb24cd4cec630940463d8d8dc25806c774f00d.\r\n\r\nDifferent LocationService might implement the different convention for\r\ntemporary table location and not necessarily always follow the\r\n\"_presto_temporary_table_${uuid}/${uuid}/\" pattern.\r\n\r\nTest plan - Travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15533", "Refactor JoinBridgeProvider", "Rebecca Schlussel", "rschlussel", "12/17/20, 02:56:02 PM", "Make JoinBridgeProvider a supplier rather than a Function since none of\r\nthe implementations were using the passed in argument\r\n\r\nTest plan - travis build\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15534", "Improve error handling for spill", "Rebecca Schlussel", "rschlussel", "12/17/20, 10:36:49 PM", "checkSuccess wrapped exceptions in an IllegalArgumentException, which\r\nmeant the actual error was buried in the stacktrace. getFutureValue\r\nwrapped the source exception in a runtime exception, but only included\r\nthe error stacktrace and not the stack to the  getFutureValue call.\r\nWithout the other stack trace, you don't know where in the operator\r\nexecution spill failed, which can make debugging harder.\r\n\r\nTest Plan - Ran tests with max spilled bytes set to 1B and looked at the\r\nerrors.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15535", "Add PrestoThriftPage", "Rongrong Zhong", "rongrong", "12/18/20, 11:47:26 PM", "Wrap List<PrestoThriftBlock> with positionCount so we can handle\r\nfunctions taking no arguments.\r\n\r\nTest plan - travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15536", "Fixes for file renaming and manifest features", "Nikhil Collooru", "NikhilCollooru", "01/04/21, 06:29:48 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1363", "NaN"], ["15537", "Deprecate Joda library for Presto Teradata functions", "Vladimirs Kotovs", "rk13", "03/08/21, 09:13:23 PM", "Test plan - (Please fill in how you tested your changes)\r\n```\r\n# make sure all test scenarios are covered\r\nmvn clean test\r\n```\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines. Don't forget to follow our [attribution guidelines](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#attribution) for any code copied from other projects.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15539", "Minor fix to Hive temporary table location", "Wenlei Xie", "wenleix", "12/23/20, 07:44:34 PM", "Temporary table name is unique and already contains UUID, doesn't need\r\nto append the extra UUID sub-directory.\r\n\r\nSee https://github.com/prestodb/presto/pull/15532 and https://github.com/prestodb/presto/pull/15486\r\n\r\nTest plan - Travis\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15544", "Add flag to set cache eviction retries", "Bin Fan", "apc999", "12/18/20, 04:39:51 AM", "Add a flag to enable retries when Alluxio cache eviction failed.\r\nThis feature is particularly helpful if there are a lot of small files cached\r\n\r\nTest plan - unit tests added\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15549", "Fix SliceDirectSelectiveStreamReader for dataLength is 0", "Ying", "yingsu00", "01/04/21, 10:19:02 PM", "For batch read mode, when dataLength is 0 and offsets array was not\r\nreset, the offsets array for previous batch would be used to create\r\nthe output Block. This causes IndexOutOfBoundsException for downstream\r\noperators. This fix initializes the offsets array to 0 for the batch\r\nread mode.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15550", "Support driver level plan in fragment result caching", "Shixuan Fan", "shixuan-fan", "12/23/20, 09:57:08 PM", "Test plan - Added unit test. Will also do verifier correctness test\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Support fragment result caching for queries with local exchange (e.g. intermediate aggregation).\r\n```\r", "NaN"], ["15551", "Upgrade Drift to 1.32", "Andrii Rosa", "arhimondr", "12/22/20, 12:11:18 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15553", "Remove redundant setter in TestHiveClientConfig", "Shixuan Fan", "shixuan-fan", "12/22/20, 11:28:53 PM", "Test plan - existing test does not fail\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15554", "Add hive partition stats based optimization", "Shixuan Fan", "shixuan-fan", "01/28/21, 05:01:30 AM", "Test plan - Added unit test for `HiveSplitManager`. Will also run verifier against internal queries.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n* Change `ConnectorTableLayoutHandle#getIdentifier` to accept a new field `Optional<ConnectorSplit> split`\r\n\r\nHive Changes\r\n* Support partition stats based optimization, including partition pruning and column domain stripping for fragment result caching.\r\n```", "NaN"], ["15555", "Fix QualifiedObjectName instantiation in FunctionAndTypeManager", "Rongrong Zhong", "rongrong", "12/23/20, 08:02:25 PM", "QualifiedObjectName expect catalog and schema to be lowercase.\r\nUse QualifiedName.parts instead of QualifiedName.originalParts when\r\ninstantiating the object in FunctionAndTypeManager.\r\n\r\nTest plan - travis\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix a bug introduced in :pr:`15313` that would cause queries to fail when using upper case in SQL function catalog schema names.\r\n```", "NaN"], ["15557", "Add Kafka Json encoder", "yang", "yangy0000", "02/05/21, 08:41:14 PM", "Cherry-pick of https://github.com/prestosql/presto/commit/85204c1bbbe3df2dacd25d8db4b9178de555c052#diff-822c322b24ab891f0ad5abf372f197351788e75be3d6b3cb11af01d7d9d6621e\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15558", "Provide more detail in MULTI_CATALOG_WRITE_CONFLICT", "Wenlei Xie", "wenleix", "12/23/20, 09:06:14 PM", "Test plan - Travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15560", "Upgrade JTS to 1.18.0", "James Gill", "jagill", "01/04/21, 02:37:59 PM", "Relevant parts:\r\n* Vastly improved overlay operations (intersection, union, etc).  This\r\n  will allow us to use JTS operations (currently using ESRI).\r\n  + TopologicalExceptions have been mostly eliminated.\r\n  + Performance is improved.  For geometries that intersect in a small\r\n    fraction of their area, performance is greatly improved.\r\n  + More accurate in some cases.\r\n  + https://locationtech.github.io/jts/javadoc/org/locationtech/jts/operation/overlayng/package-summary.html\r\n* Fix for `buffer` and `DouglasPeuckerSimplifier`: in some cases, the\r\n   majority of the polygon would be dropped:\r\n  + https://github.com/locationtech/jts/pull/655\r\n  + https://github.com/locationtech/jts/issues/498\r\n* `WKBWriter` writes empty polygons in a fashion consistent with other\r\n  libraries/tools.\r\n\r\nMore details: https://github.com/locationtech/jts/releases/tag/jts-1.18.0\r\n\r\nTest plan -\r\nAll geospatial unit tests pass.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeospatial Changes\r\n* Upgrade JTS to 1.18.0\r\n\r\n```\r", "NaN"], ["15563", "Introduce UserDefinedType", "Rongrong Zhong", "rongrong", "01/22/21, 03:32:04 AM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1372\r\n\r\nIntroduce the concept of UserDefinedType as spedified in spec16. User defined\r\ntypes are managed by FunctionNamespaceManager.\r\n\r\nThe spec has more metadata specified for user defined types but we currently\r\nonly add\r\n```\r\n<user-defined type definition> ::= CREATE TYPE <user-defined type body>\r\n<user-defined type body> ::= <schema-resolved user-defined type name>\r\n    [ AS <representation> ]\r\n<representation> ::=\r\n    <predefined type>\r\n  | <collection type>\r\n  | <member list>\r\n```\r\nFor <representation>, we use TypeSignature to represent all options above.\r\n\r\nEnum type signature format is changed from `typename(enum:type{enum_map})` to\r\n`enumtype(typename{enum_map})` so enum types can behave the same way as other\r\nbuiltin parametric types that's fully managed by\r\nBuiltInTypeAndFunctionNamespaceManager.\r\n\r\nTest plan - Travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15565", "Add QueryInterceptor to presto-jdbc", "Rob Peterson", "robbypete", "01/15/21, 01:06:01 AM", "Introduce chainable [query interceptors](https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-interceptors.html) to Presto JDBC. QueryInterceptors can modify the behavior of a query or perform side effects. QueryInterceptors' `preProcess()` and `postProcess()` methods are run before and after the query is executed in the server, respectively. If more than one QueryInterceptor classname is provided separated by a semicolon, each will be called one-by-one from left to right in the classnames provided in the connection string. Both methods can optionally return a PrestoResultSet to override the ResultSet returned by the query.\r\n\r\nAddresses issue #15142 \r\n\r\nTest plan - Run new tests in TestQueryInterceptor, TestPrestoDriverUri, and TestJdbcConnection\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nJDBC changes\r\n* Add the interface `QueryInterceptor` to allow for custom logic to be executed before or after query execution. (:pr:`15565`)", "NaN"], ["15569", "Fix memory revoking accounting", "James Petty", "pettyjamesm", "01/12/21, 07:50:20 PM", "Previously, `MemoryRevokingScheduler` (and its task-threshold flavored counterpart) would find running tasks but then operate on them by traversing their `QueryContext`. This led to two significant problems:\r\n- `MemoryRevokingScheduler` would over-count the amount of memory that was already being revoked since it would traverse each task for a given query from all other tasks in that query.\r\n- Both flavors of memory revoking scheduler would repeatedly perform the same traversals and attempt to revoke memory from the same tasks and operators based on the number of tasks a given query had\r\n\r\nThis PR also contains some cleanup commits that refactor related methods to avoid redundant work and to use cheaper methods than `SqlTask#getTaskStatus()` when possible.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix accounting for revocable memory that could cause some queries not to spill when they should.\r\n```\r", "NaN"], ["15581", "Fix output counting in OptimizedPartitionedOutputOperator", "Sean Wang", "iamhg", "01/23/21, 09:46:47 AM", "Resolves https://github.com/prestodb/presto/issues/11770\r\n\r\nOptimizedPartitionedOutputOperator used to report the output data size as the input data size. However, output data size might be larger due to loss of run-length and/or dictionary encodings. Furthermore, the output position count for the replicated case used to be the same as the input position count, while the correct value should be the input position count times partition count. This commit fixes this issue by reporting the output sizes right before flushing each partition when its buffer is full, where the accurate data size and positionCount can be obtained.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix a bug for reporting output data sizes for optimized repartitioning\r\n```", "NaN"], ["15582", "Add release notes for 0.245.1", "Ying", "yingsu00", "01/05/21, 04:06:57 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15585", "Require min 30s query.min-expire-age", null, "aweisberg", "01/05/21, 08:29:15 PM", "Tested manually with TestQueryManagerConfig to make sure the minimum is enforced. 29 seconds fails, 30 seconds works.\r\n\r\nThis turns out to be important when clients are on higher latency connections. The client library may poll for status for a query that has completed and the coordinator may have expired the completed query causing the client library to generate an error.\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Add 30 second minimum to `query.min-expire-age` configuration parameter.\r\n```\r\n\r", "NaN"], ["15588", "Implement MAP_UNION_SUM aggregation", "Sreeni Viswanadha", "kaikalur", "02/08/21, 07:16:12 AM", "This is a heavily used aggregation that aggregates maps into one by adding values for matching keys. SQL/lambda versions are inefficient. So we implement them natively.\r\n\r\nTest plan - Added tests\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n*  :func `map_union_sum`\r\n \r\n```\r", "NaN"], ["15589", "Support materialized view creation, drop and query", "Ge Gao", "gggrace14", "03/15/21, 04:38:14 AM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1438", "NaN"], ["15592", "Disable spill for probe only grouped execution", "Rebecca Schlussel", "rschlussel", "01/07/21, 10:37:06 PM", "This PR is to unblock safely rolling out spill for join.  Spill for joins where only the probe side uses grouped execution was broken because some code relied on having a fixed number of probe operators for each build operator.  These queries would fail with errors like \"5 probe operators finished out of 4 declared\" \r\n\r\nA proper fix would be to remove the dependency on a fixed number of probe operators, and instead do something similar to how JoinLifecycle works, where you keep track of how many probe operator references you have, and only destroy the build table if all probe operators have finished and the LookupJoinOperatorFactory is closed.  But that fix requires more rigorous testing to ensure we're able to keep unspilling and respilling the partitions of the hash table.\r\n\r\nTest plan - Verifier.  I had trouble writing a test that would reliably finish, but I ran this with verifier on queries I had previously seen failures on, and the errors went away.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Disable spill to disk for join queries where the probe side uses grouped execution and the build does not.  Previously these queries would fail with generic_internal_errors.\r\n```", "NaN"], ["15593", "Build presto-docs with Python3 venv", "Leiqing Cai", "caithagoras", "01/13/21, 12:01:37 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15594", "Add array_intersect SQL function that accepts array of array", "Rongrong Zhong", "rongrong", "01/08/21, 09:44:09 PM", "array_intersect(array<array<bigint>>) -> array<bigint>\r\narray_intersect(array<array<double>>) -> array<double>\r\n\r\nTest plan - Travis\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add function ``array_intersect`` that takes an array of array as input.\r\n```\r", "NaN"], ["15595", "Add setter for BlockEncodingSerde in remote function executor", "Rongrong Zhong", "rongrong", "01/09/21, 01:35:16 AM", "Remote function executor should use the same BlockEncodingSerde as the\r\nmain module rather than creating their own because plugins can register\r\nBlockEncoding.\r\n\r\nThis is a follow up to issues brought up in #15457\r\n\r\nTest plan - Travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15599", "Add isNewTable and table path to HdfsContext", "Vic Zhang", "viczhang861", "02/10/21, 09:12:36 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1378\r\n\r\n\r\nTest plan \r\n- Facebook integration test is not working as I am able to compile and build snapshot locally\r\n- End to end test results in https://github.com/facebookexternal/presto-facebook/pull/1378\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15602", "Add release notes for 0.246", "Leiqing Cai", "caithagoras", "01/27/21, 02:30:42 AM", "# Missing Release Notes\n## Bhavani Hari\n- [x] https://github.com/prestodb/presto/pull/15499 Add release notes for 0.245 (Merged by: Wenlei Xie)\n\n## Emy Sun\n- [ ] https://github.com/prestodb/presto/pull/15508 Port reverse(varbinary) from Prestosql to Prestodb (Merged by: Rongrong Zhong)\n\n## George Wang\n- [x] https://github.com/prestodb/presto/pull/14915 Avoid planning unnecessary LIMIT/TopN/Sort/DistinctLimit (Merged by: Rebecca Schlussel)\n- [x] https://github.com/prestodb/presto/pull/14915 Avoid planning unnecessary LIMIT/TopN/Sort/DistinctLimit (Merged by: Rebecca Schlussel)\n\n## shenh062326\n- [x] d2cb24cd4cec630940463d8d8dc25806c774f00d Fix temporary table deletion for exchange materialization\n\n# Extracted Release Notes\n- #15322 (Author: Rebecca Schlussel): Only allow constants for prepared statement parameters\n  - Fix an issue where prepared statements would allow some non-constant parameters.\n- #15322 (Author: Rebecca Schlussel): Only allow constants for prepared statement parameters\n  - Fix an issue where prepared statements would allow some non-constant parameters.\n- #15470 (Author: George Wang): Ensure nullable partitioning column is handled properly\n  - Fix dynamic pruning failures for joining on null keys in hive partition.\n- #15485 (Author: Sreeni Viswanadha): Disallow ORDER BY literal in window functions\n  - Disallow ORDER BY literals used with Window functions as it's not useful, expensive and most often used wrongly.\n- #15501 (Author: Rongrong Zhong): Inject null provider when thrift execution is not needed\n  - Fix injection issue when using function namespace manager.\n- #15505 (Author: Saksham Sachdev): Add listener model for TaskThresholdMemoryRevokingScheduler\n  - Add listener-based revocation model for spilling strategy PER_TASK_MEMORY_THRESHOLD.\n- #15529 (Author: Rebecca Schlussel): Split large pages in TempSingleStreamSpiller\n  - Fix a possible int overflow error when spilling to temp storage.\n- #15550 (Author: Shixuan Fan): Support driver level plan in fragment result caching\n  - Support fragment result caching for queries with local exchange (e.g. intermediate aggregation).\n- #15550 (Author: Shixuan Fan): Support driver level plan in fragment result caching\n  - Support fragment result caching for queries with local exchange (e.g. intermediate aggregation).\n- #15555 (Author: Rongrong Zhong): Fix QualifiedObjectName instantiation in FunctionAndTypeManager\n  - Fix a bug introduced in :pr:`15313` that would cause queries to fail when using upper case in SQL function catalog schema names.\n- #15560 (Author: James Gill): Upgrade JTS to 1.18.0\n  - Upgrade JTS to 1.18.0.\n- #15585 (Author: Ariel Weisberg): Require min 30s query.min-expire-age\n  - Add 30 second minimum to `query.min-expire-age` configuration parameter.\n- #15592 (Author: Rebecca Schlussel): Disable spill for probe only grouped execution\n  - Disable spill to disk for join queries where the probe side uses grouped execution and the build does not.  Previously these queries would fail with generic_internal_errors.\n\n# All Commits\n- dc00b9ff1df66f932326194b1303e677a5278f04 Disable spill for probe only grouped execution (Rebecca Schlussel)\n- 8148cb3f329bca8cc13226e19fac34d25e4a0ecc Dereference SerializedPageReferences in batches (James Petty)\n- 7ce6c1f1c80d3b39fce75a756bae47e63745ca96 Avoid unnecessary synchronization in LazyOutputBuffer (James Petty)\n- ca9466572da066f29c894d6b2ee5600aeab2dffa Add listener model for TaskThresholdMemoryRevokingScheduler (Saksham Sachdev)\n- ca7610b70370dbfa7b4a7cc7df6a03381e50cb36 Require min 30s query.min-expire-age (Ariel Weisberg)\n- ab78e27d92221a616133027ef88b3385948ec8b3 Fix SliceDirectSelectiveStreamReader for ARRAY of VARCHAR (Ying Su)\n- c3d0a4d762b387ab3c2f576e478a84527c9dc6d8 Fix testSubfieldValue in OrcTester (Ying Su)\n- 27fc29b9a7d0b2e8704dfb3a42b00851535d1212 Limit batch read mode to certain conditions for SliceDirectSelectiveStreamReader (Ying Su)\n- 70f379dd2b0287cdb4a92de59faadc71813d9a84 Fix SliceDirectSelectiveStreamReader for dataLength is 0 (Ying Su)\n- 18cbfa78167df5d9e18172886d452a3bda4feda1 Extract assertBlockEquals() in OrcTester (Ying Su)\n- 6f4e4443a21f82e1cd5014f641387dabc1e2623a Upgrade JTS to 1.18.0 (James Gill)\n- cebe0c29aa14ce7c84d40c70684c3cc76bc990f9 Store manifest only when they are preferred and file renaming is enabled (Nikhil Collooru)\n- aa1fd64545ee5be5bdfd0081a20107dad5b145a7 Track compressed manifest size when file_renaming is enabled (Nikhil Collooru)\n- 6a47cf2557087b3d0aff4af154d8f4da53928297 Add containsNumberedFileNames to PartitionUpdate (Nikhil Collooru)\n- f538920ae958a51961e0d1b2f134b40445844b5d Fix sorting file names for bucketed splits (Nikhil Collooru)\n- effd0ea39af14f737b6b09d96df5f19d8fe66ddc Disallow ORDER BY literal (ordinal) in window functions as this may be a misunderstood feature. (Sreeni Viswanadha)\n- 844dbebc2cd417835e4506fac31a7da600cf7e13 Avoid planning unnecessary Sort (George Wang)\n- 90cd1d004cf458d397dd50c40bb57cc85ac53be9 Avoid planning unnecessary LIMIT/TopN/Sort/DistinctLimit (George Wang)\n- b1846c1bae374a0bc70675166fa1903b2b1ddd83 Support driver level plan in fragment result caching (Shixuan Fan)\n- b950aec7a5d4ebc12f56aad0a127f53c3601592c Remove unused method in LocalExecutionPlanner (Shixuan Fan)\n- 8a005811dc02be9474c220a47c648a9c173b5e83 Provide more detail in MULTI_CATALOG_WRITE_CONFLICT (Wenlei Xie)\n- abdbd68f56011b439f71e5657f56b721620b0ee0 Fix javadoc method reference (James Petty)\n- 13be472fd60ef9a4d3a5a0b2620793515404e40a Fix typo in ConcurrentHashMap creation (James Petty)\n- f5a5af4b6e9d741e4d893a407b53b3a2ee9bb757 Avoid repeatedly setting memory limits in QueryContext (James Petty)\n- 1bb85b03c76ee564685d7dc0b6d32f5ce23b4aaf Fix QualifiedObjectName instantiation in FunctionAndTypeManager (Rongrong Zhong)\n- f31b960020d39189c65220a8aa2fc41e5ff40568 Minor fix to Hive temporary table location (Wenlei Xie)\n- adc7a640faf6263033fce8ea083ae3799f39ddce Avoid costly resizes of the addresses dynamic array in PagesIndex (Andrii Rosa)\n- 9dd4b7766c461c3b717ea6636569733aee6e53be Remove redundant setter in TestHiveClientConfig (Shixuan Fan)\n- 8d6ac0ee73f2bbc859f66e667c1b3a8ba99bac0a Reuse decompression buffer between multiple OrcInputStream's (Andrii Rosa)\n- 47a39d4d5252da11aabab5bdc43c89d04e15d907 Shrink cached buffers in OrcInputStream (Andrii Rosa)\n- 69b12d3bf1290cdc4bea01c6533dcab7999335c5 Improve OrcInputStream memory tracking (Andrii Rosa)\n- 839007f9b78b8fc1159ea1c14f4f23ceddd57e54 Upgrade Drift to 1.32 (Andrii Rosa)\n- bb3a2b78f97fd97b3ae005d5dbf8f46552309fcf Add PrestoThriftPage (Rongrong Zhong)\n- 7561c121a28de44ad632cfbd28b189b04553fb0d Add flag to set cache eviction retries (Bin Fan)\n- 7f3e6940da9cf187f30d8c79f5106d7b2c09db44 Improve error handling for spill (Rebecca Schlussel)\n- 6d77b0a868647d93e035ca93d160aaab8d60cbc8 Make AbstractTestXQueries classes abstract (Rebecca Schlussel)\n- 28f787bcd6b41ba2e2ff4bce4111018b3da2404d Add config to stop installation of disabled connectors (Saksham Sachdev)\n- a68b96b06f24496ababb21018d82edd5091ee74b Split large pages in TempSingleStreamSpiller (Rebecca Schlussel)\n- ab243b6ce24ac150bac4821c4a11342e89daa88a Refactor JoinBridgeProvider (Rebecca Schlussel)\n- a2939dc3afea1d407a9777f027ab3da113d6dd4a Make getExpectedOrdersTableDescription protected (Rohit Jain)\n- 382c83dc271617e7a33affb97228d2af5e84c2b4 Revert \"Fix temporary table deletion for exchange materialization\" (Wenlei Xie)\n- cfe964254b14c684beeb6d795975a14842b1b704 Replace Joda-Time libraries with java.time in Presto MongoDB (Sorin Stoiana)\n- 772621530a4b67f34f93bdeba9c29c86a4a5c978 Refactor AbstractTestQueryFramework to supply expectedQueryRunner (John Roll)\n- 1acb585f0195315fc55dc9c6e83d4405ef06fb30 Release notes for 0.244.1 (Bhavani Hari)\n- d2cb24cd4cec630940463d8d8dc25806c774f00d Fix temporary table deletion for exchange materialization (shenh062326)\n- 25c03ccad1758ee2a3d0336e73e49874e7f29ede Fix accidental per-instance logger in SerializedPageWriteListener (James Petty)\n- a4d7abded6aefddbe3526ad8ee0f01bbc55cb51f Set broadcast memory limit for Presto on Spark (Vic Zhang)\n- 3feecf3743ffd8c81c21b08ed89402ca45fe8c05 Fix broadcast memory update in RootAggregatedMemoryContext (Vic Zhang)\n- 4d5afceed18cc4f71a1574adafcd506db6e33e83 Support Serialized Presto Page in Thrift UDF service (Rongrong Zhong)\n- b49294b10a72654d6bc5a1f44d88979e1fd2f286 Move BlockEncodingManager to presto-common (Rongrong Zhong)\n- b3d92fcef735b6cce29ade81a452d5b5ba968dd6 Remove redundant code in EchoFirstInputThriftUdfService (Rongrong Zhong)\n- 8bea180fdc31be1f2c31b9e9d2b3b6f83ec47c63 Prefer default parallelism to avoid adding extra local exchange (Andrii Rosa)\n- 1eb7024d34b597367a6c4be071a8cbe0e35418bf Support different concurrency settings for table writer (Andrii Rosa)\n- b7d1a76eb2d443f65ada17357e169e7bdda6c8be Add reverse function for varbinary (Emy Sun)\n- 22efd1acfdf0f1c33aa8e3a3a2a9ead87905b8e8 Deprecating Joda library in Presto Cassandra (Moji Solgi)\n- 222b0df79bd9c0dc2799451ddb49c15643d8bfed Inject null provider when thrift execution is not needed (Rongrong Zhong)\n- 60f4d6891e28cb78665a4e9f736977b19623bae7 Fix incompatible reflection for modifying static final fields since JDK 12 (fornaix)\n- bf7bec4438accdde7bd60ec94845de7a1696be4b handle nulls properly in flatmap reader (Zhenyuan Zhao)\n- 9b3690acaf956a4e8f79dc2d6b504078400c9079 Clean up parameters related code (Rebecca Schlussel)\n- 457aa86c620f150a0837ddc0cdb456473c991c32 Only allow constants in prepared statement parameters (Rebecca Schlussel)\n- 729554d3a5b83909322417703992f8cc5e764497 Fix dynamic pruning for null keys in hive partition (George Wang)\n- 6913ec9a25187748dea95f991a139667d876526a Minor fix for memory property removal in Presto verifier (Wenlei Xie)", "NaN"], ["15604", "Use long for {Primitive}BigArray#capacity", "James Petty", "pettyjamesm", "01/14/21, 11:53:32 PM", "Previous implementations supported accesses by `long` index, but used `int` as the capacity field which could overflow. Overflowing was mostly benign but would cause `#ensureCapacity(long capacity)` to proceed down the slow-path unnecessarily.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15607", "Fix NPE in enum key lookup", "Daniel Ohayon", "daniel-ohayon", "01/20/21, 01:52:51 AM", "There is a race condition in the enum key lookup logic because we set `isFlippedEnumComputed` to `true` before actually setting `this.flippedEnum`. This only reproduces during distributed query execution.\r\n\r\nThe added tests fail on master and pass on this branch.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fixed a race condition in enum key lookup which caused queries using the `ENUM_KEY` UDF to crash occasionally with an internal error #15607\r\n\r\n```\r", "NaN"], ["15610", "Enable easy local cluster setup with cpp worker", null, "tanjialiang", "01/14/21, 08:52:24 PM", "In order to bring up a cpp worker cluster with java coordinator we used to do many hacks all over the codebase. This PR provides a clean solution for the local cluster to be built without any hacks.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15611", "Revert \"Avoid costly resizes of the addresses dynamic array in PagesI\u2026", "Rebecca Schlussel", "rschlussel", "01/14/21, 01:54:50 PM", "\u2026ndex\"\r\n\r\nThis reverts commit adc7a640faf6263033fce8ea083ae3799f39ddce.\r\nThis commit caused wrong results when there was a streaming aggregation\r\non top of a window function\r\n\r\nTest plan - Confirmed that query that had been returning wrong results now returns correct results.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15614", "Avoid costly resizes of the addresses dynamic array in PagesIndex", "Andrii Rosa", "arhimondr", "01/19/21, 01:31:16 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\nThis change was reverted as it introduced a regression in window function processing: https://github.com/prestodb/presto/pull/15611\r", "NaN"], ["15615", "Add more stats for fragment result cache manager", "Shixuan Fan", "shixuan-fan", "01/19/21, 09:30:04 PM", "Added cache size and cache removal counts\r\n\r\nTest plan - Enhanced unit test in `TestFileFragmentResultCacheManager`\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15617", "Skip Unsupported datatype from Object field in ES connector", "Reetika", "agrawalreetika", "05/27/21, 03:46:31 PM", "Skip Unsupported datatype from Object field in ES connector\r\nFixes #15616 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nElasticserarch Changes\r\n* Fix to avoid NullPointerException when there is an unsupported data type column in the Object field\r\n\r\n```", "NaN"], ["15623", "Make PageTransportErrorException extends PageTransportException", "Wenlei Xie", "wenleix", "01/21/21, 11:57:07 PM", "This allows coordinator re-categorize the Page transport error into\r\nREMOTE_HOST_GONE by looking at the error host.\r\n\r\nIt is possible for PageTransportErrorException to caused by worker\r\nrestart, such as 503 Service Unavailable.\r\n\r\nTest plan - Travis\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15626", "Reduce output buffer memory manager contention", "James Petty", "pettyjamesm", "02/03/21, 09:47:17 PM", "Addresses lock contention bottlenecks in `OutputBufferMemoryManager` by:\r\n- Reducing the scope of the critical section in `#updateMemoryUsage(long)`\r\n- Removing synchronization from `isBufferFull()` and `isOverutilized()`, these methods are already atomic reads\r\n- Avoiding redundant notifications when the `blockedOnMemory` future does not change or is already completed (aka: not blocked)\r\n- Avoiding spurious notifications from `setNoBlockOnFull()` when still blocked on memory\r\n- Adds an unsynchronized fast path in `onMemoryAvailable()` to skip the lock acquisition entirely when the buffer is full\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Improve output buffer memory tracking to reduce lock contention \r\n```\r", "NaN"], ["15628", "Add primary key to selectors table", null, "mayankgarg1990", "01/22/21, 12:36:52 AM", "This ID primary key will help with easily selecting specific rows and updating them\r\n\r\nTest plan - Ensured that TestResourceGroupsDao succeeds\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15629", "Optimize empty bucket file creation for temporary table", "Vic Zhang", "viczhang861", "02/10/21, 03:43:17 PM", "-  Creation of zero-row file for empty buckets of temporary table\r\ncan be expensive when data is sparse. A large number of file\r\ncreation operations cause reliability issue and query may\r\nfail with error HIVE_WRITER_CLOSE_ERROR. \r\n- The optimization can be extended to permanent tables but this\r\n  requires Hive version update to allow missing empty bucket file.\r\n  Given 98.2% of queries failed with \"Error write zero-row file to Hive\"\r\n   are temporary table, it can be future optimization.\r\n\r\nTest plan\r\n-  Run verifier using large batch mode, 2257 succeeded, 506 skipped, 0 resolved, 1 failed, 100.00% done\r\n    The failed query is not related (SYNTAX ERROR, table does not exist)\r\n-  Many control queries failed with \"Error write zero-row file to Hive\", but none for test queries\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n\r\n-  Optimize creation of zero-row files for temporary table empty buckets. File creation can be disabled by setting the `temporary_table_create_empty_bucket_files` session property or `hive.create-empty-bucket-files-for-temporary-table` configuration property to `false`.\r\n```", "NaN"], ["15632", "Fix iam role session name for s3 access", "Ashish", "ashishtadose", "01/28/21, 02:15:17 AM", "Test plan - (Please fill in how you tested your changes)\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines. Don't forget to follow our [attribution guidelines](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#attribution) for any code copied from other projects.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15634", "Add extra information in CacheQuota for Alluxio to use", "Ke", "kewang1024", "02/01/21, 11:38:52 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15635", "Add clientInfo triggered session property rules", null, "aweisberg", "01/28/21, 12:32:06 AM", "Just letting Travis run it right now\r\n\r\nTest plan - Unit tests. Will deploy a custom build with this to make sure it works.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for specifying session properties via regex matching on client info using :doc:`/admin/session-property-managers`\r\n```\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1392", "NaN"], ["15636", "Add warning when OR is found in JOIN", null, "chi-tsai", "02/16/21, 06:19:46 PM", "Test plan - \r\n\r\nCreated and passed JUnit tests that check for warnings in simple SQL queries that involves having an OR in a JOIN. \r\nCreated and passed JUnit tests that check no warnings appear when there is OR in JOIN in the SQL query, or if the OR is not directly in the predicate. \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* In StatementAnalyzer.java, checked if the JOIN expression needed a warning and created one if necessary\r\n* In TestAnalyzer, added JUnit tests that check for warning when OR is directly in predicate and no warning when OR not present\r", "NaN"], ["15638", "Include Identity in TempDataOperationContext", "Wenlei Xie", "wenleix", "01/27/21, 07:12:56 PM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1394\r\n\r\n\r\nTest plan - Travis\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15643", "Extract TupleDomains from a boolean column expression", "Maria Basmanova", "mbasmanova", "01/27/21, 08:54:12 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\n\r\nCC: @amitkdutta @spershin @tanjialiang", "NaN"], ["15644", "Proactively enforce memory limits in distincting accumulators", "Andrii Rosa", "arhimondr", "01/27/21, 11:12:12 PM", "GroupByHash allows to specify a memory accounting callback to enforce memory limits before the\r\nhash table expansion happens.\r\n\r\nUnfortunately distincting accumulators weren't providing a callback, so the memory limits for\r\ndistincting accumulators were enforced post factum, triggering out of memory errors on memory\r\nconstrained environments.\r\n\r\nThis patch provides a callback, so the task memory limit can be enforced.\r\n\r\nThis patch doesn't implement yield semantics for accumulators, thus the distincting accumulators\r\nwill not wait for the memory to become available in the pool. Although it is not ideal, it is\r\nan improvement over the previous version, as at least the task memory limits will be enforced\r\nproactively.\r\n\r\nTest plan:\r\n\r\ntravis + verifier\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15645", "Disable TestDistributedSpilledQueries.testCorrelatedNonAggregationScalarSubqueries", "Rebecca Schlussel", "rschlussel", "02/09/21, 07:38:35 PM", "Test hangs in travis (see https://github.com/prestodb/presto/issues/15542).  Also cleaned up other test overrides\r\n\r\nTest plan - ran class, and saw test didn't run\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15646", "First-draft of Presto client protocol documentation", "David Stryker", "djsstarburst", "02/03/21, 06:33:54 AM", "This PR adds documentation only, and contains no code changes.\r\n\r\n== NO RELEASE NOTE ==\r\n\r", "NaN"], ["15649", "Cherry-pick GH actions script from TrinoDB", null, "aweisberg", "02/24/21, 04:35:38 PM", "Test plan\r\nLook at the test results and see that they are good.\r\nYou can see it in action in my repo here https://github.com/aweisberg/presto/pull/3/checks?check_run_id=1965794614 - Finally got a passing run!\r\n\r\nIt doesn't seem to run in Presto yet, maybe it won't until we have at least one workflow landed.\r\n\r\nI think we should run both for a little bit and see how GH actions does.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\ndepends on https://github.com/facebookexternal/presto-facebook/pull/1430", "NaN"], ["15651", "Fix HiveSplitSource#bucketed parameter order", "Shixuan Fan", "shixuan-fan", "01/29/21, 01:37:23 AM", "Like calling other static functions in HiveSplitSource, caller is\r\nexpecting maxInitialSplits comes before maxOutstandingSplits. Flipping\r\nthese two arguments effectively makes maxOutstandingSplits check\r\nuseless. This will lead to failures like HIVE_EXCEEDED_SPLIT_BUFFERING_LIMIT\r\nif you are scanning a large bucketed table with multiple partitions\r\n\r\nCheck https://github.com/prestodb/presto/blob/8974bf2508711d7683cd1ad21d76e76319edfd89/presto-hive/src/main/java/com/facebook/presto/hive/HiveSplitSource.java#L192-L193 and https://github.com/prestodb/presto/blob/8974bf2508711d7683cd1ad21d76e76319edfd89/presto-hive/src/main/java/com/facebook/presto/hive/HiveSplitManager.java#L308-L309\r\nTest plan - existing tests\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Fixed a bug that query would fail with HIVE_EXCEEDED_SPLIT_BUFFERING_LIMIT when scanning large bucketed table using grouped execution \r\n```\r", "NaN"], ["15654", "Add coordinator REST endpoint for TaskInfo", null, "chi-tsai", "02/06/21, 05:29:26 AM", "Test plan -\r\n\r\nAdded TestTaskInfoResource.java to validate the response from the new endpoint for retrieving TaskInfo.\r\n\r\nFixes #15192 \r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Added a REST endpoint to get TaskInfo without directly going to worker endpoint\r", "NaN"], ["15655", "Allow to set a truststore password in InternalCommunicationConfig", "Luca Toscano", "elukey", "02/03/21, 08:33:01 PM", "Most of the java truststores need a password (if not all, I am not aware of the possibility to avoid setting one in recent JVM tools), meanwhile in Presto there seems to be no possibility to set one. This is really helpful when the CA cert is not contained in the keystore (for example, when one wants to use the system's default cacerts).\r\n\r\nMy use case: I have a cluster composed by one presto coordinator + 5 workers, with kerberos + internal https. The TLS certificates stored in the keystores are signed by a CA that it is included in the default list (`/etc/ssl/certs/java/cacerts` on Debian), but so far (up to 0.226) simply adding the following config to Presto's jvm settings made everything work:\r\n```\r\n-Djavax.net.ssl.trustStore=/etc/ssl/certs/java/cacerts\r\n-Djavax.net.ssl.trustStorePassword=somepassword\r\n``` \r\nAfter upgrading to 0.246 the above settings don't work anymore, I believe due to the explicit setting of the Truststore path happened in https://github.com/prestodb/presto/commit/6a7c4c81957cb4a85bc316bacf597c9fbe568f34#diff-474163b9662ed64fc2a0e14bfc2fb2a3d85517f387937192e4c0ce348850a502. Not sure why the password wasn't added, using a separate truststore is probably not a common use case and people didn't see the issue before.\r\n\r\nPR: #15207\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\nThis should be a trivial change, already supported by Trino/PrestoSQL from a while (see https://github.com/trinodb/trino/pull/785/files).\r\nI have a little presto testing environment, with 0.246 deployed using the release tarball downloaded from Maven Central. I checked out the prestodb repo up to the 0.246 git tag, applied this commit on top and finally built with `./mvnw clean install -DskipTests`. The copied `presto-main/target/presto-main-0.246.jar` to the testing environment and tested the new option. The https TLS authentication issues completely disappeared from the logs.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* New configuration parameter `internal-communication.https.trust-store-password` to set the Java Truststore password used for https in internal communications between nodes.\r", "NaN"], ["15659", "Upgrade to Hive 3.0.0", "Jalpreet Singh Nanda", "imjalpreet", "03/01/21, 03:53:39 PM", "This PR includes the base changes required for upgrading to Hive 3. This will help in supporting and bringing a number of interesting features in Hive 3 to Presto in the future.\r\n\r\nThis depends on https://github.com/prestodb/presto-hive-apache/releases/tag/3.0.0-2 and the following PR in tempto (https://github.com/prestodb/tempto/pull/272) \r\n\r\ndepends on https://github.com/facebookexternal/presto-facebook/pull/1414", "NaN"], ["15661", "Handle unknown type in constraints", "Sreeni Viswanadha", "kaikalur", "02/03/21, 01:34:49 AM", "Currently TypeConstraints abstraction is not used properly in SignatureBinder. So if we add a new constraint, that will not applied automatically and it needs to be duplicated in SignatureBinder. Upon close inspection, I see the only difference is in how it's handling UnknownType. So we move UnknownType to common.type (as it should be) and fix up the canBind logic to handle UnknownType properly.\r\n\r\nTest plan - tests already exist.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15663", "Refactor the OrcWriter Stats", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "02/03/21, 11:41:34 PM", "Separate current OrcWriterStats into WriterStats abstraction. The new\r\nWriterStats is able to send the metrics to a pub/sub system like Kafka.\r\nOur current deployments are Storage constrained than CPU constrained.\r\nThis change can help to gather more IO metrics.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["15665", "Supports complex types in reduce_agg", "Wenlei Xie", "wenleix", "02/17/21, 05:15:19 AM", "Test plan - Travis\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15666", "Add scalar function DESTRUCTURE_TDIGEST", "Stephen Dimmick", "sdimmick", "02/11/21, 05:28:40 AM", "* Add `sum` to TDigest and update serialization version to handle new field\r\n* Add new scalar function on TDigest type to expose the \"raw\" TDigest data\r\nstructure. The prototype of the new function is:\r\n```\r\nDESTRUCTURE_TDIGEST(column: TDIGEST) ->\r\n\r\nROW(\r\n  centroid_means: ARRAY[DOUBLE],\r\n  centroid_weights: ARRAY[INTEGER],\r\n  compression: DOUBLE,\r\n  min: DOUBLE,\r\n  max: DOUBLE,\r\n  sum: DOUBLE,\r\n  count: INTEGER\r\n)\r\n```\r\n\r\nTest plan\r\n\r\n`mvn -Dtest=\"TestTDigest,TestTDigestFunctions,TestMergeTDigestFunction\" test`\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add new scalar function on TDigest type to expose the \"raw\" TDigest data\r\nstructure. The prototype of the new function is:\r\n\r\nDESTRUCTURE_TDIGEST(column: TDIGEST) ->\r\n\r\nROW(\r\n  centroid_means: ARRAY[DOUBLE],\r\n  centroid_weights: ARRAY[INTEGER],\r\n  compression: DOUBLE,\r\n  min: DOUBLE,\r\n  max: DOUBLE,\r\n  sum: DOUBLE,\r\n  count: INTEGER\r\n)\r\n```", "NaN"], ["15667", "Evict metastore caching when partition version does not match", "Nikhil Collooru", "NikhilCollooru", "02/04/21, 06:39:31 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15669", "Support distribution of broadcast table using disk storage in Presto-on-Spark", "Arjun Gupta", "pgupta2", "03/01/21, 10:10:20 PM", "== Test plan ==\r\n- Added unit tests to trigger broadcast join using disk.\r\n- Ran verifier for around 100 broadcast join queries.\r\n\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n- Add support for distributing broadcast table using permanent storage, thereby removing spark driver from the distribution flow\r\nThis feature can be enabled/disabled using 'distribute_broadcast_table_using_disk' session property\r\n\r\nHive Changes\r\n- None\r", "NaN"], ["15671", "Fix maven scope for airlift units library", "Ajay George", "ajaygeorge", "02/03/21, 08:06:48 PM", "Airlift units library is only used from TestMemoryContexts.\r\nTherefore a test scope should be sufficient.\r\n\r\n\r\nTest plan\r\nmvn clean install -Dmaven.test.skip=true\r\nruns without any depepency-analyze errors\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["15672", "Configurable ZSTD compression level for ORC Writer", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "02/05/21, 08:17:25 AM", "OrcWriter uses ZSTD compression level 3 (It uses less CPU, but produces\r\nbigger ORC files). Support configurable ZSTD compression level so\r\nthat writer can specify higher levels (more CPU, but smaller ORC files)\r\n\r\nTest plan - \r\n* Added Comrpession Level tests in TestZstdJniCompressor\r\n* Enhanced TestOrcWriter to write DWRF file with ZSTD compression.\r\n* Using Facebook's File format verification tool.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Configurable ZSTD compression Level\r\n```\r", "NaN"], ["15673", "Resource Manager refactoring", "Timothy Meehan", "tdcmeehan", "02/10/21, 08:08:12 PM", "Test plan - verifier run\r\n\r\nExtracted from #15479\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15676", "Read maxMemoryPerNode, maxTotalMemoryPerNode and maxBroadcastMemory from Session properties", "Arjun Gupta", "pgupta2", "02/23/21, 01:40:35 AM", "Read maxMemoryPerNode, maxTotalMemoryPerNode and maxBroadcastMemory from Session properties\r\n\r\nCurrently, these configs are read from configuration properties and any changes made in session properties are not picked up. This prevents us from increasing/decreasing broadcast table memory configs.  \r\n\r\n== Test Plan == \r\n- Existing unit tests\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["15678", "Revert \"Add cpu and memory stats to NodeTaskMap\"", "Timothy Meehan", "tdcmeehan", "02/04/21, 10:11:54 PM", "Test plan - release verification\r\n\r\nThis appears to cause more frequent GC in the Presto coordinator. Reverting this to unblock the release.", "NaN"], ["15679", "Revert \"Add cpu and memory stats to NodeTaskMap\"", "Timothy Meehan", "tdcmeehan", "02/05/21, 07:02:48 PM", "Reverts prestodb/presto#15437", "NaN"], ["15681", "Integrate quota with Alluxio Cache", "Bin Fan", "apc999", "04/05/21, 05:48:23 PM", "Test plan - Unit test added\r\n\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Support Cache Quota when using Alluxio cache\r", "NaN"], ["15683", "Support named types in Type and TypeSignature", "Rongrong Zhong", "rongrong", "02/23/21, 07:51:00 PM", "* Named type's type signature will be formatted as\r\n  qualified_type_name:base_type_signature.\r\n* Introduces TypeWithName, which delegate all execution aspects of the\r\n  type to its base type.\r\n* Make SingatureBinder, type coercion, type constraint, etc to work with\r\n  TypeWithName.\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15686", "Add Checksum CRC32 to Exchange SerializedPage", "john roll", "jbroll", "02/24/21, 07:26:46 PM", "Added Feature \"exchange.checksum-enabled\"\r\nMaps to System property \"exchange_checksum\"\r\n\r\nUses CRC32 to compute the checksum.\r\n\r\nChecksum is only read / written when enabled.  The checksum existence in a page is indicated in PageCodecMarker.\r\n\r\nRefactored TestExchangeClient adding a test for checksum ok and checksum fail.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n* Added Feature \"exchange.checksum-enabled\"\r\n* Maps to System property \"exchange_checksum\"\r\n\r\nEnables CRC32 checking for SerializedPage on the Exchange data path.\r", "NaN"], ["15687", "Allow multiple overloads of an UDF to be dropped at the same time", "Leiqing Cai", "caithagoras", "02/11/21, 02:19:38 AM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support to drop multiple UDFs at the same time.\r\n```\r", "NaN"], ["15695", "Use long for partitionVersion", "Nikhil Collooru", "NikhilCollooru", "02/09/21, 07:01:44 PM", "This PR is to support the use-case where partition version is calculated based on the last modified time of the\r\npartition. For such cases, we need version to be long since it depends on unix time.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1408", "NaN"], ["15702", "Export output table names in QueryInfo", "Lung-Yen Chen", "lungyenc", "02/09/21, 11:04:54 PM", "Presto verifier rewrites queries to replace output table names with temporary table names so that presto verifier doesn't affect production tables. However, it is not easy to retrieve the temporary output table names without parsing the rewritten queries. This commit adds a field `outputTableName` to QueryInfo to export temporary output table names to presto verifier's outputs.\r\n\r\nTest plan:\r\n  (1) Updated unittest: TestDataVerification.java and ran all the unittests in presto-verifier.\r\n  (2) Ran presto verifier locally and checked the field `outputTableName` was populated correctly.\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Add output table names to Presto Verifier's outputs.\r\n```\r\n\r", "NaN"], ["15703", "Add release notes for 0.247", "Dongliang Chen", "dongliangchenfb", "02/09/21, 04:08:04 PM", "Release note for 0.247 release", "NaN"], ["15707", "Implement Spooling OutputBuffer", null, "chi-tsai", "03/25/21, 03:33:32 PM", "Test plan -\r\n\r\nUnit tested `SpoolingOutputBuffer`. \r\n\r\nVerifier tests were ran. However, the verifier runs only enqueued 1 small page into the spooling output buffer, so pages were never written into TempStorage, only in memory. \r\n\r\nTo verify the output buffer accuracy with the storage component, we ran several `SELECT` queries with output size ranging from from 25 to 6million rows twice. Once with the spooling feature turned off, and the other on, and piped the query results into a csv file to check the contents are identical. \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add Spooling OutputBuffer that writes pages into TempStorage\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15708", "Add a dummy class to presto-thrift-spec", "Leiqing Cai", "caithagoras", "02/09/21, 08:28:41 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15709", "Support exception stack and error code in remote function execution", "Leiqing Cai", "caithagoras", "02/17/21, 01:02:30 AM", "Add additional fields in ThriftUdfServiceException so that we'll be able to receive those information from the remote UDF server:\r\n- Error code\r\n- Nested structure to represent Java exception\r\n\r\nThe implementation mimics conversion between thrift object class `ExecutionFailureInfo` and Java Exception class `Failure`.\r\n- `UdfExecutionFailureInfo` mimics `com.facebook.presto.execution.ExecutionFailureInfo`\r\n- `UdfExecutionException` mimics `com.facebook.presto.execution.Failure`.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15711", "Limit the codegen class name length", "Rongrong Zhong", "rongrong", "02/11/21, 01:31:53 AM", "We use the function signature as class name for codegened classes.\r\nWhen ROW types are used, sometimes the name gets extremely long. Since\r\nwe use an AtomicLong as part of the generated class name we should be\r\nable to guarantee uniqueness without using the full name. So truncate\r\nat 100 characters.\r\n\r\nTest plan - manually tested\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15712", "Generate shorter aggregation function names", "Rongrong Zhong", "rongrong", "02/11/21, 01:44:27 AM", "Test plan - manual test\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15715", "Add readonly for file based system access control", "Reetika", "agrawalreetika", "02/24/21, 04:20:56 PM", "Add read-only for file-based system access control\r\nFixes #15714\r\n\r\n== RELEASE NOTES ==\r\n\r\nSecurity Changes\r\n\r\n-  Allow read-only configuration access in file-based system access control (#15715 )", "NaN"], ["15716", "Allow session property rules to override", null, "aweisberg", "02/11/21, 08:10:16 PM", "This is to allow rules setting resource limits to override values set as session properties. For the sneaky \"I want to bypass resource limits case\"\r\n\r\nTest plan - There are included unit tests\r\n\r\ndepends on https://github.com/facebookexternal/presto-facebook/pull/1413\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for overriding session properties using session property managers :doc:`/admin/session-property-managers`. Setting `overrideSessionProperties` to true will cause the property to be overridden and remain overridden even if subsequent rules match the property but don't have `overrideSessionProperties` set.\r\n```", "NaN"], ["15717", "Supporting adding additional modules to `DistributedQueryRunner`", null, "mayankgarg1990", "05/04/21, 01:36:40 PM", "Test plan - This PR just changes some method signatures to allow for more flexibility\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15718", "Add cpu and memory stats to NodeTaskMap", "cem cayiroglu", "cemcayiroglu", "02/22/21, 07:26:11 PM", "Test plan - Only unit tests are added. This PR is about adding CPU and memory stats as the first step to perform resource based task placement. \r\n\r\n```\r\n== NO RELEASE NOTES ==\r\n```\r\n\r\n\r\n\r\n(cherry picked from commit adaa2574364bb08f12e1c070fb779338ead8dbb8)\r\n\r\nNodeTaskMap holds the worker level aggregated split stats which is used by NodeSelectors. NodeSelector accesses to NodeTaskMap via NodeAssignmentStats. CPU and Memory stats are added to NodeTaskMap. CPU and Memory stats will used in planed new feature \"workload placement\".", "NaN"], ["15719", "Optimize joins with one or more empty sources.", null, "ahmadghazal64", "03/22/21, 09:49:12 PM", "Address the inner join case for https://github.com/prestodb/presto/issues/15412\r\n\r\n== NO RELEASE NOTE ==\r", "NaN"], ["15721", "Reorder Data Streams in OrcWriter", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "02/17/21, 07:08:05 PM", "OrcWriter orders stream by size. This will place different streams\r\nof same column separated and thereby requiring more IOs.\r\n\r\nRefactored the StreamLayout as an interface and made it configurable.\r\nAdded an option for placing the streams by column size, rather than\r\nstream size. This enables one column to be read with one IO.\r\n\r\nStreamLayout is pluggable in OrcWriterOptions. If the query pattern is\r\nknown, columns read together in popular query can be stored together\r\nby implementing a new StreamLayout functionality.\r\n\r\nTest plan - Added new test to verify the different options.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15723", "Check total memory limit when allocating user memory", "Rebecca Schlussel", "rschlussel", "02/17/21, 04:40:28 PM", "This prevents queries from exceeding the total memory limit between the\r\ntime that user memory is allocated, and the next system memory\r\nallocation.\r\n\r\nTest plan - unit test\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Enforce ``max_total_memory_per_node`` on user memory allocation\r\n```\r", "NaN"], ["15724", "Remove obsolete comment about spill", "Rebecca Schlussel", "rschlussel", "02/17/21, 05:57:13 PM", "Test plan - travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15726", "Fix adjusted queue size calculation", null, "mayankgarg1990", "02/18/21, 04:28:38 PM", "In situations that number of running queries > soft concurrency limit, the adjusted queue size is negative.\r\nThis is incorrect - since if 1 RG gets a good share of resources when the cluster is free and holds onto\r\nit, in that situation we should not have that 1 RG's negative quota impact other users, it just means\r\nthat the adjusted queue size for that RG is 0 - no waiting\r\n\r\nTest plan - This is a small change - and hence just relying on the test cases to see if something major is wrong\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15728", "Fix task worker host url", "Ajay George", "ajaygeorge", "02/18/21, 07:59:11 PM", "The removal of nodeId from TaskStatus broke the task worker host url\r\nlink from the Query Details page. This commit adds the nodeId in TaskInfo\r\nand updates the UI to use that instead.\r\n\r\nfixes https://github.com/prestodb/presto/issues/15121\r\n\r\nTest plan\r\n**Before the fix** \r\n\r\n![image](https://user-images.githubusercontent.com/236188/108320126-3b028580-7177-11eb-8cb3-268676008e36.png)\r\n\r\n**After the fix** \r\n![image](https://user-images.githubusercontent.com/236188/108320185-4f468280-7177-11eb-9c49-59c2096e330b.png)\r\n![image](https://user-images.githubusercontent.com/236188/108320202-55d4fa00-7177-11eb-8b90-840839abd50e.png)\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["15730", "Support Trino bitwise shift functions in Presto", "David Stryker", "djsstarburst", "03/16/21, 04:18:28 PM", "To promote convergence between Trino and Presto, this commit\r\nadds to Presto the bitwise functions present in Trino but missing\r\nin Presto.\r\n\r\nTest plan - Appropriate tests have been added to class TestBitwiseFunctions as part of this commit.\r\n\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* The 12 integral shift functions defined by Trino's BitwiseFunctions class have been added to Presto's BitwiseFunctions class.  These shift functions represent a complete set of shift operations on tinyints, smallints, integers and longs.\r\n\r", "NaN"], ["15731", "Update build badge url from travis-ci.org to travis-ci.com", "Ajay George", "ajaygeorge", "02/22/21, 09:30:14 PM", "Update build badge url from travis-ci.org to travis-ci.com\r\n\r\nTest plan \r\nNavigated to the build badge from my feature branch \r\nhttps://github.com/ajaygeorge/presto/tree/feature/update_travis_build_badge\r\n\r\n![image](https://user-images.githubusercontent.com/236188/108556096-4f479f00-72ab-11eb-91f7-0d8c2af5b2a3.png)\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["15736", "Hive Meta Store impersonation access", "Curt", "BlueStalker", "04/29/21, 09:28:20 PM", "This PR enabled the option of impersonation access of the Hive Metastore impersonation.\r\nIt also includes the changes to enable multiple HMS instances load-balancing and reporting the stats for each individual\r\nHMS instance.\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\nHands-on testing when enabling the impersonation\r\nPlan to add more integration testing cases during turning on the config\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\nN/A\r\n\r\nHive Changes\r\n* Hive Metastore Impersonation Access, add configuration property ``hive.metastore-impersonation-enabled`` to turn it on\r\n* Load balancing of multiple Hive Metastore instances, add configuration property ``hive.metastore.load-balancing-enabled`` to turn it on\r\n```\r", "NaN"], ["15737", "GH action product tests", null, "aweisberg", "02/25/21, 02:21:00 AM", "Test plan: Run integration/unit tests\r\n\r\nThe only one that doesn't work is Cassanda. We can leave it on Travis for a little while.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15738", "Allow to extend Kafka connector", "yang", "yangy0000", "03/20/21, 01:19:05 AM", "cherry-pick of https://github.com/trinodb/trino/commit/92e8b7b6a1b16b66ff00a87e7fe494f08db97f46\r\n\r\nCo-Authored-By: Grzegorz Kokosi\u0144ski <7569403+kokosing@users.noreply.github.com>\r\n\r\nPlanned to cherry-pick https://github.com/trinodb/trino/commit/0e379a36959dcd6676a4a1cdba2608f360a24d52 that depends on this PR.", "NaN"], ["15739", "Upgrade fastutil version", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "03/01/21, 08:22:11 AM", "Integer dictionary encoding for DWRF format is planned.\r\nFor efficient implementation, fastutil putIfAbsent is preferred\r\navailable in newer versions.  Upgrading hive-dwrf\r\nas the current version is not compatible with fastutil 8.5.2\r\n\r\nDepended by https://github.com/facebookexternal/presto-facebook/pull/1434\r\n\r\nTest plan - \r\nExisting tests should cover the change.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15741", "Register UNKNOWN type same way as other types", "Rongrong Zhong", "rongrong", "03/02/21, 03:23:13 AM", "Test plan - The reason why `UNKNOWN` type is registered differently according to the comment no longer exist. It seems to be safe to register `UNKNOWN` type the same way as other types.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15743", "Add session property to enable Spooling OutputBuffer", null, "chi-tsai", "03/25/21, 08:52:57 PM", "Test plan - \r\n\r\n```\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15744", "Support enums in SQL functions", "Rongrong Zhong", "rongrong", "02/27/21, 04:42:10 AM", "Test plan - travis\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15745", "Decom Travis", null, "aweisberg", "03/01/21, 03:47:40 PM", "Remove .travis.yml since we are almost out of credits.\r\n\r\nTest plan - Let the tests run and see that it is good\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["15746", "Refactor TempStorage interface", "Andrii Rosa", "arhimondr", "02/25/21, 10:29:57 PM", "Remove the \"initialize\" method from the user visible interface.\r\n\r\nInitialization is an internal detail of a TempStorage implementation.\r\nThe TempStorageFactory#create method should perform the initialization\r\ninternally and return an already initialized instance.\r\nClients of a TempStorage implementation shouldn't be responsible of\r\ninitialization.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15753", "CI cache maven repo", null, "aweisberg", "02/26/21, 10:58:02 PM", "Trino uses a script to retry builds, but that is kind of annoying because it loops on a lot of failures for 30 minutes. Let's try caching.\r\n\r\nThe caching is kind of poor, but I managed to make it work by fetching all the dependencies in a dedicated step and having a conditional cache population step.\r\n\r\nTest plan - See the tests pass\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15755", "Add PrestoSparkMetadataStorage", "Vic Zhang", "viczhang861", "03/03/21, 02:51:00 PM", "depended by https://github.com/facebookexternal/presto-facebook/pull/1432\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\n\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15761", "Support limiting the number of unacknowledged source splits per task", "James Petty", "pettyjamesm", "03/12/21, 07:56:04 PM", "Adds support for tracking (and optionally limiting) the number of splits that are not yet acknowledged for a given task. Previously, the only two scheduler supported limits were the number of total splits across all tasks on the worker node and the number of splits queued for a given task which included:\r\n- splits received by the worker but not yet running as of the last task status update\r\n- splits queued in the coordinator local `RemoteTask` but not yet sent to the task\r\n- splits assigned in the current scheduling batch but not yet added to the coordinator-local `RemoteTask`\r\n\r\nThe new `max_unacknowledged_splits_per_task` session property enables setting a limit on the number of splits that are either:\r\n- queued on the coordinator-local `RemoteTask` but not yet sent or at least not confirmed to have been received by the worker\r\n- assigned to the task in the current scheduling batch but not yet added to the coordinator-local `RemoteTask`\r\n\r\nThis limit enforcement takes precedence over both of the other existing split limit configurations and is designed to prevent large task update requests that might cause a query to fail.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Adds support for configuring the maximum number of unacknowledged source splits per task. This can be enabled by setting the ``max_unacknowledged_splits_per_task`` session property or ``node-scheduler.max-unacknowledged-splits-per-task`` configuration property.\r\n```\r", "NaN"], ["15762", "Allow adjusting broadcast memory via session property for PoS", "Arjun Gupta", "pgupta2", "03/02/21, 03:37:40 AM", "Broadcast memory limits in PoS could be different than Presto classic based\r\non container sizes and the user should have the flexibility to change broadcast\r\nmemory limits as per their queries requirements. On the other hand, users\r\nshould not change container level memory configs since it can result into\r\ncluster wide issues in case of mis-configuration. We should cap the global\r\nlimits by config properties but give the flexibility of changing PoS broadcast\r\nlimits via a new session property\r\n\r\nTest plan - Existing tests\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15765", "Refactor DBResourceGroupConfigurationManager to support different data providers", null, "yyujiafb", "03/15/21, 02:36:42 PM", "We want a manager that can reload resource group configuration from data stored in various places, but the current DbResourceGroupsConfigurationManager allows only for data retrieval from a database. So, we are refactoring this class so that we can reuse the code that maintains the resource groups configurations, updating and disabling them as needed, and allow for different data providers to read the basic configurations.\r\n\r\nThe following decisions were made while thinking about the refactored design:\r\n- DbResourceGroupConfigurationManager refactored to ReloadingResourceGroupConfigurationManager, which accepts a ManagerSpecProvider to be passed in that provides the basic resource group configuration definitions to the manager. \r\n- For the DB based manager, DbManagerSpecProvider handles reading from the database and provides the basic spec to the ReloadingResourceGroupConfigurationManager\r\n\r\nTest plan:\r\nTest files added to test the new classes. Ran existing tests as well to ensure that the same functionality was being preserved. \r\n\r\n== NO RELEASE NOTE ==\r\n\r\nDepends on https://github.com/facebookexternal/presto-facebook/pull/1440", "NaN"], ["15766", "Add release notes for 0.248", null, "prithvip", "03/04/21, 03:29:39 PM", "# Missing Release Notes\n## Stephen Dimmick\n- [x] https://github.com/prestodb/presto/pull/15666 Add scalar function DESTRUCTURE_TDIGEST (Merged by: Timothy Meehan)\n\n## Tim Meehan\n- [x] https://github.com/prestodb/presto/pull/15679 Revert \"Add cpu and memory stats to NodeTaskMap\" (Merged by: Timothy Meehan)\n\n## Vic Zhang\n- [x] https://github.com/prestodb/presto/pull/15629 Optimize empty bucket file creation for temporary table (Merged by: Andrii Rosa)\n- [x] https://github.com/prestodb/presto/pull/15629 Optimize empty bucket file creation for temporary table (Merged by: Andrii Rosa)\n- [x] https://github.com/prestodb/presto/pull/15629 Optimize empty bucket file creation for temporary table (Merged by: Andrii Rosa)\n\n# Extracted Release Notes\n- #15588 (Author: Sreeni Viswanadha): Implement MAP_UNION_SUM aggregation\n  - :func `map_union_sum`.\n- #15626 (Author: James Petty): Reduce output buffer memory manager contention\n  - Improve output buffer memory tracking to reduce lock contention.\n- #15636 (Author: Chi Tsai): Add warning when OR is found in JOIN\n  - In StatementAnalyzer.java, checked if the JOIN expression needed a warning and created one if necessary.\n  - In TestAnalyzer, added JUnit tests that check for warning when OR is directly in predicate and no warning when OR not present.\n- #15654 (Author: Chi Tsai): Add coordinator REST endpoint for TaskInfo\n  - Added a REST endpoint to get TaskInfo without directly going to worker endpoint.\n- #15655 (Author: Luca): Allow to set a truststore password in InternalCommunicationConfig\n  - New configuration parameter `internal-communication.https.trust-store-password` to set the Java Truststore password used for https in internal communications between nodes.\n- #15672 (Author: Arunachalam Thirupathi): Configurable ZSTD compression level for ORC Writer\n  - Configurable ZSTD compression Level.\n- #15687 (Author: Leiqing Cai): Allow multiple overloads of an UDF to be dropped at the same time\n  - Add support to drop multiple UDFs at the same time.\n- #15702 (Author: Lung-Yen Chen): Export output table names in QueryInfo\n  - Add output table names to Presto Verifier's outputs.\n- #15716 (Author: Ariel Weisberg): Allow session property rules to override\n  - Add support for overriding session properties using session property managers :doc:`/admin/session-property-managers`. Setting `overrideSessionProperties` to true will cause the property to be overridden and remain overridden even if subsequent rules match the property but don't have `overrideSessionProperties` set.\n\n# All Commits\n- c3454a9445b077cbff1f95d3269f791ea5e9c912 Support exception stack and error code in remote function execution (Leiqing Cai)\n- cb63da2b98f72c0edbb9724c2692b5cb0956c533 Add a warning for JOIN with an OR in the predicate (Chi Tsai)\n- 49b901ee38529e99576895c825f53783a163c3f4 Allow session property rules to override (Ariel Weisberg)\n- 04626d6a9abdf87abbf6e3ab67eb8b13fb56f7d8 Add scalar function DESTRUCTURE_TDIGEST (Stephen Dimmick)\n- 646648e5b4bbdd24ba6a8fac78032538957c4e68 Allow multiple overloads of an UDF to be dropped at the same time (Leiqing Cai)\n- c71cab1b16f8f7cfae683eb26e55aadc1c71640a Generate shorter aggregation function names (Rongrong Zhong)\n- c344d7bca4aad331c250efd1a4ed40f703d644b3 Limit the codegen class name length (Rongrong Zhong)\n- f6bd37f079de46073d0f38fe58580183e79c8222 Add table path to HiveTableLayoutHandle (Vic Zhang)\n- 45828a194b7970b7bf8bb927f128c92d74794d4d Add path and isCreateTable to StagingFileCommitter (Vic Zhang)\n- f7557ad1dbfcdfd90319f1c96241c434e7c4f5fc Mark new partitions for new partitioned table (Vic Zhang)\n- 09c5b096061bb22f7b086639db7f30033957395c Refactor declareIntentionToWrite to include HdfsContext (Vic Zhang)\n- eed7b37e7aba00eca745fcf62e837adf6822504b Refactor dropTable in SemiTransactionHiveMetastore (Vic Zhang)\n- af8f69e99f03a53be3d66daf38c53b009b9da657 Refactor HdfsContext used by HiveLocationService (Vic Zhang)\n- 029f8096e67267e70cfc3458f3fd96f8ced48e9e Add isNewTable and tablePath to HdfsContext (Vic Zhang)\n- 77e7d3cd3975f50fe52d86afecb503815ac72fc8 Load token_service by spark class loader (Vic Zhang)\n- cbb19ffcab47156a08e92059d604b25f18aea26c Add Drift annotations for BasicQueryInfo and NodeStatus serialization (Tim Meehan)\n- ac79506204bcab012acdaba4263585ed28502a80 Allow multiple coordinators to update memory assignments (Tim Meehan)\n- af32e7f0a6872e7b714bf89bbcc35c52af74a6e4 Allow multiple coordinators in DistributedQueryRunner (Tim Meehan)\n- 05b6200c376d50462f737cbb4f7dc71659426692 Add ResourceManager (Tim Meehan)\n- ae32d32cb34c17cf6f3b2846d5bf65c7f07f5998 Allow node to be identified as Resource Manager (Tim Meehan)\n- 863895858551e6198eb839fb7f36e78e1210009a Include exception for HIVE_FILE_NOT_FOUND error (Vic Zhang)\n- a6dc72d33518e6697914a4798f0a30037216dc7f Add session property to create empty bucket files (Vic Zhang)\n- 95daf0f38ba301a4f010bcefdf110ec77879b92b Optimize empty bucket file creation for temporary table (Vic Zhang)\n- ab60a389a8d66a8eabb0fa5626f6511aa13dcb43 Export output table names in QueryInfo (Lung-Yen Chen)\n- 23f464606544cb01ecd845deb9038ed65880c55a Add a dummy class to presto-thrift-spec (Leiqing Cai)\n- daa287d32e46092256d8771d28d11881a474a6ac Remove test \"overrides\" that are no longer relevant (Rebecca Schlussel)\n- bbda25ba8b946c1292418ce06f5cd8bdf65c4d81 Disable testCorrelatedNonAggregationScalarSubqueries with spill (Rebecca Schlussel)\n- 69ac888b7549d0c920806aff0ce21f9e4eda8969 Use long for partitionVersion (Nikhil Collooru)\n- fa1a9597f18a4089c301ce63cd8fd6bf7b53a3ba Implement MAP_UNION_SUM aggregation (Sreeni Viswanadha)\n- 4472e9cc6996b174f21bb8c57f339d0c4a731256 Add coordinator REST endpoint for TaskInfo (Chi Tsai)\n- 733ab430e21579c5b370b9cc0f2ccb3e0e8528e2 Add Kafka Json encoder (Yang Yang)\n- 04b8abfceab34e9a8848e6071481192cfbed80d0 Revert \"Add cpu and memory stats to NodeTaskMap\" (Timothy Meehan)\n- 80b9fd5df8537b3960b137bace7158f101247f5f Configurable ZSTD compression level for ORC Writer (Arunachalam Thirupathi)\n- 00b465cff52848fdabb2d1034ed2ce9dc6672ab4 Evict metastore caching when partition version does not match (Nikhil Collooru)\n- d401553d037e94df556f0b1b352c4966ffc1e544 Refactor the OrcWriter Stats (Arunachalam Thirupathi)\n- 641347b4ded84e4ae1587dbc767ec5de29da9d7b Reduce lock contention in OutputBufferMemoryManager (James Petty)\n- dff92fe4d56071dcf7230764ea10f911541f1611 Allow to set a truststore password in InternalCommunicationConfig (Luca)\n- c48825e08a5cffaf100828975f1d5255d87823c7 Fix maven scope for airlift units library (Ajay George)\n- f662badb5ed2c21f74b12a95de436993e00e6299 First-draft of Presto client protocol documentation (David Stryker)\n- 859f594241e189469ca92c6f01641ab3d12e6f3e Handle UnknownType in TypeVariableConstraint (Sreeni Viswanadha)\n- 4bd2aded74aa97055d4a51053ff1cb4fd05cc1da Add extra information in CacheQuota for Alluxio to use (Ke Wang)", "NaN"], ["15767", "Dictionary Writer refactor and Long writer first draft", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "03/16/21, 10:01:47 PM", "Add DWRF Integer Dictionary Encoding Writer\r\n\r\nRefactored the SliceDictionaryWriter to reuse the common code.\r\nAdded code for DwrfIntegerDictionary (single level).\r\nThe DwrfIntegerDictionary is disabled by default.\r\n\r\nTest plan -\r\nIntroduced tests to cover many different paths.\r\nWill run this through Facebook's DWRF teams validation service.\r\nORC String Dictionary is also changed which doesn't have\r\nenough coverage.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Refactor ORC/DWRF String Dictionary\r\n* Add DWRF Integer Dictionary encoding.\r\n```\r", "NaN"], ["15769", "Add PrestoSparkMetadataStorage", "Vic Zhang", "viczhang861", "03/05/21, 09:59:26 PM", "depended by https://github.com/facebookexternal/presto-facebook/pull/1432\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15773", "Fix LocalExecutionPlan for IndexJoin when spill enabled", "Rebecca Schlussel", "rschlussel", "03/05/21, 03:32:57 PM", "Index join does not support spill, so we don't ensure that we add\r\nexchanges with fixed distribution before an index join.  However, we\r\nwere still checking for fixed distribution in the LocalExecutionPlanner.\r\nThis removes the check for the number of operators since that is only\r\nneeded for spill\r\n\r\nTest plan - Added unit test \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix an error executing index joins when join spilling is enabled\r\n```\r", "NaN"], ["15774", "Enable different cache eviction policy", "Bin Fan", "apc999", "04/01/21, 10:32:02 PM", "Test plan - Unit test added\r\n\r", "NaN"], ["15775", "Fix over allocation of RowBlockBuilder during spilling", "Saksham", "sachdevs", "03/07/21, 06:48:01 AM", "Test plan - Deployed and tested on query that was causing JVM OOM. After this fix, the query succeeded.\r\n\r\nDescription of bug: RowBlockBuilders were using a lot more memory than their corresponding rawInputs according to the heapdump I found. This showed that we were overallocating the blockBuilders array during SpillableFinalOnlyGroupedAccumulator.evaulateIntermediate.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15778", "Add `new_partition_user_supplied_parameter` property", null, "aweisberg", "03/09/21, 01:02:09 AM", "Test plan - Includes unit test\r\n\r\nThis is to support an auditing use case where the client wants to add an extra information to the metastore entry for every partition. To keep things somewhat isolated they can specify a single key ('user_supplied') as a session parameter and then put whatever they want in the string value. JSON would be fine for instance.\r\n\r\nI didn't want them to put arbitrary keys in the the metastore extra_params to avoid the potential for collisions on key names.\r\n\r\nI had a tough time finding a good place to put this very simple unit test in. There is a large amount of ceremony required to set up the test case and I didn't want to duplicate it all.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add `new_partition_user_supplied_parameter` session property which causes all partitions created by a query to have the `user_supplied` parameter set to the supplied string in Hive metastore.\r\n```\r\n\r\ndepends on https://github.com/facebookexternal/presto-facebook/pull/1439", "NaN"], ["15779", "Support SHOW CREATE MATERIALIZED VIEW", "Ge Gao", "gggrace14", "03/15/21, 05:50:03 AM", "Depends on prestodb/presto#15589", "NaN"], ["15780", "Enforce fixed distribution for output operator", "Andrii Rosa", "arhimondr", "03/05/21, 10:18:18 PM", "Output operator in Spark does rows compaction. It has to accumulate enough\r\nrows to ensure at least 1kb of data per partition is being written to the\r\nshuffle service in a single call. Without a local exchange the output operator\r\nwill inherit the table scan driver lifecycle, effectively buffering only the\r\nrows produced by a single split. If splits are small that results into very\r\ntiny batched being flushed to the shuffle service\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15786", "Configure tempstorage for Presto on Spark", "Arjun Gupta", "pgupta2", "03/10/21, 02:58:54 AM", "Storage based broadcast join uses tempstorage for distributing\r\nhash tables across workers. tempStorage needs to configured\r\nuniquely for broadcast spills like increasing replica factor etc\r\nto improve reliability and availability of spill files. We should have \r\nthe ability to tune storage configs for a specific usecases like \r\nbroadcast join etc.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15787", "Update joda to 2.10.8 tzdata 2020d", null, "aweisberg", "03/09/21, 06:54:40 PM", "Test plan - Unit tests\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Update joda to 2.10.8 tzdata 2020d. Make sure you deploy with a JVM that is using matching tzdata.\r\n```\r\n\r\ndepends on https://github.com/facebookexternal/presto-facebook/pull/1442", "NaN"], ["15788", "Correct presto-product-tests README errors", "Jinlin Zhang", "v-jizhang", "03/16/21, 02:59:55 PM", "The instructions in \"Start Presto dependent services as Docker\r\ncontainers\" has errors. You will get \"ERROR: No such service: mysql\"\r\nif you copy and run the commands. Correct the command path should\r\nfix it.\r\n\r\n\r\n\r\nTest plan - No test plan\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15789", "Improve processing time for Presto on spark events", "Arjun Gupta", "pgupta2", "03/09/21, 12:51:49 AM", "Spark queries with very high parallelism can generate a\r\nlarge number of events. These events needs to be processed\r\nbefore spark context can stop. If processing is slower, spark\r\ndriver can take long time to exit and events can be dropped\r\nwhich would result in stale Spark UI. Driver profile shows a\r\nlarge amount of time being spent in toString() method. This\r\ninformation is not useful in Spark UI and thus we can just\r\nreturn an empty string.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15790", "Fix Input Rows reporting in TableScan operators", "Maria Basmanova", "mbasmanova", "03/09/21, 02:55:12 AM", "TableScan and ScanFilterProject operators reported input size and\r\nposition count as they were after applying pushed down filters. This\r\nmade the overal Input Rows counter for the query incorrect.\r\n\r\nFor example, select * from nation where nationkey % 2 = 0 reported\r\nInput Rows as 13 instead of 25.\r\n\r\nThis commit fixes TableScan and ScanFilterProject to report input size\r\nand position count as they are before applying pushed down filter.\r\n\r\npresto:tpch> explain analyze select sum(nationkey + 1) from nation where nationkey < 2\r\n\r\n         - ScanProject[table = TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=nation, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.nation{domains={nationkey=[ [(<min>, 2)] ]}}]'}, grouped = false, projectLocality = LOCAL] => [expr:bigint]\r\n                 CPU: 4.00ms (100.00%), Scheduled: 9.00ms (47.37%), Output: 2 rows (18B)\r\n                 Input: 25 rows (1.47kB), Filtered: 92.00%\r\n\r\nQuery 20210309_010704_00007_wbiq7, FINISHED, 2 nodes\r\nSplits: 11 total, 11 done (100.00%)\r\n0:00 [25 rows, 1.47KB] [78 rows/s, 4.64KB/s]\r\n\r\npresto:tpch> explain analyze select count(*) from nation where nationkey < 2;\r\n\r\n         - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=nation, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.nation{domains={nationkey=[ [(<min>, 2)] ]}}]'}, grouped = false] => []\r\n                 CPU: 4.00ms (100.00%), Scheduled: 6.00ms (35.29%), Output: 2 rows (0B)\r\n                 Input: 25 rows (1.47kB), Filtered: 92.00%\r\n\r\nQuery 20210309_010630_00006_wbiq7, FINISHED, 3 nodes\r\nSplits: 11 total, 11 done (100.00%)\r\n0:00 [25 rows, 1.47KB] [86 rows/s, 5.06KB/s]\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15791", "Improve SOFT_AFFINITY strategy to be more resilient to node downtime", "Nikhil Collooru", "NikhilCollooru", "04/05/21, 06:05:00 PM", "Currently with SOFT_AFFINITY node selection strategy, we sort the list of alive nodes and calculate the split placement. Now if a node goes down, the order of nodes in the sorted list is shifted and cache goes bad because it's re-distributed among the remaining alive nodes. One node going down, will affect the cache on all nodes and cache hit rate will go bad. This is worse especially for the cases where the node comes back up/becomes alive within 10-15min, because the cache will again be re-distributed. \r\nWith this PR, given a split, we try to find a node from a list of all nodes(alive+dead), and if that worker is down/dead we schedule the split to another worker. So cache hit rate will be bad only for the splits of that one died worker. Cache hit rate will be good for remaining workers.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for improving cache affinity hit rate by preventing repartitioning when nodes go down for a limited time. This can be enabled by setting the `internal-communication.memoize-dead-notes-enabled` configuration property.\r\n```\r", "NaN"], ["15794", "Minor improvement to SpillableFinalOnlyGroupedAccumulator", "James Petty", "pettyjamesm", "03/10/21, 08:17:57 PM", "Uses `IntBigArray#increment(index)` instead of `IntBigArray#set(index, IntBigArray#get(index) + 1)` which avoids traversing the 2D array twice, following up from https://github.com/prestodb/presto/pull/15775\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15795", "Bump up alluxio version", "Ke", "kewang1024", "03/10/21, 07:45:07 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15796", "Update hive-apache to 3.0.0-3", "James Petty", "pettyjamesm", "03/09/21, 11:28:53 PM", "Includes a performance improvement for the Hive JsonSerDe from https://github.com/prestodb/presto-hive-apache/pull/44\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r\ndepends on https://github.com/facebookexternal/presto-facebook/pull/1444", "NaN"], ["15799", "Adding the chi square distribution", "Tal Galili", "talgalili", "04/23/21, 08:24:23 PM", "Adding the chi square distribution, which is central to many statistical procedures (https://en.wikipedia.org/wiki/Chi-square_distribution) (#15798)\r\n\r\nTest plan (adding unit-tests)\r\n\r\nFollowing the diff template of: https://github.com/prestodb/presto/pull/11981/files\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add chisquared_cdf() and inverse_chisquared_cdf() functions.\r\n\r\n(like the beta_cds: https://prestodb.io/docs/current/release/release-0.215.html?highlight=beta_cds)\r", "NaN"], ["15800", "Upgrade hadoop-apache for refreshing S3 FileSystem Cache on Credential Change", "Jalpreet Singh Nanda", "imjalpreet", "03/11/21, 03:09:21 PM", "Fixes #15722 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Fix a bug when S3 access is compromised across s3 enabled hive catalogs (:pr:`15800`)\r\n```", "NaN"], ["15801", "Revert \"Update to Hive 3.0.0\"", null, "neeradsomanchi", "03/10/21, 09:25:09 PM", "This reverts commit fd32feac434efd7512df6a5405723816fe1f1fda.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15803", "Improving remote function work flow", "Rongrong Zhong", "rongrong", "03/23/21, 05:05:44 AM", "Test plan - integration tests\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15805", "Upgrade to Hive 3.0.0", "Jalpreet Singh Nanda", "imjalpreet", "03/11/21, 09:31:03 PM", "This PR includes the base changes required for upgrading to Hive 3. This will help in supporting and bringing a number of interesting features in Hive 3 to Presto in the future.\r\n\r\ndepends on https://github.com/facebookexternal/presto-facebook/pull/1450\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add Support for Hive Connector to work with Hive 3 metastore for basic tables (ACID Tables and tables using constraints are not supported yet) (:pr:`15805`)\r\n```", "NaN"], ["15812", "Eran/binomial functions", "Eran Amar", "eranamar", "05/05/21, 09:13:19 PM", "Adding the binomial distribution, which is central to many statistical procedures (https://en.wikipedia.org/wiki/Binomial_distribution) (#15812 )\r\n\r\nTest plan (adding unit-tests)\r\n\r\nFollowing the diff template of: https://github.com/prestodb/presto/pull/11981/files\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add :func:`binomial_cdf` and :func:`inverse_binomial_cdf` functions.\r\n```\r\n\r\n(like the beta_cds: https://prestodb.io/docs/current/release/release-0.215.html?highlight=beta_cds)", "NaN"], ["15814", "Adding the Poisson distribution", "Tal Galili", "talgalili", "05/13/21, 02:09:19 PM", "Adding the Poisson distribution, which is central to many statistical procedures (https://en.wikipedia.org/wiki/poisson_distribution) (#15798)\r\n\r\nTest plan (adding unit-tests)\r\n\r\nFollowing the diff template of: https://github.com/prestodb/presto/pull/11981/files\r\n\r\n\r\n\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add poisson_cdf and inverse_poisson_cdf functions.\r\n\r\n(like the beta_cds: https://prestodb.io/docs/current/release/release-0.215.html?highlight=beta_cds)\r", "NaN"], ["15819", "Revert \"Avoid planning unnecessary LIMIT/TopN/Sort/DistinctLimit\"", "Rongrong Zhong", "rongrong", "03/12/21, 03:54:38 AM", "This reverts commit 90cd1d004cf458d397dd50c40bb57cc85ac53be9.\r\n\r\nThis is causing a correctness SEV. Sort node is removed incorrectly in some situations.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15822", "Add test for grouping sets with limit", "Ajay George", "ajaygeorge", "03/16/21, 05:13:23 PM", "Failing test to expose the bug in grouping sets with limit\r\nS224994\r\nShould pass when https://github.com/prestodb/presto/pull/15819 is merged\r\nTest plan unit test\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15823", "Add release notes for 0.248.1", "Rongrong Zhong", "rongrong", "03/12/21, 07:36:48 PM", "NaN", "NaN"], ["15825", "Support enforcement of NOT NULL column declarations", "Jinlin Zhang", "v-jizhang", "04/02/21, 06:31:46 PM", "This PR is back ported from Trino\r\nhttps://github.com/trinodb/trino/pull/4144, originally authored by\r\ndjsstarburst. Quote description from Trino PR4144\r\n\"This commit enforces NOT NULL column declarations on write\r\nin the Presto engine, so it applies to all connectors. The\r\nexisting Postgres and Mysql tests named testInsertIntoNotNullColumn\r\nwere changed to check for the new error message...\r\n\r\nOne possible concern with this commit is that the error message\r\nissued by the Presto engine when writing a null to a NOT NULL\r\ncolumn is a different message than the Connector might issue\r\nif no value was supplied for the NOT NULL column. I think this\r\nis ok, because the error messages supplied by the Connectors are\r\ncompletely specific to the Connector.\"\r\n\r\nBecause the gap between Presto and Trino, the original PR has to be\r\nmodified.\r\n\r\nTest plan - Added new tests to suite TestMySqlIntegrationSmokeTest\r\nand TestPostgreSqlIntegrationSmokeTest.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Enforce NOT NULL constraint by checking values for NOT NULL columns. This change applies to all connectors.\r\n```\r\n\r\nFix for #15699 \r", "NaN"], ["15826", "Skip OOMing testNewBlockBuilderLikeForLargeBlockBuilder", null, "aweisberg", "03/16/21, 09:34:35 PM", "Test plan - Unit tests\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15831", "Add release notes for 0.249", "Andrii Rosa", "arhimondr", "03/15/21, 10:47:09 PM", "# Missing Release Notes\n## Andrii Rosa\n- [x] b888504726913f8ac8e521a47f37eb33b6a7e974 Revert \"Add PrestoSparkMetadataStorage\"\n- [x] 21aec0998137cc9ff2beff935aaec5c679bbefe7 Revert \"Add optional sqlLocation to PrestoSparkRunner\"\n- [x] 91b907c2d6f9cbd4258bb0c40aba7e6901c0eb1f Revert \"Add metadataStorage to PrestoSparkConfiguration\"\n\n## Ariel Weisberg\n- [x] https://github.com/prestodb/presto/pull/15787 Update joda to 2.10.8 tzdata 2020d (Merged by: Rebecca Schlussel)\n- [x] https://github.com/prestodb/presto/pull/15778 Add `new_partition_user_supplied_parameter` property (Merged by: Andrii Rosa)\n\n## Arjun Gupta\n- [x] https://github.com/prestodb/presto/pull/15669 Support distribution of broadcast table using disk storage in Presto-on-Spark (Merged by: Andrii Rosa)\n\n## John Roll\n- [x] https://github.com/prestodb/presto/pull/15686 Add Checksum CRC32 to Exchange SerializedPage (Merged by: Andrii Rosa)\n\n## agrawalreetika\n- [x] https://github.com/prestodb/presto/pull/15353 Documentation for Metastore configurations (Merged by: Rebecca Schlussel)\n- [x] https://github.com/prestodb/presto/pull/15715 Add readonly for file based system access control (Merged by: Andrii Rosa)\n\n## imjalpreet\n- [x] https://github.com/prestodb/presto/pull/15805 Upgrade to Hive 3.0.0 (Merged by: Rebecca Schlussel)\n- [x] https://github.com/prestodb/presto/pull/15805 Upgrade to Hive 3.0.0 (Merged by: Rebecca Schlussel)\n- [x] https://github.com/prestodb/presto/pull/15800 Upgrade hadoop-apache for refreshing S3 FileSystem Cache on Credential Change (Merged by: Rebecca Schlussel)\n- [x] https://github.com/prestodb/presto/pull/15659 Upgrade to Hive 3.0.0 (Merged by: Rebecca Schlussel)\n\n# Extracted Release Notes\n- #15723 (Author: Rebecca Schlussel): Check total memory limit when allocating user memory\n  - Enforce ``max_total_memory_per_node`` on user memory allocation.\n- #15773 (Author: Rebecca Schlussel): Fix LocalExecutionPlan for IndexJoin when spill enabled\n  - Fix an error executing index joins when join spilling is enabled.\n\n# All Commits\n- 51c7888d0a0a54c55f5aa66e4222a8fe4e7e6803 Revert \"Avoid planning unnecessary LIMIT/TopN/Sort/DistinctLimit\" (Rongrong Zhong)\n- a789c3cba0152f5d9640d962bb9de6ad2a8a0f8c Fix reading collection delimiter set by Hive < 3.0 (imjalpreet)\n- 3de32e6ba6a3eac18614154a6df914ce4fd74d30 Update to Hive 3.0.0 (imjalpreet)\n- 16e6006680edcb9186948a3e9026f8c2388e665c Upgrade hadoop-apache for refreshing S3 FileSystem Cache on Credential Change (imjalpreet)\n- 6619eeb68ad25e54b6868df8f7b896aebfc40be5 Revert \"Update to Hive 3.0.0\" (Neerad Somanchi)\n- bf6be9a9bd60abc1b231aa88edb00f2c28fd087c Minor improvement to SpillableFinalOnlyGroupedAccumulator (James Petty)\n- 9f44884d2b56293188754d6fcb1f072ef42b06de Bump up alluxio version (Ke Wang)\n- 1f1b1d1d0b9f625691617c4d2d5a7ed03011a762 Configure tempstorage for Presto on Spark (Arjun Gupta)\n- dfc6aebc268c6fc409ef4ec5977da9a7fa29671b Update hive-apache to 3.0.0-3 (James Petty)\n- 06f38c3b9e1e5593129a41b0eec2ea8eb5ade203 Update joda to 2.10.8 tzdata 2020d (Ariel Weisberg)\n- fa8874f1ce17dc7d044c71ef109bf68cc5c6c0d8 Fix Input Rows reporting in TableScan operators (Masha Basmanova)\n- e4a1d66cdc5b865465232b147bcd8d0ead42f45c Add `new_partition_user_supplied_parameter` property (Ariel Weisberg)\n- 0d5ed038e0ae5fef4a6f791efa6491b0456724a4 Improve processing time for Presto on spark events (Arjun Gupta)\n- 87e84b2040554cc8981b85b43c9cd941bf2215da Deprecate Joda library in Presto Teradata functions (Vladimirs Kotovs)\n- 97ac4974a006ae053298058530afacc8294cfd83 Documentation for Metastore configurations (agrawalreetika)\n- dcdedeef0541d74091e046e057845f742ecf18ea Fix overallocation of RowBlockBuilders during aggregation spill (Saksham Sachdev)\n- a66d9124d698cdf7bb0c4107a6a89e70f35b1c8a Enforce fixed distribution for output operator (Andrii Rosa)\n- b1f85b6a403232858b61d758fdc8defac86d234e Add metadataStorageType to PrestoSparkConfiguration (Vic Zhang)\n- ab351bf7d072f97f38e53841a663af79e78c5cac Add optional sqlLocation to PrestoSparkRunner (Vic Zhang)\n- e7826a85d47b2995dbf1270c1b83495fec5bc05a Add PrestoSparkMetadataStorage (Vic Zhang)\n- 1b72d26500e6a401cf7a765df02afeac08d0e896 Fix LocalExecutionPlan for IndexJoin when spill enabled (Rebecca Schlussel)\n- b888504726913f8ac8e521a47f37eb33b6a7e974 Revert \"Add PrestoSparkMetadataStorage\" (Andrii Rosa)\n- 21aec0998137cc9ff2beff935aaec5c679bbefe7 Revert \"Add optional sqlLocation to PrestoSparkRunner\" (Andrii Rosa)\n- 91b907c2d6f9cbd4258bb0c40aba7e6901c0eb1f Revert \"Add metadataStorage to PrestoSparkConfiguration\" (Andrii Rosa)\n- 8471524e3a637ef38009d0433a25d6074aa3291d Add metadataStorage to PrestoSparkConfiguration (Vic Zhang)\n- ad1753ebc6e2b57e32eff9bdee3fde22ba9c07c6 Add optional sqlLocation to PrestoSparkRunner (Vic Zhang)\n- 474d5844cb0486e2f7b37fee744e70a2da28b66d Add PrestoSparkMetadataStorage (Vic Zhang)\n- 4fb658682e5ac6eaf490959a2704d4e1d5fd973d Allow adjusting broadcast memory via session property for PoS (Arjun Gupta)\n- c372c58cbb1ed510ec79217fffa37234bfedf577 Register UNKNOWN type same way as other types (Rongrong Zhong)\n- bd933262207d87defd838b19a2a7759aa6545ae2 Implement storage based broadcast join in Presto on Spark (Arjun Gupta)\n- fd32feac434efd7512df6a5405723816fe1f1fda Update to Hive 3.0.0 (imjalpreet)\n- 866578801d36a5fdfb03bf6dc3a882b140f42332 Stop using Travis in favor of GH actions (Ariel Weisberg)\n- b043970f4122b5fd242c1916dfea2bdc348ab631 Upgrade fastutil to 8.5.2 (Arunachalam Thirupathi)\n- e7a85b3eadc87703a6320d407d9c8e550b614d93 Support enums in SQL functions (Rongrong Zhong)\n- 6259ff34917a832f6774cfd3e9ed58d2c6dd7713 CI cache maven repo (Ariel Weisberg)\n- b29e78256eb69a37fec5166b8e454e4ec75cbf64 Refactor TempStorage interface (Andrii Rosa)\n- a6b1abb31457fc07b18c088349283c6c7f563876 Enable TestBlockBuilder.testNewBlockBuilderLikeForLargeBlockBuilder (Ariel Weisberg)\n- e360668b2cc36d0ff1b04e4e53316d6cff3620c9 Add product tests to ci.yml (Ariel Weisberg)\n- a1cd3c198fd61d931704ac01956b7ec450395e72 Add Checksum CRC32 to Exchange SerializedPage (John Roll)\n- e2ceb67521d327d1f48cf06d0b9d40c984fea143 Skip OOMing testNewBlockBuilderLikeForLargeBlockBuilder (Ariel Weisberg)\n- 23f621f451fff60ef3ca09a986cd789987c40b4d Add \"America/Nuuk\" time zone (Ariel Weisberg)\n- 5f5bbe7e8e0e57d698a6fe72701d7d7e5f3c40ea Cherry-pick GH actions scripts from TrinoDB (Ariel Weisberg)\n- 06f7801731f52037027101c1763922dc1f7943cd Add readonly for file based system access control (Reetika Agrawal)\n- f6b77cad13259bfc8055e056b648c2b7f6c816e5 Support named types in Type and TypeSignature (Rongrong Zhong)\n- 0a8158f260cde2eb20cde9041a291fc27d48f9b8 Allow adjusting memory limits with session properties (Arjun Gupta)\n- 2358b019ae8428f1615cf449882b48cd7bd49722 Update build badge url from travis-ci.org to travis-ci.com (Ajay George)\n- 26a373c269cd68e5497ac670b932c54dad292e6d Add cpu and memory stats to NodeTaskMap (Cem Cayiroglu)\n- 201ef82774f523840147fa0d1f0ece97188dd978 Fix task worker host url (Ajay George)\n- 6b5268a82095d8a5c6ea5dcca662eaa681ab5b60 Fix adjusted queue size calculation (Mayank Garg)\n- d3aea63406489874b17c194f4a41dd8e5ec5d17b Reorder Data Streams in OrcWriter (Arunachalam Thirupathi)\n- 4aa0e327bc4ae795152ca1ce23d27be1b6599514 Remove obsolete comment about spill (Rebecca Schlussel)\n- d86c679cf641742fa39f3da6c305d9eac4bfbde9 Check total memory limit when allocating user memory (Rebecca Schlussel)\n- 29a5e12e084c000aac873148b4b5f1055106695c Supports complex types in reduce_agg (Wenlei Xie)", "NaN"], ["15836", "Add Presto iceberg connector", "Chunxu Tang", "ChunxuTang", "05/20/21, 12:25:08 AM", "Cherry-pick of https://github.com/trinodb/trino/commit/e82c2d5301396ca16248eb6931bec11bf1352470\r\n\r\nCo-Authored-By: Parth Brahmbhatt <pbrahmbhatt@netflix.com>\r\nCo-Authored-By: Zhenxiao Luo <zluo@twitter.com>\r\nCo-Authored-By: Beinan Wang <beinanw@twitter.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add iceberg connector\r\n```\r\n\r", "NaN"], ["15840", "Add dictionary writer benchmark", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "03/17/21, 08:23:46 PM", "Create benchmarks for comparing direct writer (Consume less CPU, but\r\ncould producer larger files) with Dictionary writer (Consume more CPU,\r\nbut could produce smaller files).\r\n\r\nThree methods are bench marked.\r\n\r\nwriteDirect - write a column using direct encoding writer.\r\nwriteDictionary - writer a column using dictionary encoding writer\r\nwriteDictionaryAndConvert - write using a dictionary writer, but convert\r\nto direct encoding later. (simulates situation where memory pressure\r\nconverts it).\r\n\r\nIn the benchmark, integer dictionary could consume 4 times the CPU.\r\nString writers could be 150 times worse when abandoning dictionary\r\nand 50 times worse when using dictionary. All the dictionary worse\r\nperforming cases happen when the dictionary contains all unique values.\r\n\r\nString performance is due to bugs and I will send the PRs to fix them.\r\nhttps://github.com/prestodb/presto/issues/15506\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\nNo product code change.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15841", "Add error categorization for out task scope errors", null, "tanjialiang", "03/17/21, 11:04:02 PM", "Test plan\r\nLocal test was done by setting up a local cpp worker and local java server. By setting up breaking points locally, it was  confirmed that the code branch was reached as intended.\r\n\r\n== RELEASE NOTES ==\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15842", "Fix and turn on Cassandra product tests", "Jinlin Zhang", "v-jizhang", "05/25/21, 06:08:30 AM", "Cherry-pick of trinodb/trino#141. This is\r\nalso a fix for #15147 which\r\ntried to backport Trino #141 but that backporting was incomplete\r\nand caused the Cassandra tests to fail.\r\n\r\nCo-authored-by: Yuya Ebihara <ebyhry@gmail.com>\r\n\r\nResolves: #15749\r\n\r\nTest plan - Run `presto-product-tests/bin/run_on_docker.sh singlenode-cassandra -g cassandra`\r\n\r\n```\r\n== RELEASE NOTES ==\r\nCassandra Connector Changes\r\n* Add support for Cassandra ``SMALLINT``, ``TINYINT`` and ``DATE`` types and fix tests.\r\n```\r", "NaN"], ["15843", "Improve performance of getRegionLogicalSizeInBytes", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "03/19/21, 06:29:36 PM", "DictionaryBlock getRegionLogicalSizeInBytes is slow when abandoning\r\nStringDictionaryEncoding. When abandoning string dictionary encoding,\r\ndictionary has large number of keys (millions) and it is converted\r\nin segements of maximum 1024. The getRegionLogicalSizeInBytes allocates\r\nsize array of number of keys as a cache.\r\n\r\nWith this change code is modified to use the cache code path only\r\nwhen the number of keys is less than the length. The conversion\r\nratio can be tuned if required.\r\n\r\nBefore this change, abandoning a dictionary with 10 million keys\r\nused to 150 times slower than the direct encoding. Now it is only\r\n50 times worse (similar to dictionary encoding).\r\n\r\nfixes https://github.com/prestodb/presto/issues/15506\r\n\r\nTest plan - \r\nAdded unit test for the non cached code path.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve Dictionary Block getRegionLogicalSizeInBytes performance\r\n```\r\n\r", "NaN"], ["15848", "Add a flag for HttpRemoteTask to avoid eager but unnecessary sendUpdate", "guhanjie", "guhanjie", "05/19/21, 01:04:17 AM", "It should not sendUpdate before task been started,\r\nas the needsUpdate flag in RemoteHttpTask is not enough to ensure this,\r\nwhich will happen on all non-leaf stage, as scheduleTask(525-535 lines)  in SqlStageExecution.java\r\nAlthough task will start soon after noMoreSplits, but we should avoid\r\nthis, which is unnecessary and not strictly accurate in principle.\r\n\r\nI have requested the issues before as [this](https://github.com/prestodb/presto/issues/15797)\r\n\r\nI have tested it, and found the not started tasks(which is on non-leaf Stage) would sendUpdate requests on Worker node eagerly.\r\nThe code lead to above case is as below,\r\n![image](https://user-images.githubusercontent.com/5463066/111728688-5bbe1800-88a8-11eb-93a4-7e95881d1103.png)\r\n\r", "NaN"], ["15851", "Extract pluggable KafkaTableDescriptionSupplier", "yang", "yangy0000", "03/31/21, 03:50:59 AM", "Cherry-pick of https://github.com/trinodb/trino/commit/0e379a36959dcd6676a4a1cdba2608f360a24d52\r\n\r\nCo-Authored-By: Grzegorz Kokosi\u0144ski <7569403+kokosing@users.noreply.github.com>\r\n\r", "NaN"], ["15852", "Add Hive schemas into InputFormat#getSplits", "Rumeshkrishnan", "rumeshkrish", "03/22/21, 05:22:40 PM", "**Changes** - Unable to use the serialization.ddl and table paramerters inside Hive custom input format. This changes bring the SerDe parameters and Table parameters into Input format as way hive query planning.\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines. Don't forget to follow our [attribution guidelines](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#attribution) for any code copied from other projects.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15856", "Fix memory leak in NodeTaskMap", null, "mayankgarg1990", "03/22/21, 02:35:35 PM", "This fixes a leak introduced recently in 26a373c. `memoryUsageTracker` and `cpuUtilizationPercentageTracker`\r\nwere being passed as both the referrant and as a part of the cleanup function. This caused 2 references to\r\nthese objects - one from the `FinalizerService#finalizers` and `FinalizerService#FinalizeReference#cleanup`\r\nand prevented a cleanup.\r\n\r\nFixing this to just pass all 3 cleanups in a single function along with the `NodeStatsTracker` object.\r\n\r\nTest plan - Using a heapdump I could now ensure that entries were indeed being cleaned up\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix a memory leak in `NodeTaskMap` which could lead to Full GC or OOMs in coordinator\r\n```", "NaN"], ["15857", "Add query retry for transient errors", "James Sun", "highker", "04/23/21, 08:24:55 PM", "depends on https://github.com/facebookexternal/presto-facebook/pull/1503\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n*  Add automatic query retry functionality for transient failures. This can be enabled by setting ``per-query-retry-limit`` to a non-zero integer to indicate the per query retry count.\r\n```", "NaN"], ["15858", "Add release notes for 0.249.1", null, "mayankgarg1990", "03/22/21, 04:16:04 PM", "Test plan - Opened the html pages and ensured that they look fine\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15864", "Improve Dictionary Writer Performance", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "04/04/21, 10:07:24 PM", "Dictionary Writers are significantly slower than the Direct writer, \r\neven when taking cost of Dictionary maintenance into account.\r\nThis change tries to close the gap. It has variety of improvements.\r\n\r\n1. DictionaryBuilder is only used for non null elements. \r\nDictionary Writer represents the null by an invalid index(-1).\r\n2. DictionaryBuilder does not expose the Block. It is used\r\nas a pure dictionary. This change opens up the possibility of\r\nusing non-sequential buffers to store the dictionary.\r\n3. On abandoning String dictionary, a dictionary block creation\r\nis avoided. Creating dictionary blocks and creating slice\r\nper row, produces too much garbage. Slices with offset and lengths\r\nare directly written to the SliceDirectColumnWriter.\r\n4. Statistics will not be recomputed when abandoning String\r\ndictionary. Existing statistics computed by String dictionary writer\r\nwill be reused.\r\n5. RowGroupIndexes are converted from IntBigArray to int[]. IntBigArray\r\nuses 1024 segments (4 KB) and default stride size is 10_000. So only\r\n8 of them could be filled at a time and wasting 4KB per stride.\r\nThis on some tests account for 20% of the memory usage.\r\n6. Rehashing of dictionary, need not verify the element equality.\r\nPreviously rehashing was verifying the element equals.\r\n\r\nThere is 20-30% improvement on dictionary encoders with this change.\r\n\r\nTest plan -\r\nAdded new additional test coverage and enhanced dictionary tests.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Improve dictionary writer performance\r\n```\r", "NaN"], ["15865", "Add release notes for 0.250", "Varun Gajjala", "varungajjala", "03/26/21, 10:15:24 PM", "# Missing Release Notes\n## David Stryker\n- [x] https://github.com/prestodb/presto/pull/15730 Support Trino bitwise shift functions in Presto (Merged by: Maria Basmanova)\n\n## Ge Gao\n- [ ] https://github.com/prestodb/presto/pull/15779 Support SHOW CREATE MATERIALIZED VIEW (Merged by: James Sun)\n- [ ] https://github.com/prestodb/presto/pull/15589 Support materialized view creation, drop and query (Merged by: James Sun)\n- [ ] https://github.com/prestodb/presto/pull/15589 Support materialized view creation, drop and query (Merged by: James Sun)\n- [ ] https://github.com/prestodb/presto/pull/15589 Support materialized view creation, drop and query (Merged by: James Sun)\n\n## Yang Yang\n- [x] https://github.com/prestodb/presto/pull/15738 Allow to extend Kafka connector (Merged by: Timothy Meehan)\n\n# Extracted Release Notes\n- #15761 (Author: James Petty): Support limiting the number of unacknowledged source splits per task\n  - Adds support for configuring the maximum number of unacknowledged source splits per task. This can be enabled by setting the ``max_unacknowledged_splits_per_task`` session property or ``node-scheduler.max-unacknowledged-splits-per-task`` configuration property.\n- #15761 (Author: James Petty): Support limiting the number of unacknowledged source splits per task\n  - Adds support for configuring the maximum number of unacknowledged source splits per task. This can be enabled by setting the ``max_unacknowledged_splits_per_task`` session property or ``node-scheduler.max-unacknowledged-splits-per-task`` configuration property.\n- #15761 (Author: James Petty): Support limiting the number of unacknowledged source splits per task\n  - Adds support for configuring the maximum number of unacknowledged source splits per task. This can be enabled by setting the ``max_unacknowledged_splits_per_task`` session property or ``node-scheduler.max-unacknowledged-splits-per-task`` configuration property.\n- #15767 (Author: Arunachalam Thirupathi): Dictionary Writer refactor and Long writer first draft\n  - Refactor ORC/DWRF String Dictionary.\n  - Add DWRF Integer Dictionary encoding.\n- #15843 (Author: Arunachalam Thirupathi): Improve performance of getRegionLogicalSizeInBytes\n  - Improve Dictionary Block getRegionLogicalSizeInBytes performance.\n- #15856 (Author: Mayank Garg): Fix memory leak in NodeTaskMap\n  - Fix a memory leak in `NodeTaskMap` which could lead to Full GC or OOMs in coordinator.\n\n# All Commits\n- 540c3588734f9cc981a174e1cd4d34b2e83591c8 Improving exception handling of remote UDF execution (Rongrong Zhong)\n- 2be4ffcff1381d893c30600bec3a6e26c4ff0250 Add additionalCpu to OperatorContext (Rongrong Zhong)\n- 3bf7ba2e7d66729cabd28e0a89ed82695f8c9c91 Change ThriftUdfService.invokeUdf API (Rongrong Zhong)\n- 333dbb0a6e52268ba0fbb20152d5958e7be5ea26 Support of inner join optimization with empty source. (Ahmad Ghazal)\n- 680baf523a22ddb70fcea3cc667fa1df25061c80 Add Hive schemas into InputFormat#getSplits (Rumeshkrishnan Mohan)\n- 2352b8d6fa183c7452ecc9c21b7b4874cef80250 Fix memory leak in NodeTaskMap (Mayank Garg)\n- 0b681a7a53ee8e0830198630feda46261158d1d2 Allow to extend Kafka connector (Yang Yang)\n- 7c7930e7bde16c396e646f743d569af95293c6c4 Improve performance of getRegionLogicalSizeInBytes (Arunachalam Thirupathi)\n- 960390692693b8abddba3571c3d5e8df2122c515 Add error categorization for out task scope errors (tanjialiang)\n- beadc29b1494effe8c006684ceb8b869a742cbe5 Add dictionary writer benchmark (Arunachalam Thirupathi)\n- 7d78886a9c449fa26ad7734229b4beba038795bd Add DWRF Integer Dictionary Encoding Writer (Arunachalam Thirupathi)\n- 427a66db94e42b83951de4b5d4fb7cced9ff16b3 Skip OOMing testNewBlockBuilderLikeForLargeBlockBuilder (Ariel Weisberg)\n- 4d3ced82b85c0e7c7a19dbad980170a3508ec232 Add test for grouping sets with limit (Ajay George)\n- 1126c2cb272569bf6f3346f00299da4b9253ac50 Support Trino bitwise shift functions in Presto (David Stryker)\n- 0ef71f60827f6f5f5a88cd38158ed33440efaf4f Correct presto-product-tests READ.me errors (v-jizhang)\n- 05b3342f628370282f890ddb8ddc03ac86cc323e Refactor DBResourceGroupConfigurationManager to support different data providers (Lisa Yang)\n- 0b1c725441147ef18ea4c48cd9f1ec2d55562e5b Support SHOW CREATE MATERIALIZED VIEW (Ge Gao)\n- fc80960407807f3714063b309c13e0bea5389ce2 Support execution of DROP MATERIALIZED VIEW (Ge Gao)\n- 4ee0795e8cb15ae01bdd23703d599aa8ef9aa240 Support DROP MATERIALIZED VIEW syntax (Ge Gao)\n- 81c80d5f0f6a0f7f13b0ab3dd506dd91c428fd66 Support execution of CREATE MATERIALIZED VIEW (Ge Gao)\n- 9a83668f301ef2f60b0325d42fcca50a2f494743 Support limiting the number of unacknowledged splits per task (James Petty)\n- 0209ef5123a3d1acf016c0b1a3dfecd76b5832b3 Avoid unnecessary node list sorting in SimpleNodeSelector (James Petty)\n- 325b7b2486a611c4e2bd692c9d0eca3718d2c9c5 Refactor NodeAssignmentStats to simplify the tracked representation (James Petty)", "NaN"], ["15867", "Increase verifier retry and determinism analysis limits", "Ajay George", "ajaygeorge", "03/29/21, 02:09:39 PM", "Increase verifier retry and determinism analysis limits\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nVerifier Changes\r\n* Increase verifier retry and determinism analysis limits.\r\n```", "NaN"], ["15868", "Fix NPE thrown while deleting hashtable spill file", "Arjun Gupta", "pgupta2", "03/24/21, 03:14:48 AM", "If broadcast stage fails due to any reason, it will return back\r\nnull output. If storage based broadcast join in enabled, then we\r\nattempt to drop the spill file. The logic does not perform null\r\nchecks and thrown NPE in this case.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15872", "Add file size and hash code for Presto On Spark", "Vic Zhang", "viczhang861", "03/30/21, 03:47:29 PM", "depended by https://github.com/facebookexternal/presto-facebook/pull/1470\r\n\r\nsee test plan in presto-facebook/pull/1470\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15873", "Disable product tests due to docker issue", "Ajay George", "ajaygeorge", "03/25/21, 02:43:03 AM", "Disable product tests due to docker issue\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15874", "Add flag to enable undo for alterPartition operation", null, "bhhari", "03/26/21, 09:36:41 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15875", "Use node location instead of nodeId to validate nodes", "Nikhil Collooru", "NikhilCollooru", "03/29/21, 08:29:29 PM", "This PR has code changes to use `node.location`  instead of `node.id` for worker node validation. Until now we are sending nodeId to NodeStatusService.isAllowed() method to figure out if a node is allowed to register with DiscoveryNodeManager. But with this PR, we will be sending nodeLocation to NodeStatusService to figure out if this node is allowed to register or not.\r\n\r\nWhen SOFT_AFFINITY node selection strategy is used, we sort the nodes based on nodeId and then calculate the split placement. The order of nodes in the sorted list plays an important role for having a good cache affinity. Currently, in Facebook deployment, we are setting nodeId as hostname. We also depend on container service to provide us the nodes for a cluster. But this container service may replace one or many nodes of cluster during kernel update etc. So when a node is replaced with a new host, the nodeId is also changed. The order of nodes in a sorted list changes when a nodeId changes and as a result the cache affinity will go bad. So in order to avoid this, we want to have nodeId as a constant and not change when underlying node changes. So for that we need nodeId to migrate away from hostname and remove any dependencies that assume nodeId to contain hostname. NodeStatusService expects nodeId to contain hostname. Hence this PR makes changes to send nodeLocation to NodeStatusService instead of nodeId. \r\n\r\n\r\nDepended by https://github.com/facebookexternal/presto-facebook/pull/1472\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* NodeStatusService implementation is now expected to validate & check if a node is allowed to register with DiscoveryNodeManager based on node.location instead of node.id configuration parameter.\r\n```\r", "NaN"], ["15879", "Allow user and password passthrough in jdbc connector", "Venki Korukanti", "vkorukanti", "03/27/21, 07:01:41 AM", "Cherry-pick of https://github.com/trinodb/trino/pull/482/\r\n\r\n```\r\n== RELEASE NOTES ==\r\nJDBC Connector Changes\r\n* JDBC connector allows user to pass username and password from CLI and other clients through use of extra credentials feature.\r\n\r\n```\r", "NaN"], ["15880", "Add flag to enable/disable undo for metastore operations", null, "bhhari", "03/27/21, 01:42:02 AM", "cherrypick from https://github.com/prestodb/presto/commit/9318af115af15fa3c4d9287b1ed548923f1ebe1c\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15882", "Do not propagate streaming preference from remote exchnage node", "Andrii Rosa", "arhimondr", "03/30/21, 04:14:40 PM", "Setting streaming preference propagates it down the plan and results\r\ninto a local exchange node being introduced right after the TableScan\r\nnode. This changes the lifecycle of the partial aggregation (or partial\r\nwindow function) operator from \"per split\" to \"per task\". Unfortunately\r\nsome of the \"partial\" operators, such as partial TopNRowNumber does not\r\nenforce memory limit for the \"partial\" buffer, resulting into premature\r\nmemory exceeding errors. While not enforcing a buffer size is a problem\r\nby itself, this change doesn't try to address that, but instead fixes a\r\nregression introduced by https://github.com/prestodb/presto/pull/15780\r\n\r\nTest plan \r\n\r\nUnit test\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15883", "Refactor presto-parquet to avoid depending on hadoop", "James Petty", "pettyjamesm", "03/31/21, 03:48:27 AM", "Refactors `MetadataReader#readFooter` and usages to avoid depending on `FSDataInputStream` and to use the `ParquetDataSource` methods instead. There are no more usages of `hadoop-apache2` except in tests after this change, so the dependency has been changed to reflect that.\r\n\r\nAs a side effect of using the `ParquetDataSource` instead of the `FSDataInputStream`, metadata reads will now count towards `ParquetDataSource` read bytes and read nanos, as well as the `FileFormatDataSourceStats` in the case of `HdfsParquetDataSource`.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15884", "Change type of destructure_tdigest.count from INTEGER to BIGINT", "Stephen Dimmick", "sdimmick", "03/30/21, 07:58:29 PM", "Also use `tDigest.getSize()` instead of unnecessarily computing sum\r\nof centroid weights. Fixes potential integer overflow.\r\n\r\nTest plan\r\n\r\n``` mvn \"-Dtest=TestTDigest*\" test```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15886", "Adding schema access rules to file based system access control", "Reetika", "agrawalreetika", "04/28/21, 03:30:12 PM", "Currently, file-based system access control allows defining catalog level rule. As a part of these changes, schema access rules can be included with file-based system access control for the below operations -\r\n\r\n- checkCanCreateSchema    : If the user has access to the catalog and is also schema owner, then access is provided. \r\n- checkCanDropSchema       : If the user has access to the catalog and is also schema owner, then access is provided. \r\n- checkCanRenameSchema  : If the user has access to the catalog and is also schema owner, then access is provided. \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nSecurity Changes\r\n\r\n- Add ability to set schema access rules in file-based system access control\r\n\r\n```\r", "NaN"], ["15888", "Fix generics related compiler warnings", "Andrii Rosa", "arhimondr", "03/30/21, 11:52:34 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15889", "Update /v1/info/state API", "Dongliang Chen", "dongliangchenfb", "04/22/21, 06:25:00 PM", "Test plan\r\n\r\nTested with:\r\n1) No cluster.required-workers-active set in config.properties\r\n2) cluster.required-workers-active set to 0\r\n3) cluster.required-workers-active set to a positive number\r\n\r\nIn all three cases, the cluster was coming up and running properly\r\n\r\nAlso tested the use case when the active workers drop below the threshold and confirmed that the state returns properly.\r\n\r\nChecked the API on the worker node confirmed it returns ACTIVE.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Update /v1/info/state API to mark coordinator active when 1) state is not shutting down, 2) active workers >= minimum required workers\r\n\r\n```", "NaN"], ["15891", "Add Drift serialization compatibility for session functions", "Timothy Meehan", "tdcmeehan", "04/19/21, 02:46:48 AM", "Test plan - Unit tests\r\n\r\nExtracted from #15479\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15892", "Add ResourceManagerClusterStateProvider", "Timothy Meehan", "tdcmeehan", "04/28/21, 02:48:26 PM", "Test plan - Unit tests, shadow tests, verifier\r\n\r\nExtracted from: #15479\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15895", "Update airlift to 0.200", "Andrii Rosa", "arhimondr", "03/31/21, 09:18:24 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15896", "Optimize splits serialization for Presto on Spark", "Andrii Rosa", "arhimondr", "04/02/21, 10:52:52 AM", "Presto on Spark has to enumerate all splits in advance to bind them\r\nto Spark tasks as Spark requires all the splits to be assigned in advance\r\nfor failure recovery reasons.\r\n\r\nEnumerating all splits in advance could be quite memory intensive,\r\nespecially when scanning large datasets.\r\n\r\nThis PR tries to reduce the memory footprint generated by the split\r\nenumeration.\r\n\r\nThere are 3 different issues this PR is addressing:\r\n\r\n1. It reorganizes the split serialization loop to avoid retaining both,\r\n   serialized and deserialized splits in memory\r\n2. It replaces a text based Json encoding with a Json compatible binary\r\n   Smile encoding.\r\n3. It uses less memory when compressing serialized splits. Instead of\r\n   serializing into a byte array first before compressing the serialization\r\n   is done directly into a compression stream. It reduces memory utilization\r\n   spikes and helps us avoid hitting the maximum byte array length\r\n   restriction that the previous implementation used to hit for certain\r\n   queries.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15898", "Add PER_QUERY_MEMORY_LIMIT spilling strategy", "Rebecca Schlussel", "rschlussel", "04/23/21, 07:43:56 PM", "Test plan - new unit test\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add new spilling strategy to spill when the user + system + revocable memory on a node exceeds max_total_memory_per_node.  This can be enabled by setting ``experimental.spiller.task-spilling-strategy`` to ``PER_QUERY_MEMORY_LIMIT``.\r\n```\r", "NaN"], ["15899", "Print query id for Presto on Spark query", "Vic Zhang", "viczhang861", "04/09/21, 02:36:26 PM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15900", "Add support for passthrough Elasticsearch queries", "Jinlin Zhang", "v-jizhang", "04/16/21, 07:15:27 AM", "Cherry-pick of https://github.com/trinodb/trino/pull/3735\r\nThis allows running queries over the results of a raw Elasticsearch query.\r\nIt extends the syntax of the enhanced ES table names with the following:\r\n\r\nSELECT * FROM es.default.\"<index>$query:<base32-encoded ES query>\"\r\n\r\nThe query is base32-encoded to avoid having to deal with escaping quotes and case\r\nsensitivity issues in table identifiers.\r\n\r\nThe result of these query tables is a table with a single row and a single column\r\nnamed \"result\" of type JSON.\r\n\r\nCo-authored-by: Martin Traverso <mtraverso@gmail.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nElasticSearch Connector Changes\r\n* Add support for passthrough Elasticsearch queries.\r\n  The Elasticsearch connector allows you to embed any valid Elasticsearch query, that uses the Elasticsearch [Query DSL](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html) in your SQL query.\r\n```\r", "NaN"], ["15901", "Optimize TaskInfo serialization for Presto on Spark", "Andrii Rosa", "arhimondr", "04/05/21, 07:33:39 PM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15903", "Remove hive-apache compile dependency in presto-orc", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "04/07/21, 07:52:43 PM", "Presto-orc only uses Murmur3.hash64 from Murmur3.\r\nThis dependency produces conflict for consumers of presto-orc,\r\nwho has also dependency on hive column vectors.\r\n\r\nTest plan - \r\n\r\nCreated new tests to verify new hash is same as the existing one.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* remove hive-apache dependency on presto-orc\r\n```\r", "NaN"], ["15904", "Optimize outer joins with one or more empty sources ", null, "ahmadghazal64", "04/23/21, 07:49:22 PM", "Address the outer join case for #15412\r\n\r\n== NO RELEASE NOTE ==", "NaN"], ["15905", "Fix spooling output buffer unit test", null, "chi-tsai", "04/08/21, 03:12:05 PM", "The unit tests for `SpoolingOutputBuffer` were disabled due to flakey tests. The executor in the `SpoolingOutputBufferFactory` was static and would get shutdown when the GC from other tests cleans up the factory, causing the tasks in `TestSpoolingOutputBuffer` to get rejected.", "NaN"], ["15906", "Fix HttpRemoteTask task stats and queue space update order", "James Petty", "pettyjamesm", "04/21/21, 01:56:32 PM", "Reorders task stats (and therefore `NodeTaskMap`) update and split queue space update in `HttpRemoteTask#processTaskUpdate`. Before this change, the scheduler could be started before the update to `NodeTaskMap` occurred and would then use stale values to assign splits.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15910", "Materialized view partition stitching for Hive connector", "Rohit Jain", "jainxrohit", "05/03/21, 12:29:29 AM", "Adding consistency support for reading materialized view. This change adds partition stitching to hive connector. A materialized view read for hive connector will automatically be stitched with base table to read consistent data.\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["15911", "Reenable product tests disabled due to dockerhub issue", "James Petty", "pettyjamesm", "04/06/21, 03:19:05 PM", "Supercedes https://github.com/prestodb/presto/pull/15907 in reenabling product tests previously disabled by 8e761e8 due to a dockerhub issue. \r\n\r\nAlso fixes `AbstractTestHiveClient#testParquetTypes` after https://github.com/prestodb/presto/pull/15883 changed the parquet page source behavior to include footer metadata reads towards completed bytes.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15913", "Fix NPE when predicate column is not contained in domain", "Shixuan Fan", "shixuan-fan", "04/07/21, 08:44:15 PM", "I don't have a test case that can reproduce this, but it seems theoretically possible so might as well patch it.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15914", "Reject @BeforeMethod in Multi-threaded tests", "Jinlin Zhang", "v-jizhang", "05/25/21, 06:09:27 AM", "This replaces #15756.\r\n\r\nExistence of @BeforeMethod or @AfterMethod most often indicates a\r\nshared mutable state that needs to be e.g. reset. This means the class\r\nshould be marked @Test(singleThreaded=true)\r\nAlso allow @BeforeMethod and @AfterMethod on non-single-threaded test\r\nclasses when the whole suite is marked as single-threaded.\r\n\r\nDepends on https://github.com/facebookexternal/presto-facebook/pull/1543\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Reject @BeforeMethod in Multi-threaded tests.\r\n```\r", "NaN"], ["15919", "Refactor SerializedPage checksum", "Andrii Rosa", "arhimondr", "04/08/21, 09:48:39 PM", "Avoid sending a hostname with every page. The remote host is known in\r\nExchangeClient.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15920", "Support non-lowercase table names in Druid connector", "Jalpreet Singh Nanda", "imjalpreet", "06/01/21, 04:18:33 AM", "This PR introduces druid.case-insensitive-name-matching to allow querying tables with mixed case/uppercase names.\r\n\r\nFixes #15587 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for querying non-lowercase table names in Druid connector (:pr:`15920`)\r\n```", "NaN"], ["15923", "Add release notes for 0.251", "Costin", "cocozianu", "04/20/21, 08:41:59 PM", "\r\n# Extracted Release Notes\r\n- #15327 (Author: George Wang): Add BigQuery Connector Support\r\n  - Add BigQuery Connector Plugin.\r\n- #15791 (Author: Nikhil Collooru): Improve SOFT_AFFINITY strategy to be more resilient to node downtime\r\n  - Adds support for memozing dead nodes. This can be enabled by setting the ``internal-communication.memoize-dead-nodes-enabled`` configuration property.\r\nnodes-enabled`` configuration property.\r\n- #15864 (Author: Arunachalam Thirupathi): Improve Dictionary Writer Performance\r\n  - Improve dictionary writer performance.\r\n- #15867 (Author: Ajay George): Increase verifier retry and determinism analysis limits\r\n  - Increase verifier retry and determinism analysis limits.\r\n- #15875 (Author: Nikhil Collooru): Use node location instead of nodeId to validate nodes\r\n  - NodeStatusService implementation is now expected to validate & check if a node is allowed to register with DiscoveryNodeManager based on node.location instead of node.id configuration parameter.\r\n- #15879 (Author: Venki Korukanti): Allow user and password passthrough in jdbc connector\r\n  - JDBC connector allows user to pass username and password from CLI and other clients through use of extra credentials feature.\r\n- #15903 (Author: Arunachalam Thirupathi): Remove hive-apache compile dependency in presto-orc\r\n  - Remove hive-apache dependency on presto-orc.\r\n\r\n# All Commits\r\n- d5ffc062a03711c567e27de039cd08aa17ccd0be Fix NPE when predicate column is not contained in domain (Shixuan Fan)\r\n- dd0a6fb67c2cd4cc8fa35566eb490f2776f4e2e1 Remove hive-apache dependency in presto-orc (Arunachalam Thirupathi)\r\n- a2ef578bb57a082cd3d1bd57bfc3b1ec5b2ef653 Revert \"Disable product tests due to docker issue\" (James Petty)\r\n- 712be766f1cef880ac140f49137535937035a36d Fix AbstractTestHiveClient parquet tests after #15883 (James Petty)\r\n- 18eca635382dcee39be472de5dc30a8f2e441364 Optimize TaskInfo serialization for Presto on Spark (Andrii Rosa)\r\n- 644e85232b940e5334ff0a707738a24607ee0755 Register accumulators as unnamed (Andrii Rosa)\r\n- 51c7bc87b19048271d73b989f6d2f9d54ffd6fe7 Include skipped non-alive preferred node stats in NodeSelectionStats (Nikhil Collooru)\r\n- a6decdf0fb3f90247c833ae977cd09efbb0b23f6 Improve SOFT_AFFINITY strategy to be more resilient to node downtime (Nikhil Collooru)\r\n- 117efd0df35f7d390907bb30245cbbd63177a379 Integrate quota with Alluxio Cache (Bin Fan)\r\n- 99847049dc357ccb721b1a8db37e73d81ac68b46 Hide BlockBuilder in DictionaryBuilder (Arunachalam Thirupathi)\r\n- 8f4750bb3e8528a10700424c8212d80f90e5d088 Optimize Null representation in Dictionary (Arunachalam Thirupathi)\r\n- b3e4540aed1bbc389b0a62e5ff0d6574151aa70d Support Slice with offset, length (Arunachalam Thirupathi)\r\n- 85a4c1522315bbf6bd1de765151d533d397b1ff9 Avoid Dictionary Block (Arunachalam Thirupathi)\r\n- 9aa0d6286e37fed6bb5aafa707f66689473a0daf Use int[] in dictionaryWriter (Arunachalam Thirupathi)\r\n- d846bd7b2a524f2c1819f286dd932a6b5bd1d23e Enhance Dictionary Writer tests (Arunachalam Thirupathi)\r\n- 2ad67dcf000be86ebc5ff7732bbb9994c8e324a8 Support enforcement of NOT NULL column declarations (v-jizhang)\r\n- b161b3b0feb58536f5635e6d15bf8e1e6889c04e Avoid serializing all splits with every task descriptor (Andrii Rosa)\r\n- a407eeda62c7ded28fab272e6de14c306fc263ea Enable more efficient Kryo serialization for SerializedPrestoSparkTaskSource (Andrii Rosa)\r\n- ad60bd579a5f3b6e24ecdc8c07e557243a37594e Refactor testAssignSourceDistributionSplits (Andrii Rosa)\r\n- 0be3863d3e6fd05d8b7674f0684d46b2161ca990 Optimize splits serialization for Presto on Spark (Andrii Rosa)\r\n- 3e1c1f31e4bd8d9b889fb98dc1ca23738f7a2513 Enable different cache eviction policy (Bin Fan)\r\n- 1b8d59c2d4f37d1a107993abd35e1bad80a25bc1 Use Airlift provided Smile support (Andrii Rosa)\r\n- 47a4ed193ee6d3b31cb6519c78679d711124e1a8 Update Airlift to 0.200 (Andrii Rosa)\r\n- c043ce1a82bb6c62018fcbc45ebe566ed72a7a7f Extract pluggable KafkaTableDescriptionSupplier (Yang Yang)\r\n- 86efbd0d5a4038753a6c1cfebe1ec77317be4e91 Refactor presto-parquet to avoid depending on hadoop (James Petty)\r\n- d58db783ca9aa4a35e2542a2cc1d6e7a64df069e Fix generics related compiler warnings (Andrii Rosa)\r\n- 784d8182a51e60f47dc92663cfa47e49b3bc6f71 Change type of destructure_tdigest.count from INTEGER to BIGINT (Stephen Dimmick)\r\n- b038e5df14fdcd7ae347b50655638239a7046d6d Do not propagate streaming preference from remote exchnage node (Andrii Rosa)\r\n- 6b52be7d89ee53153eefb7624930d1d08eef70d6 Add file size and hash code for Presto On Spark (Vic Zhang)\r\n- 0d7eda2e7fc113852d6ebc4e42903e72f5fef78c Use node location instead of nodeId to validate nodes (Nikhil Collooru)\r\n- ca3a936b2f6a0a5bf2f59ab43e2f94a334f2e6e9 Increase verifier retry and determinism analysis limits (Ajay George)\r\n- f4124c6e576acf3fef8d260abd452ef4f87d5aed Clone the JDBC properties before updating them (Venki Korukanti)\r\n- b734141ce3c54b878c7c48be77d48b365f8eb898 Add Nullable annotation in BaseJdbcConfig (Hao Luo)\r\n- 221d4784bbe172ab165baf05c2f8af2ce7e69593 Allow user and password passthrough in jdbc connector (Hao Luo)\r\n- 9318af115af15fa3c4d9287b1ed548923f1ebe1c Add flag to enable/disable undo for metastore operations (Bhavani Hari)\r\n- 56a0b48c6d15b55e8e5768d93bcc43cfe0ad09de Add session property to enable spooling output buffer (Chi Tsai)\r\n- 2739e03211f326afca2f9e3d11413ab6f115a12d Implement Spooling OutputBuffer (Chi Tsai)\r\n- 8e761e8ee115114d937dab98f13e32a1172a10a5 Disable product tests due to docker issue (Ajay George)\r\n- d690856724622379b9725a478f075ecd0b5b4a2d Fix NPE thrown while deleting hashtable spill file (Arjun Gupta)\r\n- 49942203efe72ff3321072e7a08f73d7c0b04e21 Refactor BigQuery Connector (George Wang)\r\n- a115d7892c6dff79386df732a37f3905a9ff75a2 Add BigQuery Connector doc (George Wang)\r\n- b19d29a5827cc5dc6c5ea0749294ff25b8e22d9c Add BigQuery Connector Catalog (George Wang)\r\n- 830440eac1602d451cea38e19a350ed5219ce511 Add BigQuery Connector Support (George Wang)", "NaN"], ["15925", "Reenable Presto on Spark integration Docker based test", "Andrii Rosa", "arhimondr", "04/13/21, 04:15:42 PM", "```\r\n== NO RELEASE NOTES ==\r\n```\r", "NaN"], ["15926", "Fix evictor type for Alluxio cache", "Bin Fan", "apc999", "04/12/21, 04:13:32 PM", "Test plan - unit tests\r\n\r\nFix evictor policy type for Alluxio cache and make sure the default config is practised in unit tests\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15930", "Don't rewrite IF to CASE", "Sreeni Viswanadha", "kaikalur", "04/20/21, 01:32:35 AM", "Test plan - tests already exist.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["15932", "Disable empty bucket file for temporary table", "Vic Zhang", "viczhang861", "05/05/21, 04:23:41 PM", "\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\n\r\nHive Changes\r\n*  Set default value for configuration property `hive.create-empty-bucket-files-for-temporary-table` to `false`.\r\n```\r\n\r", "NaN"], ["15933", "Fix Presto on Spark failure during startup", "Andrii Rosa", "arhimondr", "04/13/21, 12:34:48 PM", "== RELEASE NOTES ==\r\nPresto on Spark Changes\r\n* Fix a bug when Presto on Spark doesn't start because the temporary storage is not initialized", "NaN"], ["15934", "Improve bucketed table write parallelism for Presto on Spark", "Andrii Rosa", "arhimondr", "04/15/21, 12:23:08 PM", "Make sure each partitioned (bucketed) table writer task has assigned `task_partitioned_writer_count` number of buckets to keep all available writer threads busy. Currently there's only 1 bucket per task being assigned what results in threads starvation.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPresto on Spark Changes\r\n* Improve bucketed table write parallelism\r\n```\r", "NaN"], ["15937", "Do not allocate resources within test constructor", "Jinlin Zhang", "v-jizhang", "05/13/21, 08:21:53 PM", "Cherry-pick of https://github.com/trinodb/trino/pull/2412\r\n\r\nCo-authored-by: Piotr Findeisen <findepi@users.noreply.github.com>\r\n\r\nTest plan - Make sure all CI tests pass.\r\n\r\nDepends on https://github.com/facebookexternal/presto-facebook/pull/1542\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Do not allocate resources within test constructor for a cleaner code.\r\n```", "NaN"], ["15941", "Minor refactor of Presto on Spark", "Vic Zhang", "viczhang861", "04/14/21, 04:19:11 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15942", "Revert coordinator handling of out-of-task scope error categorization", null, "tanjialiang", "04/16/21, 04:45:48 AM", "Changes have been made on cpp worker side to more gracefully handle categorizations of errors generated from places outside of Task. This 'hack' on the coordinator side is no longer needed.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15943", "Move TableDescriptionSupplier to separate guice module", "yang", "yangy0000", "05/06/21, 05:52:47 PM", "Cherry-pick https://github.com/trinodb/trino/commit/696e10ba9e2cd292554b74f24e824719cf5a2449\r\n\r\nCo-Authored-By: Elon Azoulay <elon.azoulay@gmail.com>\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines. Don't forget to follow our [attribution guidelines](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#attribution) for any code copied from other projects.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15944", "A few bug fixes related to remote udf", "Rongrong Zhong", "rongrong", "04/26/21, 07:10:46 PM", "Test plan - unit test\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15947", "Enable timeout by default in Alluxio", "Ke", "kewang1024", "04/21/21, 06:35:27 PM", "```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Enable timeout by default in Alluxio\r\n\r\n```\r\n\r", "NaN"], ["15949", "Add support for returning partial results", "Nikhil Collooru", "NikhilCollooru", "04/26/21, 10:09:12 PM", "Long tail tasks that take a lot of time to finish the assigned splits and affect the query latency a lot. These tail tasks might be slow because of various reasons like bad node, low cache hit rate etc. Some clients might be okay with skipping such slow tasks and finishing the query with partial results. This commit adds support for returning partial results to such clients.\r\n\r\nIf we set `partial_results_completion_ratio_threshold=0.75`, then it means that we want the query to wait for 75% of the scan tasks to finish before becoming eligible and returning partial results. \r\n\r\nIf we set `partial_results_max_execution_time_multiplier=1.5`, then the max scan task end time is set as 1.5 * X, where X is time taken for 75% of the scan tasks to complete. Meaning any pending scan tasks that are still running after 1.5X elapsed time are cancelled and partial results are returned for the query.\r\n\r\nWhen partial results are enabled, the default values are `partial_results_completion_ratio_threshold=0.50` and `partial_results_max_execution_time_multiplier=2.0`\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for returning partial results for the queries by setting `partial_results_enabled` session property. Additionally `partial_results_max_execution_time_multiplier`, `partial_results_completion_ratio_threshold`  session properties can be set to configure the max execution time multiplier and minimum completion ratio threshold for the queries.\r\n```\r", "NaN"], ["15950", "Add support for static AWS credentials in GlueHiveMetastore", "Jinlin Zhang", "v-jizhang", "05/25/21, 07:41:58 PM", "Cherry pick of https://github.com/trinodb/trino/pull/748\r\nThis PR adds support for specifying static AWS credentials for\r\nGlueHiveMetastore through two new configuration options,\r\nhive.metastore.glue.aws-access-key and\r\nhive.metastore.glue.aws-secret-key.\r\nThis is helpful in situations where someone may want to access a glue\r\ncatalog through Presto without running the cluster on AWS\r\ninfrastructure.\r\n\r\nCo-authored-by: Philippe Gagnon <pgagnon@users.noreply.github.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add support for static AWS credentials in GlueHiveMetastore\r\n   :doc:`/connector/hive`\r\n```\r", "NaN"], ["15953", "Fix loading config properties for temp storage", "Vic Zhang", "viczhang861", "05/05/21, 04:23:21 PM", "1. Before this change, Presto classic did't load temp storage related configs from local files. This PR fixes `loadTempStorages()`\r\n2. Presto on spark doesn't need to use local temp storage. If it does, code order needs to change.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15954", "Add release notes for 0.249.2", null, "neeradsomanchi", "04/16/21, 09:53:46 PM", "Test plan - Checked the html pages locally and they look fine.\r\n\r\n<img width=\"1644\" alt=\"Screen Shot 2021-04-16 at 10 42 41 AM\" src=\"https://user-images.githubusercontent.com/22344667/115064126-6b2d8280-9ea1-11eb-8700-77d9c39b8e93.png\">\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15955", "Add Connection Params for JDBC HTTP Protocol", "Andrew Donley", "adonley", "06/02/21, 02:10:36 AM", "Fixes an issue for Java > 8_242 where the default is HTTP/2 for users with a loadbalancer infront that accepts the protocol.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Added connection param \"protocols\" that allows a user to specify which HTTP protocols the Presto client is allowed to use.\r\n```", "NaN"], ["15956", "Use segmented Slice in SliceDictionaryWriter", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "04/26/21, 06:42:09 PM", "Store elements of dictionary in Segmented Slices, instead of\r\none contiguous segment. When the number of elements\r\nin the dictionary is less than 100,000 there is no noticeable\r\nperformance degradation. When the number of elements in the\r\ndictionary reaches 10,000,000 sorting/comparing the element\r\nneeds to compute segment/offset which makes it worse by 10%.\r\nBut this is an unlikely case.\r\n\r\nTest plan - \r\nAdded new test cases for the SegmentedSlices.\r\nDictionary is covered by existing tests.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Store dictionary elements in Segmented Slice.\r\n```\r", "NaN"], ["15957", "Fix MAP_UNION aggregate function to support ORDERY BY clause", "Naveen Mahadevuni", "nmahadevuni", "04/23/21, 09:46:56 PM", "Fixes #15754 \r\n\r\nTest plan - Added tests in TestOrderedAggregation.java\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15962", "Fix order inconsistency between Hive Metastore partition information and actual partition values", "sophiashang", "XiaohuiShang", "04/20/21, 12:09:57 AM", "Test plan - tests already exist.\r\n\r\n== NO RELEASE NOTE ==", "NaN"], ["15963", "Implement toString() for JDBC driver Array result", "Junyi Huang", "junyi1313", "04/21/21, 07:37:26 AM", "Cherry-pick of https://github.com/trinodb/trino/commit/5a3c7add6a355b01d527cc0b431301af3fda98b4\r\n\r\nCo-authored-by: David Phillips <david@acz.org>\r\n\r\nTest plan - Running locally\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nJDBC Changes\r\n* Implement ``toString()`` for ``java.sql.Array`` results. \r\n```", "NaN"], ["15964", "Add Drift serialization compatibility for NodeStatus", "Timothy Meehan", "tdcmeehan", "04/19/21, 07:25:40 PM", "Test plan - Unit tests\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15966", "Upgrade zstd-jni to v1.4.9-2", "Chen Li", "lic9", "04/20/21, 12:27:22 AM", "New zstd has better performance on compression and decompression. 1%-2% overall improvement was measured on some Presto workloads.\r\nPreviously new zstd-jni caused long GC pause because of finalizer in ZstdCompressCtx and ZstdDecompressCtx (introduced by zstd-jni commit https://github.com/luben/zstd-jni/commit/5209351fc8e071084a3e6df13c0f60bd13f916f6). This has been fixed in v1.4.9-2, and we verified that no long GC occurred.\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines. Don't forget to follow our [attribution guidelines](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#attribution) for any code copied from other projects.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* ...\r\n* ...\r\n\r\nHive Changes\r\n* ...\r\n* ...\r\n```\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15967", "Upgrade to drift 1.33", null, "mayankgarg1990", "04/20/21, 10:46:19 AM", "See https://github.com/prestodb/drift/pull/34 for the bug fix information\r\n\r\nTest plan - ran the build to ensure that the upgrade did not break dependencies\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15969", "Add function version to FunctionMetadata", "Rongrong Zhong", "rongrong", "04/27/21, 12:23:17 AM", "Test plan - integration tests\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15975", "Fix hanging join test", "Rebecca Schlussel", "rschlussel", "04/21/21, 12:05:51 PM", "Test plan - unit test, ci\r\n\r\nCherry-pick of https://github.com/trinodb/trino/pull/7455/commits/42b8e33f7958f4c208a8c6f9b918d6f65ecd0456 plus some required refactoring. \r\n\r\nSpilledLookupSourceHandle needs to be disposed immediately when join operators finished processing probe data. Otherwise SpilledLookupSourceHandle is never disposed as it's not registered in PartitionedConsumption. This prevents HashBuilderOperator from finishing.\r\n\r\nThis was causing some tests to hang in TestDistributedSpilledQueries. \r\n\r\nFixes https://github.com/prestodb/presto/issues/15542 and https://github.com/prestodb/presto/issues/15921\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix hanging join operator when spill is enabled and the probe side finishes before the hash builder starts\r\n\r\n```\r", "NaN"], ["15977", "Fix code bug with param wrongly masking a field", "Sreeni Viswanadha", "kaikalur", "04/23/21, 09:31:41 PM", "Test plan - Added a unit test.\r\n\r\nName of the param to the function masks the field. But there is only one invocation of it and the checkState following it would have failed if this was ever called. So proactively fixing it.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15983", "Enable query and node heartbeats to resource manager", "Timothy Meehan", "tdcmeehan", "05/01/21, 12:14:27 AM", "Test plan - Unit tests, shadow tests, verifier\r\n\r\nExtracted from: #15479\r\n\r\nDepends on: #15892\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15984", "Add cluster-wide endpoints for query and cluster information", "Timothy Meehan", "tdcmeehan", "05/12/21, 03:34:15 AM", "Test plan - Unit tests, shadow tests, verifier\r\n\r\nExtracted from: #15479\r\n\r\nDepends on: #15983\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["15987", "Support logging formatted prepared query", "Shixuan Fan", "shixuan-fan", "04/22/21, 09:38:12 PM", "Logging prepared query is useful especially for EXECUTE queries\r\nas it could make debugging easier.\r\n\r\nTest plan \r\nPrepare query: `prepare test from select * from orders where orderkey < ?;`\r\nActual query: `execute test using 100`\r\n\r\nRunning in HiveQueryRunner with this session property enabled and disabled.\r\n<img width=\"1208\" alt=\"Screen Shot 2021-04-22 at 11 37 23 AM\" src=\"https://user-images.githubusercontent.com/12627919/115768449-29984e00-a35f-11eb-8ceb-f22ce7613895.png\">\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for logging formatted prepared query. This could be enabled by setting session property ``log_formatted_query_enabled`` to ``true``.\r\n```", "NaN"], ["15991", "Handle casting to HiveSplit", "Rohit Jain", "jainxrohit", "04/23/21, 11:28:20 PM", "If the split is not of type HiveSplit then treat it as no split. It fixes the issue\r\nhttps://github.com/prestodb/presto/issues/15982\r", "NaN"], ["15992", "Support shutting down coordinator using /info/state endpoint", null, "abhiseksaikia", "04/27/21, 08:52:54 PM", "Support shutting down coordinator using PUT /info/state\r\n\r\nCurrently we cannot shut down coordinator through /info/state endpoint. With the disagg coordinator setup, we should able to do this so that we can drain individual coordinators gracefully.\r\n\r\nTest plan - \r\n1.  Verified by running unit test\r\n2. Verified by running HiveQueryRunner in local  -\r\n        Use PUT  /v1/info endpoint with payload \"SHUTTING_DOWN\"\r\n        Before the coordinator is shutdown,  GET /v1/info/state returns  \"SHUTTING_DOWN\" as the response.\r\n        Once the coordinator is shutdown (after grace period), server refuses connection to port used by coordinator  \r\n\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support for shutting down coordinator via the /v1/info/state endpoint.\r", "NaN"], ["15993", "Fix Athena Glue table compatibility issue", "Jinlin Zhang", "v-jizhang", "05/20/21, 08:46:48 AM", "Cherry pick of Trino https://github.com/trinodb/trino/pull/1343,\r\nAthena is fine reading Glue tables that have no table type set.\r\nCurrently Presto set to default OTHER but Athena tables are external:\r\nhttps://docs.aws.amazon.com/athena/latest/ug/drop-table.html so set\r\nthe table type to EXTERNAL if it is unset.\r\n\r\nCo-authored-by: Henning Schmiedehausen <henning@schmiedehausen.org>\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Changes\r\n* Allow accessing tables in Glue metastore that do not have a table type.\r\n```\r", "NaN"], ["15996", "Support REFRESH MATERIALIZED VIEW", "Ge Gao", "gggrace14", "06/15/21, 02:24:16 AM", "Depended by https://github.com/facebookexternal/presto-facebook/pull/1509", "NaN"], ["15997", "Add ability to read session properties from HdfsContext", null, "bhhari", "04/26/21, 09:26:05 PM", "This PR adds adds the ability to read session properties from the HdfsContext.\r\nThis will help to control the storage side flow based on the session.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16000", "Fix S3 directory detection based on ContentType header", "Naveen Mahadevuni", "nmahadevuni", "04/28/21, 09:49:31 PM", "Cherry pick of https://github.com/trinodb/trino/pull/6992\r\n\r\nCo-authored-by: Micha\u0142 \u015alizak <michal.slizak+github@gmail.com>\r\n\r\nTest plan - Added test in TestPrestoS3FileSystem\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n--------------\r\n* Fix S3 table creation error when the directory location specified was created from AWS console.\r\n```\r", "NaN"], ["16003", "Add missing support for bucket sort order in Glue", "Jinlin Zhang", "v-jizhang", "05/20/21, 08:46:35 AM", "Cherry pick of https://github.com/trinodb/trino/pull/1870.\r\n\r\nCo-authored-by: Karol Sobczak <karol.sobczak@karolsobczak.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Fix a bug where the files would not be sorted when inserting into bucketed sorted tables with Glue.\r\n```\r", "NaN"], ["16006", "Rename CompressionParameters to ColumnWriterOptions", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "05/04/21, 03:44:42 AM", "1. Rename CompressionParameters to ColumnWriterOptions.\r\nThe columnWriterOptions can be used to pass String \r\nstatistics limit and whether integer dictionary encoding is\r\nenabled.\r\n2. ColumnWriterOptions will also be used to disable dictionary\r\nsorting in String dictionary encoding for DWRF format.\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\nAll changes done using IDEA refactors, except for test\r\nchanges where I modified the code to remove duplication.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16007", "Fix formatted query with PREPARE statement", "Shixuan Fan", "shixuan-fan", "04/27/21, 08:21:20 PM", "Also added \"-- Formatted Query:\" at the beginning of formatted\r\nquery.\r\n\r\nTest plan - Added unit test. Local run screenshot:\r\n\r\n<img width=\"1174\" alt=\"Screen Shot 2021-04-26 at 4 53 49 PM\" src=\"https://user-images.githubusercontent.com/12627919/116165120-fe3f9700-a6af-11eb-92e1-acf7bc073e65.png\">\r\n\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16011", "Partition schema evolution for Parquet", "Jalpreet Singh Nanda", "imjalpreet", "05/17/21, 03:56:43 AM", "Fixes #12212 \r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add support for partition schema evolution for parquet\r\n```\r", "NaN"], ["16012", "Add support for MaxResults on Glue Hive Metastore", "Jinlin Zhang", "v-jizhang", "05/19/21, 07:18:37 PM", "Cherry pick of https://github.com/trinodb/trino/pull/3024 and\r\nhttps://github.com/trinodb/trino/pull/4938\r\n\r\nCo-authored-by: Istvan <istvan@lambdainsight.com>\r\nCo-authored-by: Ashhar Hasan <hashhar_dev@outlook.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add support for MaxResults on Glue Hive Metastore\r\n```\r", "NaN"], ["16013", "Add release notes for 0.252", null, "vaishnavibatni", "05/04/21, 12:41:05 AM", "# Missing Release Notes\r\n## Andrii Rosa\r\n- [x] https://github.com/prestodb/presto/pull/15933 Fix Presto on Spark failure during startup (Merged by: Andrii Rosa)\r\n\r\n## Chi Tsai\r\n- [x] https://github.com/prestodb/presto/pull/15905 Fix spooling output buffer unit test (Merged by: Timothy Meehan)\r\n\r\n## Tal Galili\r\n- [x] https://github.com/prestodb/presto/pull/15799 Adding the chi square distribution (Merged by: Rongrong Zhong)\r\n\r\n# Extracted Release Notes\r\n- #15504 (Author: imjalpreet): Add file based password authenticator plugin\r\n  - Add file based password authenticator plugin.\r\n- #15857 (Author: James Sun): Add query retry for transient errors\r\n  - Add automatic query retry functionality for transient failures. This can be enabled by setting ``per-query-retry-limit`` to a non-zero integer to indicate the per query retry count.\r\n- #15889 (Author: Dongliang Chen): Update /v1/info/state API\r\n  - Update /v1/info/state API to mark coordinator active when 1) state is not shutting down, 2) active workers >= minimum required workers.\r\n- #15898 (Author: Rebecca Schlussel): Add PER_QUERY_MEMORY_LIMIT spilling strategy\r\n  - Add new spilling strategy to spill when the user + system + revocable memory on a node exceeds max_total_memory_per_node.  This can be enabled by setting ``experimental.spiller.task-spilling-strategy`` to ``PER_QUERY_MEMORY_LIMIT``.\r\n- #15900 (Author: v-jizhang): Add support for passthrough Elasticsearch queries\r\n  - Add support for passthrough Elasticsearch queries.\r\n- #15934 (Author: Andrii Rosa): Improve bucketed table write parallelism for Presto on Spark\r\n  - Improve bucketed table write parallelism.\r\n- #15947 (Author: Ke Wang): Enable timeout by default in Alluxio\r\n  - Enable timeout by default in Alluxio.\r\n- #15949 (Author: Nikhil Collooru): Add support for returning partial results\r\n  - Add support for returning partial results for the queries by setting `partial_results_enabled` session property. Additionally `partial_results_max_execution_time_multiplier`, `partial_results_completion_ratio_threshold`  session properties can be set to configure the max execution time multiplier and minimum completion ratio threshold for the queries.\r\n- #15956 (Author: Arunachalam Thirupathi): Use segmented Slice in SliceDictionaryWriter\r\n  - Store dictionary elements in Segmented Slice.\r\n- #15963 (Author: Junyi Huang): Implement toString() for JDBC driver Array result\r\n  - Implement ``toString()`` for ``java.sql.Array`` results.\r\n- #15975 (Author: Rebecca Schlussel): Fix hanging join test\r\n  - Fix hanging join operator when spill is enabled and the probe side finishes before the hash builder starts.\r\n- #15987 (Author: Shixuan Fan): Support logging formatted prepared query\r\n  - Add support for logging formatted prepared query. This could be enabled by setting session property ``log_formatted_query_enabled`` to ``true``.\r\n\r\n# All Commits\r\n- 99dcd7660baadbc24ab1b0803d8eead7b7d34f27 Add version to FunctionMetadata (Rongrong Zhong)\r\n- b496843cf52a132780776dd9db00980d4879abda Add support for returning partial results (Nikhil Collooru)\r\n- bb50c29bf48c6c44d9399e66e9195e115953264e Add ability to read session properties from HdfsContext (Bhavani Hari)\r\n- 130863fa8e76d6d44caae79f81d1312dce89adc5 Refactor planner to use FunctionImplementationType.isExternal (Rongrong Zhong)\r\n- 82833b873a3f9accc2fbc8fd7dd58e437a818c20 Remove redundant projection added in HashGenerationOptimizer (Rongrong Zhong)\r\n- 421d7f65ce79ac803e41b719653a650b0d994d4d Do not create local projection for constant in PlanRemoteProjections (Rongrong Zhong)\r\n- 53ebcda217e2c3de8b77b8b3c7159e22dd28f7cb Use segmented Slice in SliceDictionaryWriter (Arunachalam Thirupathi)\r\n- 3e0c6a0777036d2d8238f17f4d6c7d92cddd7fbb Handle casting to HiveSplit (Rohit Jain)\r\n- 2cfbf876fc3d781e0881a1ec34da5652ecc9db30 Fix MAP_UNION aggregate function to support ORDERY BY clause (Naveen Kumar Mahadevuni)\r\n- 3c8c4ec16af715f91320bfe9d03c8c45ed070b19 Fix code bug with param wrongly masking a field (Sreeni Viswanadha)\r\n- a34cf433047ce9a5206cc4b510ff35ca11e45ca9 Bring retry queries to the beginning of the queue (James Sun)\r\n- 2a2ac84e99f61893def912fa4b272d8f7d3d4a0a Handle duplicated retry requests (James Sun)\r\n- 338aeb239bf633b1d5c9b79875cb0b567a6b291b Add tests for query retry (James Sun)\r\n- 0fe423619590915a712023c8e25a172751f7a6e4 Support query retry only for auto commit transactions (James Sun)\r\n- 68f6cffa860bd572316c261ba51bdb84506bb1f2 Throw error upon purged retryable queries (James Sun)\r\n- ad51d4cd0e82dc5395dcc8ba819bd0be2d2dc88a Add query retry local limit enforcement (James Sun)\r\n- 2919bb932aa2ff16918c6ac520104e4a8c58ff8f Add query retry logic for transient failures (James Sun)\r\n- cce720c1a13271a300b82c63e09aa481dc877e26 Add retriable flag to error code (James Sun)\r\n- 28dcba7c38c84af98426ab392146916138d5dd36 Remove unused timeoutExecutor for queued queries (James Sun)\r\n- 6a1d1d7fde05ce36407b43ff93fbdc83f042a9f7 Adding the chi square distribution, which is central to many statistical procedures (https://en.wikipedia.org/wiki/Chi-square_distribution) (#15798) (Tal Galili)\r\n- 69a52160f74d96d2ef804500871dd8dedb291b0c Extend join optimizations based on empty source. (Ahmad Ghazal)\r\n- 6c8b45cd31c8be32d4a79b1722b6d76db0de1149 Catch Exception in MemoryRevokingSchedulers (Rebecca Schlussel)\r\n- 751e2e3dba7eab890f4b38262252885835d197ee Add per-query limit based spilling strategy (Rebecca Schlussel)\r\n- 6c09bebb547099e6a781bedc7025212130dc2fb5 Move getMemoryAlreadyBeingRevoked to utility class (Rebecca Schlussel)\r\n- 8269d72964d92d6ca8ff4585ab1386944b8e081f Refactor TaskRevocableMemoryListener (Rebecca Schlussel)\r\n- b6c170ec57ced1f3bded64fa770ee3447a3a6d02 Refactor MemoryPoolListener (Rebecca Schlussel)\r\n- 24243ad5d6ba01d6e569a2fd3a1be2453624e791 Support logging formatted prepared query (Shixuan Fan)\r\n- 60edac5a09b11d62fae066967e68a093c222eba8 Update /v1/info/state API (Dongliang Chen)\r\n- 7c8b6f5e4a68750c182e8de5039b4529b752a2b7 Enable timeout by default in Alluxio (Ke Wang)\r\n- 3e850ba3210365fa18d77102b75df70d3a388c9c Fix HttpRemoteTask task stats and queue space update order (James Petty)\r\n- a7f7a3551f012ffb2414431c4084ac4a7af346b5 Rename some tests to follow naming convention (Rebecca Schlussel)\r\n- f05484a24ff8c679b03d61fb23ddffd6d8f8ad8f Reenable join spill test (Rebecca Schlussel)\r\n- 848f7355cce26505ef0d83da3d2ab560b9306edc Fix join spill race condition (Rebecca Schlussel)\r\n- 46cd0e8f60ab1b6a9154d93523b2b4009b87e0d0 Only set join operators count if running with spill (Rebecca Schlussel)\r\n- 83fe8e7c8aafabaa64d943038202bc02d26b326b Implement toString() for JDBC driver Array result (Junyi Huang)\r\n- f084ff4ced01285769fbf8336a5782d3e24df384 Release notes for 0.251 (Costin V Cozianu)\r\n- 2dac1586f5e8622a44b7f2303ae7f59f6e75dd53 Upgrade to drift 1.33 (Mayank Garg)\r\n- 413584a51b0d6b70653e2492d620b0c9338fb7b5 Don't rewrite IF to CASE (Sreeni Viswanadha)\r\n- d0e46d58f4ee94852dd24d5ed4acf565dd25c358 Upgrade zstd-jni to v1.4.9-2 (Chen Li)\r\n- 543567c020e6b8171313ae8ccee53f1f80c394cd Fix order inconsistency between Hive Metastore partition information and actual partition values (sophiashang)\r\n- b1306fc7af8e3842887ad526d83eb102876e4940 Add Drift serialization compatibility for NodeStatus (Tim Meehan)\r\n- 3ab2e8e3dcdf43dbd4b666384d1d2711ea8c8565 Add Drift serialization compatibility for session functions (Tim Meehan)\r\n- 8c0b1ff6f12a6bc78efb13da7716c2c17a0d0436 Add support for passthrough Elasticsearch queries (v-jizhang)\r\n- 3513d076c53e74d01871a163be304855f3194b8e Revert coordinator handling of out-of-task scope error categorization (tanjialiang)\r\n- 33cdd9b3d60d5d4fcd1304835a5a0da223be3f4b Optimize partitioned table write for Presto on Spark (Andrii Rosa)\r\n- d35dbfe0b97459e139cd2a710d2b49548c0c904f Refactor PrestoSparkRddFactory (Andrii Rosa)\r\n- 75ec89cf5f2c1efc4dc981e60137ccea8acd653d Remove duplicate logger setting (Andrii Rosa)\r\n- 7aa73311dc486811c8d790d0024b88953d49e82a Reformat TestPrestoSparkQueryRunner (Andrii Rosa)\r\n- e4c9e77fb58bf30d0322765150760bfa362736b0 Run join, order by and window tests with Presto on Spark (Andrii Rosa)\r\n- 506055642f7425a8878e89d7a9b1eb667c5e72e5 Minor refactor of Presto on Spark (Vic Zhang)\r\n- d1f947e7c02b4f27a50a1d030971802cd1adb9be Fix Presto on Spark query logging (Vic Zhang)\r\n- 32cd62a75cf09e1763c89a48a368c91ba0857dbe Reenable Presto on Spark integration Docker based test (Andrii Rosa)\r\n- 94942fb4a89bf718d852f3d748f95858a7529cde Fix Presto on Spark failure during startup (Andrii Rosa)\r\n- 8803df3759a27fa7ebc1df00e42aa39b7ddcb909 Fix evictor type for Alluxio cache (Bin Fan)\r\n- 4078f509d9bda4478a5260e9d52404ad5f7a5c4b Add file based password authenticator documentation (imjalpreet)\r\n- c64f2a6cfc9113baf22a275a6bb5803750b2ab04 Add file based password authenticator plugin (imjalpreet)\r\n- b5b6ccdb113ce8cfeec96b06c2a3e6fbb7aaaa62 Print query id for Presto on Spark query (Vic Zhang)\r\n- 7f1395e755beccf554db0e49514adae8e4d5f52d Refactor SerializedPage checksum (Andrii Rosa)\r\n- 8c464c7e1ad4cb1cc02e6a869597cdfde1e833d4 Fix spooling output buffer unit test (Chi Tsai)", "NaN"], ["16014", "Add support for Glue endpoint URL", "Jinlin Zhang", "v-jizhang", "05/13/21, 01:00:23 PM", "Cherry pick of https://github.com/trinodb/trino/pull/3239\r\n\r\nCo-authored-by: Pawel Palucha <pawel.palucha@starburstdata.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Connector Changes\r\n* Add support for Glue endpoint URL\r\n  :doc:`/connector/hive`\r\n```", "NaN"], ["16016", "Revert \"Add version to FunctionMetadata\"", "Rongrong Zhong", "rongrong", "04/27/21, 11:07:47 PM", "This reverts commit 99dcd7660baadbc24ab1b0803d8eead7b7d34f27.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16017", "Adding session config: limit_larger_for_segment for Pinot connector", "Xiang Fu", "xiangfu0", "05/03/21, 04:22:03 PM", "Adding session config: limit_larger_for_segment for Pinot connector\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\nUnit test and local deployment\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16018", "Add max error retry config option to glue client", "Jinlin Zhang", "v-jizhang", "05/13/21, 01:00:39 PM", "Cherry pick of https://github.com/trinodb/trino/pull/4611 and\r\nhttps://github.com/vincentpoon/prestosql/commit/c1ac9ac257bc5a07f32a359c7aac6735fd6ef69f#diff-824c0eed6933e963a65ba98d9bc3582e5f3f96fbd78982996327b10e54bd5c67\r\n\r\nCo-authored-by: Vincent Poon <vincent.poon@salesforce.com>\r\nCo-authored-by: Philippe Gagnon <pgagnon@users.noreply.github.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\nHive Connector Changes\r\n* Add max error retry config option to glue client. Defaults to 10.\r\n  :doc:`/connector/hive`\r\n```\r", "NaN"], ["16019", "Revert \"Do not create local projection for constant in PlanRemoteProj\u2026", null, "vaishnavibatni", "04/28/21, 09:06:10 PM", "\u2026ections\"\r\n\r\n\r\n\r", "NaN"], ["16020", "Add Split Detail into maxActiveSplits endpoint", "Curt", "BlueStalker", "05/07/21, 09:54:03 AM", "These changes add detailed split info into maxActiveSplits endpoint in worker nodes,\r\nsuch information will help to debug the system, for example, HDFS data nodes slowness\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16021", "Revert \"Do not create local projection for constant in PlanRemoteProj\u2026", null, "vaishnavibatni", "04/29/21, 04:03:12 PM", "\u2026ections\"\r\n\r\nThis reverts commit 421d7f65ce79ac803e41b719653a650b0d994d4d.\r\n\r", "NaN"], ["16023", "Optimize spark split enumeration", "Andrii Rosa", "arhimondr", "04/29/21, 09:27:44 PM", "When enumerating splits for large tables holding all the splits inmemory\r\ncan potentially be very expensive. In our deployment a single HiveSplit\r\ntakes 1.5kb of memory. However multiple splits are extremely\r\ncompressible. This patch introduces iterative split enumeration. Splits\r\nare loaded and serialized+compressed in batches.\r\n\r\nTest plan:\r\n\r\n- Implement and run extensive unit tests to make sure split enumeration logic is not broken when enumerating in batches\r\n- Add an in memory integration test with a batch size set to `1`, `2` and `4` splits to make sure the code path is exercised\r\n- Run problematic production query that used to trigger driver OOM to make sure the patch resolves the problem\r\n- Run standard verifier suite with standard settings as well as with altered batch size to `1`, `2` and `5` splits\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16026", "Fix cumulativeUserMemory initial calculation & use double for cumulativeUserMemory consistently", null, "frankobe", "04/30/21, 05:51:07 PM", "Currently there is a chance that `Cumulative User Memory` is much larger than `Peak User Memory` * `Execution Time`, refer to the following screenshot of one sample query:\r\n\r\n![image](https://user-images.githubusercontent.com/2467199/116615728-8f9d4c00-a8f0-11eb-849d-5475b28bd96a.png)\r\n\r\nExtra debug logging in `TaskContext.java` shows, \r\n\r\n```\r\n2021-04-29T18:37:01.352Z    INFO    Task-20210429_183659_00001_stagingva.9.0.16-288 com.facebook.presto.operator.TaskContext    Cumulate user memory: sinceLastPeriodMillis = (34262137327073190 - 0) / \r\n1_000_000 = 34262137327.073193 ,averageMemoryForLastPeriod= (3184992 + 0) / 2 = 1592496\r\n```\r\n\r\nThe bug is that `sinceLastPeriodMillis` can equal to current system nano time if the `lastTaskStatCallNanos` is not initialized and defaults to 0.\r\n\r\nThe fix is basically initialize the `lastTaskStatCallNanos` to task start time.\r\n\r\n\r\nSigned-off-by: frankobe <mua08p@gmail.com>\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\nUnit Test\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix amplified `Cumulative User Memory` metrics calculation\r", "NaN"], ["16027", "Make HashTable computation Lazy in MapBlockBuilder", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "05/07/21, 06:14:33 PM", "https://github.com/prestodb/presto/pull/12198 made hash computation\r\nlazy in the MapBlock, but MapBlockBuilder computed the hash tables\r\nalways.\r\n\r\nAlways computing the hash table had 2 issues\r\n1. MapBlockBuilder always computed the hash and it is not used in some\r\ncases (OrcWriter). Hash computation took around 5-10% of the CPU.\r\n2. Null keys in map, failed the hash computation part. \r\n\r\nNote: Presto query engine does not support null keys in map. But we \r\nare using Presto orc writer as a stand alone library and this requires null\r\nkeys support in map.\r\n\r\nMaking the Hash table lazy solves both the issues.\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\nAdded more unit tests.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16028", "Add intelligent tiering storage class", "Jinlin Zhang", "v-jizhang", "05/12/21, 08:00:57 PM", "Cherry pick of https://github.com/trinodb/trino/pull/3032\r\n\r\nCo-authored-by Arpit Solanki <solankiarpit1997@gmail.com>\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines. Don't forget to follow our [attribution guidelines](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#attribution) for any code copied from other projects.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Connector Changes\r\n* Added intelligent tiering storage class.\r\n  The S3 storage class to use when writing the data. STANDARD and INTELLIGENT_TIERING storage classes are supported. Default storage class is STANDARD\r\n:doc:`/connector/hive`\r\n```", "NaN"], ["16030", "Include queuing time while computing query completion deadline", "Arjun Gupta", "pgupta2", "05/13/21, 01:52:24 AM", "Currently, Presto on Spark has no way to track queuing time\r\nand this would cause queries to timeout while waiting for resources.\r\nWe should add support for tracking service queuing time and use it\r\nfor query deadline time calculations.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16036", "Improve memory usage in TableFinishOperator", "Vic Zhang", "viczhang861", "05/10/21, 11:38:43 PM", "Fixes https://github.com/prestodb/presto/issues/16022\r\n\r\n- Commit `Track column statistics only in recoverable mode` will improve Presto on Spark driver memory as POS uses `TASK_COMMIT` strategy.\r\n\r\nTest plan\r\n\r\n- Built a custom package and deployed to a real cluster with shadowed queries.\r\n- Enable large batch mode, set `enable_stats_collection_for_temporary_table=true`, `hash_partition_count=16384` to increase memory used for stats collection.  Running shadow query for 24 hr, no full GC found in coordinator.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Track system memory used by column statistics in TableFinishOperator.\r\n\r", "NaN"], ["16037", "Support Dwrf Sequence Ids in Writer", "AKHIL UMESH MEHENDALE", "akhilum", "05/12/21, 02:26:24 AM", "Sequence Ids are used by Dwrf FlatMap implementations. This commit introduces sequence for DWRF writer.\r\nFollowing PR of flat map implementation will use this sequence id to add multiple streams per column. ", "NaN"], ["16038", "Use double quotes for all column names in SHOW CREATE TABLE", null, "abhiseksaikia", "05/17/21, 01:25:38 PM", "Currently SHOW CREATE TABLE does not use double quotes for column names which are reserved words. Because of this, copying the output from show create table to create the table fails. We are now using double quotes in SHOW CREATE TABLE for all column names if not already double quoted\r\n\r\nTest plan - \r\n1. Tested using unit test\r\n2. Tested by running com.facebook.presto.hive.HiveQueryRunner in local.\r\n```\r\npresto:tpch> CREATE TABLE test (\r\n          ->            a int,\r\n          ->           \"order\" int,\r\n          ->           limit int,\r\n          ->             ds varchar\r\n          ->            )\r\n          ->            WITH (\r\n          ->               format = 'DWRF',\r\n          ->               partitioned_by = ARRAY['ds']\r\n          ->            );\r\nCREATE TABLE\r\npresto:tpch> show create table test;\r\n          Create Table\r\n---------------------------------\r\n CREATE TABLE hive.tpch.test (\r\n    \"a\" integer,\r\n    \"order\" integer,\r\n    \"limit\" integer,\r\n    \"ds\" varchar\r\n )\r\n WITH (\r\n    format = 'DWRF',\r\n    partitioned_by = ARRAY['ds']\r\n )\r\n(1 row)\r\n```\r\n\r\n\r\n== NO RELEASE NOTE ==\r\n\r", "NaN"], ["16039", "Avoid unnecessary variable for ConstantExpression in PlanRemoteProjections", "Rongrong Zhong", "rongrong", "05/05/21, 05:03:33 AM", "Ran explain verifier test on build 0.253-20210504.000517-49\r\n\r\nReintroducing https://github.com/prestodb/presto/pull/15944/commits/e306f218f737b423f76670b6a4d81e700ced3a9f.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16041", "Move audience-annotations depencency to provided scope", "Timothy Meehan", "tdcmeehan", "05/04/21, 11:03:42 PM", "Test plan - run the build\r\n\r\nAdditional dependencies specified in the javadoc plugin don't seem to hit the local Maven cache.  This makes it more susceptible to issues with Maven central being flaky or down.  By marking this as a provided dependency, it will be available for the Javadoc plugin and not included in the final build artifact, but it will also leverage Maven caching.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16043", "Update airlift to version 102", "Dongliang Chen", "dongliangchenfb", "05/04/21, 09:12:15 PM", "Airlift 102 includes the includeOnlyProperty git.commit.id.abbrev that fixes the presto CLI version\r\n\r\nTest plan \r\nTested with presto cli executable jar file created, prior to this change:\r\nPresto CLI 0.253-SNAPSHOT-${git.commit.id.abbrev}\r\nAfter:\r\nPresto CLI 0.253-SNAPSHOT-ead3f71\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n-----------------\r\n* Fix the version name displayed in the UI\r\n```\r", "NaN"], ["16044", "Use alias in case of predicate stitching", "Rohit Jain", "jainxrohit", "05/17/21, 07:24:11 AM", "For materialized view processing, always use alias for the predicate stitched sub query. Without this the scope of the table is not exposed outside the predicate stitched query.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16046", "Fix NPE in HiveSpiltManager when there are upper case letters in partition columns", "Beinan", "beinan", "05/10/21, 11:31:01 PM", "We're seeing HIVE_UNKNOWN_ERROR which is caused by a NPE in HiveSplitManager as below.\r\n\r\n`com.facebook.presto.spi.PrestoException: HIVE_UNKNOWN_ERROR\r\n\tat com.facebook.presto.hive.BackgroundHiveSplitLoader$HiveSplitLoaderTask.process(BackgroundHiveSplitLoader.java:128)\r\n\tat com.facebook.presto.hive.util.ResumableTasks.safeProcessTask(ResumableTasks.java:47)\r\n\tat com.facebook.presto.hive.util.ResumableTasks.access$000(ResumableTasks.java:20)\r\n\tat com.facebook.presto.hive.util.ResumableTasks$1.run(ResumableTasks.java:35)\r\n\tat com.facebook.airlift.concurrent.BoundedExecutor.drainQueue(BoundedExecutor.java:78)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.NullPointerException\r\n\tat com.facebook.presto.hive.HiveSplitManager.lambda$getPartitionMetadata$5(HiveSplitManager.java:508)\r\n\tat com.google.common.collect.Iterators$6.transform(Iterators.java:785)\r\n\tat com.google.common.collect.TransformedIterator.next(TransformedIterator.java:47)\r\n\tat com.google.common.collect.TransformedIterator.next(TransformedIterator.java:47)\r\n\tat com.google.common.collect.Iterators$ConcatenatedIterator.hasNext(Iterators.java:1332)\r\n\tat com.facebook.presto.hive.ConcurrentLazyQueue.poll(ConcurrentLazyQueue.java:37)\r\n\tat com.facebook.presto.hive.BackgroundHiveSplitLoader.loadSplits(BackgroundHiveSplitLoader.java:188)\r\n\tat com.facebook.presto.hive.BackgroundHiveSplitLoader.access$300(BackgroundHiveSplitLoader.java:40)\r\n\tat com.facebook.presto.hive.BackgroundHiveSplitLoader$HiveSplitLoaderTask.process(BackgroundHiveSplitLoader.java:121)`\r\n\r\nIt's caused by the 'partitionName' is all lower case (makePartName make it to lower case), but the keys in partitionSplitInfo are still the original case.  So I use hivePartition.getPartitionId which is also original case instead of partitionName.\r\n\r\nCanary this change in twitter starting from tonight.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16050", "Use HIVE_COMPATIBLE function for ORC format", "Vic Zhang", "viczhang861", "05/10/21, 05:00:21 PM", "Fixes https://github.com/prestodb/presto/issues/16042\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16057", "Use getInt to avoid boxing in LookupJoinPageBuilder", "James Petty", "pettyjamesm", "05/07/21, 06:38:34 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16058", "Add documentation for Glue Catalog support in Hive", "Jinlin Zhang", "v-jizhang", "05/12/21, 08:01:13 PM", "Cherry pick of https://github.com/trinodb/trino/pull/3689 and\r\nhttps://github.com/vincentpoon/prestosql/commit/c1ac9ac257bc5a07f32a359c7aac6735fd6ef69f\r\n\r\nCo-authored-by: Philippe Gagnon <pgagnon@users.noreply.github.com>\r\nCo-authored-by: Ashhar Hasan <hashhar_dev@outlook.com>\r\n\r\n```\r\n== RELEASE NOTES ==\r\nGeneral Changes\r\n* Add documentation for Glue Catalog support in Hive.\r\n  :doc:`/connector/hive`\r\n```\r", "NaN"], ["16061", "Revert \"Fix dynamic pruning for null keys in hive partition\"", "Ke", "kewang1024", "05/10/21, 04:44:16 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16063", "[doc] fix typo", null, "linlinnn", "05/12/21, 12:34:21 AM", "```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["16064", "Short-circuit TupleDomain columnWiseUnion and intersect", "Zhenxiao Luo", "zhenxiao", "05/11/21, 02:22:18 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16065", "Bump Alluxio client version", "Bin Fan", "apc999", "05/09/21, 04:56:44 AM", "Upgrade to Alluxio v2.5.0-3 which fixes a regression in cleaning up unused cache dirs.\r\nAlso update a class name used in Presto which is renamed in v2.5.0-3\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16069", "Combine spill strategies", "Rebecca Schlussel", "rschlussel", "05/13/21, 07:40:20 PM", "Test plan - unit tests\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Remove spilling strategy ``PER_QUERY_MEMORY_LIMIT`` and instead add configuration property ``experimental.query-limit-spill-enabled`` and session property ``query_limit_spill_enabled``.  When this property is set to ``true``, and the spill strategy is not ``PER_TASK_MEMORY_THRESHOLD``, then we will spill whenever a query uses more than the per-node total memory limit in combined revocable and non-revocable memory, in addition to whenever the memory pool exceeds the spill threshold.  This fixes an issue where using the ``PER_QUERY_MEMORY_LIMIT`` spilling strategy could prevent the oom killer from running when the memory pool was full.  The issue is still present for the ``PER_TASK_MEMORY_THRESHOLD`` spilling strategy.\r\n\r\n```\r", "NaN"], ["16071", "Support Unnest in fragment result caching", "Shixuan Fan", "shixuan-fan", "05/13/21, 04:21:43 PM", "Test plan - Added unit test. Also deployed snapshot version to test certain queries\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add support to fragment result caching for unnest\r\n```\r", "NaN"], ["16072", "Add release notes for 0.253", null, "bhhari", "05/14/21, 09:48:24 PM", "# Missing Release Notes\n## Abhisek Gautam Saikia\n- [x] https://github.com/prestodb/presto/pull/15992 Support shutting down coordinator using /info/state endpoint (Merged by: Timothy Meehan)\n\n## Eran Amar\n- [x] https://github.com/prestodb/presto/pull/15812 Eran/binomial functions (Merged by: Rongrong Zhong)\n\n## Reetika Agrawal\n- [x] https://github.com/prestodb/presto/pull/15886 Adding schema access rules to file based system access control (Merged by: Rebecca Schlussel)\n\n## Zhongting Hu\n- [x] https://github.com/prestodb/presto/pull/15736 Hive Meta Store impersonation access (Merged by: Andrii Rosa)\n\n## vaishnavibatni\n- [x] https://github.com/prestodb/presto/pull/16021 Revert \"Do not create local projection for constant in PlanRemoteProj\u2026 (Merged by: Maria Basmanova)\n\n# Extracted Release Notes\n- #15932 (Author: Vic Zhang): Disable empty bucket file for temporary table\n  - Set default value for configuration property `hive.create-empty-bucket-files-for-temporary-table` to `false`.\n- #16000 (Author: Naveen Kumar Mahadevuni): Fix S3 directory detection based on ContentType header\n  - Fix S3 table creation error when the directory location specified was created from AWS console.\n- #16026 (Author: frankobe): Fix cumulativeUserMemory initial calculation & use double for cumulativeUserMemory consistently\n  - Fix amplified `Cumulative User Memory` metrics calculation.\n- #16036 (Author: Vic Zhang): Improve memory usage in TableFinishOperator\n  - Track system memory used by column statistics in TableFinishOperator.\n- #16036 (Author: Vic Zhang): Improve memory usage in TableFinishOperator\n  - Track system memory used by column statistics in TableFinishOperator.\n- #16036 (Author: Vic Zhang): Improve memory usage in TableFinishOperator\n  - Track system memory used by column statistics in TableFinishOperator.\n- #16043 (Author: Dongliang Chen): Update airlift to version 102\n  - Fix the version name displayed in the UI.\n\n# All Commits\n- ff6484ada42e1bc4a25de89a4de209833d7fc8e5 Track system memory in TableFinishOperator (Vic Zhang)\n- 6c161c1baa59c9ba3705872b8f2f64021e2a440b Allow statisitcs page reset in LifespanAndStageState (Vic Zhang)\n- eaafec018443836081df6082374ef473aec7c94d Track column statistics only in recoverable mode (Vic Zhang)\n- dbe485dbaeace0bf1ecd3f2a498ff79522016024 Fix NPE in HiveSpiltManager when there are upper case letter in the partition columns (Beinan Wang)\n- cd6ac2d24371d118edeaee927fe6c45d512b1228 Use HIVE_COMPATIBLE function for ORC format (Vic Zhang)\n- 8c3542e0ab7aca9728ee0dfae645d9d3999fe74f Use PRESTO_NATIVE for hive unsupported type (Vic Zhang)\n- 72a7afc4eba8c3302cb156e1900a8cdfb4e0f72d Revert \"Fix dynamic pruning for null keys in hive partition\" (Ke Wang)\n- f98c25e277f2010e95d8bca4b23c1cd391eb2a8e Bump Alluxio client version (Bin Fan)\n- 5f4a45e46173f76201c58b4ee1469a4fce08b773 Use getInt to avoid boxing in LookupJoinPageBuilder (James Petty)\n- a87fe3d21587d8a10c9430ddfa9f6ff9b724c8e6 Fix Size Calculation in MapBlock (Arunachalam Thirupathi)\n- 0925a963981048896ace46a49082d475230c642d Make HashTable computation Lazy in MapBlockBuilder (Arunachalam Thirupathi)\n- 55f99279442b165e3a4086a219ace7c94c74237e Update the Sample output (Zhongting Hu)\n- 07999c4215bc3d78b4680a8acaccb04302195939 Add Split Detail into maxActiveSplits endpoint (Zhongting Hu)\n- 7ee814c65ba17abf2c071bd8c12d068cd6628bf4 Move TableDescriptionSupplier to separate guice module (Yang Yang)\n- bfcd99ee9fd7173dc99628aa6d5fd5dac5bd8054 Add binomial distribution functions (Eran Amar)\n- 9543424f8aa44c6bfa9d76ee6432b22fcd15ac73 Disable empty bucket file for temporary table (Vic Zhang)\n- 6058064301b7d8b4dab22329343ad0c308c50cab Fix loading config properties for temp storage (Vic Zhang)\n- 3a28daf6c473e25f5fdf8bfdb735f5befea7abfd Avoid unnecessary variable for ConstantExpression in PlanRemoteProjections (Rongrong Zhong)\n- c2c7b783d0e95f934aff8a1a2a03f939c4b21aa8 Move audience-annotations depencency to provided scope (Tim Meehan)\n- 7a17553656594e632c27240d7fe128337fa73549 Update airlift to version 102 (Dongliang Chen)\n- eda728ee36df8734d28a912969fcbfc47ead96e4 Supporting adding additional modules to `DistributedQueryRunner` (Mayank Garg)\n- 30cd883136f01b004101d50a4169aabab8ed47b1 Add more options to ColumnWriterOptions (Arunachalam Thirupathi)\n- 969c84712d2ef6be00305b9cbcfb68dc7cc65ddc Rename CompressionParameters to ColumnWriterOptions (Arunachalam Thirupathi)\n- 1e086105fdb0808316e72d15928d83eabe81be34 Adding session config: limit_larger_for_segment for Pinot connector (Xiang Fu)\n- 35cfbd05304b9ca41ec6f5d1642a4b4a1f1889d6 Materialized view partition stitching for Hive (Rohit Jain)\n- 7e32c30f73d40180f4d3580ff0dc0206f879091c Enable query and node heartbeats to resource manager (Tim Meehan)\n- ea52ee04c4b55f538ab87d9129c8624ce796e1e0 Fix cumulativeUserMemory initial calculation & use double for cumulativeUserMemory consistently (frankobe)\n- b29aaa3f57cb413d34655e766bf31829f0375c47 Support Hive Metastore access with impersonation (Zhongting Hu)\n- d3c3445dca6b5a5f853260b374a32e1b9a64138b Enumerate and serialize splits iteratively (Andrii Rosa)\n- 1c2f7e057ce7c10e4c7e0318ae023a357c4cdc3a Use zstd streams with no finalizer (Andrii Rosa)\n- 6b5434f91158f48271822258c0b10c5197ccecd6 Revert \"Do not create local projection for constant in PlanRemoteProjections\" (vaishnavibatni)\n- b80c82ed3f78a69596b0a522129a9edf3b4b1fcf Fix S3 directory detection based on ContentType header (Naveen Kumar Mahadevuni)\n- ea8e5f4b83f128f20afeb1d79599af4f8caac286 Adding schema access rules to file based system access control (Reetika Agrawal)\n- 4710def6239d1fa7e8df247b06f03dd5c14e46c0 Add ResourceManagerClusterStateProvider (Tim Meehan)\n- 6b367064979b40a4d8ab5d163abcfe1fd1b6e613 Support shutting down coordinator using /info/state endpoint (Abhisek Gautam Saikia)\n- fc25cc49041b92465bdd8b1a12cc8724985656b2 Fix formatted query with PREPARE statement (Shixuan Fan)", "NaN"], ["16073", "Add support for custom query prerequisite logic", null, "mayankgarg1990", "05/25/21, 12:28:36 AM", "This PR adds a mechanism using which admins can check for certain prerequisites to be satisfied before a query is queued in the Presto queues. An example of this situation is in Facebook - where given our multi-region warehouse setup - we want to ensure that data is present in the region where the query is being executed before the query starts.\r\n\r\nThe high level design is:\r\n\r\n1. Add to SPI an interface `QueryPrerequisites` which allows for custom logic to be implemented and injected using the `Plugin` interface. By doing so, admins can choose to add custom logic before query execution and can ensure if some prerequisites need to be met - that can be achieved.\r\n2. Add a new state `WAITING_FOR_PREREQUISITES` as the first stage when a query starts and as the `QueryPrerequisites` is complete, it will move to `QUEUED`. This is done to ensure a clean line between when a query is ready to run and queued vs the admin needs to do some preparation before the query can be executed.\r\n\r\nTo install a custom `QueryPrerequisites` implementation, a factory instance `QueryPrerequisitesFactory` needs to be implemented which is exposed as a `Plugin` to the coordinator. Then the coordinator will load the correct factory by reading the configuration file `etc/query-prerequisites.properties` and the property `query-prerequisites.factory`. If nothing is specified, a default implementation will be loaded which finished immediately - hence it is not necessary to install a custom `QueryPrerequisites` instance.\r\n\r\nTest plan - Added tests to `TestLocalQueryDispatcher` which contains the logic for invoking and dealing with the prerequisites stage. In addition to this, on a local cluster in Facebook, developed our custom `QueryPrerequisites` logic and built and injected using the `Plugin` mechanism and ensure that the system worked\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nSPI Changes\r\n* Add support for custom query prerequisites to be checked and satisfied through ``QueryPrerequisites`` interface. See :pr:`16073`.\r\n```", "NaN"], ["16074", "Release note for 0.251.1", "Ke", "kewang1024", "05/12/21, 09:53:36 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16075", "Add queryId to MetastoreContext", "Nikhil Collooru", "NikhilCollooru", "05/17/21, 04:45:56 PM", "Add queryId, clientInfo fields to MetastoreContext. \r\n\r\ndepended by https://github.com/facebookexternal/presto-facebook/pull/1539\r\n\r\nIf release note is NOT required, use:\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16077", "Sorted range set improvements", "Zhenxiao Luo", "zhenxiao", "05/18/21, 02:13:52 AM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16082", "Revert \"Bring retry queries to the beginning of the queue\"", null, "bhhari", "05/12/21, 08:29:16 PM", "This reverts commit a34cf433047ce9a5206cc4b510ff35ca11e45ca9, which is causing a memory leak\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16083", "Catch UncheckedIOException while reading broadcast table from storage", "Arjun Gupta", "pgupta2", "05/14/21, 06:33:12 PM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16084", "Fix iterator in IndexedPriorityQueue", "Timothy Meehan", "tdcmeehan", "05/13/21, 02:22:23 AM", "Calling remove() on the iterator returned from this class results in a memory leak.\r\n\r\nTest plan - unit tests\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16085", "Revert \"Bring retry queries to the beginning of the queue\"", null, "bhhari", "05/13/21, 01:55:44 AM", "This reverts commit a34cf433047ce9a5206cc4b510ff35ca11e45ca9.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16086", "Revert \"Bring retry queries to the beginning of the queue\"", null, "bhhari", "05/13/21, 02:16:29 AM", "This reverts commit a34cf433047ce9a5206cc4b510ff35ca11e45ca9.\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16087", "Define custom json serializer for DataSize and Duration", null, "abhiseksaikia", "05/13/21, 02:25:46 AM", "Currently thrift encoding leverages ThriftCodec implementations that convert DataSize as well as Duration objects to double for optimization. So even though the resource manager thrift client sends both DataSize as well as Duration objects with well formatted unit, the unit information is getting lost on Resource manager server side. This commit is to address this issue by defining custom json serializer so that UI can display well formatted json value for DataSize and Duration.\r\n\r\nTest plan -  Verified by running TpchQueryRunner", "NaN"], ["16089", "Refactor OrcWriterOptions to builder pattern", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "05/27/21, 02:02:53 PM", "OrcWriterOptions has 10 with* options to customize the OrcWriterOptions.\r\nEach one of them changes one field, but copies the rest. So adding a new\r\noption will require updating the existing 10 with* methods. There are few\r\nadditional options that are going to be introduced like SortDictionary Keys, \r\nStripeCache and FlatMapWriter.\r\n\r\nRefactored the OrcWriterOptions to use builder pattern. With the new \r\npattern any new options will require only OrcWriterOptions constructor,\r\nbuilder, copy and build methods to be updated.\r\n\r\nTest plan\r\nExisting tests and added new tests for the new code path.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16095", "Add properties to update memory in TableFinishOperator", "Vic Zhang", "viczhang861", "05/14/21, 08:32:25 PM", "When system memory used by TableFinishOperator is tracked\r\nand included in query total memory, query may hit memory\r\nlimit and fail.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Memory tracking in TableFinishOperator can be enabled by setting the `table-finish-operator-memory-tracking-enabled` configuration property to `true`.\r\n\r", "NaN"], ["16096", "Add properties to update memory in TableFinishOperator", "Vic Zhang", "viczhang861", "05/14/21, 08:32:39 PM", "This feature is not in older version, no release note needed.\r\nCherry pick from https://github.com/prestodb/presto/pull/16095\r\n\r\n\r\nFacebook integration was done manually using branches:\r\npresto/release-0.253\r\npresto-facebook/release-0.253\r\n\r\nResult:  build success\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16098", "Revert \"Revert \"Bring retry queries to the beginning of the queue\"\"", null, "mayankgarg1990", "05/14/21, 09:47:36 PM", "This reverts commit 150fb77b9300857e696077858d107cf37adf2f34. `IndexedPriorityQueue`'s\r\niterator has been fixed to not have this leak and hence we can reinstate this commit\r\n\r\nTest plan - This PR is a revert of a revert. The actual issue was fixed in https://github.com/prestodb/presto/pull/16084\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16099", "Allow classes that inherit from AbstractTestQueryFramework to supply expectedQueryRunner.", null, "spershin", "05/14/21, 10:34:58 PM", "Allow classes that inherit from AbstractTestQueryFramework to supply expectedQueryRunner.\r\n\r\nTest plan - Existing tests. Nothing really changes.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16100", "Initial commit for adding MBean counter to TaskUpdateRequest", null, "basarhamdionat", "06/04/21, 05:30:40 PM", "Due to large sized TaskUpdateRequest created, coordinator has to invoke a full GC. \r\nTo identify and understand this, we are introducing MBean that tracks the memory size of the serialized TaskUpdateRequests.\r\n\r\nTest plan - (Please fill in how you tested your changes)\r\n\r\n1. Having a unit test for the code that is written \r\n2. Running a query and having jconsole that shows that jmx beans are created\r\n![Screen Shot 2021-05-24 at 6 14 51 PM](https://user-images.githubusercontent.com/76267197/119426238-afd8e480-bcbd-11eb-8533-d481cd71d5f8.png)\r\n![Screen Shot 2021-05-24 at 6 15 00 PM](https://user-images.githubusercontent.com/76267197/119426243-b0717b00-bcbd-11eb-9e59-b2451b179448.png)\r\n\r\n\r\nPlease make sure your submission complies with our [Development](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#development), [Formatting](https://github.com/prestodb/presto/wiki/Presto-Development-Guidelines#formatting), and [Commit Message](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#commit-formatting-and-pull-requests) guidelines. Don't forget to follow our [attribution guidelines](https://github.com/prestodb/presto/wiki/Review-and-Commit-guidelines#attribution) for any code copied from other projects.\r\n\r\nFill in the release notes towards the bottom of the PR description.\r\nSee [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) for details.\r\n\r\n\r\n== NO RELEASE NOTE ==\r\n\r", "NaN"], ["16101", "Add release notes for 0.253", null, "bhhari", "05/14/21, 10:53:52 PM", "cherry-pick e2e475185e9a05d828a57b07da9f09eb39fe7f31\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16102", "Plumb DWRF stripe cache info from proto to model", "Sergii Druzkin", "sdruzkin", "05/17/21, 08:26:50 PM", "Propagate DWRF stripe cache size, mode, and offsets from the DWRF proto objects to the metadata model.\r\n\r\nThis change is a part of effort to enable DWRF stripe cache in the reader. The changes are mostly the same as in the original large PR https://github.com/prestodb/presto/pull/15893/commits/8b42d1dc118338db8f668eed4f6ddccd3991bb2b, all notes from the original PR are addressed.\r\n\r\nTest plan:\r\n- added new unit tests for DwrfMetadataReader\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16109", "Use searchInterruptible for Joni regexp matching", "Sreeni Viswanadha", "kaikalur", "05/24/21, 08:12:13 PM", "Joni regexp search can be very slow. This can cause runaway splits issue. So we use the searchInterruptible method so it can be interrupted in such cases.\r\n\r\nTest plan - None\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n*  Fix an issue where regular expression functions were not interruptible and could  keep running for a long time after the query they were a part of failed. \r\n```\r", "NaN"], ["16111", "Interrupt runaway splits", null, "aweisberg", "05/26/21, 09:02:14 PM", "Test plan - Included unit test, deploy to a cluster and test with a known bad query\r\n\r\nWe do use thread interruption as part of the query cancellation mechanism so it is probably safer then I thought it was in some earlier discussions.\r\n\r\nWhat this is really implementing is eager split termination where we don't honor the user specified query timeout. Because of that I am limiting this to only killing a split that looks like it is blocked in Joni. We can add more if we find other common culprits.\r\n\r\n@arhimondr brought up the possibility of edge cases like JDBC where we might have an expensive query that might not yield a page for 10 minutes as it tries to generate a small number of rows from a system that isn't very fast. Even with Hive + HDFS if it is needle in a haystack type stuff this could occur. I think a targeted approach of interrupting allow listed things is a good place to start.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add interruption for runaway splits blocked in known situations controlled by ``task.interrupt-runaway-splits-timeout`` property which defaults to ``600s``.\r\n```\r", "NaN"], ["16112", "Use username for comparison only when impersonation is enabled", "Nikhil Collooru", "NikhilCollooru", "05/19/21, 11:30:19 PM", "When impersonation is NOT enabled, we still want the MetastoreContext to have correct username and not a dummy value. So this PR moves the impersonation logic into `KeyAndContext` . The `equals()` and `hashCode()` methods of KeyAndContext will use username ONLY when impersonation is enabled. \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16113", "Add support for partition cache verification", "Nikhil Collooru", "NikhilCollooru", "05/19/21, 11:29:54 PM", "\r\n```\r\n== RELEASE NOTES ==\r\n\r\nHive Changes\r\n* Add support for partition cache validation. This can be enabled by setting `hive.partition-cache-validation-percentage` configuration parameter.\r\n```\r", "NaN"], ["16114", "Allow StorageOrcFileTailSource to read DWRF stripe cache data", "Sergii Druzkin", "sdruzkin", "05/20/21, 01:06:48 PM", "This change allows StorageOrcFileTailSource to load the DWRF stripe cache data from the source/file into OrcFileTail. This feature is disabled by default. \r\n\r\nTo be able to reduce number of IOs while reading the file tail I made the value of the tail read size (the size of the first read operation) configurable.\r\n\r\nThis is a continuation of https://github.com/prestodb/presto/pull/16102\r\n\r\nTest plan:\r\n- added new unit tests for StorageOrcFileTailSource\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16118", "Add predicate stitching for more nodes", "Rohit Jain", "jainxrohit", "05/25/21, 04:29:35 PM", "Adding predicate stitching for With, Union, GroupingSet node types.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16120", "Reduce memory utilization on the Driver during the commit phase", "Andrii Rosa", "arhimondr", "05/20/21, 03:53:06 PM", "In Presto each writer produces a PartitionUpdate object that contains file names and other meta information for the files being written. This information is then collected on the driver to perform a final commit (do file renames). In some cases this meta information could be quite large. This patch tries to optimize several things:\r\n\r\n- Reduce `PartitionUpdate` memory footprint on the Driver by serializing to SMILE instead of JSON and applying ZSTD compression\r\n- Release serializes and compressed pages as soon as they read by the engine on the driver. This should help avoid double memory utilization\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPresto on Spark Changes\r\n* Reduce commit memory footprint on the Driver\r\n\r\n```", "NaN"], ["16121", "Add session property for temp storage buffer size", "Vic Zhang", "viczhang861", "05/20/21, 04:35:34 PM", "Issue:\r\nBuffered data used by spiller can use more than a few GB memory depending on buffer size and number of spiller created. For small heap size under environment of Presto on Spark, it is important to tune buffer size frequently depending on operator type and container size.\r\n\r\n\r\n\r\nTest:\r\nTake heap dump before and after tuning this session property.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16122", "Fix resource manager tests and add to CI", "Timothy Meehan", "tdcmeehan", "06/02/21, 08:20:26 PM", "Test plan - Test run\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16125", "Fix ZeroRowFileCreator to use DataSink instead of OutputStream interface", "Arjun Gupta", "pgupta2", "05/20/21, 07:13:10 PM", "\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16127", "Disable flaky TestQueues.testQueuedQueryInteraction", "Vic Zhang", "viczhang861", "05/20/21, 04:35:12 PM", "\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16128", "Fix the partition cache invalidation logic to be more efficient", "Nikhil Collooru", "NikhilCollooru", "05/20/21, 09:01:06 PM", "depended by https://github.com/facebookexternal/presto-facebook/pull/1548\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16129", "Implement efficient way to fetch partition name and values", null, "jzzhaofb", "05/22/21, 03:41:16 AM", "NaN", "NaN"], ["16130", "Fix MetastoreContext creation in presto-iceberg module", "Nikhil Collooru", "NikhilCollooru", "05/20/21, 02:06:43 PM", "Fix MetastoreContext object creation in presto-iceberg module. The MetastoreContext constructor was changed recently and looks like tests were not re-run before merging in PR https://github.com/prestodb/presto/pull/15836. The build is broken and this PR fixes it.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16131", "Add Option to disable DWRF string dictionary sorting", "ARUNACHALAM THIRUPATHI", "arunthirupathi", "06/08/21, 06:41:04 AM", "DWRF format does not require the string dictionary to\r\nbe sorted. Sorting DWRF string dictionary keys takes\r\nlarge amount of time. A new option is added to disable\r\nstring dictionary sorting in DWRF dictionary encoding.\r\n\r\nTest plan \r\nEnhanced existing tests and added new tests.\r\nRan the validation service for this change.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16133", "Support spilling for Presto on Spark", "Vic Zhang", "viczhang861", "05/20/21, 10:33:03 PM", "This PR only makes spilling work,  it is not targeted for memory improvement after enabling spilling.\r\n\r\nTest\r\n- Unit test\r\n     Enable spilling and manually verify spilled file is created.\r\n- Integration test\r\n   Run large Presto on Spark query and verify data is spilled in logging.\r\n\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nPresto on Spark\r\n* Add session property `spark_memory_revoking_threshold` and configuration property `spark.memory-revoking-threshold`, spilling is triggered when total memory is beyond this threshold.\r\n\r", "NaN"], ["16136", "Fix PlanRemoteProjections", "Rongrong Zhong", "rongrong", "05/20/21, 11:42:18 PM", "When a function/operator only has constants as input, processedArguments.size() could be 0.\r\nPrevious logic can produce index out of bound exception.\r\n\r\nTest plan - unit test\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix a bug where queries that have both remote functions and a local function with only constant arguments could throw an IndexOutOfBoundException during planning. The bug was introduced in release 0.253 by :pr:`16039`.\r\n```\r", "NaN"], ["16139", "Make NodeTaskMap inner classes static", "James Petty", "pettyjamesm", "05/21/21, 09:21:25 PM", "Minor cleanup of unnecessary outer instance scope from `NodeTaskMap` inner-classes.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16140", "Upgrade airlift dependency to 0.201", null, "abhiseksaikia", "05/26/21, 08:57:26 PM", "As part of this PR, we are updating the version for airlift dependency to 0.201 version.\r\nRefer to https://github.com/prestodb/airlift/pull/41 for bugfix.\r\n\r\nTest plan - Ran build and tests to make sure upgrade does not break anything\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16141", "Add release notes for 0.254", null, "sujay-jain", "06/01/21, 08:59:05 PM", "# Missing Release Notes\n## Abhisek Gautam Saikia\n- [x] https://github.com/prestodb/presto/pull/16087 Define custom json serializer for DataSize and Duration (Merged by: Timothy Meehan)\n\n## Akhil Umesh Mehendale\n- [x] https://github.com/prestodb/presto/pull/16037 Support Dwrf Sequence Ids in Writer (Merged by: Rebecca Schlussel)\n\n## Tal Galili\n- [x] https://github.com/prestodb/presto/pull/15814 Adding the Poisson distribution (Merged by: Maria Basmanova)\n\n## Vic Zhang\n- [x] https://github.com/prestodb/presto/pull/16133 Support spilling for Presto on Spark (Merged by: Andrii Rosa)\n- [x] https://github.com/prestodb/presto/pull/16133 Support spilling for Presto on Spark (Merged by: Andrii Rosa)\n- [x] https://github.com/prestodb/presto/pull/16133 Support spilling for Presto on Spark (Merged by: Andrii Rosa)\n- [x] https://github.com/prestodb/presto/pull/16133 Support spilling for Presto on Spark (Merged by: Andrii Rosa)\n- [x] https://github.com/prestodb/presto/pull/16133 Support spilling for Presto on Spark (Merged by: Andrii Rosa)\n- [x] https://github.com/prestodb/presto/pull/16133 Support spilling for Presto on Spark (Merged by: Andrii Rosa)\n\n## guhanjie\n- [x] https://github.com/prestodb/presto/pull/15848 Add a flag for HttpRemoteTask to avoid eager but unnecessary sendUpdate (Merged by: Andrii Rosa)\n\n# Extracted Release Notes\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15836 (Author: Chunxu Tang): Add Presto iceberg connector\n  - Add iceberg connector.\n- #15937 (Author: v-jizhang): Do not allocate resources within test constructor\n  - Do not allocate resources within test constructor for a cleaner code.\n- #15993 (Author: v-jizhang): Fix Athena Glue table compatibility issue\n  - Allow accessing tables in Glue metastore that do not have a table type.\n- #16003 (Author: v-jizhang): Add missing support for bucket sort order in Glue\n  - Add support for bucket sort order in Glue when creating or updating a table or partition.\n- #16011 (Author: Jalpreet Singh Nanda (:imjalpreet)): Partition schema evolution for Parquet\n  - Add support for partition schema evolution for parquet.\n- #16011 (Author: Jalpreet Singh Nanda (:imjalpreet)): Partition schema evolution for Parquet\n  - Add support for partition schema evolution for parquet.\n- #16012 (Author: v-jizhang): Add support for MaxResults on Glue Hive Metastore\n  - Add support for MaxResults on Glue Hive Metastore.\n- #16014 (Author: v-jizhang): Add support for Glue endpoint URL\n  - Add support for Glue endpoint URL :doc:`/connector/hive`.\n- #16018 (Author: v-jizhang): Add max error retry config option to glue client\n  - Add max error retry config option to glue client. Defaults to 10. :doc:`/connector/hive`.\n- #16028 (Author: v-jizhang): Add intelligent tiering storage class\n  - Added intelligent tiering storage class. The S3 storage class to use when writing the data. STANDARD and INTELLIGENT_TIERING storage classes are supported. Default storage class is STANDARD :doc:`/connector/hive`.\n- #16058 (Author: v-jizhang): Add documentation for Glue Catalog support in Hive\n  - Add documentation for Glue Catalog support in Hive. :doc:`/connector/hive`.\n- #16069 (Author: Rebecca Schlussel): Combine spill strategies\n  - Remove spilling strategy ``PER_QUERY_MEMORY_LIMIT`` and instead add configuration property ``experimental.query-limit-spill-enabled`` and session property ``query_limit_spill_enabled``.  When this property is set to ``true``, and the spill strategy is not ``PER_TASK_MEMORY_THRESHOLD``, then we will spill whenever a query uses more than the per-node total memory limit in combined revocable and non-revocable memory, in addition to whenever the memory pool exceeds the spill threshold.  This fixes an issue where using the ``PER_QUERY_MEMORY_LIMIT`` spilling strategy could prevent the oom killer from running when the memory pool was full.  The issue is still present for the ``PER_TASK_MEMORY_THRESHOLD`` spilling strategy.\n- #16069 (Author: Rebecca Schlussel): Combine spill strategies\n  - Remove spilling strategy ``PER_QUERY_MEMORY_LIMIT`` and instead add configuration property ``experimental.query-limit-spill-enabled`` and session property ``query_limit_spill_enabled``.  When this property is set to ``true``, and the spill strategy is not ``PER_TASK_MEMORY_THRESHOLD``, then we will spill whenever a query uses more than the per-node total memory limit in combined revocable and non-revocable memory, in addition to whenever the memory pool exceeds the spill threshold.  This fixes an issue where using the ``PER_QUERY_MEMORY_LIMIT`` spilling strategy could prevent the oom killer from running when the memory pool was full.  The issue is still present for the ``PER_TASK_MEMORY_THRESHOLD`` spilling strategy.\n- #16069 (Author: Rebecca Schlussel): Combine spill strategies\n  - Remove spilling strategy ``PER_QUERY_MEMORY_LIMIT`` and instead add configuration property ``experimental.query-limit-spill-enabled`` and session property ``query_limit_spill_enabled``.  When this property is set to ``true``, and the spill strategy is not ``PER_TASK_MEMORY_THRESHOLD``, then we will spill whenever a query uses more than the per-node total memory limit in combined revocable and non-revocable memory, in addition to whenever the memory pool exceeds the spill threshold.  This fixes an issue where using the ``PER_QUERY_MEMORY_LIMIT`` spilling strategy could prevent the oom killer from running when the memory pool was full.  The issue is still present for the ``PER_TASK_MEMORY_THRESHOLD`` spilling strategy.\n- #16069 (Author: Rebecca Schlussel): Combine spill strategies\n  - Remove spilling strategy ``PER_QUERY_MEMORY_LIMIT`` and instead add configuration property ``experimental.query-limit-spill-enabled`` and session property ``query_limit_spill_enabled``.  When this property is set to ``true``, and the spill strategy is not ``PER_TASK_MEMORY_THRESHOLD``, then we will spill whenever a query uses more than the per-node total memory limit in combined revocable and non-revocable memory, in addition to whenever the memory pool exceeds the spill threshold.  This fixes an issue where using the ``PER_QUERY_MEMORY_LIMIT`` spilling strategy could prevent the oom killer from running when the memory pool was full.  The issue is still present for the ``PER_TASK_MEMORY_THRESHOLD`` spilling strategy.\n- #16071 (Author: Shixuan Fan): Support Unnest in fragment result caching\n  - Add support to fragment result caching for unnest.\n- #16095 (Author: Vic Zhang): Add properties to update memory in TableFinishOperator\n  - Memory tracking in TableFinishOperator can be enabled by setting the `table-finish-operator-memory-tracking-enabled` configuration property to `true`.\n- #16113 (Author: Nikhil Collooru): Add support for partition cache verification\n  - Add support for partition cache validation. This can be enabled by setting `hive.partition-cache-validation-percentage` configuration parameter.\n- #16120 (Author: Andrii Rosa): Reduce memory utilization on the Driver during the commit phase\n  - Reduce commit memory footprint on the Driver.\n- #16120 (Author: Andrii Rosa): Reduce memory utilization on the Driver during the commit phase\n  - Reduce commit memory footprint on the Driver.\n- #16120 (Author: Andrii Rosa): Reduce memory utilization on the Driver during the commit phase\n  - Reduce commit memory footprint on the Driver.\n- #16136 (Author: Rongrong Zhong): Fix PlanRemoteProjections\n  - Fix a bug where queries that have both remote functions and a local function with only constant arguments could throw an IndexOutOfBoundException during planning. The bug was introduced in release 0.253 by :pr:`16039`.\n\n# All Commits\n- 8aa29612584603a6aab836a54e445790eefc59bc Fix PlanRemoteProjections (Rongrong Zhong)\n- e2944c678f79eef09096f961e80c50c497fabfd9 Disable flaky test AbstractTestQueries.testEmptyJoins (Vic Zhang)\n- 8d7d0a9470b8768db027b8b2d5c9dc4655b1d0cd Add spilling threshold for Presto on Spark (Vic Zhang)\n- 5f11dd2b61316b49cf206c86c28cc97b83731d60 Include revocable memory for peak node total memory (Vic Zhang)\n- 5f692c0bd985d3421b3b49e7a1dd7c043d11c30a Include revocable memory for memory limit error (Vic Zhang)\n- 4d17bb089170d7fe1415244e6da6d9b86dc8e532 Support spilling for Presto on Spark (Vic Zhang)\n- 129aab3bfd6574ff5468e30d68fc8377f81192c4 Rename TestPrestoSparkAbstractTestAggregations (Vic Zhang)\n- 4428cf732e8f6b3743d209d2c2327b848cc4c893 Fix the partition cache invalidation logic to be more efficient (Nikhil Collooru)\n- a6d52705ee6b80fba898b9e4dff1e132b8b56779 Fix ZeroRowFileCreator to use DataSink instead of OutputStream interface (Arjun Gupta)\n- 5eb36ae64a70f22fffa44731a910ec90e083d9c2 Add session property for temp storage buffer size (Vic Zhang)\n- 100c120d403e03827001b0f862fdbe64c9d5c4fa Disable flaky TestQueues.testQueuedQueryInteraction (Vic Zhang)\n- 4bd5dc5d5f25d92cf0bfed74f455353d529aebfa Log sizes for pages received on the Driver (Andrii Rosa)\n- 2692f3004f8919d77f866f7c97af735c9b3319f1 Release inmemory input pages incrementally (Andrii Rosa)\n- a31785bed1fdb582b84bfd7192c55bd4373f21c0 Support compression for PartitionUpdate in Hive connector (Andrii Rosa)\n- 81e45abfb0893a9b6a276003fd8949f1fde0f79d Fix MetastoreContext creation in presto-iceberg module (Nikhil Collooru)\n- baea41bb7dee6384aa5b30d7eb27af4c16635e3d Allow StorageOrcFileTailSource to read DWRF stripe cache data (Sergii Druzkin)\n- 9958e198c4c40d36f594f5a1a93c4dc1449383cf Make ORC read tail size configurable (Sergii Druzkin)\n- 6681e91b9f3db800c25f8fa118897059b412e0ca Fix Athena Glue table compatibility issue (v-jizhang)\n- e87999e56c0d4639b59e26cf144890e9a7768dbd Add missing support for bucket sort order in Glue (v-jizhang)\n- b8b0e522e9a287828e76a45aa4b85c24ca74816e Upgrade to 0.254 (Chunxu Tang)\n- 97953b1ae0dc705b48bc989d526525b6b6ba3052 Adjust the test change of AbstractTestDistributedQueries (Chunxu Tang)\n- 1769297678ebaa73bfd30859a67408d74f501648 Include presto-iceberg in example config (Chunxu Tang)\n- 70fd449dca7f4cad53e53b361b7a9c0e1080d733 Remove unnecessary comments (Chunxu Tang)\n- cea7117c6b7ee4f34978d7f18376be8f42057d1e Add MetastoreContext in the iceberg HiveTableOperations (Chunxu Tang)\n- 656f7afe31174df6e2f5fe5218fdca98dc64198a Adjust the usage of Hive Metastore with MetastoreContext (Chunxu Tang)\n- 68003043b31cbadce2fd8bb85a20be3e0da41be2 Fix review comments (Chunxu Tang)\n- 8180ce3d5bcee4e4be05fd69eb413ceafb76ac08 Upgrade Presto to 0.252 (Chunxu Tang)\n- d749f3f087ae7e42894c9c54f1b5581379f105d7 Refactor HdfsFileIo names (Chunxu Tang)\n- b6ba9134f1edfa798586251f0f95aed64c71ef14 Fix parquet version conflict (Chunxu Tang)\n- ddb67db8ebc734f186198353b9b3c5afa88db999 Include iceberg connector in the root POM (Chunxu Tang)\n- 2d37a11debbbef6ffb66bd738acdc4e490c11342 Set up query tests (Chunxu Tang)\n- f738ed3bed0ce290d3bc61e540b0a2cf89b527a1 Add iceberg connector (Chunxu Tang)\n- 28f558b2fa55014d8bfb7b0dff5731c21f5819ac upgrade parquet to 1.11.0 (Chunxu Tang)\n- a7a98121aee1fce65ede01ba6edac02e838a0936 Use username for comparison only when impersonation is enabled (Nikhil Collooru)\n- fca1f2429069b44c04a41d02818e5b38974b596b Add support for partition cache verification (Nikhil Collooru)\n- 8f4ef27fc72aa68facb6f0f5049a460d2b649333 Add support for MaxResults on Glue Hive Metastore (v-jizhang)\n- 8be15cfd2acf882067bdbb052c09e02769d6e35c Add a flag for HttpRemoteTask to avoid eager but unnecessary sendUpdate (guhanjie)\n- e9c4a306f114b9a68f2cc6498d03a79dc44e23cb Decode Parquet dictionary faster (Zhenxiao Luo)\n- c9571e1e449a76740c0f3fad46f3679b95bf653d Fix generics in declaration of ValueSet.copyOf (Zhenxiao Luo)\n- 6a4f0ab55fc1709f775b642e0b1c42f44a8bd48b Benchmark Parquet dictionary to Domain conversion (Zhenxiao Luo)\n- b302af11b64eea922e7c3a523544931a64dc3dd9 Add a test for BenchmarkSortedRangeSet (Zhenxiao Luo)\n- ca09cd31fedba77611975d5717cbaae80d2e984b Simplify Range consumption (Zhenxiao Luo)\n- d887f690ae79d8de4bb5d1c2e720acbbb6445b8c Add benchmark for SortedRangeSet's getOrderedRanges (Zhenxiao Luo)\n- 3faafe1de59d6fcc3f1e8d4fe50d88ffc2e5962b Benchmark SortedRangeSet methods (Zhenxiao Luo)\n- 494b7f5dcbbcde8bb658a2d04be2d21da6e01f74 Plumb DWRF stripe cache info from proto to model (Sergii Druzkin)\n- 54c3304e0c651d87675501aaaf395eea789b3b48 Add queryId to MetastoreContext (Nikhil Collooru)\n- c63a7c7773e62fc8d96e9a9ec00ffb5edef37def Use double quotes for all column names in SHOW CREATE TABLE (Abhisek Gautam Saikia)\n- 18f7e1786265c82ca8ba0949d8de69ac7e89b663 Use alias in case of predicate stitching (Rohit Jain)\n- 2e180bafba1452a13a61e18ab5c4a9f7dd90bfc0 Add support for partition schema evolution for Parquet (Jalpreet Singh Nanda (:imjalpreet))\n- 9e07362f03517320001bb7923a8050a5b113a885 Introduce TableToPartitionMapping (Jalpreet Singh Nanda (:imjalpreet))\n- 865753c0b6d3508ab0343eb13b0af447e6ab3200 Allow classes that inherit from AbstractTestQueryFramework to supply expectedQueryRunner. (Sergey Pershin)\n- d441316da87c5e814f6b8ba6ed7ab09124946b64 Revert \"Revert \"Bring retry queries to the beginning of the queue\"\" (Mayank Garg)\n- 779afcc460625f5dc77d5520e0924e1591407533 Add properties to update memory in TableFinishOperator (Vic Zhang)\n- 7f9e2426360f87718e634c8c35bd811f4e3096f5 Catch UncheckedIOException while reading broadcast table from storage (Arjun Gupta)\n- aa59a8d51fdae1d412e063afc3a6a0d35a4b0c02 Do not allocate resources within test constructor (v-jizhang)\n- f434ea10a590ef0f79d3226608632fb2b15f5140 Disable flaky test in TestJdbcClient (Rebecca Schlussel)\n- 970e67e3e1486559af3e7757cd878e8aa51436ce Disable flaky test TestPrestoDriver.testQueryCancelByInterrupt (Rebecca Schlussel)\n- 44240cce3d84bdd11506f5e0529f0ab17f7e0b24 Combine memory pool and query based spill strategies (Rebecca Schlussel)\n- eae526629ed41fa62044019c429859073a4e4158 Remove periodic check from MemoryRevokingScheduler (Rebecca Schlussel)\n- dec319521f6a76b9de83afc74387ed4ed0bfa940 Support Unnest in fragment result caching (Shixuan Fan)\n- 39c66e30d87cf10345827194469a7a847751f18d Adding the Poisson distribution (Tal Galili)\n- 77989bf483be56098af47a662a1b549fea6d2874 Add max error retry config option to glue client (v-jizhang)\n- 012e9bd554eb413cd19cd69e188dbe0f7446d7be Add support for Glue endpoint URL (v-jizhang)\n- 486f4e214b16e53e776f2a122af1ba6313d83b3c Define custom json serializer for DataSize and Duration (Abhisek Gautam Saikia)\n- 54bde4ced47b984b3ca32103e9693feae33e72e3 Fix iterator in IndexedPriorityQueue (Tim Meehan)\n- 88a14ba1e4580be885cf990051edd96ff1f1a821 Include queuing time while computing query completion deadline (Arjun Gupta)\n- cdc9dca2ae1e82a9484703f368bc75943de5ca0d Release note for 0.251.1 (Ke Wang)\n- 150fb77b9300857e696077858d107cf37adf2f34 Revert \"Bring retry queries to the beginning of the queue\" (Bhavani Hari)\n- 408c2a19c95ab55b478c8295e6b09cc3c5e4ca72 Add documentation for Glue Catalog support in Hive (v-jizhang)\n- 7e4fe3d909a598d561b7e9b8b37c606a9df7241c Add intelligent tiering storage class (v-jizhang)\n- 3d78f134dadc33827ed2074583cb79dc7649495d Add cluster-wide endpoints for query and cluster information (Tim Meehan)\n- f6611cbc0ae3dee90e6f5476a2c2aa027999b200 Support Dwrf Sequence Ids in Writer (Akhil Umesh Mehendale)\n- 54a7ec79a344a4766e8fcf06e7ff4d10b7f2c380 Fix typo in connectors.rst (linjunhua)\n- 299933063dd20a9c39d4d018ce76f4190dba0b1a Short-circuit TupleDomain columnWiseUnion and intersect (Zhenxiao Luo)", "NaN"], ["16142", "Add StripeMetadataSourceFactory to consume loaded DWRF stripe cache", "Sergii Druzkin", "sdruzkin", "05/27/21, 10:36:33 PM", "This change allows OrcReader to populate DWRF stripe cache using the stripe cache slices from the OrcFileTail and the CacheOffsets from the footer. The created DWRF stripe cache then passed into the StripeMetadataSource via a new interface StripeMetadataSourceFactory. This new interface is added after the discussion with @shixuan-fan and @highker  to avoid adding mutating methods into StripeMetadataSource, and still be able to pass DWRF stripe cache directly from the file/source into the StripeMetadataSource.\r\n\r\nAt this point the DWRF stripe cache can be already used if we add a new implementation of StripeMetadataSource that uses the DWRF stripe cache and falls back to the StorageStripeMetadataSource. I plan to add this new class in a next PR to keep the scope of this PR manageable.\r\n\r\nThis is a continuation of https://github.com/prestodb/presto/pull/16114\r\n\r\nTest plan:\r\n- add new unit test that reads files created by BBIO with various DWRF stripe cache settings\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16146", "Add iceberg connector to devel config", "Beinan", "beinan", "05/25/21, 05:05:07 PM", "Add iceberg to the devel config in presto-main\r\nFix load iceberg module failure when connecting hive-metastore\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16148", "Fix MySQL connection failure on startup", null, "aweisberg", "05/25/21, 06:07:39 AM", "Test plan - Tested that PrestoServer starts up from IDE\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16150", "Add iceberg connector doc", "Chunxu Tang", "ChunxuTang", "05/26/21, 08:13:55 AM", "Add iceberg connector doc\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16152", "Fix the import of SMALLINT in ParquetReader", "Chen Liang", "chliang71", "05/26/21, 05:22:43 AM", "Currently Parquet imports SMALLINT from\r\n`com.facebook.presto.common.type.StandardTypes.SMALLINT`\r\n\r\nThis is a string \"SMALLINT\", the expected type here is actually a SMALLINT type object. Namely:\r\n`com.facebook.presto.common.type.SmallintType.SMALLINT`\r\n\r\ncc. @vkorukanti do you mind taking a look?\r", "NaN"], ["16153", "Improve type errors messages for structs", "Amit Adhikari", "adkri", "05/26/21, 08:02:37 PM", "Struct fields in query should behave like regular columns with a clear\r\nerror message. This fix gives better support for error messages with\r\nstruct fields. Error with nested struct fields will come as\r\ndot-separated in the error message for more human readable message.\r\n\r\nTest plan\r\nRan all tests for presto-main. Wrote additional tests to check for type errors in nested structs\r\n\r\nExample:\r\n```\r\n> CREATE TABLE test(a BIGINT, b ROW(w BIGINT, x ROW(y BIGINT, z DOUBLE)));\r\n> INSERT INTO test VALUES(10, ROW(20, ROW(30, ROW(50))));\r\nQuery 20210525_011822_00022_jdrc5 failed: line 1:42: Mismatch at column 2: 'b.x.z' is of type double but expression is of type row(integer)\r\n```\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r", "NaN"], ["16159", "Implement ARRAY_NORMALIZE function", "Zhan Yuan", "yuanzhanhku", "06/03/21, 07:35:45 AM", "fixes #16134\r\n\r\nNormalizes array ``x`` by dividing each element by the p-norm of the array.\r\nIt is equivalent to\r\n  TRANSFORM(array, v -> v / REDUCE(array, 0, (a, v) -> a + POW(ABS(v), p), a -> POW(a, 1 / p)).\r\nBut the reduce part is only executed once to improve performance.\r\nReturns null if the array is null or there are null array elements.\r\n\r\nTest plan\r\n - Added unit tests in src/test/java/com/facebook/presto/operator/scalar/TestArrayNormalizeFunction.java.\r\n - Also started a presto server locally and tested the change manually.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Added array_normalize function\r\n\r\n```\r", "NaN"], ["16160", "Avoid over-retaining objects via operator info supplier", "James Petty", "pettyjamesm", "06/09/21, 06:53:39 PM", "Port of applicable (and some additional) changes from https://github.com/trinodb/trino/pull/8086 which follows up on a fix made in https://github.com/trinodb/trino/pull/7947 (that didn't occur in presto). At a high level, these changes are intended to avoid the pattern of making operator info suppliers from an operator instance method since that can retain the operator and associated memory for longer than necessary. In particular, before this change tasks that failed with some drivers still running could retain references to those driver's active operators for the remaining lifespan of the failed task's `TaskContext` via embedded references in `OperatorContext#infoSupplier`.\r\n\r\nThe commits in this PR:\r\n- Makes `OperatorContext#infoSupplier` covariant so that suppliers of OperatorInfo subtypes can be passed directly to `OperatorContext#setInfoSupplier`, ie: it changes `OperatorContext#infoSupplier` from `Supplier<OperatorInfo>` (invariant) to `Supplier<? extends OperatorInfo>` (covariant).\r\n- Memoizes the result of and then clears any reference to the original `OperatorContext#infoSupplier` inside of `OperatorContext#destroy()`\r\n- Refactor various operators to create `Supplier<OperatorInfo>` instances using a static method that takes only the required fields instead of using a method reference in the form of `Operator.this#getInfo()` which retains the whole operator\r\n- Adds a minor the allocation rate improvement to the `WindowOperator` info supplier as well as to `WindowInfo#mergeWith`\r\n- Adds a minor allocation rate improvement to scan operator info suppliers by reusing the same instance between calls to `Supplier#get`\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16161", "Remove duplicate UtcEquivalentName check from TimeZoneKey", null, "henneberger", "05/27/21, 04:01:06 AM", "```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16164", "Fix Config Validation", "Darren F", "darrenfu", "06/01/21, 05:06:04 PM", "```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix config validation. Based on JSR 303, validation should be applied on getter methods only. This fix will move all setter-side javax validation annotations to getter methods. This applies to all three Config Bean files.\r\n```\r\n\r\nReference: \r\n* [JSR 303 official link](https://beanvalidation.org/1.0/spec/#constraintdeclarationvalidationprocess-requirements-object)\r\n* [JSR 303 bean validation - Why on getter and not setter discussion link](https://stackoverflow.com/questions/6283726/jsr-303-bean-validation-why-on-getter-and-not-setter)\r\n* [Errors without this fix](https://docs.google.com/document/d/1anSc1D6lV07zpm1RlSRs67_qQbS4xTeZRhlYPLsnHmw/edit?usp=sharing)\r\n\r\nSimilar PR as the one applied to 0.228: https://github.com/prestodb/presto/pull/16162", "NaN"], ["16172", "Revert map optimizations", "Andrii Rosa", "arhimondr", "05/27/21, 03:57:29 PM", "This PR reverts optimizations introduced in https://github.com/prestodb/presto/pull/16027. The optimization causes significant regression in certain cases.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16173", "Revert \"Use segmented Slice in SliceDictionaryWriter\"", "Andrii Rosa", "arhimondr", "05/27/21, 03:57:38 PM", "This PR reverts optimizations introduced in https://github.com/prestodb/presto/pull/15956. The optimization causes significant regression in certain cases.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16174", "Avoid accidental H2 databse name collisions when using System.nanoTime()", "James Petty", "pettyjamesm", "05/28/21, 02:05:12 AM", "Supplements H2 in memory database name generation by adding a random component on top of `System.nanoTime()`. Using `System.nanoTime()` alone leaves open the possibility of different threads starting tests within the same window (especially after global synchronization events like GC pauses) and accidentally sharing the same database concurrently.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16177", "Verify all table elements in materialized view tests", "Grace Xin", "gracexin2003", "06/08/21, 06:43:29 AM", "This branch checks all elements in the materialized view tables and base tables \r\nare the same, instead of using the row count to verify the results.\r\nIt also sorts the rows of the tables based on certain columns, so that each table\r\nis ordered in the same way.\r\n\r\nResolves: #16149\r\nResolves: #16189 \r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16180", "Fix NPE when desc table in iceberg connector", "Beinan", "beinan", "06/03/21, 11:32:13 PM", "Fix NPE when desc table in iceberg connector.   \r\nAs far as I know, the json file (metadata) of some iceberg table might be missing during listing tables, so we could throw a TableNotFoundException here (The exception thrown would be handled in listing table functions).\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16181", "Use double quotes for all field names within struct in SHOW CREATE TABLE", null, "abhiseksaikia", "06/03/21, 08:47:59 PM", "As part of https://github.com/prestodb/presto/pull/16038, we made changes to use double quotes for all column names in SHOW CREATE TABLE to handle usage of reserved words. We need to do the same for field names within struct\r\n\r\nTest plan- unit test and local testing using cli\r\n \r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16182", "Add 0.253.1 release notes", null, "mayankgarg1990", "06/02/21, 12:25:43 PM", "Test plan - `mvn clean package -DskipTests -am -pl presto-docs` and accessed the generated HTML pages to ensure that the links are correct\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16184", "Support parameters in lambda expressions for prepare statement", "Zhan Yuan", "yuanzhanhku", "06/08/21, 06:42:56 AM", "fixes #16005\r\n\r\nOverride visitLambdaExpression in ParameterExtractingVisitor to support\r\nextracting parameters form lambda expressions. This commit does not add the\r\noverride function to DefaultTraversalVisitor because there are other\r\nvisitors which do not expect to visit lambdas. For example,\r\nVariablesExtractor doesn't want to extract variables from lambdas and\r\ntreat it as dependencies from the source plan.\r\n\r\nTest plan\r\n - Added query tests in presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java.\r\n - Also started a presto server locally and tested the change manually.\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Fix a bug that parameters are not supported in lambda expressions for prepare statements.\r\n\r\n```", "NaN"], ["16185", "Add StripeMetadataSource implementation working with DwrfStripeCache", "Sergii Druzkin", "sdruzkin", "06/02/21, 05:45:08 PM", "Added a new implementation of the StripeMetadataSource that can load stripe footer\r\nand stripe index streams from the DwrfStripeCache or from the underlying delegate.\r\nThe new class can be layered with existing CachingStripeMetadataSource and \r\nStorageStripeMetadataSource.\r\n\r\nTest plan:\r\n- updated existing test cases\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16186", "Avoid shaded guava imports in Presto on Spark codebase", "Andrii Rosa", "arhimondr", "06/02/21, 03:43:32 PM", "Should resolve the issue mentioned here: https://github.com/prestodb/presto/pull/16154#issuecomment-852342015\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16187", "Fix missing QueryCompleteEvent for Presto on Spark", "Vic Zhang", "viczhang861", "06/08/21, 03:35:36 PM", "Issue:\r\n User error with invalid session property is not tracked correctly for Presto on Spark \r\n\r\nFix missing QueryCompleteEvent for Presto on Spark\r\n - Catch INVALID_SESSION_PROPERTY error\r\n - QueryCompleteEvent is created in catch block\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16190", "Reduce memory usage in Analysis getter", null, "prithvip", "06/04/21, 01:28:36 AM", "The getter, Analysis::getColumnReferences(), is\r\ncalled by multiple analyzers, and it creates a\r\ncopy of the map, resulting in multiple copies of\r\nthe map unnecessarily. This change returns the\r\nmap directly rather than creating a copy,\r\navoiding unnecessary memory usage.\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16197", "Add additional query time statistics", null, "tanjialiang", "06/08/21, 09:48:29 PM", "The newly added coordinator level query time statistics provides additional insights on query latency bottlenecks. This is beneficial for performance debugging.\r\n\r\ndepends on https://github.com/facebookexternal/presto-facebook/pull/1562\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r\n\r", "NaN"], ["16202", "Fix invalid output from GraphvizPrinter", "Xiangxi Guo (Ryan)", "StrongerXi", "06/08/21, 12:24:04 AM", "## Overview\r\n[GraphViz's node label grammar](https://graphviz.org/doc/info/shapes.html#record) prohibits certain characters in string intended to be displayed. `GraphVizPrinter` is handling some of those by escaping them, but missed the `{` and `}` cases, which results in malformed GraphViz output for simple queries. This PR covers the missing cases and adds regression tests.\r\n\r\n## Test plan\r\n\r\n1. Added regression unit tests\r\n2. Use [GraphViz Renderer](http://graphviz.it/) to render the output of the following query:\r\n\r\n```\r\nEXPLAIN (format graphviz, type distributed) SELECT * from tpch.tiny.nation\r\n```\r\n\r\n## Release\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16204", "Revert \"Support Dwrf Sequence Ids in Writer\"", "Shixuan Fan", "shixuan-fan", "06/04/21, 04:00:08 PM", "This reverts commit f6611cbc0ae3dee90e6f5476a2c2aa027999b200.\r\n\r\nTest plan - revert offending commit\r\n\r\n```\r\nHive Changes\r\n* Revert :pr:`16204`\r\n```\r", "NaN"], ["16208", "Add quotes around printed domain values", null, "lightseba", "06/09/21, 11:49:50 PM", "The domain intervals can be ambiguous in cases like\r\n`SELECT * FROM table WHERE col >= 'a' AND col <= 'b'`\r\nvs.\r\n`SELECT * FROM table WHERE col = 'a, b'`\r\nwhich both have the domain interval `[a, b]`.\r\nThis is fixed by adding quotes to the output (`[\"a\", \"b\"]` vs. `[\"a, b\"]`)\r\nand escaping quotes which may appear in the string itself.\r\nAlso made `PlanPrinter.formatDomain()` use the formatting code in\r\n`Range.toString()`, which is exactly the same.\r\n\r\nTest plan\r\nIn addition to the unit tests added, I ran\r\n```\r\nCREATE TABLE test_orders WITH (partitioned_by = ARRAY['orderkey', 'processing']) AS \r\nSELECT custkey, orderkey, orderstatus = 'P' processing FROM orders WHERE orderkey < 3\r\n```\r\n```\r\nEXPLAIN SELECT * FROM test_orders WHERE orderkey < 2 AND orderkey > 20\r\n```\r\nThis query should show quotes around the domain intervals.\r\n\r\n\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add double quotes around the column domain values in the text query plan. \r\n  Literal double quotes in values will be escaped with a backslash (ab\"c -> ab\\\"c).\r\n```\r", "NaN"], ["16213", "Bind shared scheduled executor in verifier", "Andrii Rosa", "arhimondr", "06/07/21, 01:35:48 PM", "This executor can be used by the verifier submodules to run scheduled tasks\r\n\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16215", "Add query statistic cumulative total memory", "Jim Apple", "jbapple", "06/10/21, 04:10:03 AM", "This statistic counts not just user memory, but also system memory.\r\n\r\nTest plan - Ran the unit tests and check_webui.sh\r\n\r\n```\r\n== RELEASE NOTES ==\r\n\r\nGeneral Changes\r\n* Add a new total cumulative memory statistic\r\n\r\nWeb UI\r\n* Add cumulative total memory to the Memory and Resource Utilization Summary tables on the QueryDetail page.\r\n```\r", "NaN"], ["16216", "Add release notes for 0.255", "Rohit Jain", "jainxrohit", "06/15/21, 02:24:21 PM", "# Missing Release Notes\n## Chen\n- [x] https://github.com/prestodb/presto/pull/16152 Fix the import of SMALLINT in ParquetReader (Merged by: Venki Korukanti)\n\n## Darren Fu\n- [x] https://github.com/prestodb/presto/pull/16164 Fix Config Validation (Merged by: Andrii Rosa)\n\n## Julian Zhuoran Zhao\n- [x] https://github.com/prestodb/presto/pull/16129 Implement efficient way to fetch partition name and values (Merged by: James Sun)\n\n## Shixuan Fan\n- [x] https://github.com/prestodb/presto/pull/16204 Revert \"Support Dwrf Sequence Ids in Writer\" (Merged by: Shixuan Fan)\n\n# Extracted Release Notes\n- #15617 (Author: Reetika Agrawal): Skip Unsupported datatype from Object field in ES connector\n  - Fix to avoid NullPointerException when there is an unsupported data type column in the Object field.\n- #15842 (Author: v-jizhang): Fix and turn on Cassandra product tests\n  - Add support for Cassandra ``SMALLINT``, ``TINYINT`` and ``DATE`` types and fix tests.\n- #15914 (Author: v-jizhang): Reject @BeforeMethod in Multi-threaded tests\n  - Reject @BeforeMethod in Multi-threaded tests.\n- #15920 (Author: Jalpreet Singh Nanda (:imjalpreet)): Support non-lowercase table names in Druid connector\n  - Add support for querying non-lowercase table names in Druid connector (:pr:`15920`).\n- #15950 (Author: v-jizhang): Add support for static AWS credentials in GlueHiveMetastore\n  - Add support for static AWS credentials in GlueHiveMetastore :doc:`/connector/hive`.\n- #15955 (Author: Andrew Donley): Add Connection Params for JDBC HTTP Protocol\n  - Added connection param \"protocols\" that allows a user to specify which HTTP protocols the Presto client is allowed to use.\n- #16073 (Author: Mayank Garg): Add support for custom query prerequisite logic\n  - Add support for custom query prerequisites to be checked and satisfied through ``QueryPrerequisites`` interface. See :pr:`16073`.\n- #16073 (Author: Mayank Garg): Add support for custom query prerequisite logic\n  - Add support for custom query prerequisites to be checked and satisfied through ``QueryPrerequisites`` interface. See :pr:`16073`.\n- #16109 (Author: Sreeni Viswanadha): Use searchInterruptible for Joni regexp matching\n  - Fix an issue where regular expression functions were not interruptible and could  keep running for a long time after the query they were a part of failed.\n- #16111 (Author: Ariel Weisberg): Interrupt runaway splits\n  - Add interruption for runaway splits blocked in known situations controlled by ``task.interrupt-runaway-splits-timeout`` property which defaults to ``600s``.\n- #16159 (Author: Zhan Yuan): Implement ARRAY_NORMALIZE function\n  - Added array_normalize function.\n\n# All Commits\n- fe48d865fbf4aab072e8443a251a42dfb5399356 Adding a counter to track TaskUpdateRequest size (Basar Hamdi Onat)\n- 42590d86527b2b4428b8fe7706125a4048edb306 Revert \"Support Dwrf Sequence Ids in Writer\" (Shixuan Fan)\n- 6ea61ba39268e43e508cf412957c8646f532d8b1 Reduce memory usage in Analysis getter (prithvip)\n- 7f7747321b18656445223eba762c951ca859b3c1 Fix NPE when desc table in iceberg connector (beinan)\n- afbfd21aaee90889f5638561f5440d792d3917fa Use double quotes for all field names within struct in SHOW CREATE TABLE. (Abhisek Gautam Saikia)\n- 9095346f47c9237ed79ef6eaf0df24a8938c2f6a Implement ARRAY_NORMALIZE function (Zhan Yuan)\n- faf0cfec04adb82efcc8128f8d9b8afa46b3edb7 Temporarily disable test (Tim Meehan)\n- 4cc76ded774803f6ef7830b166a17076e3bf4ab1 Add resource manager profile to presto-tests and enable in CI (Tim Meehan)\n- 96c1a22435a26c5a9a3400449c4d0a2f78af2c9f Fix TestDistributedQueryResource (Tim Meehan)\n- 867dd6f73ac9e03b6bebf78abefef0d199b8404f Fix TestDistributedClusterStatsResource (Tim Meehan)\n- 5865c76837c1ac71b81f16b7969bdc1961febb77 Add StripeMetadataSource implementation working with DwrfStripeCache (Sergii Druzkin)\n- 84907066c57c23cdbabb1bdcea86dabeafb58b24 Avoid shaded guava imports in Presto on Spark codebase (Andrii Rosa)\n- 6a0257522531941731a71e2c768e1bcef4fb5780 Add 0.253.1 release notes (Mayank Garg)\n- add05d0c16729dfa5bde0b41a13356ff00fc829e Add Connection Params for JDBC HTTP Protocol (Andrew Donley)\n- 02bb379ef6e2fda78525566b00c60180ea7feed6 Fix Config Validation issue (Darren Fu)\n- 42b9ef81b9b2a2789587ee252d520ce026664d83 Implement case-insensitive name matching for Druid (Jalpreet Singh Nanda (:imjalpreet))\n- 269758ca0dea82bf3c77f672ae4862d0e1dce0f4 Avoid accidental H2 databse name collisions when using System.nanoTime() (James Petty)\n- 59b469cf2d8a4ea117b239943b4a6e164b27fa3b Add StripeMetadataSourceFactory to consume loaded DWRF stripe cache (Sergii Druzkin)\n- 5706c39f5697d7d1a4dacb55e439ec6f863bfc04 Revert \"Use segmented Slice in SliceDictionaryWriter\" (Andrii Rosa)\n- 278e87deeade3a6d2052af07a17dc34dd1e522d4 Revert \"Make HashTable computation Lazy in MapBlockBuilder\" (Andrii Rosa)\n- 91ef9deb64b59a4f303dffa59d0b2bee9823ea90 Revert \"Fix Size Calculation in MapBlock\" (Andrii Rosa)\n- 058be9c06e545781bde5e7874019b953b9e7213d Log warning on Unsupported datatype from Object field in ES connector (Reetika Agrawal)\n- aff4a75253ea769993944ea4415c8a50114f115d Refactor OrcWriterOptions to builder pattern (Arunachalam Thirupathi)\n- ac6187d74b00b52526140eba7c00971bdca878e0 Remove duplicate UtcEquivalentName check from TimeZoneKey (henneberger)\n- 8e8a546fcd4367fadda181e3eb334891215bba93 Interrupt runaway splits in known locations (Ariel Weisberg)\n- 399d80955c4475690384f451e65e134509160074 Upgrade airlift dependency to 0.201 (Abhisek Gautam Saikia)\n- f93db75eca4cabf89f54da34b6fd41a5bf382af0 Improve type errors messages for structs (Amit Adhikari)\n- 127949e5914add4653cc3e7edb1cd6269f61a52c Add iceberg connector doc (Chunxu Tang)\n- 1463e683708fd8887eca3d6042f3afec88bbb7ce Fix the import of SMALLINT in ParquetReader (Chen)\n- a178e5dcf5fd337bf44ab1211e165a6715e3c3cc Add support for static AWS credentials in GlueHiveMetastore (v-jizhang)\n- 27f338e606c17cf06d304d6b6d4b2a42df436dfc Fix PartitionMutator not bound error during server start (beinan)\n- edcc4f7ac4aecfeab316e722a90c88f0bedaab54 Add iceberg connector to devel configs (beinan)\n- 96a9acafc22a1147cdbcac6ad73d498a12293ed9 Add predicate stitching for more nodes (Rohit Jain)\n- 56c5e52a154378d0a9e9214f50940283e56d6468 Reject @BeforeMethod in Multi-threaded tests (v-jizhang)\n- f38548b9a60091720e3fb649cc51fbad6a696003 Fix and turn on Cassandra product tests (v-jizhang)\n- cc944e2d79768abf7832dbd10c693f3386a06414 Fix MySQL connection failure on startup (Ariel Weisberg)\n- bf94d65472945880d6522a662e76d428795cac11 Add support to introduce custom prerequisite waiting logic (Mayank Garg)\n- b011a55edf4e9ceb43a9df71069f383fdfcf015c Add new query state `WAITING_FOR_PREREQUISITES` (Mayank Garg)\n- e20b4b6dd0af672d6a75e82622ca6ee293d91930 Use searchInterruptible so that we can interrupt runaway regexp matching thread. (Sreeni Viswanadha)\n- 0aeca14a21998a4f3a7bcf2e0aebe6b43ca0c1b1 Implement efficient way to fetch partition name and values (Julian Zhuoran Zhao)\n- 72d714532ba44e36ac04e05694bb119e95c5d1e7 Make NodeTaskMap inner classes static (James Petty)", "NaN"], ["16218", "Add release notes for 0.254.1", null, "mayankgarg1990", "06/06/21, 08:40:56 PM", "Test plan - generated presto-docs artifacts and checked in the browser that the generated pages work fine\r\n\r\nOriginal PR - https://github.com/prestodb/presto/pull/16211\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16219", "Add tests of the iceberg connector", "Chunxu Tang", "ChunxuTang", "06/08/21, 05:32:25 AM", "Cherry-pick unit tests of https://github.com/trinodb/trino/commit/e82c2d5301396ca16248eb6931bec11bf1352470\r\n\r\nCo-Authored-By: Parth Brahmbhatt <pbrahmbhatt@netflix.com>\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16226", "Test Iceberg connector configs", "Chunxu Tang", "ChunxuTang", "06/08/21, 09:49:45 PM", "Cherry-pick unit tests of https://github.com/trinodb/trino/commit/4efbbea7b0772bc4575572f86bc020fa918f2d6c\r\n\r\nCo-Authored-By: Xingyuan Lin <linxingyuan1102@gmail.com>\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```", "NaN"], ["16239", "Remove typeBound from TypeVariableConstraint", "Rongrong Zhong", "rongrong", "06/10/21, 09:43:05 PM", "EnumType now is implemented as a standard paremetric type so using\r\nthe existing variadicBound as type constraint will work.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"], ["16247", "Refactor StreamReader::startStripe to take the Stripe directly", "RindsSchei225e", "HuamengJiang", "06/15/21, 02:23:51 AM", "This PR is in preparation for dictionary sharing implementation in \r\nDWRF format, a file format level feature that allows\r\nmultiple keys in a flat map to share the same dictionary data stream.\r\nThis PR is refactor only and contains no logic change.\r\n\r\nTest plan - existing unit tests. We sampled the top 3 namespaces for \r\ntables to run integration tests on and the result is clean.\r\n\r\n```\r\n== NO RELEASE NOTE ==\r\n```\r", "NaN"]]